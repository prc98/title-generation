{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_TITLE_GENERATION_(2).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MElzMPtl9RJG",
        "eldkGaYp8R4S",
        "7XoHBg97-2gl",
        "U9-xPP1oIg3k"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1koJ-8-HpjFt",
        "outputId": "b6fc1b20-9888-4abc-feb6-1a1167ea2d22"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkZ1wIEBqQtq"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from torch.optim import lr_scheduler\n",
        "from PIL import Image\n",
        "import timeit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy import data\n",
        "import random\n",
        "## For reproducibility\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MElzMPtl9RJG"
      },
      "source": [
        "## Create Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBrhIMlF8tBX"
      },
      "source": [
        "tokenize =  lambda s: s.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEOpVU2H9YmN"
      },
      "source": [
        "import re  \n",
        "def cleanup_text(texts):\n",
        "    cleaned_text = []\n",
        "    for text in texts:\n",
        "        # remove punctuation\n",
        "        text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
        "        # remove multiple spaces\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        # remove newline\n",
        "        text = re.sub(r'\\n', ' ', text)\n",
        "        #replace digits with '# symbol\n",
        "        text = re.sub('[0-9]', '#', text)\n",
        "        cleaned_text.append(text)\n",
        "    return cleaned_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ouAXeTl9vuW",
        "outputId": "3faf26a7-cc48-408d-9d5e-0654471e0f15"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eldkGaYp8R4S"
      },
      "source": [
        "#### Create two torch text fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9t25sIfv0qf"
      },
      "source": [
        "ABS = data.Field(tokenize = tokenize,init_token='<sos>',eos_token='<eos>',stop_words=stop,pad_first=True,lower = True,preprocessing=cleanup_text)\n",
        "TITLE = data.Field(tokenize = tokenize,init_token='<sos>',eos_token='<eos>',lower = True,preprocessing=cleanup_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XoHBg97-2gl"
      },
      "source": [
        "#### Read tabular dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhmY8osUNTyb"
      },
      "source": [
        "fields = [('Abstract',ABS),('Title',TITLE)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB47SHu8qYPn"
      },
      "source": [
        "dataset = data.TabularDataset(path='./drive/MyDrive/data.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sh3SdmaRaLr"
      },
      "source": [
        "import random\n",
        "train_data, valid_data = dataset.split(split_ratio=0.9, random_state=random.seed(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KUNtYUFR1Ad",
        "outputId": "d54c534e-4015-41a0-d78d-0fff5afdf30b"
      },
      "source": [
        "print(len(train_data.examples))\n",
        "print(len(valid_data.examples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36900\n",
            "4100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUqJbls8Tx4E",
        "outputId": "f55ed7cd-1d0e-4000-face-068d4189e7eb"
      },
      "source": [
        "train_data[0].Abstract"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['human',\n",
              " 'communication',\n",
              " 'typically',\n",
              " 'underlying',\n",
              " 'structure ',\n",
              " 'reflected',\n",
              " 'fact',\n",
              " 'many',\n",
              " 'user',\n",
              " 'generated',\n",
              " 'videos ',\n",
              " 'starting',\n",
              " 'point ',\n",
              " 'ending ',\n",
              " 'certain',\n",
              " 'objective',\n",
              " 'steps',\n",
              " 'two',\n",
              " 'identified ',\n",
              " 'paper ',\n",
              " 'propose',\n",
              " 'method',\n",
              " 'parsing',\n",
              " 'video',\n",
              " 'semantic',\n",
              " 'steps',\n",
              " 'unsupervised',\n",
              " 'way ',\n",
              " 'proposed',\n",
              " 'method',\n",
              " 'capable',\n",
              " 'providing',\n",
              " 'semantic',\n",
              " ' storyline ',\n",
              " 'video',\n",
              " 'composed',\n",
              " 'objective',\n",
              " 'steps ',\n",
              " 'accomplish',\n",
              " 'using',\n",
              " 'visual',\n",
              " 'language',\n",
              " 'cues',\n",
              " 'joint',\n",
              " 'generative',\n",
              " 'model ',\n",
              " 'proposed',\n",
              " 'method',\n",
              " 'also',\n",
              " 'provide',\n",
              " 'textual',\n",
              " 'description',\n",
              " 'identified',\n",
              " 'semantic',\n",
              " 'steps',\n",
              " 'video',\n",
              " 'segments ',\n",
              " 'evaluate',\n",
              " 'method',\n",
              " 'large',\n",
              " 'number',\n",
              " 'complex',\n",
              " 'youtube',\n",
              " 'videos',\n",
              " 'show',\n",
              " 'results',\n",
              " 'unprecedented',\n",
              " 'quality',\n",
              " 'intricate',\n",
              " 'impactful',\n",
              " 'problem ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC2qQT8vSALx"
      },
      "source": [
        "ABS.build_vocab(train_data.Abstract,train_data.Title,max_size=40000,min_freq=2)\n",
        "\n",
        "TITLE.vocab= ABS.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sWkiIVDt8iv",
        "outputId": "67876907-ea9c-460e-b847-ffdd6f8262f3"
      },
      "source": [
        "len(TITLE.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLb__KCTYghZ",
        "outputId": "5321f753-67a2-4537-d52e-00f3445b3ef0"
      },
      "source": [
        "ABS.vocab.freqs['of']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9616"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXhRvMRuTM6L"
      },
      "source": [
        "assert(TITLE.vocab.stoi ==  ABS.vocab.stoi) #check if both share the same vocab or not"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9-xPP1oIg3k"
      },
      "source": [
        "#### Create Iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWrev9KYXIOH"
      },
      "source": [
        "BATCH_SIZE =64\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "    batch_size = BATCH_SIZE, sort_key = lambda x: len(x.Abstract), sort_within_batch = True,shuffle=True,sort=False,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD57vcqJlRWV",
        "outputId": "96a3bd63-3f77-47c5-c426-36a729259a03"
      },
      "source": [
        "len(valid_iterator.dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJoE4bdsY20x"
      },
      "source": [
        "##Baseline Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT1wPVtoY5Zx"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, num_layers, dropout): \n",
        "        super().__init__()   \n",
        "        self.hid_dim = hid_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)  \n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers, dropout = dropout)    \n",
        "        self.dropout = nn.Dropout(dropout)       \n",
        "    def forward(self, input_idx):\n",
        "        #print(input_idx)\n",
        "        embedded = self.dropout(self.embedding(input_idx))  \n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #embedded = seq_len,batch_size,embed_dim\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        #outputs are always from the top hidden layer\n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63FdSj3ZZBbi"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,input_dim,emb_dim,hid_dim,num_layers,dropout,output_dim):\n",
        "    super().__init__()\n",
        "    self.hid_dim = hid_dim\n",
        "    self.num_layers=num_layers\n",
        "    self.embedding = nn.Embedding(input_dim,emb_dim)\n",
        "    self.lstm = nn.LSTM(emb_dim,hid_dim,num_layers,dropout=dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(hid_dim,output_dim) \n",
        "    self.output_dim=output_dim\n",
        "\n",
        "  def forward(self,input_idx,context_vector,cell_state):\n",
        "    #input_idx = [batch_size]\n",
        "    input_idx = input_idx.unsqueeze(0) #Adding a dimenstion at the the first = 1 = seq_len as we are sending word by word\n",
        "    embedded = self.dropout(self.embedding(input_idx))\n",
        "    #embedded = [1,batch_size,embed_size]\n",
        "    #print(embedded.size())\n",
        "    #print(\"G\")\n",
        "    outputs, (hidden, cell) = self.lstm(embedded,(context_vector,cell_state))\n",
        "    #outputs_size = (1,batch_size,hidden_dim)\n",
        "    outputs = outputs.reshape(-1,self.hid_dim)\n",
        "    #outputs_size=(batch_size,hid_dim)\n",
        "    prediction = self.fc(outputs)\n",
        "    #prediction_size = (batch_size,out_dim)\n",
        "    return prediction,hidden,cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoULj4AeZDTc"
      },
      "source": [
        "class Seq2Seq(nn.Module): #Combining the encoder and decoder\n",
        "  def __init__(self,encoder,decoder,device):\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.device =  device\n",
        "  def forward(self,input_batches,output_batches,tfr=0.5):\n",
        "    #input_batches dimension - (seq_len,batch_size)\n",
        "    #output_batches_dimension - (seq_len,batch_size)\n",
        "    batch_size = input_batches.shape[1]\n",
        "    title_len = output_batches.shape[0]\n",
        "    title_vocab_size = self.decoder.output_dim\n",
        "    predictions = torch.zeros(title_len, batch_size, title_vocab_size).to(device)\n",
        "    #print(input_batches.size())\n",
        "    hidden_state, cell_state = self.encoder(input_batches) \n",
        "    #hidden_state/cell_state dimension = num_layers,batch_size,hidden_dim\n",
        "\n",
        "    x = output_batches[0,:] # Trigger token <SOS>\n",
        "\n",
        "    for i in range(1, title_len):\n",
        "      pred, hidden_state, cell_state = self.decoder(x, hidden_state, cell_state)\n",
        "      #pred = [batch_size,output_dim(vocab_size)]\n",
        "      predictions[i] = pred\n",
        "      best_guess = pred.argmax(1) \n",
        "      x = output_batches[i,:] if random.random() < tfr else best_guess\n",
        "    return predictions  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAucn09mZF5y"
      },
      "source": [
        "INPUT_DIM = len(ABS.vocab)\n",
        "OUTPUT_DIM = len(TITLE.vocab)\n",
        "ENC_EMB_DIM = 100\n",
        "DEC_EMB_DIM = 100\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 3\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(INPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT,OUTPUT_DIM)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZdlgqRHa88Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063c4a78-336d-40dd-cc2f-1c2eb63b1992"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.1, 0.1)\n",
        "        \n",
        "model.apply(init_weights)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(40004, 100)\n",
              "    (lstm): LSTM(100, 512, num_layers=3, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(40004, 100)\n",
              "    (lstm): LSTM(100, 512, num_layers=3, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (fc): Linear(in_features=512, out_features=40004, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmAgQGFwbCy-"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5C4oZm4bNwn"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i,batch in enumerate(iterator):\n",
        "        \n",
        "        abstract = batch.Abstract\n",
        "        title = batch.Title\n",
        "        #abstract,title = [seq_len,batch_size]\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(abstract, title,0.5)\n",
        "        #predictions = [seq_len_title,batch_size,title_vocab]\n",
        "        output_dim = predictions.shape[-1]\n",
        "        \n",
        "        predictions = predictions[1:].view(-1, output_dim)#ignoring the first value is the <sos> token\n",
        "        title = title[1:].view(-1)\n",
        "        \n",
        "        loss = criterion(predictions, title)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndUAmUxWPqmp"
      },
      "source": [
        "def test(model, iterator, criterion):    \n",
        "    model.eval() \n",
        "    epoch_loss = 0 \n",
        "    with torch.no_grad():   \n",
        "        for i, batch in enumerate(iterator):\n",
        "          abstract = batch.Abstract\n",
        "          title = batch.Title\n",
        "          #abstract,title = [seq_len,batch_size]\n",
        "          predictions = model(abstract, title,0)\n",
        "          #predictions = [seq_len_title,batch_size,title_vocab]\n",
        "          output_dim = predictions.shape[-1]\n",
        "          predictions = predictions[1:].view(-1, output_dim)#ignoring the first value is the <sos> token\n",
        "          title = title[1:].view(-1)\n",
        "          loss = criterion(predictions, title)  \n",
        "          epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WXQSpnNQ7VD"
      },
      "source": [
        "#to generate title for one abstract\n",
        "def translate(model,abs,max_length):\n",
        "  #abs = A single abstract to generate title, data_type = str\n",
        "\n",
        "  #Preprocessing as done during training\n",
        "  abs=abs.lower()\n",
        "  abs = tokenize(abs)\n",
        "  abs = cleanup_text(abs)\n",
        "  #convert to a list of idx corresponding to abstract vocab\n",
        "  num_abs = []\n",
        "  num_abs.append(ABS.vocab.stoi['<sos>'])\n",
        "  for w in abs:\n",
        "    if w in ABS.vocab.stoi:\n",
        "      num_abs.append(ABS.vocab.stoi[w])\n",
        "    else:\n",
        "      num_abs.append(ABS.vocab.stoi['<unk'])\n",
        "  num_abs.append(ABS.vocab.stoi['<eos>'])\n",
        "  #covert to Long Tensor\n",
        "  num_abs = torch.LongTensor(num_abs)\n",
        "  #add batch_size = 1\n",
        "  num_abs = num_abs.unsqueeze(1)\n",
        "  #load them to device\n",
        "  num_abs = num_abs.to(device)\n",
        "  #print(num_abs.size())\n",
        "  #model = model.to(device)\n",
        "\n",
        "  #model.eval()\n",
        "  #Forward pass through the encoder\n",
        "  with torch.no_grad():\n",
        "    hidden_state, cell_state = model.encoder(num_abs)\n",
        "  \n",
        "  #Starts with <sos>\n",
        "  pred = [ABS.vocab.stoi[\"<sos>\"]]\n",
        "  for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([pred[-1]]).to(device)\n",
        "        #print(previous_word.size())\n",
        "        with torch.no_grad():\n",
        "            output, hidden_state, cell_state = model.decoder(previous_word, hidden_state, cell_state)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        pred.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == ABS.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "  translated_sentence = [ABS.vocab.itos[idx] for idx in pred]\n",
        "  return translated_sentence[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlsqbW7QKJlo"
      },
      "source": [
        "demo_sentence = \"We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgjlZW3zP6d5",
        "outputId": "a26179c2-b3c5-4dce-9eaa-2ce17f1226e1"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(40004, 100)\n",
              "    (lstm): LSTM(100, 512, num_layers=3, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(40004, 100)\n",
              "    (lstm): LSTM(100, 512, num_layers=3, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (fc): Linear(in_features=512, out_features=40004, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7eTgM7dYcbw"
      },
      "source": [
        "def checkpoint_and_save(model, min_loss, epoch, optimizer):\n",
        "    print()\n",
        "    state = {'model': model,'min_loss': min_loss,'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),}\n",
        "    path =  './drive/MyDrive/Colab Notebooks/net.pt'\n",
        "    torch.save(state, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPjXox43bwtD",
        "outputId": "c2564b31-5154-434a-b0ac-88d4dd15c1f0"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "import time\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(\"Time taken : \",((end_time-start_time)/60),'min')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 6.676\n",
            "\tTest Loss: 6.513\n",
            "After epoch 0 , generated title is ['a', 'the', 'of', 'of', 'for', '<eos>']\n",
            "Time taken :  3.70690389474233 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 6.297\n",
            "\tTest Loss: 6.521\n",
            "After epoch 1 , generated title is ['<unk>', 'of', 'of', 'for', 'for', '<eos>']\n",
            "Time taken :  3.745437733332316 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 6.168\n",
            "\tTest Loss: 6.698\n",
            "After epoch 2 , generated title is ['a', '<unk>', 'approach', 'for', 'for', 'for', '<eos>']\n",
            "Time taken :  3.7392032027244566 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 6.065\n",
            "\tTest Loss: 6.548\n",
            "After epoch 3 , generated title is ['<unk>', 'a', 'of', 'for', 'for', '<eos>']\n",
            "Time taken :  3.733981561660767 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.962\n",
            "\tTest Loss: 6.500\n",
            "After epoch 4 , generated title is ['<unk>', 'deep', 'neural', 'networks', 'for', 'image', 'recognition', '<eos>']\n",
            "Time taken :  3.7391348679860434 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.830\n",
            "\tTest Loss: 6.438\n",
            "After epoch 5 , generated title is ['<unk>', 'a', 'of', 'for', 'for', '<eos>']\n",
            "Time taken :  3.741561532020569 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.699\n",
            "\tTest Loss: 6.358\n",
            "After epoch 6 , generated title is ['<unk>', 'deep', 'neural', 'networks', 'for', 'for', '<eos>']\n",
            "Time taken :  3.7491976658503217 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.566\n",
            "\tTest Loss: 6.337\n",
            "After epoch 7 , generated title is ['<unk>', 'a', 'of', 'for', 'for', 'for', '<eos>']\n",
            "Time taken :  3.7410315672556558 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.457\n",
            "\tTest Loss: 6.318\n",
            "After epoch 8 , generated title is ['<unk>', 'neural', 'neural', 'networks', 'for', 'for', '<eos>']\n",
            "Time taken :  3.754911784331004 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.345\n",
            "\tTest Loss: 6.313\n",
            "After epoch 9 , generated title is ['<unk>', 'neural', 'neural', 'network', 'for', 'for', '<eos>']\n",
            "Time taken :  3.743460257848104 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.268\n",
            "\tTest Loss: 6.286\n",
            "After epoch 10 , generated title is ['<unk>', 'neural', 'neural', 'networks', 'for', 'for', '<eos>']\n",
            "Time taken :  3.7564907908439635 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.160\n",
            "\tTest Loss: 6.306\n",
            "After epoch 11 , generated title is ['<unk>', 'recurrent', 'neural', 'networks', 'for', 'sentence', 'embeddings', '<eos>']\n",
            "Time taken :  3.7509630918502808 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.058\n",
            "\tTest Loss: 6.317\n",
            "After epoch 12 , generated title is ['<unk>', 'neural', 'networks', 'for', 'for', 'speech', 'recognition', '<eos>']\n",
            "Time taken :  3.7582721869150797 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.955\n",
            "\tTest Loss: 6.369\n",
            "After epoch 13 , generated title is ['<unk>', 'neural', 'neural', 'networks', 'for', 'for', 'question', 'answering', '<eos>']\n",
            "Time taken :  3.755064634482066 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.878\n",
            "\tTest Loss: 6.364\n",
            "After epoch 14 , generated title is ['<unk>', 'neural', 'networks', 'for', 'for', 'speech', 'recognition', '<eos>']\n",
            "Time taken :  3.7453364372253417 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.805\n",
            "\tTest Loss: 6.386\n",
            "After epoch 15 , generated title is ['<unk>', 'a', 'neural', 'for', 'for', 'for', '<eos>']\n",
            "Time taken :  3.7508170048395795 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.724\n",
            "\tTest Loss: 6.382\n",
            "After epoch 16 , generated title is ['<unk>', 'attention', 'neural', 'networks', 'for', 'for', 'question', 'answering', '<eos>']\n",
            "Time taken :  3.7425969441731772 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.649\n",
            "\tTest Loss: 6.443\n",
            "After epoch 17 , generated title is ['<unk>', 'attention', 'networks', 'for', 'neural', 'neural', 'networks', '<eos>']\n",
            "Time taken :  3.760417652130127 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.584\n",
            "\tTest Loss: 6.435\n",
            "After epoch 18 , generated title is ['<unk>', 'attention', 'neural', 'networks', 'for', 'question', 'answering', '<eos>']\n",
            "Time taken :  3.7578323086102805 min\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.498\n",
            "\tTest Loss: 6.508\n",
            "After epoch 19 , generated title is ['<unk>', 'attention', 'neural', 'networks', 'for', 'text', 'question', 'answering', '<eos>']\n",
            "Time taken :  3.7567429184913634 min\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlwJI6bjtjpZ"
      },
      "source": [
        "### Some more examples \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnaGOfqwl7vj"
      },
      "source": [
        "path =  './drive/MyDrive/Colab Notebooks/net.pt'\n",
        "checkpoint = torch.load(path)\n",
        "model1 = checkpoint['model']\n",
        "model1.load_state_dict( checkpoint['model_state_dict'])\n",
        "min_loss = checkpoint['min_loss']\n",
        "epoch = checkpoint['epoch']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8X-BEMDulgw"
      },
      "source": [
        "df = pd.read_csv('./drive/MyDrive/data_summaries.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM3cxHE0IbBM"
      },
      "source": [
        "<br>(test_data.csv already contains 1000 randomly selected abstracts)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRFBFIKftuAv"
      },
      "source": [
        "df1={'Abstract':[],'Title':[],'Generated Title':[]}\n",
        "ID = data.Field(use_vocab=False,sequential=False,preprocessing=int)\n",
        "fields = [('Id',ID),('Abstract',ABS),('Title',TITLE),('sum1',None),('sum2',None),('sum3',None),('sum4',None),('sum5',None),('sum6',None),('sum7',None)]\n",
        "ran_dataset = data.TabularDataset(path='./drive/MyDrive/test_data.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKksvIMBuHoQ"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "BATCH_SIZE =1\n",
        "iterator=data.Iterator(\n",
        "    ran_dataset,\n",
        "    batch_size = BATCH_SIZE,shuffle=False,device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v8ABEbkIh-1"
      },
      "source": [
        "Generate headlines and store it in a file called \"generated_titles_baseline.csv\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpi1uKo0uNHh"
      },
      "source": [
        "for i,batch in enumerate(iterator):\n",
        "  id1 = batch.Id.item()\n",
        "  abs= \"\".join(list(df.loc[df['Id'] == id1]['Abstract']))\n",
        "  title= \"\".join(list(df.loc[df['Id'] == id1]['Title']))\n",
        "  #print(batch)\n",
        "  #print(\"Abstract : \")\n",
        "  #print(\"\".join(list(df.loc[df['Id'] == id1]['Abstract'])))\n",
        "  #print(\"Actual Title : \")\n",
        "  #print(\"\".join(list(df.loc[df['Id'] == id1]['Title'])))\n",
        "  #print(\"Generated Title : \")\n",
        "  #print(\" \".join(translate(model1,abs,10)))\n",
        "  \n",
        "  df1['Abstract'].append(abs)\n",
        "  df1['Title'].append(title)\n",
        "  df1['Generated Title'].append(\" \".join(translate(model1,abs,10)))\n",
        "  #df1['Generated_Title'][id1]=translate(model1,batch,15))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf5HB9EzEsuT"
      },
      "source": [
        "df2 = pd.DataFrame(df1)\n",
        "df2.to_csv('./drive/MyDrive/generated_titles_baseline.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}