{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_TITLE_GENERATION_(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMe2sqOnTDMzNq0IvpATH91",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prc98/title-generation/blob/main/NLP_TITLE_GENERATION_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1koJ-8-HpjFt",
        "outputId": "0ce7c3d5-7c6f-424d-fa58-656399564e02"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkZ1wIEBqQtq"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from torch.optim import lr_scheduler\n",
        "from PIL import Image\n",
        "import timeit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy import data\n",
        "import random\n",
        "## For reproducibility\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MElzMPtl9RJG"
      },
      "source": [
        "## Create Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBrhIMlF8tBX"
      },
      "source": [
        "tokenize =  lambda s: s.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEOpVU2H9YmN"
      },
      "source": [
        "import re  \n",
        "def cleanup_text(texts):\n",
        "    cleaned_text = []\n",
        "    for text in texts:\n",
        "        # remove punctuation\n",
        "        text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
        "        # remove multiple spaces\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        # remove newline\n",
        "        text = re.sub(r'\\n', ' ', text)\n",
        "        \n",
        "        cleaned_text.append(text)\n",
        "    return cleaned_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ouAXeTl9vuW",
        "outputId": "cdcfb9fe-12c3-4e78-8dd6-034130a2049f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eldkGaYp8R4S"
      },
      "source": [
        "#### Create two torch text fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9t25sIfv0qf"
      },
      "source": [
        "ABS = data.Field(tokenize = tokenize,init_token='<sos>',eos_token='<eos>',pad_first=True,lower = True,stop_words=stop,preprocessing=cleanup_text)\n",
        "TITLE = data.Field(tokenize = tokenize,init_token='<sos>',eos_token='<eos>',lower = True,preprocessing=cleanup_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XoHBg97-2gl"
      },
      "source": [
        "#### Read tabular dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhmY8osUNTyb"
      },
      "source": [
        "fields = [('Abstract',ABS),('Title',TITLE)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB47SHu8qYPn"
      },
      "source": [
        "dataset = data.TabularDataset(path='./drive/MyDrive/data.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sh3SdmaRaLr"
      },
      "source": [
        "import random\n",
        "train_data, valid_data = dataset.split(split_ratio=0.9, random_state=random.seed(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KUNtYUFR1Ad",
        "outputId": "b8659f77-e160-4fcc-b61a-93ce1538163f"
      },
      "source": [
        "print(len(train_data.examples))\n",
        "print(len(valid_data.examples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36900\n",
            "4100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUqJbls8Tx4E",
        "outputId": "8bf15e55-6364-4fb8-8dd6-5c49dffcd89a"
      },
      "source": [
        "train_data[0].Abstract"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['human',\n",
              " 'communication',\n",
              " 'typically',\n",
              " 'underlying',\n",
              " 'structure ',\n",
              " 'reflected',\n",
              " 'fact',\n",
              " 'many',\n",
              " 'user',\n",
              " 'generated',\n",
              " 'videos ',\n",
              " 'starting',\n",
              " 'point ',\n",
              " 'ending ',\n",
              " 'certain',\n",
              " 'objective',\n",
              " 'steps',\n",
              " 'two',\n",
              " 'identified ',\n",
              " 'paper ',\n",
              " 'propose',\n",
              " 'method',\n",
              " 'parsing',\n",
              " 'video',\n",
              " 'semantic',\n",
              " 'steps',\n",
              " 'unsupervised',\n",
              " 'way ',\n",
              " 'proposed',\n",
              " 'method',\n",
              " 'capable',\n",
              " 'providing',\n",
              " 'semantic',\n",
              " ' storyline ',\n",
              " 'video',\n",
              " 'composed',\n",
              " 'objective',\n",
              " 'steps ',\n",
              " 'accomplish',\n",
              " 'using',\n",
              " 'visual',\n",
              " 'language',\n",
              " 'cues',\n",
              " 'joint',\n",
              " 'generative',\n",
              " 'model ',\n",
              " 'proposed',\n",
              " 'method',\n",
              " 'also',\n",
              " 'provide',\n",
              " 'textual',\n",
              " 'description',\n",
              " 'identified',\n",
              " 'semantic',\n",
              " 'steps',\n",
              " 'video',\n",
              " 'segments ',\n",
              " 'evaluate',\n",
              " 'method',\n",
              " 'large',\n",
              " 'number',\n",
              " 'complex',\n",
              " 'youtube',\n",
              " 'videos',\n",
              " 'show',\n",
              " 'results',\n",
              " 'unprecedented',\n",
              " 'quality',\n",
              " 'intricate',\n",
              " 'impactful',\n",
              " 'problem ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMpsIGBbSNhc",
        "outputId": "b22bb08d-8287-4315-a8d4-725a85726879"
      },
      "source": [
        "for i in train_data.Abstract:\n",
        "  print(i)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['human', 'communication', 'typically', 'underlying', 'structure ', 'reflected', 'fact', 'many', 'user', 'generated', 'videos ', 'starting', 'point ', 'ending ', 'certain', 'objective', 'steps', 'two', 'identified ', 'paper ', 'propose', 'method', 'parsing', 'video', 'semantic', 'steps', 'unsupervised', 'way ', 'proposed', 'method', 'capable', 'providing', 'semantic', ' storyline ', 'video', 'composed', 'objective', 'steps ', 'accomplish', 'using', 'visual', 'language', 'cues', 'joint', 'generative', 'model ', 'proposed', 'method', 'also', 'provide', 'textual', 'description', 'identified', 'semantic', 'steps', 'video', 'segments ', 'evaluate', 'method', 'large', 'number', 'complex', 'youtube', 'videos', 'show', 'results', 'unprecedented', 'quality', 'intricate', 'impactful', 'problem ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC2qQT8vSALx"
      },
      "source": [
        "ABS.build_vocab(train_data.Abstract,train_data.Title,max_size=40000,min_freq=2)\n",
        "TITLE.build_vocab)\n",
        "TITLE.vocab= ABS.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sWkiIVDt8iv",
        "outputId": "e28699a7-6461-436c-a24d-079bb00a5ec8"
      },
      "source": [
        "len(TITLE.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40004"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLb__KCTYghZ",
        "outputId": "6ccb10c6-2373-4cd1-f49d-78ea4c7fe99f"
      },
      "source": [
        "ABS.vocab.freqs['of']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9616"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXhRvMRuTM6L"
      },
      "source": [
        "assert(TITLE.vocab.stoi ==  ABS.vocab.stoi) #check if both share the same vocab or not"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9-xPP1oIg3k"
      },
      "source": [
        "#### Create Iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWrev9KYXIOH"
      },
      "source": [
        "BATCH_SIZE =64\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "    batch_size = BATCH_SIZE, sort_key = lambda x: len(x.Abstract), sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "447OZxoxXN9g",
        "outputId": "936d24f1-c4e2-40b6-acc0-d4ab809e34a1"
      },
      "source": [
        "batch = next(iter(train_iterator))\n",
        "batch.Title"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
              "        [  76,   76, 7886,  ...,   78,   49,  988],\n",
              "        [2608, 2213,  409,  ..., 2812,  845,  605],\n",
              "        ...,\n",
              "        [   1,    1,    1,  ...,    1,    1,    1],\n",
              "        [   1,    1,    1,  ...,    1,    1,    1],\n",
              "        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJoE4bdsY20x"
      },
      "source": [
        "#### Baseline Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT1wPVtoY5Zx"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, num_layers, dropout): \n",
        "        super().__init__()   \n",
        "        self.hid_dim = hid_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)  \n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers, dropout = dropout)    \n",
        "        self.dropout = nn.Dropout(dropout)       \n",
        "    def forward(self, input_idx):\n",
        "        #print(input_idx)\n",
        "        embedded = self.dropout(self.embedding(input_idx))  \n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #embedded = seq_len,batch_size,embed_dim\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        #outputs are always from the top hidden layer\n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63FdSj3ZZBbi"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,input_dim,emb_dim,hid_dim,num_layers,dropout,output_dim):\n",
        "    super().__init__()\n",
        "    self.hid_dim = hid_dim\n",
        "    self.num_layers=num_layers\n",
        "    self.embedding = nn.Embedding(input_dim,emb_dim)\n",
        "    self.lstm = nn.LSTM(emb_dim,hid_dim,num_layers,dropout=dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(hid_dim,output_dim) \n",
        "    self.output_dim=output_dim\n",
        "\n",
        "  def forward(self,input_idx,context_vector,cell_state):\n",
        "    #input_idx = [batch_size]\n",
        "    input_idx = input_idx.unsqueeze(0) #Adding a dimenstion at the the first = 1 = seq_len as we are sending word by word\n",
        "    embedded = self.dropout(self.embedding(input_idx))\n",
        "    #embedded = [1,batch_size,embed_size]\n",
        "    #print(embedded.size())\n",
        "    #print(\"G\")\n",
        "    outputs, (hidden, cell) = self.lstm(embedded,(context_vector,cell_state))\n",
        "    #outputs_size = (1,batch_size,hidden_dim)\n",
        "    outputs = outputs.reshape(-1,self.hid_dim)\n",
        "    #outputs_size=(batch_size,hid_dim)\n",
        "    prediction = self.fc(outputs)\n",
        "    #prediction_size = (batch_size,out_dim)\n",
        "    return prediction,hidden,cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoULj4AeZDTc"
      },
      "source": [
        "class Seq2Seq(nn.Module): #Combining the encoder and decoder\n",
        "  def __init__(self,encoder,decoder,device):\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.device =  device\n",
        "  def forward(self,input_batches,output_batches,tfr=0.5):\n",
        "    #input_batches dimension - (seq_len,batch_size)\n",
        "    #output_batches_dimension - (seq_len,batch_size)\n",
        "    batch_size = input_batches.shape[1]\n",
        "    title_len = output_batches.shape[0]\n",
        "    title_vocab_size = self.decoder.output_dim\n",
        "    predictions = torch.zeros(title_len, batch_size, title_vocab_size).to(device)\n",
        "    #print(input_batches.size())\n",
        "    hidden_state, cell_state = self.encoder(input_batches) \n",
        "    #hidden_state/cell_state dimension = num_layers,batch_size,hidden_dim\n",
        "\n",
        "    x = output_batches[0,:] # Trigger token <SOS>\n",
        "\n",
        "    for i in range(1, title_len):\n",
        "      pred, hidden_state, cell_state = self.decoder(x, hidden_state, cell_state)\n",
        "      #pred = [batch_size,output_dim(vocab_size)]\n",
        "      predictions[i] = pred\n",
        "      best_guess = pred.argmax(1) \n",
        "      x = output_batches[i,:] if random.random() < tfr else best_guess\n",
        "    return predictions  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO12iW3Mhmw8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAucn09mZF5y"
      },
      "source": [
        "INPUT_DIM = len(ABS.vocab)\n",
        "OUTPUT_DIM = len(TITLE.vocab)\n",
        "ENC_EMB_DIM = 100\n",
        "DEC_EMB_DIM = 100\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 3\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(INPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT,OUTPUT_DIM)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZdlgqRHa88Q",
        "outputId": "b62941f1-534d-44ab-d7a9-cf9a272460cb"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.1, 0.1)\n",
        "        \n",
        "model.apply(init_weights)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(40004, 100)\n",
              "    (lstm): LSTM(100, 512, num_layers=4, dropout=0.1)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(40004, 100)\n",
              "    (lstm): LSTM(100, 512, num_layers=4, dropout=0.1)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (fc): Linear(in_features=512, out_features=40004, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmAgQGFwbCy-"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5C4oZm4bNwn"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i,batch in enumerate(iterator):\n",
        "        \n",
        "        abstract = batch.Abstract\n",
        "        title = batch.Title\n",
        "        #abstract,title = [seq_len,batch_size]\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(abstract, title,0.9)\n",
        "        #predictions = [seq_len_title,batch_size,title_vocab]\n",
        "        output_dim = predictions.shape[-1]\n",
        "        \n",
        "        predictions = predictions[1:].view(-1, output_dim)#ignoring the first value is the <sos> token\n",
        "        title = title[1:].view(-1)\n",
        "        \n",
        "        loss = criterion(predictions, title)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndUAmUxWPqmp"
      },
      "source": [
        "def test(model, iterator, criterion):    \n",
        "    model.eval() \n",
        "    epoch_loss = 0 \n",
        "    with torch.no_grad():   \n",
        "        for i, batch in enumerate(iterator):\n",
        "          abstract = batch.Abstract\n",
        "          title = batch.Title\n",
        "          #abstract,title = [seq_len,batch_size]\n",
        "          predictions = model(abstract, title,0)\n",
        "          #predictions = [seq_len_title,batch_size,title_vocab]\n",
        "          output_dim = predictions.shape[-1]\n",
        "          predictions = predictions[1:].view(-1, output_dim)#ignoring the first value is the <sos> token\n",
        "          title = title[1:].view(-1)\n",
        "          loss = criterion(predictions, title)  \n",
        "          epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WXQSpnNQ7VD"
      },
      "source": [
        "#to generate title for one abstract\n",
        "def translate(model,abs,max_length):\n",
        "  #abs = A single abstract to generate title, data_type = str\n",
        "\n",
        "  #Preprocessing as done during training\n",
        "  abs=abs.lower()\n",
        "  abs = tokenize(abs)\n",
        "  abs = cleanup_text(abs)\n",
        "  #convert to a list of idx corresponding to abstract vocab\n",
        "  num_abs = []\n",
        "  num_abs.append(ABS.vocab.stoi['<sos>'])\n",
        "  for w in abs:\n",
        "    if w in ABS.vocab.stoi:\n",
        "      num_abs.append(ABS.vocab.stoi[w])\n",
        "    else:\n",
        "      num_abs.append(ABS.vocab.stoi['<unk'])\n",
        "  num_abs.append(ABS.vocab.stoi['<eos>'])\n",
        "  #covert to Long Tensor\n",
        "  num_abs = torch.LongTensor(num_abs)\n",
        "  #add batch_size = 1\n",
        "  num_abs = num_abs.unsqueeze(1)\n",
        "  #load them to device\n",
        "  num_abs = num_abs.to(device)\n",
        "  #print(num_abs.size())\n",
        "  #model = model.to(device)\n",
        "\n",
        "  #model.eval()\n",
        "  #Forward pass through the encoder\n",
        "  with torch.no_grad():\n",
        "    hidden_state, cell_state = model.encoder(num_abs)\n",
        "  \n",
        "  #Starts with <sos>\n",
        "  pred = [ABS.vocab.stoi[\"<sos>\"]]\n",
        "  for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([pred[-1]]).to(device)\n",
        "        #print(previous_word.size())\n",
        "        with torch.no_grad():\n",
        "            output, hidden_state, cell_state = model.decoder(previous_word, hidden_state, cell_state)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        pred.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == ABS.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "  translated_sentence = [ABS.vocab.itos[idx] for idx in pred]\n",
        "  return translated_sentence[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfomoGllfPWO",
        "outputId": "5012a5cf-7eee-4fde-b363-80f0cc49b1b6"
      },
      "source": [
        "model.parameters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(61763, 256)\n",
              "    (lstm): LSTM(256, 600, num_layers=4, dropout=0.1)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(61763, 256)\n",
              "    (lstm): LSTM(256, 600, num_layers=4, dropout=0.1)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (fc): Linear(in_features=600, out_features=61763, bias=True)\n",
              "  )\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlsqbW7QKJlo"
      },
      "source": [
        "demo_sentence = \"We propose an architecture for VQA which utilizes recurrent layers to\\ngenerate visual and textual attention. The memory characteristic of the\\nproposed recurrent attention units offers a rich joint embedding of visual and\\ntextual features and enables the model to reason relations between several\\nparts of the image and question. Our single model outperforms the first place\\nwinner on the VQA 1.0 dataset, performs within margin to the current\\nstate-of-the-art ensemble model. We also experiment with replacing attention\\nmechanisms in other state-of-the-art models with our implementation and show\\nincreased accuracy. In both cases, our recurrent attention mechanism improves\\nperformance in tasks requiring sequential or relational reasoning on the VQA\\ndataset\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPjXox43bwtD",
        "outputId": "30d896a2-ca3d-45db-a6e2-657a6f249d76"
      },
      "source": [
        "N_EPOCHS = 150\n",
        "CLIP = 1\n",
        "import time\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    #epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "      \n",
        "    #print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        " \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 6.752\n",
            "\tTest Loss: 6.508\n",
            "After epoch 0 , generated title is ['<unk>', 'of', 'for', 'for', 'for', '<eos>']\n",
            "\tTrain Loss: 6.356\n",
            "\tTest Loss: 6.656\n",
            "After epoch 1 , generated title is ['<unk>', 'learning', 'of', '<unk>', 'of', '<unk>', '<eos>']\n",
            "\tTrain Loss: 6.071\n",
            "\tTest Loss: 6.914\n",
            "After epoch 2 , generated title is ['<unk>', 'a', '<unk>', 'approach', 'for', 'the', 'and', '<unk>', '<eos>']\n",
            "\tTrain Loss: 5.895\n",
            "\tTest Loss: 7.114\n",
            "After epoch 3 , generated title is ['<unk>', 'a', '<unk>', 'approach', 'for', 'the', '<unk>', '<eos>']\n",
            "\tTrain Loss: 5.747\n",
            "\tTest Loss: 6.993\n",
            "After epoch 4 , generated title is ['<unk>', 'a', '<unk>', 'approach', 'for', 'the', '<unk>', 'of', 'the', '<eos>']\n",
            "\tTrain Loss: 5.618\n",
            "\tTest Loss: 7.009\n",
            "After epoch 5 , generated title is ['<unk>', 'a', '<unk>', 'approach', 'for', 'the', '<unk>', '<eos>']\n",
            "\tTrain Loss: 5.461\n",
            "\tTest Loss: 7.093\n",
            "After epoch 6 , generated title is ['<unk>', 'a', 'deep', 'convolutional', 'neural', 'network', 'for', 'image', 'classification', '<eos>']\n",
            "\tTrain Loss: 5.297\n",
            "\tTest Loss: 6.938\n",
            "After epoch 7 , generated title is ['<unk>', 'a', 'deep', 'learning', 'approach', 'for', 'image', 'classification', '<eos>']\n",
            "\tTrain Loss: 5.122\n",
            "\tTest Loss: 7.091\n",
            "After epoch 8 , generated title is ['<unk>', 'a', 'deep', 'learning', 'approach', 'for', 'image', 'classification', '<eos>']\n",
            "\tTrain Loss: 4.948\n",
            "\tTest Loss: 7.168\n",
            "After epoch 9 , generated title is ['<unk>', 'a', 'deep', 'neural', 'network', 'for', 'predicting', 'the', '<unk>', '<eos>']\n",
            "\tTrain Loss: 4.803\n",
            "\tTest Loss: 7.266\n",
            "After epoch 10 , generated title is ['<unk>', 'a', 'deep', 'learning', 'approach', 'for', 'automatic', 'post editing', '<eos>']\n",
            "\tTrain Loss: 4.645\n",
            "\tTest Loss: 7.411\n",
            "After epoch 11 , generated title is ['<unk>', 'a', 'deep', 'learning', 'approach', 'for', 'password', 'guessing', '<eos>']\n",
            "\tTrain Loss: 4.527\n",
            "\tTest Loss: 7.167\n",
            "After epoch 12 , generated title is ['<unk>', 'a', 'deep', 'neural', 'network', 'for', '<unk>', '<eos>']\n",
            "\tTrain Loss: 4.361\n",
            "\tTest Loss: 7.411\n",
            "After epoch 13 , generated title is ['<unk>', 'a', 'deep', 'learning', 'approach', 'for', 'password', 'guessing', '<eos>']\n",
            "\tTrain Loss: 4.242\n",
            "\tTest Loss: 7.522\n",
            "After epoch 14 , generated title is ['<unk>', 'a', 'deep', 'learning', 'framework', 'for', 'abuse', 'detection', '<eos>']\n",
            "\tTrain Loss: 4.118\n",
            "\tTest Loss: 7.611\n",
            "After epoch 15 , generated title is ['<unk>', 'a', 'deep', 'learning', 'approach', 'for', 'password', 'guessing', '<eos>']\n",
            "\tTrain Loss: 3.998\n",
            "\tTest Loss: 7.633\n",
            "After epoch 16 , generated title is ['<unk>', 'a', 'recurrent', 'neural', 'network', 'for', 'text', 'classification', '<eos>']\n",
            "\tTrain Loss: 3.867\n",
            "\tTest Loss: 7.728\n",
            "After epoch 17 , generated title is ['<unk>', 'a', 'recurrent', 'neural', 'network', 'for', 'text', 'classification', '<eos>']\n",
            "\tTrain Loss: 3.733\n",
            "\tTest Loss: 7.798\n",
            "After epoch 18 , generated title is ['<unk>', 'a', 'deep', 'neural', 'network', 'for', 'text', 'classification', '<eos>']\n",
            "\tTrain Loss: 3.636\n",
            "\tTest Loss: 7.814\n",
            "After epoch 19 , generated title is ['<unk>', 'a', 'deep', 'learning', 'approach', 'for', 'password', 'guessing', '<eos>']\n",
            "\tTrain Loss: 3.535\n",
            "\tTest Loss: 7.907\n",
            "After epoch 20 , generated title is ['<unk>', 'a', 'recurrent', 'neural', 'network', 'for', 'text', 'summarization', '<eos>']\n",
            "\tTrain Loss: 3.451\n",
            "\tTest Loss: 7.992\n",
            "After epoch 21 , generated title is ['<unk>', 'a', 'recurrent', 'neural', 'network', 'for', 'multimodal', 'language', 'generation', '<eos>']\n",
            "\tTrain Loss: 3.355\n",
            "\tTest Loss: 8.092\n",
            "After epoch 22 , generated title is ['<unk>', 'a', 'recurrent', 'neural', 'network', 'for', 'multimodal', 'language', 'generation', '<eos>']\n",
            "\tTrain Loss: 3.254\n",
            "\tTest Loss: 8.092\n",
            "After epoch 23 , generated title is ['<unk>', 'a', 'deep', 'neural', 'network', 'for', 'multimodal', 'question', 'answering', '<eos>']\n",
            "\tTrain Loss: 3.134\n",
            "\tTest Loss: 8.246\n",
            "After epoch 24 , generated title is ['<unk>', 'a', 'pytorch', 'reimplementation', 'of', 'visual', 'attention', 'for', 'visual', 'question']\n",
            "\tTrain Loss: 3.056\n",
            "\tTest Loss: 8.342\n",
            "After epoch 25 , generated title is ['<unk>', 'a', 'pytorch', 'reimplementation', 'of', 'image', 'and', 'visual', 'attention', '<eos>']\n",
            "\tTrain Loss: 2.964\n",
            "\tTest Loss: 8.436\n",
            "After epoch 26 , generated title is ['<unk>', 'a', 'pytorch', 'reimplementation', 'of', 'image', 'and', 'visual', 'question', 'answering']\n",
            "\tTrain Loss: 2.896\n",
            "\tTest Loss: 8.511\n",
            "After epoch 27 , generated title is ['<unk>', 'multimodal', 'attention', 'for', 'neural', 'machine', 'translation', '<eos>']\n",
            "\tTrain Loss: 2.812\n",
            "\tTest Loss: 8.534\n",
            "After epoch 28 , generated title is ['<unk>', 'a', 'multimodal', 'neural', 'network', 'for', 'multimodal', 'question', 'answering', '<eos>']\n",
            "\tTrain Loss: 2.690\n",
            "\tTest Loss: 8.745\n",
            "After epoch 29 , generated title is ['<unk>', 'a', 'multimodal', 'neural', 'network', 'for', 'multimodal', 'question', 'answering', '<eos>']\n",
            "\tTrain Loss: 2.634\n",
            "\tTest Loss: 8.700\n",
            "After epoch 30 , generated title is ['<unk>', 'visual', 'question', 'answering', 'with', 'image caption', 'supervision', '<eos>']\n",
            "\tTrain Loss: 2.562\n",
            "\tTest Loss: 8.862\n",
            "After epoch 31 , generated title is ['<unk>', 'a', 'pytorch', 'reimplementation', 'of', 'attention', 'for', 'visual', 'question', 'answering']\n",
            "\tTrain Loss: 2.465\n",
            "\tTest Loss: 8.823\n",
            "After epoch 32 , generated title is ['<unk>', 'answering', 'to', 'commands', 'with', 'captions', '<eos>']\n",
            "\tTrain Loss: 2.373\n",
            "\tTest Loss: 9.050\n",
            "After epoch 33 , generated title is ['<unk>', 'visual', 'captioning', 'with', 'visual', 'attention', '<eos>']\n",
            "\tTrain Loss: 2.307\n",
            "\tTest Loss: 9.156\n",
            "After epoch 34 , generated title is ['<unk>', 'a', 'multimodal', 'neural', 'network', 'for', 'multimodal', 'apparent', 'visual', 'question']\n",
            "\tTrain Loss: 2.251\n",
            "\tTest Loss: 9.185\n",
            "After epoch 35 , generated title is ['<unk>', 'multimodal', 'attention', 'for', 'referring', 'speech', 'recognition', '<eos>']\n",
            "\tTrain Loss: 2.157\n",
            "\tTest Loss: 9.389\n",
            "After epoch 36 , generated title is ['ask', 'me', 'anything ', 'image', 'captioning', 'with', 'large scale', 'web', '<eos>']\n",
            "\tTrain Loss: 2.102\n",
            "\tTest Loss: 9.337\n",
            "After epoch 37 , generated title is ['ask', 'your', 'neurons ', 'a', 'novel', 'dataset', 'for', 'visual', 'question', 'answering']\n",
            "\tTrain Loss: 2.021\n",
            "\tTest Loss: 9.456\n",
            "After epoch 38 , generated title is ['<unk>', 'visual', 'question', 'answering', 'with', 'visual', 'attention', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnScepimbkU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48913be5-d108-4717-ee23-3105259b1115"
      },
      "source": [
        "original_title = \"Dual Recurrent Attention Units for Visual Question Answering\"\n",
        "generated_title = ['<unk>', 'visual', 'question', 'answering', 'with', 'visual', 'attention', '<eos>']\n",
        "print(\"Abstract\")\n",
        "print(demo_sentence)\n",
        "print(\"Original Title :\",original_title)\n",
        "print(\"Generated Title (after ignoring <unk> and <eos> symbols) : \", \" \".join(generated_title[1:-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Abstract\n",
            "We propose an architecture for VQA which utilizes recurrent layers to\n",
            "generate visual and textual attention. The memory characteristic of the\n",
            "proposed recurrent attention units offers a rich joint embedding of visual and\n",
            "textual features and enables the model to reason relations between several\n",
            "parts of the image and question. Our single model outperforms the first place\n",
            "winner on the VQA 1.0 dataset, performs within margin to the current\n",
            "state-of-the-art ensemble model. We also experiment with replacing attention\n",
            "mechanisms in other state-of-the-art models with our implementation and show\n",
            "increased accuracy. In both cases, our recurrent attention mechanism improves\n",
            "performance in tasks requiring sequential or relational reasoning on the VQA\n",
            "dataset\n",
            "Original Title : Dual Recurrent Attention Units for Visual Question Answering\n",
            "Generated Title (after ignoring <unk> and <eos> symbols) :  visual question answering with visual attention\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}