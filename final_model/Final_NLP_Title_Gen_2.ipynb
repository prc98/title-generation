{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_NLP_Title_Gen_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cFlmpmODdvSr",
        "9Oqz5VAMdy9M",
        "jGEDhNyXT15G",
        "SM5k8SGcHaHn",
        "osefEj6yjp7-"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJx8HaEbhoxG"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from torch.optim import lr_scheduler\n",
        "from PIL import Image\n",
        "import timeit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy import data\n",
        "import random\n",
        "## For reproducibility\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul0enV_f6fVk",
        "outputId": "c9b4e316-4aa5-4eec-c29c-3cd463318c0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iryvSLmxh1vw"
      },
      "source": [
        "# Create Preprocessing pipeline for summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBy0Wh1zidtB"
      },
      "source": [
        "tokenize =  lambda s: s.split()\n",
        "import re  \n",
        "def cleanup_text(texts):\n",
        "    cleaned_text = []\n",
        "    for text in texts:\n",
        "        # remove punctuation\n",
        "        text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
        "        # remove multiple spaces\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        # remove newline\n",
        "        text = re.sub(r'\\n', ' ', text)\n",
        "        #replace digits with '# symbol\n",
        "        text = re.sub('[0-9]', '#', text)\n",
        "        cleaned_text.append(text)\n",
        "    return cleaned_text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJbowwh3LpyA",
        "outputId": "ac47e14e-9c96-4deb-a7e2-9f478becc8aa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2MHOhPtklHS"
      },
      "source": [
        "## Create torchtext fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CVDphMZkWaP"
      },
      "source": [
        "#Field for summaries (removed stop words)\n",
        "SUM = data.Field(tokenize = tokenize,init_token='<sos>',eos_token='<eos>',pad_first=True,stop_words=stop,lower = True,preprocessing=cleanup_text)\n",
        "#Field for title\n",
        "TITLE = data.Field(tokenize = tokenize,init_token='<sos>',eos_token='<eos>',lower = True,preprocessing=cleanup_text)\n",
        "#Field for Id\n",
        "#ID = data.Field(use_vocab=False,sequential=False,dtype=torch.LongTensor,postprocessing=data.Pipeline(lambda x: int(x)))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r-2ePKbnx62"
      },
      "source": [
        "fields = [('Id',None),('Abstract',None),('Title',TITLE),('sum1',SUM),('sum2',SUM),('sum3',SUM),('sum4',SUM),('sum5',SUM),('sum6',SUM),('sum7',SUM)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J3EKi0ooi6f"
      },
      "source": [
        "## Read data into tabular dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgf0hafhomaE"
      },
      "source": [
        "dataset = data.TabularDataset(path='./drive/MyDrive/data_summaries.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tleM8xWxpMmf",
        "outputId": "92d95346-9610-4bf7-cf11-0dcd9f62bed2"
      },
      "source": [
        "print(vars(dataset[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Title': ['dual', 'recurrent', 'attention', 'units', 'for', 'visual', 'question', 'answering'], 'sum1': ['propose', 'architecture', 'vqa', 'utilizes', 'recurrent', 'layers', 'generate', 'visual', 'textual', 'attention the', 'memory', 'characteristic', 'proposed', 'recurrent', 'attention', 'units', 'offers', 'rich', 'joint', 'embedding', 'visual', 'textual', 'features', 'enables', 'model', 'reason', 'relations', 'several', 'parts', 'image', 'question '], 'sum2': ['propose', 'architecture', 'vqa', 'utilizes', 'recurrent', 'layers', 'generate', 'visual', 'textual', 'attention in', 'cases ', 'recurrent', 'attention', 'mechanism', 'improves', 'performance', 'tasks', 'requiring', 'sequential', 'relational', 'reasoning', 'vqa', 'dataset '], 'sum3': ['memory', 'characteristic', 'proposed', 'recurrent', 'attention', 'units', 'offers', 'rich', 'joint', 'embedding', 'visual', 'textual', 'features', 'enables', 'model', 'reason', 'relations', 'several', 'parts', 'image', 'question our', 'single', 'model', 'outperforms', 'first', 'place', 'winner', 'vqa', '# #', 'dataset ', 'performs', 'within', 'margin', 'current', 'state of the art', 'ensemble', 'model '], 'sum4': ['memory', 'characteristic', 'proposed', 'recurrent', 'attention', 'units', 'offers', 'rich', 'joint', 'embedding', 'visual', 'textual', 'features', 'enables', 'model', 'reason', 'relations', 'several', 'parts', 'image', 'question our', 'single', 'model', 'outperforms', 'first', 'place', 'winner', 'vqa', '# #', 'dataset ', 'performs', 'within', 'margin', 'current', 'state of the art', 'ensemble', 'model '], 'sum5': ['memory', 'characteristic', 'proposed', 'recurrent', 'attention', 'units', 'offers', 'rich', 'joint', 'embedding', 'visual', 'textual', 'features', 'enables', 'model', 'reason', 'relations', 'several', 'parts', 'image', 'question in', 'cases ', 'recurrent', 'attention', 'mechanism', 'improves', 'performance', 'tasks', 'requiring', 'sequential', 'relational', 'reasoning', 'vqa', 'dataset '], 'sum6': ['single', 'model', 'outperforms', 'first', 'place', 'winner', 'vqa', '# #', 'dataset ', 'performs', 'within', 'margin', 'current', 'state of the art', 'ensemble', 'model in', 'cases ', 'recurrent', 'attention', 'mechanism', 'improves', 'performance', 'tasks', 'requiring', 'sequential', 'relational', 'reasoning', 'vqa', 'dataset '], 'sum7': ['propose', 'architecture', 'vqa', 'utilizes', 'recurrent', 'layers', 'generate', 'visual', 'textual', 'attention in', 'cases ', 'recurrent', 'attention', 'mechanism', 'improves', 'performance', 'tasks', 'requiring', 'sequential', 'relational', 'reasoning', 'vqa', 'dataset ']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKlMN3jMqmJm"
      },
      "source": [
        "## Create training data and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vlRSnYiqjIk"
      },
      "source": [
        "import random\n",
        "train_data, valid_data = dataset.split(split_ratio=0.9, random_state=random.seed(0))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwPMSsKN6r05",
        "outputId": "023526b6-2ba3-4457-dd16-fb18fad65551"
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(valid_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36900\n",
            "4100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhhG460cyQVW",
        "outputId": "d59ae9a2-abfc-4a7c-a054-8a023f06576c"
      },
      "source": [
        "print(vars(train_data[5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Title': ['adaptively', 'learning', 'the', 'crowd', 'kernel'], 'sum1': ['introduce', 'algorithm', 'that ', 'given', 'n', 'objects ', 'learns', 'similarity', 'matrix', 'n #', 'pairs ', 'crowdsourced', 'data', 'alone the', 'algorithm', 'samples', 'responses', 'adaptively', 'chosen', 'triplet based', 'relative similarity', 'queries '], 'sum2': ['introduce', 'algorithm', 'that ', 'given', 'n', 'objects ', 'learns', 'similarity', 'matrix', 'n #', 'pairs ', 'crowdsourced', 'data', 'alone svms', 'reveal', 'crowd', 'kernel', 'captures', 'prominent', 'subtle', 'features', 'across', 'number', 'domains ', ' is', 'striped ', 'among', 'neckties', ' vowel', 'vs ', 'consonant ', 'among', 'letters '], 'sum3': ['introduce', 'algorithm', 'that ', 'given', 'n', 'objects ', 'learns', 'similarity', 'matrix', 'n #', 'pairs ', 'crowdsourced', 'data', 'alone svms', 'reveal', 'crowd', 'kernel', 'captures', 'prominent', 'subtle', 'features', 'across', 'number', 'domains ', ' is', 'striped ', 'among', 'neckties', ' vowel', 'vs ', 'consonant ', 'among', 'letters '], 'sum4': ['output', 'embedding', 'objects', 'euclidean', 'space', ' like', 'mds ', 'refer', ' crowd', 'kernel svms', 'reveal', 'crowd', 'kernel', 'captures', 'prominent', 'subtle', 'features', 'across', 'number', 'domains ', ' is', 'striped ', 'among', 'neckties', ' vowel', 'vs ', 'consonant ', 'among', 'letters '], 'sum5': ['introduce', 'algorithm', 'that ', 'given', 'n', 'objects ', 'learns', 'similarity', 'matrix', 'n #', 'pairs ', 'crowdsourced', 'data', 'alone and', 'chosen', 'maximally', 'informative', 'given', 'preceding', 'responses '], 'sum6': ['introduce', 'algorithm', 'that ', 'given', 'n', 'objects ', 'learns', 'similarity', 'matrix', 'n #', 'pairs ', 'crowdsourced', 'data', 'alone the', 'algorithm', 'samples', 'responses', 'adaptively', 'chosen', 'triplet based', 'relative similarity', 'queries '], 'sum7': ['introduce', 'algorithm', 'that ', 'given', 'n', 'objects ', 'learns', 'similarity', 'matrix', 'n #', 'pairs ', 'crowdsourced', 'data', 'alone svms', 'reveal', 'crowd', 'kernel', 'captures', 'prominent', 'subtle', 'features', 'across', 'number', 'domains ', ' is', 'striped ', 'among', 'neckties', ' vowel', 'vs ', 'consonant ', 'among', 'letters ']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzsrWel-rwik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a770ee99-3e1a-4cfe-9b6e-a3890aa5f415"
      },
      "source": [
        "SUM.build_vocab(train_data.sum1,train_data.sum2,train_data.sum3,train_data.sum4,train_data.sum5,\\\n",
        "                train_data.sum6,train_data.sum7,train_data.Title,max_size=40000,vectors='glove.6B.100d')\n",
        "TITLE.vocab= SUM.vocab\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.40MB/s]                          \n",
            " 99%|█████████▉| 397542/400000 [00:14<00:00, 27781.74it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMTZoMzTwj9-",
        "outputId": "027163a6-6153-4fc7-af3e-b0bb805cf189"
      },
      "source": [
        "print(len(SUM.vocab))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dodGaMxD-zFL"
      },
      "source": [
        "## Create Bucket iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5V0C7iZ_E5N"
      },
      "source": [
        "def cal_length(x):\n",
        "  return len(x.sum1)+len(x.sum2)+len(x.sum3)+len(x.sum4)+len(x.sum5)+len(x.sum6)+len(x.sum7)+len(x.Title)\n",
        "from torchtext.legacy import data\n",
        "BATCH_SIZE =64\n",
        "train_iterator, valid_iterator =data.BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "    batch_size = BATCH_SIZE, sort_key = lambda x: cal_length(x), sort_within_batch = True,shuffle=True,sort=False,\n",
        "    device = device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMUTzw0tiJQ-"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU4Ix8W6iNpn"
      },
      "source": [
        "## Encoder layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qOs589nrh8p"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, num_layers, dropout): \n",
        "        super().__init__()   \n",
        "        self.hid_dim = hid_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)  \n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers, dropout = dropout)    \n",
        "        self.dropout = nn.Dropout(dropout)       \n",
        "    def forward(self, input_idx):\n",
        "        #print(input_idx)\n",
        "        input_idx=input_idx.to(device)\n",
        "        embedded = self.dropout(self.embedding(input_idx))\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #embedded = seq_len,batch_size,embed_dim\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        #outputs are always from the top hidden layer\n",
        "        return outputs,hidden"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEOlyNZxiQEF"
      },
      "source": [
        "## Control layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjdf2BQ6rjSM"
      },
      "source": [
        "class ControlLayer(nn.Module):\n",
        "    def __init__(self, input_dim,hid_dim): \n",
        "        super().__init__()   \n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        #self.embedding = nn.Embedding(input_dim, emb_dim)  \n",
        "        self.lstm = nn.LSTM(input_dim, hid_dim)    \n",
        "             \n",
        "    def forward(self, sum_hidden):\n",
        "        #print(input_idx)\n",
        "        #sum_hidden = seq_len(=7),batch_size,embed_dim(=encoder_hidden_dimension)\n",
        "        outputs, (hidden, cell) = self.lstm(sum_hidden)\n",
        "        #embedded = seq_len,batch_size,embed_dim\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        #outputs are always from the top hidden layer\n",
        "        return outputs,hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa4b7-0diabA"
      },
      "source": [
        "##Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfPmC8aGtaOu"
      },
      "source": [
        "class ComplexAttention(nn.Module):\n",
        "   def __init__(self,dec_hid_dim,cnt_hid_dim,enc_hid_dim):\n",
        "    super().__init__() \n",
        "    self.cnt_hid_dim=cnt_hid_dim\n",
        "    self.enc_hid_dim = enc_hid_dim\n",
        "    self.dec_hid_dim = dec_hid_dim\n",
        "   def forward(self,cnt_hid_states,enc_hid_states,dec_hid_states):\n",
        "     #cnt_hid_states = [7,batch_size,cnt_hid_dim]\n",
        "     #enc_hid_states = [7,seq_len*,batch_size,enc_hid_dim], list of hidden states for every summary\n",
        "     #dec_hid_states = [1,batch_size,dec_hid_dim]\n",
        "     '''Calculate summary level attention'''\n",
        "     cnt_hid_states=cnt_hid_states.permute(1,0,2)\n",
        "     dec_hid_states=dec_hid_states.permute(1,2,0)\n",
        "     #dec_hid_states=[batch_size,dec_hid_dim,1]\n",
        "     alpha = torch.bmm(cnt_hid_states,dec_hid_states)\n",
        "     #alpha=[batch_size,7,1]\n",
        "     #alpha=alpha.squeeze(2)\n",
        "     alpha=F.softmax(alpha,dim=1)\n",
        "     #alpha=[batch_size,7,1]\n",
        "     '''Calculate word level attention'''\n",
        "     batch_size = alpha.size()[0]\n",
        "     context_vec=torch.zeros(batch_size,1,self.enc_hid_dim).to(device)\n",
        "     context_vec_k=torch.zeros(7,batch_size,self.enc_hid_dim).to(device)\n",
        "     for k,sum_hid_states in enumerate(enc_hid_states):\n",
        "       #sum_hid_states = [seq_len_k,batch_size,enc_hid_dim]\n",
        "       sum_hid_states=sum_hid_states.permute(1,0,2)\n",
        "       beta = torch.bmm(sum_hid_states,dec_hid_states)\n",
        "       #beta = [batch_size,seq_len_1,1]\n",
        "       #beta=beta.squeeze(2)\n",
        "       beta=F.softmax(beta,dim=1)\n",
        "       beta=beta.permute(0,2,1)\n",
        "       #beta = [batch_size,1,seq_len]\n",
        "       #sum_hid_states = [batch_size,seq_len_size,enc_hid_dim]\n",
        "       context_vec_k[k] = torch.bmm(beta,sum_hid_states).squeeze(1)\n",
        "       #context_vec_k = [batch_size,1,enc_hid_dim].squeeze(1)\n",
        "     '''Combining both and returning context_vector'''\n",
        "     context_vec_k=context_vec_k.permute(1,0,2)\n",
        "     alpha=alpha.permute(0,2,1)\n",
        "     context_vec = torch.bmm(alpha,context_vec_k)\n",
        "     del context_vec_k\n",
        "     torch.cuda.empty_cache()\n",
        "     return alpha,beta,context_vec\n",
        "\n",
        "       "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0JC9VKMQ79t"
      },
      "source": [
        "class SimpleAttention(nn.Module):\n",
        "   def __init__(self,dec_hid_dim,cnt_hid_dim,enc_hid_dim,split):\n",
        "    super().__init__() \n",
        "    self.cnt_hid_dim=cnt_hid_dim\n",
        "    self.enc_hid_dim = enc_hid_dim\n",
        "    self.dec_hid_dim = dec_hid_dim\n",
        "    self.split = split\n",
        "   def forward(self,cnt_hid_states,enc_hid_states,dec_hid_states):\n",
        "     #cnt_hid_states = [7,batch_size,cnt_hid_dim]\n",
        "     #enc_hid_states = [7,seq_len*,batch_size,enc_hid_dim], list of hidden states for every summary\n",
        "     #dec_hid_states = [num_layers(=1),batch_size,dec_hid_dim]\n",
        "     '''Calculate summary level attention'''\n",
        "     cnt_hid_states_context=cnt_hid_states.permute(1,0,2)[:,:,:self.split]\n",
        "     dec_hid_states_context=dec_hid_states.permute(1,2,0)[:,:self.split,:]\n",
        "     cnt_hid_states_wgt=cnt_hid_states.permute(1,0,2)[:,:,self.split:]\n",
        "     dec_hid_states_wgt=dec_hid_states.permute(1,2,0)[:,self.split:,:]\n",
        "     #dec_hid_states_wgt=[batch_size,dec_hid_dim,1]\n",
        "     alpha = torch.bmm(cnt_hid_states_wgt,dec_hid_states_wgt)\n",
        "     #alpha=[batch_size,7,1]\n",
        "     #alpha=alpha.squeeze(2)\n",
        "     alpha=F.softmax(alpha,dim=1)\n",
        "     #alpha=[batch_size,7,1]\n",
        "     '''Calculate word level attention'''\n",
        "     batch_size = alpha.size()[0]\n",
        "     context_vec=torch.zeros(batch_size,1,self.split).to(device)\n",
        "     context_vec_k=torch.zeros(7,batch_size,self.split).to(device)\n",
        "     for k,sum_hid_states in enumerate(enc_hid_states):\n",
        "       #sum_hid_states = [seq_len_k,batch_size,enc_hid_dim]\n",
        "       sum_hid_states_wgt=sum_hid_states.permute(1,0,2)[:,:,self.split:]\n",
        "       sum_hid_states_context=sum_hid_states.permute(1,0,2)[:,:,:self.split]\n",
        "       beta = torch.bmm(sum_hid_states_wgt,dec_hid_states_wgt)\n",
        "       #beta = [batch_size,seq_len_1,1]\n",
        "       #beta=beta.squeeze(2)\n",
        "       beta=F.softmax(beta,dim=1)\n",
        "       beta=beta.permute(0,2,1)\n",
        "       #beta = [batch_size,1,seq_len]\n",
        "       #sum_hid_states = [batch_size,seq_len_size,enc_hid_dim]\n",
        "       context_vec_k[k] = torch.bmm(beta,sum_hid_states_context).squeeze(1)\n",
        "       #context_vec_k = [batch_size,1,enc_hid_dim].squeeze(1)\n",
        "     '''Combining both and returning context_vector'''\n",
        "     context_vec_k=context_vec_k.permute(1,0,2)\n",
        "     alpha=alpha.permute(0,2,1)\n",
        "     context_vec = torch.bmm(alpha,context_vec_k)\n",
        "     del context_vec_k\n",
        "     torch.cuda.empty_cache()\n",
        "     return alpha,beta,context_vec\n",
        "\n",
        "       "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3fLV-xHVtKr"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbbagtapVxkt"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim,con_hid_dim,attention,attention_type):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.con_hid_dim = con_hid_dim\n",
        "        self.attention = attention      \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        if attention_type=='complex':      \n",
        "          self.lstm = nn.LSTM(input_size=(enc_hid_dim + emb_dim),hidden_size= dec_hid_dim)\n",
        "          self.fc_out = nn.Linear( enc_hid_dim + dec_hid_dim + emb_dim, output_dim)\n",
        "        else:\n",
        "          self.lstm = nn.LSTM((attention.split+emb_dim), dec_hid_dim)\n",
        "          self.fc_out = nn.Linear( (2*attention.split)  + emb_dim, output_dim)    \n",
        "        #self.dropout = nn.Dropout(dropout)\n",
        "        self.attention_type=attention_type\n",
        "    def forward(self, input_idx,cnt_hid_states,enc_hid_states,dec_hid_states,cell_state):\n",
        "      #input = [batch_size]\n",
        "      input_idx = input_idx.unsqueeze(0)#Adding a dimenstion at the the first = 1 = seq_len as we are sending word by word\n",
        "      #input = [1,batch_size] \n",
        "      embedded = self.embedding(input_idx)\n",
        "      #embedded = [1,batch_size,embed_size]\n",
        "      '''Getting the context vector'''\n",
        "      _,_,context_vector=self.attention(cnt_hid_states,enc_hid_states,dec_hid_states)\n",
        "      #context_vector=[batch_size,1,hid_state]\n",
        "      context_vector=context_vector.permute(1,0,2)\n",
        "      #context_vector=[1,batch_size,hid_state]\n",
        "      lstm_in = torch.cat((embedded,context_vector),dim=2)\n",
        "      #lstm_in = [1,batch_size,context_vector_size+embed_size]\n",
        "      #print(lstm_in.size())\n",
        "      outputs, (hidden, cell) = self.lstm(lstm_in,(dec_hid_states,cell_state))\n",
        "      #output=[seq_len(=1),batch_size,hid_dim]\n",
        "      #hidden=[num_layers(=1),batch_size,hid_dim]\n",
        "      assert(outputs==hidden).all()\n",
        "\n",
        "      embedded=embedded.squeeze(0)\n",
        "      context_vector=context_vector.squeeze(0)\n",
        "      outputs = outputs.squeeze(0)\n",
        "      if self.attention_type=='complex':\n",
        "        prediction = self.fc_out(torch.cat((outputs,context_vector,embedded),dim=1))\n",
        "      else:\n",
        "        prediction = self.fc_out(torch.cat((outputs[:,:self.attention.split],context_vector,embedded),dim=1))\n",
        "      #prediction_size = (batch_size,out_dim)\n",
        "      return prediction,hidden,cell\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7w-CoTWgqMo"
      },
      "source": [
        "##Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OidIY-BVW4OB"
      },
      "source": [
        "class Seq2Seq(nn.Module): #Combining the encoder,control_layer & decoder\n",
        "  def __init__(self,encoder,control_layer,decoder,device):\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    self.control_layer = control_layer\n",
        "    self.decoder=decoder\n",
        "    self.device =  device\n",
        "  def forward(self,input_batches,output_batches,tfr=0.5):\n",
        "    #input_batches dimension - NOT A TENSOR. ENTIRE BATCH OBJECT IS SENT. \n",
        "    #output_batches_dimension - (seq_len,batch_size)\n",
        "    \n",
        "    batch_size = output_batches.shape[1]\n",
        "    title_len = output_batches.shape[0]\n",
        "    title_vocab_size = self.decoder.output_dim\n",
        "    predictions = torch.zeros(title_len, batch_size, title_vocab_size).to(device)\n",
        "    #print(input_batches.size())\n",
        "    '''Pass each summary through the encoder'''\n",
        "    sum1=input_batches.sum1\n",
        "    sum2=input_batches.sum2\n",
        "    sum3=input_batches.sum3\n",
        "    sum4=input_batches.sum4\n",
        "    sum5=input_batches.sum5\n",
        "    sum6=input_batches.sum6\n",
        "    sum7=input_batches.sum7\n",
        "    sum=[sum1,sum2,sum3,sum4,sum5,sum6,sum7]\n",
        "    control_input=torch.zeros((7,batch_size,self.control_layer.hid_dim)).to(device)\n",
        "    encoder_hidden_states = []\n",
        "    for s in range(7):\n",
        "      output,hidden=self.encoder(sum[s])\n",
        "      #output = [s.length,batch_size,hid_dim]\n",
        "      #hidden=[num_layers,batch_size,hid_dim]\n",
        "      #print(\"enc_output device\",output.device)\n",
        "      encoder_hidden_states.append(output)\n",
        "      control_input[s]=hidden[-1]\n",
        "    \n",
        "    '''Pass the last hidden state to control layer for each summary'''\n",
        "    output,hidden_state,cell_state = self.control_layer(control_input)\n",
        "    control_hidden_states = output\n",
        "    #prprint(\"S_c\")\n",
        "    '''Pass the merged representation to decoder along with encoder and control layer hidden states for implementing attention'''\n",
        "    \n",
        "    \n",
        "    x = output_batches[0,:] # Trigger token <SOS>\n",
        "\n",
        "    for i in range(1, title_len):\n",
        "      pred, hidden_state, cell_state = self.decoder(x,control_hidden_states,encoder_hidden_states,hidden_state, cell_state)\n",
        "      #pred = [batch_size,output_dim(vocab_size)]\n",
        "      predictions[i] = pred\n",
        "      best_guess = pred.argmax(1) \n",
        "      x = output_batches[i,:] if random.random() < tfr else best_guess\n",
        "    return predictions  "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3vYL04BOAgk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrgYtkNKOdUR"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i,batch in enumerate(iterator):\n",
        "        \n",
        "        #abstract = batch.Abstract\n",
        "        title = batch.Title\n",
        "        #abstract,title = [seq_len,batch_size]\n",
        "        optimizer.zero_grad()\n",
        "        #print(\"batch device \",batch.device)\n",
        "        predictions = model(batch, title,0.5)\n",
        "        \n",
        "        #predictions = [seq_len_title,batch_size,title_vocab]\n",
        "        output_dim = predictions.shape[-1]\n",
        "        \n",
        "        predictions = predictions[1:].view(-1, output_dim)#ignoring the first value is the <sos> token\n",
        "        title = title[1:].view(-1)\n",
        "        \n",
        "        loss = criterion(predictions, title)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W52SADa2OQij"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUeVTA61Odl5"
      },
      "source": [
        "def test(model, iterator, criterion):    \n",
        "    model.eval() \n",
        "    epoch_loss = 0 \n",
        "    with torch.no_grad():   \n",
        "        for i, batch in enumerate(iterator):\n",
        "          #abstract = batch.Abstract\n",
        "          title = batch.Title\n",
        "          #abstract,title = [seq_len,batch_size]\n",
        "          predictions = model(batch, title,0)\n",
        "          #predictions = [seq_len_title,batch_size,title_vocab]\n",
        "          output_dim = predictions.shape[-1]\n",
        "          predictions = predictions[1:].view(-1, output_dim)#ignoring the first value is the <sos> token\n",
        "          title = title[1:].view(-1)\n",
        "          loss = criterion(predictions, title)  \n",
        "          epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPORcxCzOZEi"
      },
      "source": [
        "##Translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaIcc_hIOe_i"
      },
      "source": [
        "#to generate title for one abstract\n",
        "def translate(model,batch,max_len):\n",
        "  predictions=[]\n",
        "  model.eval()\n",
        "  with torch.no_grad():   \n",
        "    batch_size = 1\n",
        "    title_vocab_size = model.decoder.output_dim\n",
        "   # predictions = torch.zeros(title_len, batch_size, title_vocab_size).to(device)\n",
        "    #print(input_batches.size())\n",
        "    '''Pass each summary through the encoder'''\n",
        "    sum1=batch.sum1\n",
        "    sum2=batch.sum2\n",
        "    sum3=batch.sum3\n",
        "    sum4=batch.sum4\n",
        "    sum5=batch.sum5\n",
        "    sum6=batch.sum6\n",
        "    sum7=batch.sum7\n",
        "    sum=[sum1,sum2,sum3,sum4,sum5,sum6,sum7]\n",
        "    control_input=torch.zeros((7,batch_size,model.control_layer.hid_dim)).to(device)\n",
        "    encoder_hidden_states = []\n",
        "    for s in range(7):\n",
        "      output,hidden=model.encoder(sum[s])\n",
        "      #output = [s.length,batch_size,hid_dim]\n",
        "      #hidden=[num_layers,batch_size,hid_dim]\n",
        "      #print(\"enc_output device\",output.device)\n",
        "      encoder_hidden_states.append(output)\n",
        "      control_input[s]=hidden[-1]\n",
        "    \n",
        "    '''Pass the last hidden state to control layer for each summary'''\n",
        "    output,hidden_state,cell_state = model.control_layer(control_input)\n",
        "    control_hidden_states = output\n",
        "    #prprint(\"S_c\")\n",
        "    '''Pass the merged representation to decoder along with encoder and control layer hidden states for implementing attention'''\n",
        "    \n",
        "    \n",
        "    x =  torch.LongTensor([SUM.vocab.stoi['<sos>']]).to(device)\n",
        "\n",
        "    for i in range(1, max_len):\n",
        "      pred, hidden_state, cell_state = model.decoder(x,control_hidden_states,encoder_hidden_states,hidden_state, cell_state)\n",
        "      #pred = [1,output_dim(vocab_size)]\n",
        "      best_guess = pred.argmax(1)\n",
        "      predictions.append(best_guess.item())\n",
        "      x = best_guess\n",
        "      # Model predicts it's the end of the sentence\n",
        "      if predictions[-1] == SUM.vocab.stoi[\"<eos>\"]:\n",
        "        break\n",
        "\n",
        "      translated_sentence = [SUM.vocab.itos[idx] for idx in predictions]\n",
        "  return translated_sentence[1:]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipr7F5cN6SKA"
      },
      "source": [
        "### Randomly pick 1000 abstracts from the dataset\n",
        "<br> This will be used later for generating titles. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVIqlySR6JD4"
      },
      "source": [
        "df = pd.read_csv('./drive/MyDrive/data_summaries.csv')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCoXANHQvP07"
      },
      "source": [
        "#Create the file only once. To be used for experiments.\n",
        "idx = np.random.randint(0,df.shape[0],1000)\n",
        "df1 = df.loc[idx]\n",
        "df1.to_csv('./drive/MyDrive/test_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boOqkDtIOhPS"
      },
      "source": [
        "##Start training and testing!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFlmpmODdvSr"
      },
      "source": [
        "### Experiement 1 - Complex Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1U8noU9OcZ4"
      },
      "source": [
        "INPUT_DIM = len(SUM.vocab)\n",
        "OUTPUT_DIM = len(TITLE.vocab)\n",
        "ENC_EMB_DIM = 100\n",
        "DEC_EMB_DIM = 100\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 3\n",
        "ENC_DROPOUT = 0\n",
        "DEC_DROPOUT = 0\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "con = ControlLayer(HID_DIM,HID_DIM)\n",
        "'''trying with complex attention first'''\n",
        "attention = ComplexAttention(HID_DIM,HID_DIM,HID_DIM)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,HID_DIM,HID_DIM,attention,'complex')\n",
        "\n",
        "model = Seq2Seq(enc,con, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpq0dBD1Ocrh",
        "outputId": "e9d487fc-7bbf-40d3-e378-2bce628baa75"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      #print(name)\n",
        "      nn.init.uniform_(param.data, -0.1, 0.1)  \n",
        "model.apply(init_weights)\n",
        "pretrained_embeddings = SUM.vocab.vectors\n",
        "model.encoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.decoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-fJZPAwfNVF"
      },
      "source": [
        "def checkpoint_and_save(model, min_loss, epoch, optimizer):\n",
        "    print()\n",
        "    state = {'model': model,'min_loss': min_loss,'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),}\n",
        "    path =  './drive/MyDrive/Colab Notebooks/final_net.pt'\n",
        "    torch.save(state, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1vltVXHj2Oz"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiO38agbOn-k",
        "outputId": "bfe836e0-0bd3-4cf1-e7cc-260158f395ca"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 6.402\n",
            "\tTest Loss: 6.523\n",
            "Time taken : 7.524mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.931\n",
            "\tTest Loss: 6.506\n",
            "Time taken : 7.692mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.748\n",
            "\tTest Loss: 6.520\n",
            "Time taken : 7.695mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.573\n",
            "\tTest Loss: 6.534\n",
            "Time taken : 7.702mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.455\n",
            "\tTest Loss: 6.547\n",
            "Time taken : 7.697mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.370\n",
            "\tTest Loss: 6.550\n",
            "Time taken : 7.683mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.274\n",
            "\tTest Loss: 6.594\n",
            "Time taken : 7.713mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.164\n",
            "\tTest Loss: 6.586\n",
            "Time taken : 7.722mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.100\n",
            "\tTest Loss: 6.594\n",
            "Time taken : 7.707mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.023\n",
            "\tTest Loss: 6.603\n",
            "Time taken : 7.705mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.964\n",
            "\tTest Loss: 6.613\n",
            "Time taken : 7.759mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.868\n",
            "\tTest Loss: 6.607\n",
            "Time taken : 7.721mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.807\n",
            "\tTest Loss: 6.611\n",
            "Time taken : 7.710mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.700\n",
            "\tTest Loss: 6.680\n",
            "Time taken : 7.735mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.602\n",
            "\tTest Loss: 6.513\n",
            "Time taken : 7.749mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.509\n",
            "\tTest Loss: 6.483\n",
            "Time taken : 7.751mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.356\n",
            "\tTest Loss: 6.472\n",
            "Time taken : 7.750mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.189\n",
            "\tTest Loss: 6.462\n",
            "Time taken : 7.759mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.059\n",
            "\tTest Loss: 6.404\n",
            "Time taken : 7.760mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.915\n",
            "\tTest Loss: 6.480\n",
            "Time taken : 7.769mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Oqz5VAMdy9M"
      },
      "source": [
        "### Experiement 2 - Simple Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBs4tvMcA53a"
      },
      "source": [
        "INPUT_DIM = len(SUM.vocab)\n",
        "OUTPUT_DIM = len(TITLE.vocab)\n",
        "ENC_EMB_DIM = 100\n",
        "DEC_EMB_DIM = 100\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 3\n",
        "ENC_DROPOUT = 0\n",
        "DEC_DROPOUT = 0\n",
        "SPLIT = 472\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "con = ControlLayer(HID_DIM,HID_DIM)\n",
        "'''trying with complex attention first'''\n",
        "attention = SimpleAttention(HID_DIM,HID_DIM,HID_DIM,SPLIT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,HID_DIM,HID_DIM,attention,'simple')\n",
        "\n",
        "model = Seq2Seq(enc,con, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0U2HVi5A53c",
        "outputId": "cc790111-d9b9-4d69-95c5-4c9ca160d32e"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      #print(name)\n",
        "      nn.init.uniform_(param.data, -0.1, 0.1)  \n",
        "model.apply(init_weights)\n",
        "pretrained_embeddings = SUM.vocab.vectors\n",
        "model.encoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.decoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0eBrsz2A53f"
      },
      "source": [
        "def checkpoint_and_save(model, min_loss, epoch, optimizer):\n",
        "    print()\n",
        "    state = {'model': model,'min_loss': min_loss,'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),'train_iterator':train_iterator.state_dict(),'valid_iterator':valid_iterator.state_dict()}\n",
        "    path =  './drive/MyDrive/Colab Notebooks/final_net_2.1.pt'\n",
        "    torch.save(state, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kdefe9PA53h"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkBlstVXA53i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7929b384-e8a7-4237-c9d5-4c476133da44"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 50\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch} - Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 6.239\n",
            "\tTest Loss: 6.492\n",
            "Epoch 0 - Time taken : 8.695mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.899\n",
            "\tTest Loss: 6.497\n",
            "Epoch 1 - Time taken : 8.679mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.714\n",
            "\tTest Loss: 6.523\n",
            "Epoch 2 - Time taken : 8.692mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.565\n",
            "\tTest Loss: 6.612\n",
            "Epoch 3 - Time taken : 8.696mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.453\n",
            "\tTest Loss: 6.531\n",
            "Epoch 4 - Time taken : 8.678mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.357\n",
            "\tTest Loss: 6.564\n",
            "Epoch 5 - Time taken : 8.702mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.261\n",
            "\tTest Loss: 6.532\n",
            "Epoch 6 - Time taken : 8.724mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.115\n",
            "\tTest Loss: 6.464\n",
            "Epoch 7 - Time taken : 8.725mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.961\n",
            "\tTest Loss: 6.336\n",
            "Epoch 8 - Time taken : 8.755mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.799\n",
            "\tTest Loss: 6.277\n",
            "Epoch 9 - Time taken : 8.762mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.628\n",
            "\tTest Loss: 6.341\n",
            "Epoch 10 - Time taken : 8.750mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.466\n",
            "\tTest Loss: 6.272\n",
            "Epoch 11 - Time taken : 8.772mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.303\n",
            "\tTest Loss: 6.353\n",
            "Epoch 12 - Time taken : 8.764mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.115\n",
            "\tTest Loss: 6.429\n",
            "Epoch 13 - Time taken : 8.768mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.971\n",
            "\tTest Loss: 6.364\n",
            "Epoch 14 - Time taken : 8.759mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.814\n",
            "\tTest Loss: 6.426\n",
            "Epoch 15 - Time taken : 8.749mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.628\n",
            "\tTest Loss: 6.554\n",
            "Epoch 16 - Time taken : 8.765mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.442\n",
            "\tTest Loss: 6.691\n",
            "Epoch 17 - Time taken : 8.759mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.314\n",
            "\tTest Loss: 6.718\n",
            "Epoch 18 - Time taken : 8.780mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.076\n",
            "\tTest Loss: 6.883\n",
            "Epoch 19 - Time taken : 8.759mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.901\n",
            "\tTest Loss: 6.917\n",
            "Epoch 20 - Time taken : 8.757mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.666\n",
            "\tTest Loss: 7.090\n",
            "Epoch 21 - Time taken : 8.761mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.505\n",
            "\tTest Loss: 7.223\n",
            "Epoch 22 - Time taken : 8.768mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.335\n",
            "\tTest Loss: 7.293\n",
            "Epoch 23 - Time taken : 8.770mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.108\n",
            "\tTest Loss: 7.434\n",
            "Epoch 24 - Time taken : 8.783mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.910\n",
            "\tTest Loss: 7.564\n",
            "Epoch 25 - Time taken : 8.790mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.756\n",
            "\tTest Loss: 7.643\n",
            "Epoch 26 - Time taken : 8.775mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.537\n",
            "\tTest Loss: 7.874\n",
            "Epoch 27 - Time taken : 8.784mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.360\n",
            "\tTest Loss: 8.005\n",
            "Epoch 28 - Time taken : 8.779mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.205\n",
            "\tTest Loss: 8.167\n",
            "Epoch 29 - Time taken : 8.739mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.032\n",
            "\tTest Loss: 8.385\n",
            "Epoch 30 - Time taken : 8.738mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 0.876\n",
            "\tTest Loss: 8.565\n",
            "Epoch 31 - Time taken : 8.755mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e9223c43ed01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\tTrain Loss: {train_loss:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-534606c595b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhhR8UlO7y9f"
      },
      "source": [
        "#### Call translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDlHeS9D8nDg"
      },
      "source": [
        "df1={'Abstract':[],'Title':[],'Generated Title':[]}"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9We7viJmFhhV",
        "outputId": "d275e7d4-718f-4513-c65c-a77b229079bc"
      },
      "source": [
        "df1"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Abstract': [], 'Generated Title': [], 'Title': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E53x0NA8nDj"
      },
      "source": [
        "ID = data.Field(use_vocab=False,sequential=False,preprocessing=int)\n",
        "fields = [('Id',ID),('Abstract',None),('Title',TITLE),('sum1',SUM),('sum2',SUM),('sum3',SUM),('sum4',SUM),('sum5',SUM),('sum6',SUM),('sum7',SUM)]\n",
        "ran_dataset = data.TabularDataset(path='./drive/MyDrive/test_data.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG5ZBjpc-J9_"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "BATCH_SIZE =1\n",
        "iterator=data.Iterator(\n",
        "    ran_dataset,\n",
        "    batch_size = BATCH_SIZE,shuffle=False,device=device)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taMJEYPA-NRg",
        "outputId": "7be247c6-a7bb-4863-83dd-608543558024"
      },
      "source": [
        "len(iterator)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cngyig-8-pP"
      },
      "source": [
        "path =  './drive/MyDrive/Colab Notebooks/final_net_2.1.pt'\n",
        "checkpoint = torch.load(path)\n",
        "#print(checkpoint)\n",
        "model1 = checkpoint['model']\n",
        "model1.load_state_dict( checkpoint['model_state_dict'])\n",
        "min_loss = checkpoint['min_loss']\n",
        "epoch = checkpoint['epoch']\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBYW-IDv9IXY"
      },
      "source": [
        "for i,batch in enumerate(iterator):\n",
        "  id1 = batch.Id.item()\n",
        "  #print(batch)\n",
        "  '''print(\"Abstract : \")\n",
        "  print(\"\".join(list(df.loc[df['Id'] == id1]['Abstract'])))\n",
        "  print(\"Actual Title : \")\n",
        "  print(\"\".join(list(df.loc[df['Id'] == id1]['Title'])))\n",
        "  print(\"Generated Title : \")\n",
        "  print(\" \".join(translate(model1,batch,10)))'''\n",
        "  abs= \"\".join(list(df.loc[df['Id'] == id1]['Abstract']))\n",
        "  title= \"\".join(list(df.loc[df['Id'] == id1]['Title']))\n",
        "  df1['Abstract'].append(abs)\n",
        "  df1['Title'].append(title)\n",
        "  df1['Generated Title'].append(\" \".join(translate(model1,batch,10)))\n",
        "  #df1['Generated_Title'][id1]=translate(model1,batch,15))\n",
        "\n",
        "  "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNc5GI_y-sr9",
        "outputId": "9819978d-a7cf-4dcb-eefa-ed5d0acd8aff"
      },
      "source": [
        "len(df1['Abstract'])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDVL3Dur-mLX"
      },
      "source": [
        "df2 = pd.DataFrame(df1)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYbKyAV2-mLc"
      },
      "source": [
        "df2.to_csv('./drive/MyDrive/generated_titles_2.csv')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_i-6DvbUM9e",
        "outputId": "fd99fabf-21ab-4b89-8942-db6e0fd5b330"
      },
      "source": [
        "##Calculate BLEU Score\n",
        "ref = []\n",
        "pred= []\n",
        "ref=[[t.split()] for t in df1['Title']]\n",
        "pred=[t.split() for t in df1['Generated Title']]\n",
        "from torchtext.data.metrics import bleu_score\n",
        "bleu_score(pred,ref,weights=[1,0,0,0])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGEDhNyXT15G"
      },
      "source": [
        "### Experiment 3 - Simple Attention,tfr =0.5, Dropout = 0.5\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2gNb5Msaqzv"
      },
      "source": [
        "INPUT_DIM = len(SUM.vocab)\n",
        "OUTPUT_DIM = len(TITLE.vocab)\n",
        "ENC_EMB_DIM = 100\n",
        "DEC_EMB_DIM = 100\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 3\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SPLIT = 472\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "con = ControlLayer(HID_DIM,HID_DIM)\n",
        "'''trying with complex attention first'''\n",
        "attention = SimpleAttention(HID_DIM,HID_DIM,HID_DIM,SPLIT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,HID_DIM,HID_DIM,attention,'simple')\n",
        "\n",
        "model = Seq2Seq(enc,con, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6B8R05uaqzz",
        "outputId": "729d513f-e138-4bdc-9c42-3e447a6566a4"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      #print(name)\n",
        "      nn.init.uniform_(param.data, -0.1, 0.1)  \n",
        "model.apply(init_weights)\n",
        "pretrained_embeddings = SUM.vocab.vectors\n",
        "model.encoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.decoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg90mZtLaqz4"
      },
      "source": [
        "def checkpoint_and_save(model, min_loss, epoch, optimizer):\n",
        "    print()\n",
        "    state = {'model': model,'min_loss': min_loss,'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),'train_iterator':train_iterator.state_dict(),'valid_iterator':valid_iterator.state_dict()}\n",
        "    path =  './drive/MyDrive/Colab Notebooks/final_net_3.pt'\n",
        "    torch.save(state, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jERTEwfaqz-"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P28dujUpaqz_",
        "outputId": "b2d29441-e735-4da8-c3fa-5548503150e4"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch} - Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 6.398\n",
            "\tTest Loss: 6.491\n",
            "Epoch 0 - Time taken : 7.194mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.933\n",
            "\tTest Loss: 6.505\n",
            "Epoch 1 - Time taken : 7.219mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.736\n",
            "\tTest Loss: 6.537\n",
            "Epoch 2 - Time taken : 7.198mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.576\n",
            "\tTest Loss: 6.423\n",
            "Epoch 3 - Time taken : 7.211mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.348\n",
            "\tTest Loss: 6.327\n",
            "Epoch 4 - Time taken : 7.238mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.181\n",
            "\tTest Loss: 6.303\n",
            "Epoch 5 - Time taken : 7.203mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.000\n",
            "\tTest Loss: 6.239\n",
            "Epoch 6 - Time taken : 7.226mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.786\n",
            "\tTest Loss: 6.225\n",
            "Epoch 7 - Time taken : 7.238mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.613\n",
            "\tTest Loss: 6.224\n",
            "Epoch 8 - Time taken : 7.245mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.436\n",
            "\tTest Loss: 6.263\n",
            "Epoch 9 - Time taken : 7.220mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.265\n",
            "\tTest Loss: 6.302\n",
            "Epoch 10 - Time taken : 7.231mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.064\n",
            "\tTest Loss: 6.335\n",
            "Epoch 11 - Time taken : 7.225mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.872\n",
            "\tTest Loss: 6.436\n",
            "Epoch 12 - Time taken : 7.249mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.692\n",
            "\tTest Loss: 6.519\n",
            "Epoch 13 - Time taken : 7.236mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.492\n",
            "\tTest Loss: 6.536\n",
            "Epoch 14 - Time taken : 7.265mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.385\n",
            "\tTest Loss: 6.632\n",
            "Epoch 15 - Time taken : 7.238mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.187\n",
            "\tTest Loss: 6.667\n",
            "Epoch 16 - Time taken : 7.261mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.054\n",
            "\tTest Loss: 6.781\n",
            "Epoch 17 - Time taken : 7.247mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.861\n",
            "\tTest Loss: 6.859\n",
            "Epoch 18 - Time taken : 7.243mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.717\n",
            "\tTest Loss: 6.951\n",
            "Epoch 19 - Time taken : 7.253mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF03zOQmdanb"
      },
      "source": [
        "#### Call translate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faH6jPBrA-xf"
      },
      "source": [
        "df1={'Abstract':[],'Title':[],'Generated Title':[]}"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiWkWbBI0ohT"
      },
      "source": [
        "ID = data.Field(use_vocab=False,sequential=False,preprocessing=int)\n",
        "fields = [('Id',ID),('Abstract',None),('Title',TITLE),('sum1',SUM),('sum2',SUM),('sum3',SUM),('sum4',SUM),('sum5',SUM),('sum6',SUM),('sum7',SUM)]\n",
        "ran_dataset = data.TabularDataset(path='./drive/MyDrive/test_data.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI6rLEhhefgR"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "BATCH_SIZE =1\n",
        "iterator=data.Iterator(\n",
        "    ran_dataset,\n",
        "    batch_size = BATCH_SIZE,shuffle=False,device=device)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Do9McV9Apc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5afca0-1c92-410d-c403-0b96f0c03c86"
      },
      "source": [
        "len(iterator)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVgbpP61eNVR"
      },
      "source": [
        "path =  './drive/MyDrive/Colab Notebooks/final_net_3.pt'\n",
        "checkpoint = torch.load(path)\n",
        "#print(checkpoint)\n",
        "model1 = checkpoint['model']\n",
        "model1.load_state_dict( checkpoint['model_state_dict'])\n",
        "min_loss = checkpoint['min_loss']\n",
        "epoch = checkpoint['epoch']\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b5dMFSefD6K"
      },
      "source": [
        "\n",
        "for i,batch in enumerate(iterator):\n",
        "  id1 = batch.Id.item()\n",
        "  #print(batch)\n",
        "  #print(\"Abstract : \")\n",
        "  #print(\"\".join(list(df.loc[df['Id'] == id1]['Abstract'])))\n",
        "  #print(\"Actual Title : \")\n",
        "  #print(\"\".join(list(df.loc[df['Id'] == id1]['Title'])))\n",
        "  #print(\"Generated Title : \")\n",
        "  #print(\" \".join(translate(model1,batch,10)))\n",
        "  abs= \"\".join(list(df.loc[df['Id'] == id1]['Abstract']))\n",
        "  title= \"\".join(list(df.loc[df['Id'] == id1]['Title']))\n",
        "  df1['Abstract'].append(abs)\n",
        "  df1['Title'].append(title)\n",
        "  df1['Generated Title'].append(\" \".join(translate(model1,batch,10)))\n",
        "  #df1['Generated_Title'][id1]=translate(model1,batch,15))\n",
        "\n",
        "  "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ja56laqyaFl"
      },
      "source": [
        "df2 = pd.DataFrame(df1)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgS-aNedX4rL"
      },
      "source": [
        "df2.to_csv('./drive/MyDrive/generated_titles_3.csv')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRV8zxw7vmCT",
        "outputId": "003af872-6b0c-408b-f43b-c45da7480fc4"
      },
      "source": [
        "# Calculate BLEU SCORE\n",
        "ref = []\n",
        "pred= []\n",
        "ref=[[t.split()] for t in df1['Title']]\n",
        "pred=[t.split() for t in df1['Generated Title']]\n",
        "from torchtext.data.metrics import bleu_score\n",
        "bleu_score(pred,ref,weights=[1,0,0,0])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM5k8SGcHaHn"
      },
      "source": [
        "### Experiment 4 ; Simple Attention, encoder num layers = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY2HIy2ZHXnj"
      },
      "source": [
        "INPUT_DIM = len(SUM.vocab)\n",
        "OUTPUT_DIM = len(TITLE.vocab)\n",
        "ENC_EMB_DIM = 100\n",
        "DEC_EMB_DIM = 100\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 1\n",
        "ENC_DROPOUT = 0\n",
        "DEC_DROPOUT = 0\n",
        "SPLIT = 472\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "con = ControlLayer(HID_DIM,HID_DIM)\n",
        "attention = SimpleAttention(HID_DIM,HID_DIM,HID_DIM,SPLIT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,HID_DIM,HID_DIM,attention,'simple')\n",
        "\n",
        "model = Seq2Seq(enc,con, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGjEBHmjIZWD",
        "outputId": "13bc2bda-8778-4df7-ab84-55874dd9bf35"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      #print(name)\n",
        "      nn.init.uniform_(param.data, -0.1, 0.1)  \n",
        "model.apply(init_weights)\n",
        "pretrained_embeddings = SUM.vocab.vectors\n",
        "model.encoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.decoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_qs2l1aIi7H"
      },
      "source": [
        "def checkpoint_and_save(model, min_loss, epoch, optimizer):\n",
        "    print()\n",
        "    state = {'model': model,'min_loss': min_loss,'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),'train_iterator':train_iterator.state_dict(),'valid_iterator':valid_iterator.state_dict()}\n",
        "    path =  './drive/MyDrive/Colab Notebooks/final_net_4.pt'\n",
        "    torch.save(state, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeI5VLCBIi7K"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dweIbaLfIi7M",
        "outputId": "d46f5538-847f-463e-a68b-fbef3e71e1c7"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch} - Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 6.323\n",
            "\tTest Loss: 6.135\n",
            "Epoch 0 - Time taken : 5.394mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.392\n",
            "\tTest Loss: 5.861\n",
            "Epoch 1 - Time taken : 5.327mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.816\n",
            "\tTest Loss: 5.798\n",
            "Epoch 2 - Time taken : 5.299mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.308\n",
            "\tTest Loss: 5.779\n",
            "Epoch 3 - Time taken : 5.324mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.804\n",
            "\tTest Loss: 5.884\n",
            "Epoch 4 - Time taken : 5.322mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.371\n",
            "\tTest Loss: 5.995\n",
            "Epoch 5 - Time taken : 5.297mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.981\n",
            "\tTest Loss: 6.151\n",
            "Epoch 6 - Time taken : 5.294mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.615\n",
            "\tTest Loss: 6.297\n",
            "Epoch 7 - Time taken : 5.312mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.300\n",
            "\tTest Loss: 6.453\n",
            "Epoch 8 - Time taken : 5.338mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.988\n",
            "\tTest Loss: 6.672\n",
            "Epoch 9 - Time taken : 5.489mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.701\n",
            "\tTest Loss: 6.880\n",
            "Epoch 10 - Time taken : 5.524mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.434\n",
            "\tTest Loss: 7.103\n",
            "Epoch 11 - Time taken : 5.506mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIIoDRgJSOxO"
      },
      "source": [
        "#### Call translate (final_model_4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YML8B8rgSLpR"
      },
      "source": [
        "df1={'Abstract':[],'Title':[],'Generated Title':[]}"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu2HO7j8SLpT",
        "outputId": "b00ff38c-2d06-450f-8f01-083a32146616"
      },
      "source": [
        "df1"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Abstract': [], 'Generated Title': [], 'Title': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJa0P6CZSLpV"
      },
      "source": [
        "ID = data.Field(use_vocab=False,sequential=False,preprocessing=int)\n",
        "fields = [('Id',ID),('Abstract',None),('Title',TITLE),('sum1',SUM),('sum2',SUM),('sum3',SUM),('sum4',SUM),('sum5',SUM),('sum6',SUM),('sum7',SUM)]\n",
        "ran_dataset = data.TabularDataset(path='./drive/MyDrive/test_data.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nED7m4gcSLpW"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "BATCH_SIZE =1\n",
        "iterator=data.Iterator(\n",
        "    ran_dataset,\n",
        "    batch_size = BATCH_SIZE,shuffle=False,device=device)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksTPG-h1SLpX",
        "outputId": "39d4cae1-f1e6-42d8-c01c-6ed01d3d7f85"
      },
      "source": [
        "len(iterator)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzpVe2WKSLpY"
      },
      "source": [
        "path =  './drive/MyDrive/Colab Notebooks/final_net_4.pt'\n",
        "checkpoint = torch.load(path)\n",
        "#print(checkpoint)\n",
        "model1 = checkpoint['model']\n",
        "model1.load_state_dict( checkpoint['model_state_dict'])\n",
        "min_loss = checkpoint['min_loss']\n",
        "epoch = checkpoint['epoch']"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDJMt51BSLpc"
      },
      "source": [
        "for i,batch in enumerate(iterator):\n",
        "  id1 = batch.Id.item()\n",
        "  #print(batch)\n",
        "  '''print(\"Abstract : \")\n",
        "  print(\"\".join(list(df.loc[df['Id'] == id1]['Abstract'])))\n",
        "  print(\"Actual Title : \")\n",
        "  print(\"\".join(list(df.loc[df['Id'] == id1]['Title'])))\n",
        "  print(\"Generated Title : \")\n",
        "  print(\" \".join(translate(model1,batch,10)))'''\n",
        "  abs= \"\".join(list(df.loc[df['Id'] == id1]['Abstract']))\n",
        "  title= \"\".join(list(df.loc[df['Id'] == id1]['Title']))\n",
        "  df1['Abstract'].append(abs)\n",
        "  df1['Title'].append(title)\n",
        "  df1['Generated Title'].append(\" \".join(translate(model1,batch,10)))\n",
        "  #df1['Generated_Title'][id1]=translate(model1,batch,15))\n",
        "\n",
        "  "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0vqLDIYSLpd",
        "outputId": "273a77b3-266e-4b20-ca95-93bd5ce63e06"
      },
      "source": [
        "len(df1['Abstract'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBeWyGU8SLpg"
      },
      "source": [
        "df2 = pd.DataFrame(df1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A51Jm9IOSLpg"
      },
      "source": [
        "df2.to_csv('./drive/MyDrive/generated_titles_4.csv')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f6BpwHUUr7d"
      },
      "source": [
        "### Experiment 5 - Simple attention, encoder layer = 1, Hidden_dim = 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3HS1nFRU1bR"
      },
      "source": [
        "INPUT_DIM = len(SUM.vocab)\n",
        "OUTPUT_DIM = len(TITLE.vocab)\n",
        "ENC_EMB_DIM = 100\n",
        "DEC_EMB_DIM = 100\n",
        "HID_DIM = 128\n",
        "N_LAYERS = 1\n",
        "ENC_DROPOUT = 0\n",
        "DEC_DROPOUT = 0\n",
        "SPLIT = 100\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "con = ControlLayer(HID_DIM,HID_DIM)\n",
        "'''trying with complex attention first'''\n",
        "attention = SimpleAttention(HID_DIM,HID_DIM,HID_DIM,SPLIT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,HID_DIM,HID_DIM,attention,'simple')\n",
        "\n",
        "model = Seq2Seq(enc,con, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVfFmtCgU1bT",
        "outputId": "37ac5cce-1443-419c-a3e3-3d2117a87858"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      #print(name)\n",
        "      nn.init.uniform_(param.data, -0.1, 0.1)  \n",
        "model.apply(init_weights)\n",
        "pretrained_embeddings = SUM.vocab.vectors\n",
        "model.encoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.decoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ_VIVz6U1bV"
      },
      "source": [
        "def checkpoint_and_save(model, min_loss, epoch, optimizer):\n",
        "    print()\n",
        "    state = {'model': model,'min_loss': min_loss,'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),'train_iterator':train_iterator.state_dict(),'valid_iterator':valid_iterator.state_dict()}\n",
        "    path =  './drive/MyDrive/Colab Notebooks/final_net_5.pt'\n",
        "    torch.save(state, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AknTYGCCU1bX"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYJzZxboU1bY",
        "outputId": "f58e590b-ed03-49e4-a51b-7580c364200c"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch} - Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 6.501\n",
            "\tTest Loss: 6.345\n",
            "Epoch 0 - Time taken : 2.972mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.800\n",
            "\tTest Loss: 6.123\n",
            "Epoch 1 - Time taken : 2.960mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.406\n",
            "\tTest Loss: 5.982\n",
            "Epoch 2 - Time taken : 2.960mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.114\n",
            "\tTest Loss: 5.951\n",
            "Epoch 3 - Time taken : 2.966mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.878\n",
            "\tTest Loss: 5.868\n",
            "Epoch 4 - Time taken : 2.954mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.638\n",
            "\tTest Loss: 5.849\n",
            "Epoch 5 - Time taken : 2.962mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.431\n",
            "\tTest Loss: 5.868\n",
            "Epoch 6 - Time taken : 2.969mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.223\n",
            "\tTest Loss: 5.908\n",
            "Epoch 7 - Time taken : 2.970mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.053\n",
            "\tTest Loss: 5.912\n",
            "Epoch 8 - Time taken : 2.976mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.871\n",
            "\tTest Loss: 5.967\n",
            "Epoch 9 - Time taken : 2.970mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osefEj6yjp7-"
      },
      "source": [
        "#### Resume Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BFF7xtg9TVL"
      },
      "source": [
        "path =  './drive/MyDrive/Colab Notebooks/final_net_5.pt'\n",
        "checkpoint = torch.load(path)\n",
        "#print(checkpoint)\n",
        "model = checkpoint['model']\n",
        "model.load_state_dict( checkpoint['model_state_dict'])\n",
        "min_loss = checkpoint['min_loss']\n",
        "epoch = checkpoint['epoch']\n",
        "optimizer1 = checkpoint['optimizer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDuCBu_B9TVO"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "optimizer.load_state_dict(optimizer1)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-qw3bEUjnQ8",
        "outputId": "caf60c32-6545-4ee6-ca7e-c72fe62b7cd1"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(10,N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch} - Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 3.716\n",
            "\tTest Loss: 6.044\n",
            "Epoch 10 - Time taken : 2.947mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.554\n",
            "\tTest Loss: 6.077\n",
            "Epoch 11 - Time taken : 2.968mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.398\n",
            "\tTest Loss: 6.143\n",
            "Epoch 12 - Time taken : 2.952mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.264\n",
            "\tTest Loss: 6.199\n",
            "Epoch 13 - Time taken : 2.965mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.134\n",
            "\tTest Loss: 6.276\n",
            "Epoch 14 - Time taken : 2.969mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.002\n",
            "\tTest Loss: 6.372\n",
            "Epoch 15 - Time taken : 2.972mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.883\n",
            "\tTest Loss: 6.448\n",
            "Epoch 16 - Time taken : 2.960mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.772\n",
            "\tTest Loss: 6.518\n",
            "Epoch 17 - Time taken : 2.962mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.657\n",
            "\tTest Loss: 6.604\n",
            "Epoch 18 - Time taken : 2.971mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.541\n",
            "\tTest Loss: 6.674\n",
            "Epoch 19 - Time taken : 2.972mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnv3O_D_FbX3"
      },
      "source": [
        "#### Run the model for 10 more epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voK692bm95W2",
        "outputId": "23d4e2f5-4745-4919-a420-ef63e265bf38"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(10,N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch} - Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 2.460\n",
            "\tTest Loss: 6.773\n",
            "Epoch 10 - Time taken : 2.893mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.366\n",
            "\tTest Loss: 6.848\n",
            "Epoch 11 - Time taken : 2.911mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.272\n",
            "\tTest Loss: 6.957\n",
            "Epoch 12 - Time taken : 2.899mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.178\n",
            "\tTest Loss: 7.015\n",
            "Epoch 13 - Time taken : 2.901mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.087\n",
            "\tTest Loss: 7.127\n",
            "Epoch 14 - Time taken : 2.909mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.035\n",
            "\tTest Loss: 7.195\n",
            "Epoch 15 - Time taken : 2.900mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.947\n",
            "\tTest Loss: 7.300\n",
            "Epoch 16 - Time taken : 2.891mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.861\n",
            "\tTest Loss: 7.396\n",
            "Epoch 17 - Time taken : 2.903mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.804\n",
            "\tTest Loss: 7.463\n",
            "Epoch 18 - Time taken : 2.898mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.735\n",
            "\tTest Loss: 7.582\n",
            "Epoch 19 - Time taken : 2.895mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXMtwYUX_bsN"
      },
      "source": [
        "#### Call Translate(final_model_5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U84Wg7VrEsuC"
      },
      "source": [
        "df1={'Abstract':[],'Title':[],'Generated Title':[]}"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mytSHmisEsuE",
        "outputId": "29aa9ede-345b-4fad-8fcb-7c0dbb675876"
      },
      "source": [
        "df1"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Abstract': [], 'Generated Title': [], 'Title': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqf5ses_EsuJ"
      },
      "source": [
        "ID = data.Field(use_vocab=False,sequential=False,preprocessing=int)\n",
        "fields = [('Id',ID),('Abstract',None),('Title',TITLE),('sum1',SUM),('sum2',SUM),('sum3',SUM),('sum4',SUM),('sum5',SUM),('sum6',SUM),('sum7',SUM)]\n",
        "ran_dataset = data.TabularDataset(path='./drive/MyDrive/test_data.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qCLM0i7EsuL"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "BATCH_SIZE =1\n",
        "iterator=data.Iterator(\n",
        "    ran_dataset,\n",
        "    batch_size = BATCH_SIZE,shuffle=False,device=device)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC701uP3EsuM"
      },
      "source": [
        "path =  './drive/MyDrive/Colab Notebooks/final_net_5.pt'\n",
        "checkpoint = torch.load(path)\n",
        "#print(checkpoint)\n",
        "model1 = checkpoint['model']\n",
        "model1.load_state_dict( checkpoint['model_state_dict'])\n",
        "min_loss = checkpoint['min_loss']\n",
        "epoch = checkpoint['epoch']"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPFG2P5HEsuN"
      },
      "source": [
        "for i,batch in enumerate(iterator):\n",
        "  id1 = batch.Id.item()\n",
        "  #print(batch)\n",
        "  #print(\"Abstract : \")\n",
        "  #print(\"\".join(list(df.loc[df['Id'] == id1]['Abstract'])))\n",
        "  #print(\"Actual Title : \")\n",
        "  #print(\"\".join(list(df.loc[df['Id'] == id1]['Title'])))\n",
        "  #print(\"Generated Title : \")\n",
        "  #print(\" \".join(translate(model1,batch,10)))\n",
        "  abs= \"\".join(list(df.loc[df['Id'] == id1]['Abstract']))\n",
        "  title= \"\".join(list(df.loc[df['Id'] == id1]['Title']))\n",
        "  df1['Abstract'].append(abs)\n",
        "  df1['Title'].append(title)\n",
        "  df1['Generated Title'].append(\" \".join(translate(model1,batch,10)))\n",
        "  #df1['Generated_Title'][id1]=translate(model1,batch,15))\n",
        "\n",
        "  "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf5HB9EzEsuT"
      },
      "source": [
        "df2 = pd.DataFrame(df1)\n",
        "df2.to_csv('./drive/MyDrive/generated_titles_5.csv')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmQoIvoIF3Ot"
      },
      "source": [
        "#Calculate BLEU SCORE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEhtRprZF3Ow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2582568-f00c-40f0-9dcb-e0311520934c"
      },
      "source": [
        "ref = []\n",
        "pred= []\n",
        "ref=[[t.split()] for t in df1['Title']]\n",
        "pred=[t.split() for t in df1['Generated Title']]\n",
        "from torchtext.data.metrics import bleu_score\n",
        "bleu_score(pred,ref,weights=[1,0,0,0])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12435863708392587"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFQowqVg31yd"
      },
      "source": [
        "# Final model selected = final_model_5 ( from experiment 5)\n",
        "# Cherry Picking some examples from generated titles and comparing with Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcyWxYTu-WIe"
      },
      "source": [
        "final_titles = pd.read_csv('./drive/MyDrive/generated_titles_5.csv')\n",
        "base_titles = pd.read_csv('./drive/MyDrive/generated_titles_baseline.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3PRC8fR_wJN"
      },
      "source": [
        "idx = [41,42,58,61,121,143,160,162,188,192,231,232,240,291,403,452,947,992,941,883] ##Cheery picking\n",
        "compare_titles = base_titles.loc[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuftZm9EAnWc"
      },
      "source": [
        "compare_titles['Generated Title(Final)']=final_titles.loc[idx]['Generated Title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X14GTbj-B1aa"
      },
      "source": [
        "compare_titles.columns = ['No.','Abstract', 'Actual title','Generated Title(Baseline)','Generated Title(Final)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NykQzACvBF25",
        "outputId": "b6a24897-f182-4629-a281-fb54c71521cb"
      },
      "source": [
        "pd.set_option(\"display.max_colwidth\",1000)\n",
        "compare_titles.iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Actual title</th>\n",
              "      <th>Generated Title(Baseline)</th>\n",
              "      <th>Generated Title(Final)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>This paper reviews Kunchenko's polynomials using as template matching method\\nto recognize template in one-dimensional input signal. Kunchenko's polynomials\\nmethod is compared with classical methods - cross-correlation and sum of\\nsquared differences according to numerical statistical example.</td>\n",
              "      <td>Kunchenko's Polynomials for Template Matching</td>\n",
              "      <td>a new framework for learning work quality of phylogenetic qualitative</td>\n",
              "      <td>polynomials for template matching</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Although many successful ensemble clustering approaches have been developed\\nin recent years, there are still two limitations to most of the existing\\napproaches. First, they mostly overlook the issue of uncertain links, which may\\nmislead the overall consensus process. Second, they generally lack the ability\\nto incorporate global information to refine the local links. To address these\\ntwo limitations, in this paper, we propose a novel ensemble clustering approach\\nbased on sparse graph representation and probability trajectory analysis. In\\nparticular, we present the elite neighbor selection strategy to identify the\\nuncertain links by locally adaptive thresholds and build a sparse graph with a\\nsmall number of probably reliable links. We argue that a small number of\\nprobably reliable links can lead to significantly better consensus results than\\nusing all graph links regardless of their reliability. The random walk process\\ndriven by a new transition probability matrix is util...</td>\n",
              "      <td>Robust Ensemble Clustering Using Probability Trajectories</td>\n",
              "      <td>a new framework for work single linkage optimization &lt;eos&gt;</td>\n",
              "      <td>ensemble clustering with sparse ensemble</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>We study black-box attacks on machine learning classifiers where each query\\nto the model incurs some cost or risk of detection to the adversary. We focus\\nexplicitly on minimizing the number of queries as a major objective.\\nSpecifically, we consider the problem of attacking machine learning classifiers\\nsubject to a budget of feature modification cost while minimizing the number of\\nqueries, where each query returns only a class and confidence score. We\\ndescribe an approach that uses Bayesian optimization to minimize the number of\\nqueries, and find that the number of queries can be reduced to approximately\\none tenth of the number needed through a random strategy for scenarios where\\nthe feature modification cost budget is low.</td>\n",
              "      <td>Query-limited Black-box Attacks to Classifiers</td>\n",
              "      <td>a new approach to work &lt;unk&gt; of &lt;unk&gt; ai based &lt;eos&gt;</td>\n",
              "      <td>black box attacks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>We consider probabilistic topic models and more recent word embedding\\ntechniques from a perspective of learning hidden semantic representations.\\nInspired by a striking similarity of the two approaches, we merge them and\\nlearn probabilistic embeddings with online EM-algorithm on word co-occurrence\\ndata. The resulting embeddings perform on par with Skip-Gram Negative Sampling\\n(SGNS) on word similarity tasks and benefit in the interpretability of the\\ncomponents. Next, we learn probabilistic document embeddings that outperform\\nparagraph2vec on a document similarity task and require less memory and time\\nfor training. Finally, we employ multimodal Additive Regularization of Topic\\nModels (ARTM) to obtain a high sparsity and learn embeddings for other\\nmodalities, such as timestamps and categories. We observe further improvement\\nof word similarity performance and meaningful inter-modality similarities.</td>\n",
              "      <td>Interpretable probabilistic embeddings: bridging the gap between topic\\n  models and neural networks</td>\n",
              "      <td>a filters of work &lt;unk&gt;  roughly authorship  pseudo likelihood and and</td>\n",
              "      <td>probabilistic embeddings  bridging the topic models</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>The production of color language is essential for grounded language\\ngeneration. Color descriptions have many challenging properties: they can be\\nvague, compositionally complex, and denotationally rich. We present an\\neffective approach to generating color descriptions using recurrent neural\\nnetworks and a Fourier-transformed color representation. Our model outperforms\\nprevious work on a conditional language modeling task over a large corpus of\\nnaturalistic color descriptions. In addition, probing the model's output\\nreveals that it can accurately produce not only basic color terms but also\\ndescriptors with non-convex denotations (\"greenish\"), bare modifiers (\"bright\",\\n\"dull\"), and compositional phrases (\"faded teal\") not seen in training.</td>\n",
              "      <td>Learning to Generate Compositional Color Descriptions</td>\n",
              "      <td>&lt;unk&gt; a &lt;unk&gt; for hmm flaws structure  &lt;eos&gt;</td>\n",
              "      <td>to generate compositional color descriptions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>Short-term tracking is an open and challenging problem for which\\ndiscriminative correlation filters (DCF) have shown excellent performance. We\\nintroduce the channel and spatial reliability concepts to DCF tracking and\\nprovide a novel learning algorithm for its efficient and seamless integration\\nin the filter update and the tracking process. The spatial reliability map\\nadjusts the filter support to the part of the object suitable for tracking.\\nThis both allows to enlarge the search region and improves tracking of\\nnon-rectangular objects. Reliability scores reflect channel-wise quality of the\\nlearned filters and are used as feature weighting coefficients in localization.\\nExperimentally, with only two simple standard features, HoGs and Colornames,\\nthe novel CSR-DCF method -- DCF with Channel and Spatial Reliability --\\nachieves state-of-the-art results on VOT 2016, VOT 2015 and OTB100. The CSR-DCF\\nruns in real-time on a CPU.</td>\n",
              "      <td>Discriminative Correlation Filter with Channel and Spatial Reliability</td>\n",
              "      <td>a new approach to work &lt;unk&gt; of audience  ai based using</td>\n",
              "      <td>correlation filter for for multi object tracking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>We consider the problem of using sentence compression techniques to\\nfacilitate query-focused multi-document summarization. We present a\\nsentence-compression-based framework for the task, and design a series of\\nlearning-based compression models built on parse trees. An innovative beam\\nsearch decoder is proposed to efficiently find highly probable compressions.\\nUnder this framework, we show how to integrate various indicative metrics such\\nas linguistic motivation and query relevance into the compression process by\\nderiving a novel formulation of a compression scoring function. Our best model\\nachieves statistically significant improvement over the state-of-the-art\\nsystems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2\\nrespectively) for the DUC 2006 and 2007 summarization task.</td>\n",
              "      <td>A Sentence Compression Based Framework to Query-Focused Multi-Document\\n  Summarization</td>\n",
              "      <td>a bayesian model for leverages developments  digital in a squared loss</td>\n",
              "      <td>sentence compression for automatic text summarization</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>Neural sequence-to-sequence models have provided a viable new approach for\\nabstractive text summarization (meaning they are not restricted to simply\\nselecting and rearranging passages from the original text). However, these\\nmodels have two shortcomings: they are liable to reproduce factual details\\ninaccurately, and they tend to repeat themselves. In this work we propose a\\nnovel architecture that augments the standard sequence-to-sequence attentional\\nmodel in two orthogonal ways. First, we use a hybrid pointer-generator network\\nthat can copy words from the source text via pointing, which aids accurate\\nreproduction of information, while retaining the ability to produce novel words\\nthrough the generator. Second, we use coverage to keep track of what has been\\nsummarized, which discourages repetition. We apply our model to the CNN / Daily\\nMail summarization task, outperforming the current abstractive state-of-the-art\\nby at least 2 ROUGE points.</td>\n",
              "      <td>Get To The Point: Summarization with Pointer-Generator Networks</td>\n",
              "      <td>a new framework for work single linkage learning of deep neural</td>\n",
              "      <td>abstractive summarization with pointer generator networks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>Ezhil is a Tamil language based interpreted procedural programming language.\\nTamil keywords and grammar are chosen to make the native Tamil speaker write\\nprograms in the Ezhil system. Ezhil allows easy representation of computer\\nprogram closer to the Tamil language logical constructs equivalent to the\\nconditional, branch and loop statements in modern English based programming\\nlanguages. Ezhil is a compact programming language aimed towards Tamil speaking\\nnovice computer users. Grammar for Ezhil and a few example programs are\\nreported here, from the initial proof-of-concept implementation using the\\nPython programming language1. To the best of our knowledge, Ezhil language is\\nthe first freely available Tamil programming language.</td>\n",
              "      <td>Ezhil: A Tamil Programming Language</td>\n",
              "      <td>&lt;unk&gt; a deep neural network for standardized learning &lt;eos&gt;</td>\n",
              "      <td>a tamil programming language</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>Modeling emotional-cognition is in a nascent stage and therefore wide-open\\nfor new ideas and discussions. In this paper the author looks at the modeling\\nproblem by bringing in ideas from axiomatic mathematics, information theory,\\ncomputer science, molecular biology, non-linear dynamical systems and quantum\\ncomputing and explains how ideas from these disciplines may have applications\\nin modeling emotional-cognition.</td>\n",
              "      <td>A novice looks at emotional cognition</td>\n",
              "      <td>a new approach to work autoencoding problem problem &lt;eos&gt;</td>\n",
              "      <td>novice looks at emotional cognition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>We consider additive models built with trend filtering, i.e., additive models\\nwhose components are each regularized by the (discrete) total variation of\\ntheir $(k+1)$st (discrete) derivative, for a chosen integer $k \\geq 0$. This\\nresults in $k$th degree piecewise polynomial components, (e.g., $k=0$ gives\\npiecewise constant components, $k=1$ gives piecewise linear, $k=2$ gives\\npiecewise quadratic, etc.). In univariate nonparametric regression, the\\nlocalized nature of the total variation regularizer used by trend filtering has\\nbeen shown to produce estimates with superior local adaptivity to those from\\nsmoothing splines (and linear smoothers, more generally) (Tibshirani [2014]).\\nFurther, the structured nature of this regularizer has been shown to lead to\\nhighly efficient computational routines for trend filtering (Kim et al. [2009],\\nRamdas and Tibshirani [2016]). In this paper, we argue that both of these\\nproperties carry over to the additive models setting. We derive fas...</td>\n",
              "      <td>Additive Models with Trend Filtering</td>\n",
              "      <td>a bayesian optimization approach to find work discussion of biology</td>\n",
              "      <td>additive models with trend filtering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>Dictionary learning for sparse representations is traditionally approached\\nwith sequential atom updates, in which an optimized atom is used immediately\\nfor the optimization of the next atoms. We propose instead a Jacobi version, in\\nwhich groups of atoms are updated independently, in parallel. Extensive\\nnumerical evidence for sparse image representation shows that the parallel\\nalgorithms, especially when all atoms are updated simultaneously, give better\\ndictionaries than their sequential counterparts.</td>\n",
              "      <td>Overcomplete Dictionary Learning with Jacobi Atom Updates</td>\n",
              "      <td>a new approach to work l # norm &lt;unk&gt; problem &lt;eos&gt;</td>\n",
              "      <td>dictionary learning with jacobi atom updates</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>Evolutionary algorithms (EAs) are population-based general-purpose\\noptimization algorithms, and have been successfully applied in various\\nreal-world optimization tasks. However, previous theoretical studies often\\nemploy EAs with only a parent or offspring population and focus on specific\\nproblems. Furthermore, they often only show upper bounds on the running time,\\nwhile lower bounds are also necessary to get a complete understanding of an\\nalgorithm. In this paper, we analyze the running time of the\\n($\\mu$+$\\lambda$)-EA (a general population-based EA with mutation only) on the\\nclass of pseudo-Boolean functions with a unique global optimum. By applying the\\nrecently proposed switch analysis approach, we prove the lower bound $\\Omega(n\\n\\ln n+ \\mu + \\lambda n\\ln\\ln n/ \\ln n)$ for the first time. Particularly on the\\ntwo widely-studied problems, OneMax and LeadingOnes, the derived lower bound\\ndiscloses that the ($\\mu$+$\\lambda$)-EA will be strictly slower than the\\n(1+1)-EA wh...</td>\n",
              "      <td>A Lower Bound Analysis of Population-based Evolutionary Algorithms for\\n  Pseudo-Boolean Functions</td>\n",
              "      <td>a bayesian optimization approach to raising and separate generator &lt;eos&gt;</td>\n",
              "      <td>lower bound for population based evolutionary algorithms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>This paper proposes to use probabilistic model checking to synthesize optimal\\nrobot policies in multi-tasking autonomous systems that are subject to\\nhuman-robot interaction. Given the convincing empirical evidence that human\\nbehavior can be related to reinforcement models, we take as input a\\nwell-studied Q-table model of the human behavior for flexible scenarios. We\\nfirst describe an automated procedure to distill a Markov decision process\\n(MDP) for the human in an arbitrary but fixed scenario. The distinctive issue\\nis that -- in contrast to existing models -- under-specification of the human\\nbehavior is included. Probabilistic model checking is used to predict the\\nhuman's behavior. Finally, the MDP model is extended with a robot model.\\nOptimal robot policies are synthesized by analyzing the resulting two-player\\nstochastic game. Experimental results with a prototypical implementation using\\nPRISM show promising results.</td>\n",
              "      <td>Probabilistic Model Checking for Complex Cognitive Tasks -- A case study\\n  in human-robot interaction</td>\n",
              "      <td>a new approach to work &lt;unk&gt; of audience  skip connections and</td>\n",
              "      <td>model checking for cognitive behavior in a probabilistic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>We analyze differences between two information-theoretically motivated\\napproaches to statistical inference and model selection: the Minimum\\nDescription Length (MDL) principle, and the Minimum Message Length (MML)\\nprinciple. Based on this analysis, we present two revised versions of MML: a\\npointwise estimator which gives the MML-optimal single parameter model, and a\\nvolumewise estimator which gives the MML-optimal region in the parameter space.\\nOur empirical results suggest that with small data sets, the MDL approach\\nyields more accurate predictions than the MML estimators. The empirical results\\nalso demonstrate that the revised MML estimators introduced here perform better\\nthan the original MML estimator suggested by Wallace and Freeman.</td>\n",
              "      <td>Minimum Encoding Approaches for Predictive Modeling</td>\n",
              "      <td>a filters of work &lt;unk&gt; algorithm for work &lt;unk&gt; of</td>\n",
              "      <td>encoding approaches for predictive analysis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>Flower pollination algorithm is a new nature-inspired algorithm, based on the\\ncharacteristics of flowering plants. In this paper, we extend this flower\\nalgorithm to solve multi-objective optimization problems in engineering. By\\nusing the weighted sum method with random weights, we show that the proposed\\nmulti-objective flower algorithm can accurately find the Pareto fronts for a\\nset of test functions. We then solve a bi-objective disc brake design problem,\\nwhich indeed converges quickly.</td>\n",
              "      <td>Multi-objective Flower Algorithm for Optimization</td>\n",
              "      <td>a bayesian model for leverages developments  come &lt;eos&gt;</td>\n",
              "      <td>flower algorithm for optimization</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>947</th>\n",
              "      <td>In this work, we present a new dataset for computational humor, specifically\\ncomparative humor ranking, which attempts to eschew the ubiquitous binary\\napproach to humor detection. The dataset consists of tweets that are humorous\\nresponses to a given hashtag. We describe the motivation for this new dataset,\\nas well as the collection process, which includes a description of our\\nsemi-automated system for data collection. We also present initial experiments\\nfor this dataset using both unsupervised and supervised approaches. Our best\\nsupervised system achieved 63.7% accuracy, suggesting that this task is much\\nmore difficult than comparable humor detection tasks. Initial experiments\\nindicate that a character-level model is more suitable for this task than a\\ntoken-level model, likely due to a large amount of puns that can be captured by\\na character-level model.</td>\n",
              "      <td>#HashtagWars: Learning a Sense of Humor</td>\n",
              "      <td>a new approach to work autoencoding of in structure  soccer</td>\n",
              "      <td>a deep learning of humor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992</th>\n",
              "      <td>We introduce GAMSEL (Generalized Additive Model Selection), a penalized\\nlikelihood approach for fitting sparse generalized additive models in high\\ndimension. Our method interpolates between null, linear and additive models by\\nallowing the effect of each variable to be estimated as being either zero,\\nlinear, or a low-complexity curve, as determined by the data. We present a\\nblockwise coordinate descent procedure for efficiently optimizing the penalized\\nlikelihood objective over a dense grid of the tuning parameter, producing a\\nregularization path of additive models. We demonstrate the performance of our\\nmethod on both real and simulated data examples, and compare it with existing\\ntechniques for additive model selection.</td>\n",
              "      <td>Generalized Additive Model Selection</td>\n",
              "      <td>a &lt;unk&gt; approach to work &lt;unk&gt; certain of cifar soccer</td>\n",
              "      <td>additive model selection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>941</th>\n",
              "      <td>Skin cancer, the most common human malignancy, is primarily diagnosed\\nvisually by physicians [1]. Classification with an automated method like CNN\\n[2, 3] shows potential for challenging tasks [1]. By now, the deep\\nconvolutional neural networks are on par with human dermatologist [1]. This\\nabstract is dedicated on developing a Deep Learning method for ISIC [5] 2017\\nSkin Lesion Detection Competition hosted at [6] to classify the dermatology\\npictures, which is aimed at improving the diagnostic accuracy rate and general\\nlevel of the human health. The challenge falls into three sub-challenges,\\nincluding Lesion Segmentation, Lesion Dermoscopic Feature Extraction and Lesion\\nClassification. This project only participates in the Lesion Classification\\npart. This algorithm is comprised of three steps: (1) original images\\npreprocessing, (2) modelling the processed images using CNN [2, 3] in Caffe [4]\\nframework, (3) predicting the test images and calculating the scores that\\nreprese...</td>\n",
              "      <td>Using Deep Learning Method for Classification: A Proposed Algorithm for\\n  the ISIC 2017 Skin Lesion Classification Challenge</td>\n",
              "      <td>a new approach to sw image cohorts &lt;eos&gt;</td>\n",
              "      <td>lesion analysis using deep learning challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>Recent studies have highlighted the vulnerability of deep neural networks\\n(DNNs) to adversarial examples - a visually indistinguishable adversarial image\\ncan easily be crafted to cause a well-trained model to misclassify. Existing\\nmethods for crafting adversarial examples are based on $L_2$ and $L_\\infty$\\ndistortion metrics. However, despite the fact that $L_1$ distortion accounts\\nfor the total variation and encourages sparsity in the perturbation, little has\\nbeen developed for crafting $L_1$-based adversarial examples. In this paper, we\\nformulate the process of attacking DNNs via adversarial examples as an\\nelastic-net regularized optimization problem. Our elastic-net attacks to DNNs\\n(EAD) feature $L_1$-oriented adversarial examples and include the\\nstate-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,\\nCIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial\\nexamples with small $L_1$ distortion and attains similar attack perfor...</td>\n",
              "      <td>EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial\\n  Examples</td>\n",
              "      <td>a new approach to work &lt;unk&gt; of speed up  dnns in</td>\n",
              "      <td>elastic net attacks to adversarial deep neural networks</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Abstract  ...                                     Generated Title(Final)\n",
              "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   This paper reviews Kunchenko's polynomials using as template matching method\\nto recognize template in one-dimensional input signal. Kunchenko's polynomials\\nmethod is compared with classical methods - cross-correlation and sum of\\nsquared differences according to numerical statistical example.  ...                          polynomials for template matching\n",
              "42   Although many successful ensemble clustering approaches have been developed\\nin recent years, there are still two limitations to most of the existing\\napproaches. First, they mostly overlook the issue of uncertain links, which may\\nmislead the overall consensus process. Second, they generally lack the ability\\nto incorporate global information to refine the local links. To address these\\ntwo limitations, in this paper, we propose a novel ensemble clustering approach\\nbased on sparse graph representation and probability trajectory analysis. In\\nparticular, we present the elite neighbor selection strategy to identify the\\nuncertain links by locally adaptive thresholds and build a sparse graph with a\\nsmall number of probably reliable links. We argue that a small number of\\nprobably reliable links can lead to significantly better consensus results than\\nusing all graph links regardless of their reliability. The random walk process\\ndriven by a new transition probability matrix is util...  ...                   ensemble clustering with sparse ensemble\n",
              "58                                                                                                                                                                                                                                                                     We study black-box attacks on machine learning classifiers where each query\\nto the model incurs some cost or risk of detection to the adversary. We focus\\nexplicitly on minimizing the number of queries as a major objective.\\nSpecifically, we consider the problem of attacking machine learning classifiers\\nsubject to a budget of feature modification cost while minimizing the number of\\nqueries, where each query returns only a class and confidence score. We\\ndescribe an approach that uses Bayesian optimization to minimize the number of\\nqueries, and find that the number of queries can be reduced to approximately\\none tenth of the number needed through a random strategy for scenarios where\\nthe feature modification cost budget is low.  ...                                          black box attacks\n",
              "61                                                                                     We consider probabilistic topic models and more recent word embedding\\ntechniques from a perspective of learning hidden semantic representations.\\nInspired by a striking similarity of the two approaches, we merge them and\\nlearn probabilistic embeddings with online EM-algorithm on word co-occurrence\\ndata. The resulting embeddings perform on par with Skip-Gram Negative Sampling\\n(SGNS) on word similarity tasks and benefit in the interpretability of the\\ncomponents. Next, we learn probabilistic document embeddings that outperform\\nparagraph2vec on a document similarity task and require less memory and time\\nfor training. Finally, we employ multimodal Additive Regularization of Topic\\nModels (ARTM) to obtain a high sparsity and learn embeddings for other\\nmodalities, such as timestamps and categories. We observe further improvement\\nof word similarity performance and meaningful inter-modality similarities.  ...        probabilistic embeddings  bridging the topic models\n",
              "121                                                                                                                                                                                                                                                      The production of color language is essential for grounded language\\ngeneration. Color descriptions have many challenging properties: they can be\\nvague, compositionally complex, and denotationally rich. We present an\\neffective approach to generating color descriptions using recurrent neural\\nnetworks and a Fourier-transformed color representation. Our model outperforms\\nprevious work on a conditional language modeling task over a large corpus of\\nnaturalistic color descriptions. In addition, probing the model's output\\nreveals that it can accurately produce not only basic color terms but also\\ndescriptors with non-convex denotations (\"greenish\"), bare modifiers (\"bright\",\\n\"dull\"), and compositional phrases (\"faded teal\") not seen in training.  ...               to generate compositional color descriptions\n",
              "143                                                       Short-term tracking is an open and challenging problem for which\\ndiscriminative correlation filters (DCF) have shown excellent performance. We\\nintroduce the channel and spatial reliability concepts to DCF tracking and\\nprovide a novel learning algorithm for its efficient and seamless integration\\nin the filter update and the tracking process. The spatial reliability map\\nadjusts the filter support to the part of the object suitable for tracking.\\nThis both allows to enlarge the search region and improves tracking of\\nnon-rectangular objects. Reliability scores reflect channel-wise quality of the\\nlearned filters and are used as feature weighting coefficients in localization.\\nExperimentally, with only two simple standard features, HoGs and Colornames,\\nthe novel CSR-DCF method -- DCF with Channel and Spatial Reliability --\\nachieves state-of-the-art results on VOT 2016, VOT 2015 and OTB100. The CSR-DCF\\nruns in real-time on a CPU.  ...           correlation filter for for multi object tracking\n",
              "160                                                                                                                                                                                               We consider the problem of using sentence compression techniques to\\nfacilitate query-focused multi-document summarization. We present a\\nsentence-compression-based framework for the task, and design a series of\\nlearning-based compression models built on parse trees. An innovative beam\\nsearch decoder is proposed to efficiently find highly probable compressions.\\nUnder this framework, we show how to integrate various indicative metrics such\\nas linguistic motivation and query relevance into the compression process by\\nderiving a novel formulation of a compression scoring function. Our best model\\nachieves statistically significant improvement over the state-of-the-art\\nsystems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2\\nrespectively) for the DUC 2006 and 2007 summarization task.  ...      sentence compression for automatic text summarization\n",
              "162                                    Neural sequence-to-sequence models have provided a viable new approach for\\nabstractive text summarization (meaning they are not restricted to simply\\nselecting and rearranging passages from the original text). However, these\\nmodels have two shortcomings: they are liable to reproduce factual details\\ninaccurately, and they tend to repeat themselves. In this work we propose a\\nnovel architecture that augments the standard sequence-to-sequence attentional\\nmodel in two orthogonal ways. First, we use a hybrid pointer-generator network\\nthat can copy words from the source text via pointing, which aids accurate\\nreproduction of information, while retaining the ability to produce novel words\\nthrough the generator. Second, we use coverage to keep track of what has been\\nsummarized, which discourages repetition. We apply our model to the CNN / Daily\\nMail summarization task, outperforming the current abstractive state-of-the-art\\nby at least 2 ROUGE points.  ...  abstractive summarization with pointer generator networks\n",
              "188                                                                                                                                                                                                                                                               Ezhil is a Tamil language based interpreted procedural programming language.\\nTamil keywords and grammar are chosen to make the native Tamil speaker write\\nprograms in the Ezhil system. Ezhil allows easy representation of computer\\nprogram closer to the Tamil language logical constructs equivalent to the\\nconditional, branch and loop statements in modern English based programming\\nlanguages. Ezhil is a compact programming language aimed towards Tamil speaking\\nnovice computer users. Grammar for Ezhil and a few example programs are\\nreported here, from the initial proof-of-concept implementation using the\\nPython programming language1. To the best of our knowledge, Ezhil language is\\nthe first freely available Tamil programming language.  ...                               a tamil programming language\n",
              "192                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Modeling emotional-cognition is in a nascent stage and therefore wide-open\\nfor new ideas and discussions. In this paper the author looks at the modeling\\nproblem by bringing in ideas from axiomatic mathematics, information theory,\\ncomputer science, molecular biology, non-linear dynamical systems and quantum\\ncomputing and explains how ideas from these disciplines may have applications\\nin modeling emotional-cognition.  ...                        novice looks at emotional cognition\n",
              "231  We consider additive models built with trend filtering, i.e., additive models\\nwhose components are each regularized by the (discrete) total variation of\\ntheir $(k+1)$st (discrete) derivative, for a chosen integer $k \\geq 0$. This\\nresults in $k$th degree piecewise polynomial components, (e.g., $k=0$ gives\\npiecewise constant components, $k=1$ gives piecewise linear, $k=2$ gives\\npiecewise quadratic, etc.). In univariate nonparametric regression, the\\nlocalized nature of the total variation regularizer used by trend filtering has\\nbeen shown to produce estimates with superior local adaptivity to those from\\nsmoothing splines (and linear smoothers, more generally) (Tibshirani [2014]).\\nFurther, the structured nature of this regularizer has been shown to lead to\\nhighly efficient computational routines for trend filtering (Kim et al. [2009],\\nRamdas and Tibshirani [2016]). In this paper, we argue that both of these\\nproperties carry over to the additive models setting. We derive fas...  ...                       additive models with trend filtering\n",
              "232                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Dictionary learning for sparse representations is traditionally approached\\nwith sequential atom updates, in which an optimized atom is used immediately\\nfor the optimization of the next atoms. We propose instead a Jacobi version, in\\nwhich groups of atoms are updated independently, in parallel. Extensive\\nnumerical evidence for sparse image representation shows that the parallel\\nalgorithms, especially when all atoms are updated simultaneously, give better\\ndictionaries than their sequential counterparts.  ...               dictionary learning with jacobi atom updates\n",
              "240  Evolutionary algorithms (EAs) are population-based general-purpose\\noptimization algorithms, and have been successfully applied in various\\nreal-world optimization tasks. However, previous theoretical studies often\\nemploy EAs with only a parent or offspring population and focus on specific\\nproblems. Furthermore, they often only show upper bounds on the running time,\\nwhile lower bounds are also necessary to get a complete understanding of an\\nalgorithm. In this paper, we analyze the running time of the\\n($\\mu$+$\\lambda$)-EA (a general population-based EA with mutation only) on the\\nclass of pseudo-Boolean functions with a unique global optimum. By applying the\\nrecently proposed switch analysis approach, we prove the lower bound $\\Omega(n\\n\\ln n+ \\mu + \\lambda n\\ln\\ln n/ \\ln n)$ for the first time. Particularly on the\\ntwo widely-studied problems, OneMax and LeadingOnes, the derived lower bound\\ndiscloses that the ($\\mu$+$\\lambda$)-EA will be strictly slower than the\\n(1+1)-EA wh...  ...   lower bound for population based evolutionary algorithms\n",
              "291                                                         This paper proposes to use probabilistic model checking to synthesize optimal\\nrobot policies in multi-tasking autonomous systems that are subject to\\nhuman-robot interaction. Given the convincing empirical evidence that human\\nbehavior can be related to reinforcement models, we take as input a\\nwell-studied Q-table model of the human behavior for flexible scenarios. We\\nfirst describe an automated procedure to distill a Markov decision process\\n(MDP) for the human in an arbitrary but fixed scenario. The distinctive issue\\nis that -- in contrast to existing models -- under-specification of the human\\nbehavior is included. Probabilistic model checking is used to predict the\\nhuman's behavior. Finally, the MDP model is extended with a robot model.\\nOptimal robot policies are synthesized by analyzing the resulting two-player\\nstochastic game. Experimental results with a prototypical implementation using\\nPRISM show promising results.  ...   model checking for cognitive behavior in a probabilistic\n",
              "403                                                                                                                                                                                                                                                     We analyze differences between two information-theoretically motivated\\napproaches to statistical inference and model selection: the Minimum\\nDescription Length (MDL) principle, and the Minimum Message Length (MML)\\nprinciple. Based on this analysis, we present two revised versions of MML: a\\npointwise estimator which gives the MML-optimal single parameter model, and a\\nvolumewise estimator which gives the MML-optimal region in the parameter space.\\nOur empirical results suggest that with small data sets, the MDL approach\\nyields more accurate predictions than the MML estimators. The empirical results\\nalso demonstrate that the revised MML estimators introduced here perform better\\nthan the original MML estimator suggested by Wallace and Freeman.  ...                encoding approaches for predictive analysis\n",
              "452                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Flower pollination algorithm is a new nature-inspired algorithm, based on the\\ncharacteristics of flowering plants. In this paper, we extend this flower\\nalgorithm to solve multi-objective optimization problems in engineering. By\\nusing the weighted sum method with random weights, we show that the proposed\\nmulti-objective flower algorithm can accurately find the Pareto fronts for a\\nset of test functions. We then solve a bi-objective disc brake design problem,\\nwhich indeed converges quickly.  ...                          flower algorithm for optimization\n",
              "947                                                                                                                            In this work, we present a new dataset for computational humor, specifically\\ncomparative humor ranking, which attempts to eschew the ubiquitous binary\\napproach to humor detection. The dataset consists of tweets that are humorous\\nresponses to a given hashtag. We describe the motivation for this new dataset,\\nas well as the collection process, which includes a description of our\\nsemi-automated system for data collection. We also present initial experiments\\nfor this dataset using both unsupervised and supervised approaches. Our best\\nsupervised system achieved 63.7% accuracy, suggesting that this task is much\\nmore difficult than comparable humor detection tasks. Initial experiments\\nindicate that a character-level model is more suitable for this task than a\\ntoken-level model, likely due to a large amount of puns that can be captured by\\na character-level model.  ...                                   a deep learning of humor\n",
              "992                                                                                                                                                                                                                                                                        We introduce GAMSEL (Generalized Additive Model Selection), a penalized\\nlikelihood approach for fitting sparse generalized additive models in high\\ndimension. Our method interpolates between null, linear and additive models by\\nallowing the effect of each variable to be estimated as being either zero,\\nlinear, or a low-complexity curve, as determined by the data. We present a\\nblockwise coordinate descent procedure for efficiently optimizing the penalized\\nlikelihood objective over a dense grid of the tuning parameter, producing a\\nregularization path of additive models. We demonstrate the performance of our\\nmethod on both real and simulated data examples, and compare it with existing\\ntechniques for additive model selection.  ...                                   additive model selection\n",
              "941  Skin cancer, the most common human malignancy, is primarily diagnosed\\nvisually by physicians [1]. Classification with an automated method like CNN\\n[2, 3] shows potential for challenging tasks [1]. By now, the deep\\nconvolutional neural networks are on par with human dermatologist [1]. This\\nabstract is dedicated on developing a Deep Learning method for ISIC [5] 2017\\nSkin Lesion Detection Competition hosted at [6] to classify the dermatology\\npictures, which is aimed at improving the diagnostic accuracy rate and general\\nlevel of the human health. The challenge falls into three sub-challenges,\\nincluding Lesion Segmentation, Lesion Dermoscopic Feature Extraction and Lesion\\nClassification. This project only participates in the Lesion Classification\\npart. This algorithm is comprised of three steps: (1) original images\\npreprocessing, (2) modelling the processed images using CNN [2, 3] in Caffe [4]\\nframework, (3) predicting the test images and calculating the scores that\\nreprese...  ...              lesion analysis using deep learning challenge\n",
              "883  Recent studies have highlighted the vulnerability of deep neural networks\\n(DNNs) to adversarial examples - a visually indistinguishable adversarial image\\ncan easily be crafted to cause a well-trained model to misclassify. Existing\\nmethods for crafting adversarial examples are based on $L_2$ and $L_\\infty$\\ndistortion metrics. However, despite the fact that $L_1$ distortion accounts\\nfor the total variation and encourages sparsity in the perturbation, little has\\nbeen developed for crafting $L_1$-based adversarial examples. In this paper, we\\nformulate the process of attacking DNNs via adversarial examples as an\\nelastic-net regularized optimization problem. Our elastic-net attacks to DNNs\\n(EAD) feature $L_1$-oriented adversarial examples and include the\\nstate-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,\\nCIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial\\nexamples with small $L_1$ distortion and attains similar attack perfor...  ...    elastic net attacks to adversarial deep neural networks\n",
              "\n",
              "[20 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}