{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_NLP_Title_Gen_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cFlmpmODdvSr",
        "9Oqz5VAMdy9M",
        "jGEDhNyXT15G",
        "SM5k8SGcHaHn",
        "osefEj6yjp7-"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJx8HaEbhoxG"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from torch.optim import lr_scheduler\n",
        "from PIL import Image\n",
        "import timeit\n",
        "from sklearn.pipeline import Pipeline\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy import data\n",
        "import random\n",
        "## For reproducibility\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul0enV_f6fVk",
        "outputId": "1747b714-783b-477f-8ae5-542c9ca567e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iryvSLmxh1vw"
      },
      "source": [
        "# Create Preprocessing pipeline for summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBy0Wh1zidtB"
      },
      "source": [
        "tokenize =  lambda s: s.split()\n",
        "import re  \n",
        "def cleanup_text(texts):\n",
        "    cleaned_text = []\n",
        "    for text in texts:\n",
        "        # remove punctuation\n",
        "        text = re.sub('[^a-zA-Z0-9]', ' ', text)\n",
        "        # remove multiple spaces\n",
        "        text = re.sub(r' +', ' ', text)\n",
        "        # remove newline\n",
        "        text = re.sub(r'\\n', ' ', text)\n",
        "        #replace digits with '# symbol\n",
        "        text = re.sub('[0-9]', '#', text)\n",
        "        cleaned_text.append(text)\n",
        "    return cleaned_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJbowwh3LpyA",
        "outputId": "aeafc611-3220-4a6d-9155-55b168607edb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2MHOhPtklHS"
      },
      "source": [
        "## Create torchtext fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CVDphMZkWaP"
      },
      "source": [
        "#Field for summaries (removed stop words)\n",
        "SUM = data.Field(tokenize = tokenize,init_token='<sos>',eos_token='<eos>',pad_first=True,stop_words=stop,lower = True,preprocessing=cleanup_text)\n",
        "#Field for title\n",
        "TITLE = data.Field(tokenize = tokenize,init_token='<sos>',eos_token='<eos>',lower = True,preprocessing=cleanup_text)\n",
        "#Field for Id\n",
        "#ID = data.Field(use_vocab=False,sequential=False,dtype=torch.LongTensor,postprocessing=data.Pipeline(lambda x: int(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r-2ePKbnx62"
      },
      "source": [
        "fields = [('Id',None),('Abstract',None),('Title',TITLE),('sum1',SUM),('sum2',SUM),('sum3',SUM),('sum4',SUM),('sum5',SUM),('sum6',SUM),('sum7',SUM)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J3EKi0ooi6f"
      },
      "source": [
        "## Read data into tabular dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgf0hafhomaE"
      },
      "source": [
        "dataset = data.TabularDataset(path='./drive/MyDrive/data_summaries.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tleM8xWxpMmf",
        "outputId": "f7d8aefa-b556-4c6c-eebd-1c9b519396e0"
      },
      "source": [
        "print(vars(dataset[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Title': ['dual', 'recurrent', 'attention', 'units', 'for', 'visual', 'question', 'answering'], 'sum1': ['propose', 'architecture', 'vqa', 'utilizes', 'recurrent', 'layers', 'generate', 'visual', 'textual', 'attention the', 'memory', 'characteristic', 'proposed', 'recurrent', 'attention', 'units', 'offers', 'rich', 'joint', 'embedding', 'visual', 'textual', 'features', 'enables', 'model', 'reason', 'relations', 'several', 'parts', 'image', 'question '], 'sum2': ['propose', 'architecture', 'vqa', 'utilizes', 'recurrent', 'layers', 'generate', 'visual', 'textual', 'attention in', 'cases ', 'recurrent', 'attention', 'mechanism', 'improves', 'performance', 'tasks', 'requiring', 'sequential', 'relational', 'reasoning', 'vqa', 'dataset '], 'sum3': ['memory', 'characteristic', 'proposed', 'recurrent', 'attention', 'units', 'offers', 'rich', 'joint', 'embedding', 'visual', 'textual', 'features', 'enables', 'model', 'reason', 'relations', 'several', 'parts', 'image', 'question our', 'single', 'model', 'outperforms', 'first', 'place', 'winner', 'vqa', '# #', 'dataset ', 'performs', 'within', 'margin', 'current', 'state of the art', 'ensemble', 'model '], 'sum4': ['memory', 'characteristic', 'proposed', 'recurrent', 'attention', 'units', 'offers', 'rich', 'joint', 'embedding', 'visual', 'textual', 'features', 'enables', 'model', 'reason', 'relations', 'several', 'parts', 'image', 'question our', 'single', 'model', 'outperforms', 'first', 'place', 'winner', 'vqa', '# #', 'dataset ', 'performs', 'within', 'margin', 'current', 'state of the art', 'ensemble', 'model '], 'sum5': ['memory', 'characteristic', 'proposed', 'recurrent', 'attention', 'units', 'offers', 'rich', 'joint', 'embedding', 'visual', 'textual', 'features', 'enables', 'model', 'reason', 'relations', 'several', 'parts', 'image', 'question in', 'cases ', 'recurrent', 'attention', 'mechanism', 'improves', 'performance', 'tasks', 'requiring', 'sequential', 'relational', 'reasoning', 'vqa', 'dataset '], 'sum6': ['single', 'model', 'outperforms', 'first', 'place', 'winner', 'vqa', '# #', 'dataset ', 'performs', 'within', 'margin', 'current', 'state of the art', 'ensemble', 'model in', 'cases ', 'recurrent', 'attention', 'mechanism', 'improves', 'performance', 'tasks', 'requiring', 'sequential', 'relational', 'reasoning', 'vqa', 'dataset '], 'sum7': ['propose', 'architecture', 'vqa', 'utilizes', 'recurrent', 'layers', 'generate', 'visual', 'textual', 'attention in', 'cases ', 'recurrent', 'attention', 'mechanism', 'improves', 'performance', 'tasks', 'requiring', 'sequential', 'relational', 'reasoning', 'vqa', 'dataset ']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKlMN3jMqmJm"
      },
      "source": [
        "## Create training data and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vlRSnYiqjIk"
      },
      "source": [
        "import random\n",
        "train_data, valid_data = dataset.split(split_ratio=0.9, random_state=random.seed(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwPMSsKN6r05",
        "outputId": "86586833-c71e-4117-d721-beefade2f2fe"
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(valid_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36900\n",
            "4100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhhG460cyQVW",
        "outputId": "d59ae9a2-abfc-4a7c-a054-8a023f06576c"
      },
      "source": [
        "print(vars(train_data[5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Title': ['adaptively', 'learning', 'the', 'crowd', 'kernel'], 'sum1': ['introduce', 'algorithm', 'that ', 'given', 'n', 'objects ', 'learns', 'similarity', 'matrix', 'n #', 'pairs ', 'crowdsourced', 'data', 'alone the', 'algorithm', 'samples', 'responses', 'adaptively', 'chosen', 'triplet based', 'relative similarity', 'queries '], 'sum2': ['introduce', 'algorithm', 'that ', 'given', 'n', 'objects ', 'learns', 'similarity', 'matrix', 'n #', 'pairs ', 'crowdsourced', 'data', 'alone svms', 'reveal', 'crowd', 'kernel', 'captures', 'prominent', 'subtle', 'features', 'across', 'number', 'domains ', ' is', 'striped ', 'among', 'neckties', ' vowel', 'vs ', 'consonant ', 'among', 'letters '], 'sum3': ['introduce', 'algorithm', 'that ', 'given', 'n', 'objects ', 'learns', 'similarity', 'matrix', 'n #', 'pairs ', 'crowdsourced', 'data', 'alone svms', 'reveal', 'crowd', 'kernel', 'captures', 'prominent', 'subtle', 'features', 'across', 'number', 'domains ', ' is', 'striped ', 'among', 'neckties', ' vowel', 'vs ', 'consonant ', 'among', 'letters '], 'sum4': ['output', 'embedding', 'objects', 'euclidean', 'space', ' like', 'mds ', 'refer', ' crowd', 'kernel svms', 'reveal', 'crowd', 'kernel', 'captures', 'prominent', 'subtle', 'features', 'across', 'number', 'domains ', ' is', 'striped ', 'among', 'neckties', ' vowel', 'vs ', 'consonant ', 'among', 'letters '], 'sum5': ['introduce', 'algorithm', 'that ', 'given', 'n', 'objects ', 'learns', 'similarity', 'matrix', 'n #', 'pairs ', 'crowdsourced', 'data', 'alone and', 'chosen', 'maximally', 'informative', 'given', 'preceding', 'responses '], 'sum6': ['introduce', 'algorithm', 'that ', 'given', 'n', 'objects ', 'learns', 'similarity', 'matrix', 'n #', 'pairs ', 'crowdsourced', 'data', 'alone the', 'algorithm', 'samples', 'responses', 'adaptively', 'chosen', 'triplet based', 'relative similarity', 'queries '], 'sum7': ['introduce', 'algorithm', 'that ', 'given', 'n', 'objects ', 'learns', 'similarity', 'matrix', 'n #', 'pairs ', 'crowdsourced', 'data', 'alone svms', 'reveal', 'crowd', 'kernel', 'captures', 'prominent', 'subtle', 'features', 'across', 'number', 'domains ', ' is', 'striped ', 'among', 'neckties', ' vowel', 'vs ', 'consonant ', 'among', 'letters ']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzsrWel-rwik"
      },
      "source": [
        "SUM.build_vocab(train_data.sum1,train_data.sum2,train_data.sum3,train_data.sum4,train_data.sum5,\\\n",
        "                train_data.sum6,train_data.sum7,train_data.Title,max_size=40000,vectors='glove.6B.100d')\n",
        "TITLE.vocab= SUM.vocab\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMTZoMzTwj9-",
        "outputId": "5dde9967-b99f-4f15-9543-9c2b515d9cc8"
      },
      "source": [
        "print(len(SUM.vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dodGaMxD-zFL"
      },
      "source": [
        "## Create Bucket iterator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5V0C7iZ_E5N"
      },
      "source": [
        "def cal_length(x):\n",
        "  return len(x.sum1)+len(x.sum2)+len(x.sum3)+len(x.sum4)+len(x.sum5)+len(x.sum6)+len(x.sum7)+len(x.Title)\n",
        "from torchtext.legacy import data\n",
        "BATCH_SIZE =64\n",
        "train_iterator, valid_iterator =data.BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "    batch_size = BATCH_SIZE, sort_key = lambda x: cal_length(x), sort_within_batch = True,shuffle=True,sort=False,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMUTzw0tiJQ-"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU4Ix8W6iNpn"
      },
      "source": [
        "## Encoder layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qOs589nrh8p"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, num_layers, dropout): \n",
        "        super().__init__()   \n",
        "        self.hid_dim = hid_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)  \n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers, dropout = dropout)    \n",
        "        self.dropout = nn.Dropout(dropout)       \n",
        "    def forward(self, input_idx):\n",
        "        #print(input_idx)\n",
        "        input_idx=input_idx.to(device)\n",
        "        embedded = self.dropout(self.embedding(input_idx))\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #embedded = seq_len,batch_size,embed_dim\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        #outputs are always from the top hidden layer\n",
        "        return outputs,hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEOlyNZxiQEF"
      },
      "source": [
        "## Control layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjdf2BQ6rjSM"
      },
      "source": [
        "class ControlLayer(nn.Module):\n",
        "    def __init__(self, input_dim,hid_dim): \n",
        "        super().__init__()   \n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        #self.embedding = nn.Embedding(input_dim, emb_dim)  \n",
        "        self.lstm = nn.LSTM(input_dim, hid_dim)    \n",
        "             \n",
        "    def forward(self, sum_hidden):\n",
        "        #print(input_idx)\n",
        "        #sum_hidden = seq_len(=7),batch_size,embed_dim(=encoder_hidden_dimension)\n",
        "        outputs, (hidden, cell) = self.lstm(sum_hidden)\n",
        "        #embedded = seq_len,batch_size,embed_dim\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        #outputs are always from the top hidden layer\n",
        "        return outputs,hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa4b7-0diabA"
      },
      "source": [
        "##Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfPmC8aGtaOu"
      },
      "source": [
        "class ComplexAttention(nn.Module):\n",
        "   def __init__(self,dec_hid_dim,cnt_hid_dim,enc_hid_dim):\n",
        "    super().__init__() \n",
        "    self.cnt_hid_dim=cnt_hid_dim\n",
        "    self.enc_hid_dim = enc_hid_dim\n",
        "    self.dec_hid_dim = dec_hid_dim\n",
        "   def forward(self,cnt_hid_states,enc_hid_states,dec_hid_states):\n",
        "     #cnt_hid_states = [7,batch_size,cnt_hid_dim]\n",
        "     #enc_hid_states = [7,seq_len*,batch_size,enc_hid_dim], list of hidden states for every summary\n",
        "     #dec_hid_states = [1,batch_size,dec_hid_dim]\n",
        "     '''Calculate summary level attention'''\n",
        "     cnt_hid_states=cnt_hid_states.permute(1,0,2)\n",
        "     dec_hid_states=dec_hid_states.permute(1,2,0)\n",
        "     #dec_hid_states=[batch_size,dec_hid_dim,1]\n",
        "     alpha = torch.bmm(cnt_hid_states,dec_hid_states)\n",
        "     #alpha=[batch_size,7,1]\n",
        "     #alpha=alpha.squeeze(2)\n",
        "     alpha=F.softmax(alpha,dim=1)\n",
        "     #alpha=[batch_size,7,1]\n",
        "     '''Calculate word level attention'''\n",
        "     batch_size = alpha.size()[0]\n",
        "     context_vec=torch.zeros(batch_size,1,self.enc_hid_dim).to(device)\n",
        "     context_vec_k=torch.zeros(7,batch_size,self.enc_hid_dim).to(device)\n",
        "     for k,sum_hid_states in enumerate(enc_hid_states):\n",
        "       #sum_hid_states = [seq_len_k,batch_size,enc_hid_dim]\n",
        "       sum_hid_states=sum_hid_states.permute(1,0,2)\n",
        "       beta = torch.bmm(sum_hid_states,dec_hid_states)\n",
        "       #beta = [batch_size,seq_len_1,1]\n",
        "       #beta=beta.squeeze(2)\n",
        "       beta=F.softmax(beta,dim=1)\n",
        "       beta=beta.permute(0,2,1)\n",
        "       #beta = [batch_size,1,seq_len]\n",
        "       #sum_hid_states = [batch_size,seq_len_size,enc_hid_dim]\n",
        "       context_vec_k[k] = torch.bmm(beta,sum_hid_states).squeeze(1)\n",
        "       #context_vec_k = [batch_size,1,enc_hid_dim].squeeze(1)\n",
        "     '''Combining both and returning context_vector'''\n",
        "     context_vec_k=context_vec_k.permute(1,0,2)\n",
        "     alpha=alpha.permute(0,2,1)\n",
        "     context_vec = torch.bmm(alpha,context_vec_k)\n",
        "     del context_vec_k\n",
        "     torch.cuda.empty_cache()\n",
        "     return alpha,beta,context_vec\n",
        "\n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0JC9VKMQ79t"
      },
      "source": [
        "class SimpleAttention(nn.Module):\n",
        "   def __init__(self,dec_hid_dim,cnt_hid_dim,enc_hid_dim,split):\n",
        "    super().__init__() \n",
        "    self.cnt_hid_dim=cnt_hid_dim\n",
        "    self.enc_hid_dim = enc_hid_dim\n",
        "    self.dec_hid_dim = dec_hid_dim\n",
        "    self.split = split\n",
        "   def forward(self,cnt_hid_states,enc_hid_states,dec_hid_states):\n",
        "     #cnt_hid_states = [7,batch_size,cnt_hid_dim]\n",
        "     #enc_hid_states = [7,seq_len*,batch_size,enc_hid_dim], list of hidden states for every summary\n",
        "     #dec_hid_states = [num_layers(=1),batch_size,dec_hid_dim]\n",
        "     '''Calculate summary level attention'''\n",
        "     cnt_hid_states_context=cnt_hid_states.permute(1,0,2)[:,:,:self.split]\n",
        "     dec_hid_states_context=dec_hid_states.permute(1,2,0)[:,:self.split,:]\n",
        "     cnt_hid_states_wgt=cnt_hid_states.permute(1,0,2)[:,:,self.split:]\n",
        "     dec_hid_states_wgt=dec_hid_states.permute(1,2,0)[:,self.split:,:]\n",
        "     #dec_hid_states_wgt=[batch_size,dec_hid_dim,1]\n",
        "     alpha = torch.bmm(cnt_hid_states_wgt,dec_hid_states_wgt)\n",
        "     #alpha=[batch_size,7,1]\n",
        "     #alpha=alpha.squeeze(2)\n",
        "     alpha=F.softmax(alpha,dim=1)\n",
        "     #alpha=[batch_size,7,1]\n",
        "     '''Calculate word level attention'''\n",
        "     batch_size = alpha.size()[0]\n",
        "     context_vec=torch.zeros(batch_size,1,self.split).to(device)\n",
        "     context_vec_k=torch.zeros(7,batch_size,self.split).to(device)\n",
        "     for k,sum_hid_states in enumerate(enc_hid_states):\n",
        "       #sum_hid_states = [seq_len_k,batch_size,enc_hid_dim]\n",
        "       sum_hid_states_wgt=sum_hid_states.permute(1,0,2)[:,:,self.split:]\n",
        "       sum_hid_states_context=sum_hid_states.permute(1,0,2)[:,:,:self.split]\n",
        "       beta = torch.bmm(sum_hid_states_wgt,dec_hid_states_wgt)\n",
        "       #beta = [batch_size,seq_len_1,1]\n",
        "       #beta=beta.squeeze(2)\n",
        "       beta=F.softmax(beta,dim=1)\n",
        "       beta=beta.permute(0,2,1)\n",
        "       #beta = [batch_size,1,seq_len]\n",
        "       #sum_hid_states = [batch_size,seq_len_size,enc_hid_dim]\n",
        "       context_vec_k[k] = torch.bmm(beta,sum_hid_states_context).squeeze(1)\n",
        "       #context_vec_k = [batch_size,1,enc_hid_dim].squeeze(1)\n",
        "     '''Combining both and returning context_vector'''\n",
        "     context_vec_k=context_vec_k.permute(1,0,2)\n",
        "     alpha=alpha.permute(0,2,1)\n",
        "     context_vec = torch.bmm(alpha,context_vec_k)\n",
        "     del context_vec_k\n",
        "     torch.cuda.empty_cache()\n",
        "     return alpha,beta,context_vec\n",
        "\n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3fLV-xHVtKr"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbbagtapVxkt"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim,con_hid_dim,attention,attention_type):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.con_hid_dim = con_hid_dim\n",
        "        self.attention = attention      \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        if attention_type=='complex':      \n",
        "          self.lstm = nn.LSTM(input_size=(enc_hid_dim + emb_dim),hidden_size= dec_hid_dim)\n",
        "          self.fc_out = nn.Linear( enc_hid_dim + dec_hid_dim + emb_dim, output_dim)\n",
        "        else:\n",
        "          self.lstm = nn.LSTM((attention.split+emb_dim), dec_hid_dim)\n",
        "          self.fc_out = nn.Linear( (2*attention.split)  + emb_dim, output_dim)    \n",
        "        #self.dropout = nn.Dropout(dropout)\n",
        "        self.attention_type=attention_type\n",
        "    def forward(self, input_idx,cnt_hid_states,enc_hid_states,dec_hid_states,cell_state):\n",
        "      #input = [batch_size]\n",
        "      input_idx = input_idx.unsqueeze(0)#Adding a dimenstion at the the first = 1 = seq_len as we are sending word by word\n",
        "      #input = [1,batch_size] \n",
        "      embedded = self.embedding(input_idx)\n",
        "      #embedded = [1,batch_size,embed_size]\n",
        "      '''Getting the context vector'''\n",
        "      _,_,context_vector=self.attention(cnt_hid_states,enc_hid_states,dec_hid_states)\n",
        "      #context_vector=[batch_size,1,hid_state]\n",
        "      context_vector=context_vector.permute(1,0,2)\n",
        "      #context_vector=[1,batch_size,hid_state]\n",
        "      lstm_in = torch.cat((embedded,context_vector),dim=2)\n",
        "      #lstm_in = [1,batch_size,context_vector_size+embed_size]\n",
        "      #print(lstm_in.size())\n",
        "      outputs, (hidden, cell) = self.lstm(lstm_in,(dec_hid_states,cell_state))\n",
        "      #output=[seq_len(=1),batch_size,hid_dim]\n",
        "      #hidden=[num_layers(=1),batch_size,hid_dim]\n",
        "      assert(outputs==hidden).all()\n",
        "\n",
        "      embedded=embedded.squeeze(0)\n",
        "      context_vector=context_vector.squeeze(0)\n",
        "      outputs = outputs.squeeze(0)\n",
        "      if self.attention_type=='complex':\n",
        "        prediction = self.fc_out(torch.cat((outputs,context_vector,embedded),dim=1))\n",
        "      else:\n",
        "        prediction = self.fc_out(torch.cat((outputs[:,:self.attention.split],context_vector,embedded),dim=1))\n",
        "      #prediction_size = (batch_size,out_dim)\n",
        "      return prediction,hidden,cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7w-CoTWgqMo"
      },
      "source": [
        "##Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OidIY-BVW4OB"
      },
      "source": [
        "class Seq2Seq(nn.Module): #Combining the encoder,control_layer & decoder\n",
        "  def __init__(self,encoder,control_layer,decoder,device):\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    self.control_layer = control_layer\n",
        "    self.decoder=decoder\n",
        "    self.device =  device\n",
        "  def forward(self,input_batches,output_batches,tfr=0.5):\n",
        "    #input_batches dimension - NOT A TENSOR. ENTIRE BATCH OBJECT IS SENT. \n",
        "    #output_batches_dimension - (seq_len,batch_size)\n",
        "    \n",
        "    batch_size = output_batches.shape[1]\n",
        "    title_len = output_batches.shape[0]\n",
        "    title_vocab_size = self.decoder.output_dim\n",
        "    predictions = torch.zeros(title_len, batch_size, title_vocab_size).to(device)\n",
        "    #print(input_batches.size())\n",
        "    '''Pass each summary through the encoder'''\n",
        "    sum1=input_batches.sum1\n",
        "    sum2=input_batches.sum2\n",
        "    sum3=input_batches.sum3\n",
        "    sum4=input_batches.sum4\n",
        "    sum5=input_batches.sum5\n",
        "    sum6=input_batches.sum6\n",
        "    sum7=input_batches.sum7\n",
        "    sum=[sum1,sum2,sum3,sum4,sum5,sum6,sum7]\n",
        "    control_input=torch.zeros((7,batch_size,self.control_layer.hid_dim)).to(device)\n",
        "    encoder_hidden_states = []\n",
        "    for s in range(7):\n",
        "      output,hidden=self.encoder(sum[s])\n",
        "      #output = [s.length,batch_size,hid_dim]\n",
        "      #hidden=[num_layers,batch_size,hid_dim]\n",
        "      #print(\"enc_output device\",output.device)\n",
        "      encoder_hidden_states.append(output)\n",
        "      control_input[s]=hidden[-1]\n",
        "    \n",
        "    '''Pass the last hidden state to control layer for each summary'''\n",
        "    output,hidden_state,cell_state = self.control_layer(control_input)\n",
        "    control_hidden_states = output\n",
        "    #prprint(\"S_c\")\n",
        "    '''Pass the merged representation to decoder along with encoder and control layer hidden states for implementing attention'''\n",
        "    \n",
        "    \n",
        "    x = output_batches[0,:] # Trigger token <SOS>\n",
        "\n",
        "    for i in range(1, title_len):\n",
        "      pred, hidden_state, cell_state = self.decoder(x,control_hidden_states,encoder_hidden_states,hidden_state, cell_state)\n",
        "      #pred = [batch_size,output_dim(vocab_size)]\n",
        "      predictions[i] = pred\n",
        "      best_guess = pred.argmax(1) \n",
        "      x = output_batches[i,:] if random.random() < tfr else best_guess\n",
        "    return predictions  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3vYL04BOAgk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrgYtkNKOdUR"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i,batch in enumerate(iterator):\n",
        "        \n",
        "        #abstract = batch.Abstract\n",
        "        title = batch.Title\n",
        "        #abstract,title = [seq_len,batch_size]\n",
        "        optimizer.zero_grad()\n",
        "        #print(\"batch device \",batch.device)\n",
        "        predictions = model(batch, title,0.5)\n",
        "        \n",
        "        #predictions = [seq_len_title,batch_size,title_vocab]\n",
        "        output_dim = predictions.shape[-1]\n",
        "        \n",
        "        predictions = predictions[1:].view(-1, output_dim)#ignoring the first value is the <sos> token\n",
        "        title = title[1:].view(-1)\n",
        "        \n",
        "        loss = criterion(predictions, title)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W52SADa2OQij"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUeVTA61Odl5"
      },
      "source": [
        "def test(model, iterator, criterion):    \n",
        "    model.eval() \n",
        "    epoch_loss = 0 \n",
        "    with torch.no_grad():   \n",
        "        for i, batch in enumerate(iterator):\n",
        "          #abstract = batch.Abstract\n",
        "          title = batch.Title\n",
        "          #abstract,title = [seq_len,batch_size]\n",
        "          predictions = model(batch, title,0)\n",
        "          #predictions = [seq_len_title,batch_size,title_vocab]\n",
        "          output_dim = predictions.shape[-1]\n",
        "          predictions = predictions[1:].view(-1, output_dim)#ignoring the first value is the <sos> token\n",
        "          title = title[1:].view(-1)\n",
        "          loss = criterion(predictions, title)  \n",
        "          epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPORcxCzOZEi"
      },
      "source": [
        "##Translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaIcc_hIOe_i"
      },
      "source": [
        "#to generate title for one abstract\n",
        "def translate(model,batch,max_len):\n",
        "  predictions=[]\n",
        "  model.eval()\n",
        "  with torch.no_grad():   \n",
        "    batch_size = 1\n",
        "    title_vocab_size = model.decoder.output_dim\n",
        "   # predictions = torch.zeros(title_len, batch_size, title_vocab_size).to(device)\n",
        "    #print(input_batches.size())\n",
        "    '''Pass each summary through the encoder'''\n",
        "    sum1=batch.sum1\n",
        "    sum2=batch.sum2\n",
        "    sum3=batch.sum3\n",
        "    sum4=batch.sum4\n",
        "    sum5=batch.sum5\n",
        "    sum6=batch.sum6\n",
        "    sum7=batch.sum7\n",
        "    sum=[sum1,sum2,sum3,sum4,sum5,sum6,sum7]\n",
        "    control_input=torch.zeros((7,batch_size,model.control_layer.hid_dim)).to(device)\n",
        "    encoder_hidden_states = []\n",
        "    for s in range(7):\n",
        "      output,hidden=model.encoder(sum[s])\n",
        "      #output = [s.length,batch_size,hid_dim]\n",
        "      #hidden=[num_layers,batch_size,hid_dim]\n",
        "      #print(\"enc_output device\",output.device)\n",
        "      encoder_hidden_states.append(output)\n",
        "      control_input[s]=hidden[-1]\n",
        "    \n",
        "    '''Pass the last hidden state to control layer for each summary'''\n",
        "    output,hidden_state,cell_state = model.control_layer(control_input)\n",
        "    control_hidden_states = output\n",
        "    #prprint(\"S_c\")\n",
        "    '''Pass the merged representation to decoder along with encoder and control layer hidden states for implementing attention'''\n",
        "    \n",
        "    \n",
        "    x =  torch.LongTensor([SUM.vocab.stoi['<sos>']]).to(device)\n",
        "\n",
        "    for i in range(1, max_len):\n",
        "      pred, hidden_state, cell_state = model.decoder(x,control_hidden_states,encoder_hidden_states,hidden_state, cell_state)\n",
        "      #pred = [1,output_dim(vocab_size)]\n",
        "      best_guess = pred.argmax(1)\n",
        "      predictions.append(best_guess.item())\n",
        "      x = best_guess\n",
        "      # Model predicts it's the end of the sentence\n",
        "      if predictions[-1] == SUM.vocab.stoi[\"<eos>\"]:\n",
        "        break\n",
        "\n",
        "      translated_sentence = [SUM.vocab.itos[idx] for idx in predictions]\n",
        "  return translated_sentence[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipr7F5cN6SKA"
      },
      "source": [
        "### Randomly pick 1000 abstracts from the dataset\n",
        "<br> This will be used later for generating titles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVIqlySR6JD4"
      },
      "source": [
        "df = pd.read_csv('./drive/MyDrive/data_summaries.csv')\n",
        "idx = np.random.randint(0,df.shape[0],1000)\n",
        "df1 = df.loc[idx]\n",
        "df1.to_csv('./drive/MyDrive/test_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boOqkDtIOhPS"
      },
      "source": [
        "##Start training and testing!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFlmpmODdvSr"
      },
      "source": [
        "### Experiement 1 - Complex Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1U8noU9OcZ4"
      },
      "source": [
        "INPUT_DIM = len(SUM.vocab)\n",
        "OUTPUT_DIM = len(TITLE.vocab)\n",
        "ENC_EMB_DIM = 100\n",
        "DEC_EMB_DIM = 100\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 3\n",
        "ENC_DROPOUT = 0\n",
        "DEC_DROPOUT = 0\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "con = ControlLayer(HID_DIM,HID_DIM)\n",
        "'''trying with complex attention first'''\n",
        "attention = ComplexAttention(HID_DIM,HID_DIM,HID_DIM)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,HID_DIM,HID_DIM,attention,'complex')\n",
        "\n",
        "model = Seq2Seq(enc,con, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpq0dBD1Ocrh",
        "outputId": "e9d487fc-7bbf-40d3-e378-2bce628baa75"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      #print(name)\n",
        "      nn.init.uniform_(param.data, -0.1, 0.1)  \n",
        "model.apply(init_weights)\n",
        "pretrained_embeddings = SUM.vocab.vectors\n",
        "model.encoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.decoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-fJZPAwfNVF"
      },
      "source": [
        "def checkpoint_and_save(model, min_loss, epoch, optimizer):\n",
        "    print()\n",
        "    state = {'model': model,'min_loss': min_loss,'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),}\n",
        "    path =  './drive/MyDrive/Colab Notebooks/final_net.pt'\n",
        "    torch.save(state, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1vltVXHj2Oz"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiO38agbOn-k",
        "outputId": "bfe836e0-0bd3-4cf1-e7cc-260158f395ca"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 6.402\n",
            "\tTest Loss: 6.523\n",
            "Time taken : 7.524mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.931\n",
            "\tTest Loss: 6.506\n",
            "Time taken : 7.692mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.748\n",
            "\tTest Loss: 6.520\n",
            "Time taken : 7.695mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.573\n",
            "\tTest Loss: 6.534\n",
            "Time taken : 7.702mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.455\n",
            "\tTest Loss: 6.547\n",
            "Time taken : 7.697mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.370\n",
            "\tTest Loss: 6.550\n",
            "Time taken : 7.683mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.274\n",
            "\tTest Loss: 6.594\n",
            "Time taken : 7.713mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.164\n",
            "\tTest Loss: 6.586\n",
            "Time taken : 7.722mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.100\n",
            "\tTest Loss: 6.594\n",
            "Time taken : 7.707mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.023\n",
            "\tTest Loss: 6.603\n",
            "Time taken : 7.705mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.964\n",
            "\tTest Loss: 6.613\n",
            "Time taken : 7.759mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.868\n",
            "\tTest Loss: 6.607\n",
            "Time taken : 7.721mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.807\n",
            "\tTest Loss: 6.611\n",
            "Time taken : 7.710mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.700\n",
            "\tTest Loss: 6.680\n",
            "Time taken : 7.735mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.602\n",
            "\tTest Loss: 6.513\n",
            "Time taken : 7.749mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.509\n",
            "\tTest Loss: 6.483\n",
            "Time taken : 7.751mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.356\n",
            "\tTest Loss: 6.472\n",
            "Time taken : 7.750mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.189\n",
            "\tTest Loss: 6.462\n",
            "Time taken : 7.759mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.059\n",
            "\tTest Loss: 6.404\n",
            "Time taken : 7.760mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.915\n",
            "\tTest Loss: 6.480\n",
            "Time taken : 7.769mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Oqz5VAMdy9M"
      },
      "source": [
        "### Experiement 2 - Simple Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBs4tvMcA53a"
      },
      "source": [
        "INPUT_DIM = len(SUM.vocab)\n",
        "OUTPUT_DIM = len(TITLE.vocab)\n",
        "ENC_EMB_DIM = 100\n",
        "DEC_EMB_DIM = 100\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 3\n",
        "ENC_DROPOUT = 0\n",
        "DEC_DROPOUT = 0\n",
        "SPLIT = 472\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "con = ControlLayer(HID_DIM,HID_DIM)\n",
        "'''trying with complex attention first'''\n",
        "attention = SimpleAttention(HID_DIM,HID_DIM,HID_DIM,SPLIT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,HID_DIM,HID_DIM,attention,'simple')\n",
        "\n",
        "model = Seq2Seq(enc,con, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0U2HVi5A53c",
        "outputId": "cc790111-d9b9-4d69-95c5-4c9ca160d32e"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      #print(name)\n",
        "      nn.init.uniform_(param.data, -0.1, 0.1)  \n",
        "model.apply(init_weights)\n",
        "pretrained_embeddings = SUM.vocab.vectors\n",
        "model.encoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.decoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0eBrsz2A53f"
      },
      "source": [
        "def checkpoint_and_save(model, min_loss, epoch, optimizer):\n",
        "    print()\n",
        "    state = {'model': model,'min_loss': min_loss,'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),'train_iterator':train_iterator.state_dict(),'valid_iterator':valid_iterator.state_dict()}\n",
        "    path =  './drive/MyDrive/Colab Notebooks/final_net_2.1.pt'\n",
        "    torch.save(state, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kdefe9PA53h"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkBlstVXA53i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7929b384-e8a7-4237-c9d5-4c476133da44"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 50\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch} - Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 6.239\n",
            "\tTest Loss: 6.492\n",
            "Epoch 0 - Time taken : 8.695mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.899\n",
            "\tTest Loss: 6.497\n",
            "Epoch 1 - Time taken : 8.679mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.714\n",
            "\tTest Loss: 6.523\n",
            "Epoch 2 - Time taken : 8.692mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.565\n",
            "\tTest Loss: 6.612\n",
            "Epoch 3 - Time taken : 8.696mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.453\n",
            "\tTest Loss: 6.531\n",
            "Epoch 4 - Time taken : 8.678mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.357\n",
            "\tTest Loss: 6.564\n",
            "Epoch 5 - Time taken : 8.702mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.261\n",
            "\tTest Loss: 6.532\n",
            "Epoch 6 - Time taken : 8.724mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.115\n",
            "\tTest Loss: 6.464\n",
            "Epoch 7 - Time taken : 8.725mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.961\n",
            "\tTest Loss: 6.336\n",
            "Epoch 8 - Time taken : 8.755mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.799\n",
            "\tTest Loss: 6.277\n",
            "Epoch 9 - Time taken : 8.762mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.628\n",
            "\tTest Loss: 6.341\n",
            "Epoch 10 - Time taken : 8.750mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.466\n",
            "\tTest Loss: 6.272\n",
            "Epoch 11 - Time taken : 8.772mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.303\n",
            "\tTest Loss: 6.353\n",
            "Epoch 12 - Time taken : 8.764mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.115\n",
            "\tTest Loss: 6.429\n",
            "Epoch 13 - Time taken : 8.768mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.971\n",
            "\tTest Loss: 6.364\n",
            "Epoch 14 - Time taken : 8.759mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.814\n",
            "\tTest Loss: 6.426\n",
            "Epoch 15 - Time taken : 8.749mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.628\n",
            "\tTest Loss: 6.554\n",
            "Epoch 16 - Time taken : 8.765mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.442\n",
            "\tTest Loss: 6.691\n",
            "Epoch 17 - Time taken : 8.759mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.314\n",
            "\tTest Loss: 6.718\n",
            "Epoch 18 - Time taken : 8.780mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.076\n",
            "\tTest Loss: 6.883\n",
            "Epoch 19 - Time taken : 8.759mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.901\n",
            "\tTest Loss: 6.917\n",
            "Epoch 20 - Time taken : 8.757mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.666\n",
            "\tTest Loss: 7.090\n",
            "Epoch 21 - Time taken : 8.761mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.505\n",
            "\tTest Loss: 7.223\n",
            "Epoch 22 - Time taken : 8.768mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.335\n",
            "\tTest Loss: 7.293\n",
            "Epoch 23 - Time taken : 8.770mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.108\n",
            "\tTest Loss: 7.434\n",
            "Epoch 24 - Time taken : 8.783mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.910\n",
            "\tTest Loss: 7.564\n",
            "Epoch 25 - Time taken : 8.790mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.756\n",
            "\tTest Loss: 7.643\n",
            "Epoch 26 - Time taken : 8.775mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.537\n",
            "\tTest Loss: 7.874\n",
            "Epoch 27 - Time taken : 8.784mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.360\n",
            "\tTest Loss: 8.005\n",
            "Epoch 28 - Time taken : 8.779mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.205\n",
            "\tTest Loss: 8.167\n",
            "Epoch 29 - Time taken : 8.739mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.032\n",
            "\tTest Loss: 8.385\n",
            "Epoch 30 - Time taken : 8.738mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 0.876\n",
            "\tTest Loss: 8.565\n",
            "Epoch 31 - Time taken : 8.755mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e9223c43ed01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\tTrain Loss: {train_loss:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-534606c595b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhhR8UlO7y9f"
      },
      "source": [
        "#### Call translate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDlHeS9D8nDg"
      },
      "source": [
        "df1={'Abstract':[],'Title':[],'Generated Title':[]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9We7viJmFhhV",
        "outputId": "13c7c515-5632-4604-f190-833f95225e26"
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Abstract': [], 'Generated Title': [], 'Title': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E53x0NA8nDj"
      },
      "source": [
        "ID = data.Field(use_vocab=False,sequential=False,preprocessing=int)\n",
        "fields = [('Id',ID),('Abstract',None),('Title',TITLE),('sum1',SUM),('sum2',SUM),('sum3',SUM),('sum4',SUM),('sum5',SUM),('sum6',SUM),('sum7',SUM)]\n",
        "ran_dataset = data.TabularDataset(path='./drive/MyDrive/test_data.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG5ZBjpc-J9_"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "BATCH_SIZE =1\n",
        "iterator=data.Iterator(\n",
        "    ran_dataset,\n",
        "    batch_size = BATCH_SIZE,shuffle=False,device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taMJEYPA-NRg",
        "outputId": "e109bdc3-a177-4dc0-888b-934d14fc1947"
      },
      "source": [
        "len(iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cngyig-8-pP"
      },
      "source": [
        "path =  './drive/MyDrive/Colab Notebooks/final_net_2.1.pt'\n",
        "checkpoint = torch.load(path)\n",
        "#print(checkpoint)\n",
        "model1 = checkpoint['model']\n",
        "model1.load_state_dict( checkpoint['model_state_dict'])\n",
        "min_loss = checkpoint['min_loss']\n",
        "epoch = checkpoint['epoch']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBYW-IDv9IXY"
      },
      "source": [
        "for i,batch in enumerate(iterator):\n",
        "  id1 = batch.Id.item()\n",
        "  #print(batch)\n",
        "  print(\"Abstract : \")\n",
        "  print(\"\".join(list(df.loc[df['Id'] == id1]['Abstract'])))\n",
        "  print(\"Actual Title : \")\n",
        "  print(\"\".join(list(df.loc[df['Id'] == id1]['Title'])))\n",
        "  print(\"Generated Title : \")\n",
        "  print(\" \".join(translate(model1,batch,10)))\n",
        "  abs= \"\".join(list(df.loc[df['Id'] == id1]['Abstract']))\n",
        "  title= \"\".join(list(df.loc[df['Id'] == id1]['Title']))\n",
        "  df1['Abstract'].append(abs)\n",
        "  df1['Title'].append(title)\n",
        "  df1['Generated Title'].append(\" \".join(translate(model1,batch,10)))\n",
        "  #df1['Generated_Title'][id1]=translate(model1,batch,15))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNc5GI_y-sr9",
        "outputId": "4f2a9ca6-e3e5-4f19-afb5-6e94df0f1ff4"
      },
      "source": [
        "len(df1['Abstract'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDVL3Dur-mLX"
      },
      "source": [
        "df2 = pd.DataFrame(df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYbKyAV2-mLc"
      },
      "source": [
        "df2.to_csv('./drive/MyDrive/generated_titles_2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGEDhNyXT15G"
      },
      "source": [
        "### Experiment 3 - Simple Attention,tfr =0.5, Dropout = 0.5\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5W0oK2sYt0a"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i,batch in enumerate(iterator):\n",
        "        \n",
        "        #abstract = batch.Abstract\n",
        "        title = batch.Title\n",
        "        #abstract,title = [seq_len,batch_size]\n",
        "        optimizer.zero_grad()\n",
        "        #print(\"batch device \",batch.device)\n",
        "        predictions = model(batch, title,0.5)\n",
        "        \n",
        "        #predictions = [seq_len_title,batch_size,title_vocab]\n",
        "        output_dim = predictions.shape[-1]\n",
        "        \n",
        "        predictions = predictions[1:].view(-1, output_dim)#ignoring the first value is the <sos> token\n",
        "        title = title[1:].view(-1)\n",
        "        \n",
        "        loss = criterion(predictions, title)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2gNb5Msaqzv"
      },
      "source": [
        "INPUT_DIM = len(SUM.vocab)\n",
        "OUTPUT_DIM = len(TITLE.vocab)\n",
        "ENC_EMB_DIM = 100\n",
        "DEC_EMB_DIM = 100\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 3\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SPLIT = 472\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "con = ControlLayer(HID_DIM,HID_DIM)\n",
        "'''trying with complex attention first'''\n",
        "attention = SimpleAttention(HID_DIM,HID_DIM,HID_DIM,SPLIT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,HID_DIM,HID_DIM,attention,'simple')\n",
        "\n",
        "model = Seq2Seq(enc,con, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6B8R05uaqzz",
        "outputId": "a8e20f09-115a-4836-8d67-60a2e2e0a6bd"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      #print(name)\n",
        "      nn.init.uniform_(param.data, -0.1, 0.1)  \n",
        "model.apply(init_weights)\n",
        "pretrained_embeddings = SUM.vocab.vectors\n",
        "model.encoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.decoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg90mZtLaqz4"
      },
      "source": [
        "def checkpoint_and_save(model, min_loss, epoch, optimizer):\n",
        "    print()\n",
        "    state = {'model': model,'min_loss': min_loss,'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),'train_iterator':train_iterator.state_dict(),'valid_iterator':valid_iterator.state_dict()}\n",
        "    path =  './drive/MyDrive/Colab Notebooks/final_net_3.pt'\n",
        "    torch.save(state, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jERTEwfaqz-"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P28dujUpaqz_",
        "outputId": "c6caa915-35fa-4eb5-8062-3a0a7b38edef"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 25\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch} - Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 6.406\n",
            "\tTest Loss: 6.501\n",
            "Epoch 0 - Time taken : 14.785mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.928\n",
            "\tTest Loss: 6.579\n",
            "Epoch 1 - Time taken : 14.835mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.726\n",
            "\tTest Loss: 6.502\n",
            "Epoch 2 - Time taken : 14.822mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.595\n",
            "\tTest Loss: 6.516\n",
            "Epoch 3 - Time taken : 14.853mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.458\n",
            "\tTest Loss: 6.512\n",
            "Epoch 4 - Time taken : 14.843mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.360\n",
            "\tTest Loss: 6.490\n",
            "Epoch 5 - Time taken : 14.827mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.163\n",
            "\tTest Loss: 6.478\n",
            "Epoch 6 - Time taken : 14.831mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.022\n",
            "\tTest Loss: 6.300\n",
            "Epoch 7 - Time taken : 14.851mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.854\n",
            "\tTest Loss: 6.318\n",
            "Epoch 8 - Time taken : 14.841mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.664\n",
            "\tTest Loss: 6.282\n",
            "Epoch 9 - Time taken : 14.860mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.518\n",
            "\tTest Loss: 6.334\n",
            "Epoch 10 - Time taken : 14.861mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF03zOQmdanb"
      },
      "source": [
        "#### Call translate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faH6jPBrA-xf"
      },
      "source": [
        "df1={'Abstract':[],'Title':[],'Generated Title':[]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiWkWbBI0ohT"
      },
      "source": [
        "ID = data.Field(use_vocab=False,sequential=False,preprocessing=int)\n",
        "fields = [('Id',ID),('Abstract',None),('Title',TITLE),('sum1',SUM),('sum2',SUM),('sum3',SUM),('sum4',SUM),('sum5',SUM),('sum6',SUM),('sum7',SUM)]\n",
        "ran_dataset = data.TabularDataset(path='./drive/MyDrive/test_data.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI6rLEhhefgR"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "BATCH_SIZE =1\n",
        "iterator=data.Iterator(\n",
        "    ran_dataset,\n",
        "    batch_size = BATCH_SIZE,shuffle=False,device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Do9McV9Apc4"
      },
      "source": [
        "len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVgbpP61eNVR"
      },
      "source": [
        "path =  './drive/MyDrive/Colab Notebooks/final_net_3.pt'\n",
        "checkpoint = torch.load(path)\n",
        "#print(checkpoint)\n",
        "model1 = checkpoint['model']\n",
        "model1.load_state_dict( checkpoint['model_state_dict'])\n",
        "min_loss = checkpoint['min_loss']\n",
        "epoch = checkpoint['epoch']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b5dMFSefD6K"
      },
      "source": [
        "\n",
        "for i,batch in enumerate(iterator):\n",
        "  id1 = batch.Id.item()\n",
        "  #print(batch)\n",
        "  #print(\"Abstract : \")\n",
        "  #print(\"\".join(list(df.loc[df['Id'] == id1]['Abstract'])))\n",
        "  #print(\"Actual Title : \")\n",
        "  #print(\"\".join(list(df.loc[df['Id'] == id1]['Title'])))\n",
        "  #print(\"Generated Title : \")\n",
        "  #print(\" \".join(translate(model1,batch,10)))\n",
        "  abs= \"\".join(list(df.loc[df['Id'] == id1]['Abstract']))\n",
        "  title= \"\".join(list(df.loc[df['Id'] == id1]['Title']))\n",
        "  df1['Abstract'].append(abs)\n",
        "  df1['Title'].append(title)\n",
        "  df1['Generated Title'].append(\" \".join(translate(model1,batch,10)))\n",
        "  #df1['Generated_Title'][id1]=translate(model1,batch,15))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ja56laqyaFl"
      },
      "source": [
        "df2 = pd.DataFrame(df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgS-aNedX4rL"
      },
      "source": [
        "df2.to_csv('./drive/MyDrive/generated_titles_3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM5k8SGcHaHn"
      },
      "source": [
        "### Experiment 4 ; Simple Attention, encoder num layers = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY2HIy2ZHXnj"
      },
      "source": [
        "INPUT_DIM = len(SUM.vocab)\n",
        "OUTPUT_DIM = len(TITLE.vocab)\n",
        "ENC_EMB_DIM = 100\n",
        "DEC_EMB_DIM = 100\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 1\n",
        "ENC_DROPOUT = 0\n",
        "DEC_DROPOUT = 0\n",
        "SPLIT = 472\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "con = ControlLayer(HID_DIM,HID_DIM)\n",
        "attention = SimpleAttention(HID_DIM,HID_DIM,HID_DIM,SPLIT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,HID_DIM,HID_DIM,attention,'simple')\n",
        "\n",
        "model = Seq2Seq(enc,con, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGjEBHmjIZWD",
        "outputId": "1c8bd466-c63d-49ac-b3df-88e5144d029e"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      #print(name)\n",
        "      nn.init.uniform_(param.data, -0.1, 0.1)  \n",
        "model.apply(init_weights)\n",
        "pretrained_embeddings = SUM.vocab.vectors\n",
        "model.encoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.decoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_qs2l1aIi7H"
      },
      "source": [
        "def checkpoint_and_save(model, min_loss, epoch, optimizer):\n",
        "    print()\n",
        "    state = {'model': model,'min_loss': min_loss,'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),'train_iterator':train_iterator.state_dict(),'valid_iterator':valid_iterator.state_dict()}\n",
        "    path =  './drive/MyDrive/Colab Notebooks/final_net_4.pt'\n",
        "    torch.save(state, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeI5VLCBIi7K"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dweIbaLfIi7M",
        "outputId": "c1cc6989-c13a-468a-ee88-5af0c9f90454"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch} - Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 6.323\n",
            "\tTest Loss: 6.135\n",
            "Epoch 0 - Time taken : 5.285mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.392\n",
            "\tTest Loss: 5.861\n",
            "Epoch 1 - Time taken : 5.340mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.816\n",
            "\tTest Loss: 5.798\n",
            "Epoch 2 - Time taken : 5.329mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.308\n",
            "\tTest Loss: 5.779\n",
            "Epoch 3 - Time taken : 5.347mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.804\n",
            "\tTest Loss: 5.884\n",
            "Epoch 4 - Time taken : 5.355mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.371\n",
            "\tTest Loss: 5.995\n",
            "Epoch 5 - Time taken : 5.338mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.981\n",
            "\tTest Loss: 6.151\n",
            "Epoch 6 - Time taken : 5.328mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.615\n",
            "\tTest Loss: 6.297\n",
            "Epoch 7 - Time taken : 5.348mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.300\n",
            "\tTest Loss: 6.453\n",
            "Epoch 8 - Time taken : 5.355mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.988\n",
            "\tTest Loss: 6.672\n",
            "Epoch 9 - Time taken : 5.337mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.701\n",
            "\tTest Loss: 6.880\n",
            "Epoch 10 - Time taken : 5.344mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.434\n",
            "\tTest Loss: 7.103\n",
            "Epoch 11 - Time taken : 5.330mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-024d119ea7c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\tTrain Loss: {train_loss:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-534606c595b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIIoDRgJSOxO"
      },
      "source": [
        "#### Call translate (final_model_4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqBtiJuwSLpM"
      },
      "source": [
        "df = pd.read_csv('./drive/MyDrive/data_summaries.csv')\n",
        "idx = np.random.randint(0,df.shape[0],1000)\n",
        "df1 = df.loc[idx]\n",
        "df1.to_csv('./drive/MyDrive/test_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YML8B8rgSLpR"
      },
      "source": [
        "df1={'Abstract':[],'Title':[],'Generated Title':[]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu2HO7j8SLpT",
        "outputId": "c32282d0-9b76-4ab1-8d3b-db90ca3c3bda"
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Abstract': [], 'Generated Title': [], 'Title': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJa0P6CZSLpV"
      },
      "source": [
        "ID = data.Field(use_vocab=False,sequential=False,preprocessing=int)\n",
        "fields = [('Id',ID),('Abstract',None),('Title',TITLE),('sum1',SUM),('sum2',SUM),('sum3',SUM),('sum4',SUM),('sum5',SUM),('sum6',SUM),('sum7',SUM)]\n",
        "ran_dataset = data.TabularDataset(path='./drive/MyDrive/test_data.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nED7m4gcSLpW"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "BATCH_SIZE =1\n",
        "iterator=data.Iterator(\n",
        "    ran_dataset,\n",
        "    batch_size = BATCH_SIZE,shuffle=False,device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksTPG-h1SLpX",
        "outputId": "291b7e5e-1983-499d-f9c0-7b0444b977b7"
      },
      "source": [
        "len(iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzpVe2WKSLpY"
      },
      "source": [
        "path =  './drive/MyDrive/Colab Notebooks/final_net_4.pt'\n",
        "checkpoint = torch.load(path)\n",
        "#print(checkpoint)\n",
        "model1 = checkpoint['model']\n",
        "model1.load_state_dict( checkpoint['model_state_dict'])\n",
        "min_loss = checkpoint['min_loss']\n",
        "epoch = checkpoint['epoch']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMCZKxdhSLpa",
        "outputId": "96c6a61c-8330-4f9a-c26a-ebf046e222f1"
      },
      "source": [
        "epoch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDJMt51BSLpc",
        "outputId": "3f1ff208-d9a3-47e7-fa89-df7db49930d5"
      },
      "source": [
        "for i,batch in enumerate(iterator):\n",
        "  id1 = batch.Id.item()\n",
        "  #print(batch)\n",
        "  print(\"Abstract : \")\n",
        "  print(\"\".join(list(df.loc[df['Id'] == id1]['Abstract'])))\n",
        "  print(\"Actual Title : \")\n",
        "  print(\"\".join(list(df.loc[df['Id'] == id1]['Title'])))\n",
        "  print(\"Generated Title : \")\n",
        "  print(\" \".join(translate(model1,batch,10)))\n",
        "  abs= \"\".join(list(df.loc[df['Id'] == id1]['Abstract']))\n",
        "  title= \"\".join(list(df.loc[df['Id'] == id1]['Title']))\n",
        "  df1['Abstract'].append(abs)\n",
        "  df1['Title'].append(title)\n",
        "  df1['Generated Title'].append(\" \".join(translate(model1,batch,10)))\n",
        "  #df1['Generated_Title'][id1]=translate(model1,batch,15))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "information, we developed a classification scheme based on neuro-fuzzy modeling\n",
            "of the AU intensity, which is robust to intensity variations, 2) using both\n",
            "geometric and appearance-based features, and applying efficient dimension\n",
            "reduction techniques, our system is robust to illumination changes and it can\n",
            "represent the subtle changes as well as temporal information involved in\n",
            "formation of the facial expressions, and 3) by continuous values of intensity\n",
            "and employing top-down hierarchical rule-based classifiers, we can develop\n",
            "accurate human-interpretable AU-to-expression converters. Extensive experiments\n",
            "on Cohn-Kanade database show the superiority of the proposed method, in\n",
            "comparison with support vector machines, hidden Markov models, and neural\n",
            "network classifiers. Keywords: biased discriminant analysis (BDA), classifier\n",
            "design and evaluation, facial action units (AUs), hybrid learning, neuro-fuzzy\n",
            "modeling.\n",
            "Actual Title : \n",
            "Analysis, Interpretation, and Recognition of Facial Action Units and\n",
            "  Expressions Using Neuro-Fuzzy Modeling\n",
            "Generated Title : \n",
            "interpretation  and recognition of of facial action units\n",
            "Abstract : \n",
            "Recent years have witnessed amazing progress in AI related fields such as\n",
            "computer vision, machine learning and autonomous vehicles. As with any rapidly\n",
            "growing field, however, it becomes increasingly difficult to stay up-to-date or\n",
            "enter the field as a beginner. While several topic specific survey papers have\n",
            "been written, to date no general survey on problems, datasets and methods in\n",
            "computer vision for autonomous vehicles exists. This paper attempts to narrow\n",
            "this gap by providing a state-of-the-art survey on this topic. Our survey\n",
            "includes both the historically most relevant literature as well as the current\n",
            "state-of-the-art on several specific topics, including recognition,\n",
            "reconstruction, motion estimation, tracking, scene understanding and end-to-end\n",
            "learning. Towards this goal, we first provide a taxonomy to classify each\n",
            "approach and then analyze the performance of the state-of-the-art on several\n",
            "challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes.\n",
            "Besides, we discuss open problems and current research challenges. To ease\n",
            "accessibility and accommodate missing references, we will also provide an\n",
            "interactive platform which allows to navigate topics and methods, and provides\n",
            "additional information and project links for each paper.\n",
            "Actual Title : \n",
            "Computer Vision for Autonomous Vehicles: Problems, Datasets and\n",
            "  State-of-the-Art\n",
            "Generated Title : \n",
            "vision and vision for autonomous vehicles  problems  a\n",
            "Abstract : \n",
            "We present a new similarity measure based on information theoretic measures\n",
            "which is superior than Normalized Compression Distance for clustering problems\n",
            "and inherits the useful properties of conditional Kolmogorov complexity. We\n",
            "show that Normalized Compression Dictionary Size and Normalized Compression\n",
            "Dictionary Entropy are computationally more efficient, as the need to perform\n",
            "the compression itself is eliminated. Also they scale linearly with exponential\n",
            "vector size growth and are content independent. We show that normalized\n",
            "compression dictionary distance is compressor independent, if limited to\n",
            "lossless compressors, which gives space for optimizations and implementation\n",
            "speed improvement for real-time and big data applications. The introduced\n",
            "measure is applicable for machine learning tasks of parameter-free unsupervised\n",
            "clustering, supervised learning such as classification and regression, feature\n",
            "selection, and is applicable for big data problems with order of magnitude\n",
            "speed increase.\n",
            "Actual Title : \n",
            "Generalized Compression Dictionary Distance as Universal Similarity\n",
            "  Measure\n",
            "Generated Title : \n",
            "compression generalized similarity measures for unsupervised compression\n",
            "Abstract : \n",
            "With the range and sensitivity of algorithmic decisions expanding at a\n",
            "break-neck speed, it is imperative that we aggressively investigate whether\n",
            "programs are biased. We propose a novel probabilistic program analysis\n",
            "technique and apply it to quantifying bias in decision-making programs.\n",
            "Specifically, we (i) present a sound and complete automated verification\n",
            "technique for proving quantitative properties of probabilistic programs; (ii)\n",
            "show that certain notions of bias, recently proposed in the fairness\n",
            "literature, can be phrased as quantitative correctness properties; and (iii)\n",
            "present FairSquare, the first verification tool for quantifying program bias,\n",
            "and evaluate it on a range of decision-making programs.\n",
            "Actual Title : \n",
            "Quantifying Program Bias\n",
            "Generated Title : \n",
            "bias bias bias bias\n",
            "Abstract : \n",
            "2D face analysis techniques, such as face landmarking, face recognition and\n",
            "face verification, are reasonably dependent on illumination conditions which\n",
            "are usually uncontrolled and unpredictable in the real world. An illumination\n",
            "robust preprocessing method thus remains a significant challenge in reliable\n",
            "face analysis. In this paper we propose a novel approach for improving lighting\n",
            "normalization through building the underlying reflectance model which\n",
            "characterizes interactions between skin surface, lighting source and camera\n",
            "sensor, and elaborates the formation of face color appearance. Specifically,\n",
            "the proposed illumination processing pipeline enables the generation of\n",
            "Chromaticity Intrinsic Image (CII) in a log chromaticity space which is robust\n",
            "to illumination variations. Moreover, as an advantage over most prevailing\n",
            "methods, a photo-realistic color face image is subsequently reconstructed which\n",
            "eliminates a wide variety of shadows whilst retaining the color information and\n",
            "identity details. Experimental results under different scenarios and using\n",
            "various face databases show the effectiveness of the proposed approach to deal\n",
            "with lighting variations, including both soft and hard shadows, in face\n",
            "recognition.\n",
            "Actual Title : \n",
            "Improving Shadow Suppression for Illumination Robust Face Recognition\n",
            "Generated Title : \n",
            "face recognition using robust face recognition\n",
            "Abstract : \n",
            "We present Deeply Supervised Object Detector (DSOD), a framework that can\n",
            "learn object detectors from scratch. State-of-the-art object objectors rely\n",
            "heavily on the off-the-shelf networks pre-trained on large-scale classification\n",
            "datasets like ImageNet, which incurs learning bias due to the difference on\n",
            "both the loss functions and the category distributions between classification\n",
            "and detection tasks. Model fine-tuning for the detection task could alleviate\n",
            "this bias to some extent but not fundamentally. Besides, transferring\n",
            "pre-trained models from classification to detection between discrepant domains\n",
            "is even more difficult (e.g. RGB to depth images). A better solution to tackle\n",
            "these two critical problems is to train object detectors from scratch, which\n",
            "motivates our proposed DSOD. Previous efforts in this direction mostly failed\n",
            "due to much more complicated loss functions and limited training data in object\n",
            "detection. In DSOD, we contribute a set of design principles for training\n",
            "object detectors from scratch. One of the key findings is that deep\n",
            "supervision, enabled by dense layer-wise connections, plays a critical role in\n",
            "learning a good detector. Combining with several other principles, we develop\n",
            "DSOD following the single-shot detection (SSD) framework. Experiments on PASCAL\n",
            "VOC 2007, 2012 and MS COCO datasets demonstrate that DSOD can achieve better\n",
            "results than the state-of-the-art solutions with much more compact models. For\n",
            "instance, DSOD outperforms SSD on all three benchmarks with real-time detection\n",
            "speed, while requires only 1/2 parameters to SSD and 1/10 parameters to Faster\n",
            "RCNN. Our code and models are available at: https://github.com/szq0214/DSOD .\n",
            "Actual Title : \n",
            "DSOD: Learning Deeply Supervised Object Detectors from Scratch\n",
            "Generated Title : \n",
            "learning deeply supervised object detectors\n",
            "Abstract : \n",
            "We present an approach for real-time, robust and accurate hand pose\n",
            "estimation from moving egocentric RGB-D cameras in cluttered real environments.\n",
            "Existing methods typically fail for hand-object interactions in cluttered\n",
            "scenes imaged from egocentric viewpoints, common for virtual or augmented\n",
            "reality applications. Our approach uses two subsequently applied Convolutional\n",
            "Neural Networks (CNNs) to localize the hand and regress 3D joint locations.\n",
            "Hand localization is achieved by using a CNN to estimate the 2D position of the\n",
            "hand center in the input, even in the presence of clutter and occlusions. The\n",
            "localized hand position, together with the corresponding input depth value, is\n",
            "used to generate a normalized cropped image that is fed into a second CNN to\n",
            "regress relative 3D hand joint locations in real time. For added accuracy,\n",
            "robustness and temporal stability, we refine the pose estimates using a\n",
            "kinematic pose tracking energy. To train the CNNs, we introduce a new\n",
            "photorealistic dataset that uses a merged reality approach to capture and\n",
            "synthesize large amounts of annotated data of natural hand interaction in\n",
            "cluttered scenes. Through quantitative and qualitative evaluation, we show that\n",
            "our method is robust to self-occlusion and occlusions by objects, particularly\n",
            "in moving egocentric perspectives.\n",
            "Actual Title : \n",
            "Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor\n",
            "Generated Title : \n",
            "hand tracking under occlusion rgb d data\n",
            "Abstract : \n",
            "Accurate segmentation of the heart is an important step towards evaluating\n",
            "cardiac function. In this paper, we present a fully automated framework for\n",
            "segmentation of the left (LV) and right (RV) ventricular cavities and the\n",
            "myocardium (Myo) on short-axis cardiac MR images. We investigate various 2D and\n",
            "3D convolutional neural network architectures for this task. We investigate the\n",
            "suitability of various state-of-the art 2D and 3D convolutional neural network\n",
            "architectures, as well as slight modifications thereof, for this task.\n",
            "Experiments were performed on the ACDC 2017 challenge training dataset\n",
            "comprising cardiac MR images of 100 patients, where manual reference\n",
            "segmentations were made available for end-diastolic (ED) and end-systolic (ES)\n",
            "frames. We find that processing the images in a slice-by-slice fashion using 2D\n",
            "networks is beneficial due to a relatively large slice thickness. However, the\n",
            "exact network architecture only plays a minor role. We report mean Dice\n",
            "coefficients of $0.950$ (LV), $0.893$ (RV), and $0.899$ (Myo), respectively\n",
            "with an average evaluation time of 1.1 seconds per volume on a modern GPU.\n",
            "Actual Title : \n",
            "An Exploration of 2D and 3D Deep Learning Techniques for Cardiac MR\n",
            "  Image Segmentation\n",
            "Generated Title : \n",
            "learning with #d convolutional neural networks for #d\n",
            "Abstract : \n",
            "Tensor decompositions are invaluable tools in analyzing multimodal datasets.\n",
            "In many real-world scenarios, such datasets are far from being static, to the\n",
            "contrary they tend to grow over time. For instance, in an online social network\n",
            "setting, as we observe new interactions over time, our dataset gets updated in\n",
            "its \"time\" mode. How can we maintain a valid and accurate tensor decomposition\n",
            "of such a dynamically evolving multimodal dataset, without having to re-compute\n",
            "the entire decomposition after every single update? In this paper we introduce\n",
            "SaMbaTen, a Sampling-based Batch Incremental Tensor Decomposition algorithm,\n",
            "which incrementally maintains the decomposition given new updates to the tensor\n",
            "dataset. SaMbaTen is able to scale to datasets that the state-of-the-art in\n",
            "incremental tensor decomposition is unable to operate on, due to its ability to\n",
            "effectively summarize the existing tensor and the incoming updates, and perform\n",
            "all computations in the reduced summary space. We extensively evaluate SaMbaTen\n",
            "using synthetic and real datasets. Indicatively, SaMbaTen achieves comparable\n",
            "accuracy to state-of-the-art incremental and non-incremental techniques, while\n",
            "being 25-30 times faster. Furthermore, SaMbaTen scales to very large sparse and\n",
            "dense dynamically evolving tensors of dimensions up to 100K x 100K x 100K where\n",
            "state-of-the-art incremental approaches were not able to operate.\n",
            "Actual Title : \n",
            "SamBaTen: Sampling-based Batch Incremental Tensor Decomposition\n",
            "Generated Title : \n",
            "tensor batch incremental tensor decomposition\n",
            "Abstract : \n",
            "Normally a decision support system is build to solve problem where\n",
            "multi-criteria decisions are involved. The knowledge base is the vital part of\n",
            "the decision support containing the information or data that is used in\n",
            "decision-making process. This is the field where engineers and scientists have\n",
            "applied several intelligent techniques and heuristics to obtain optimal\n",
            "decisions from imprecise information. In this paper, we present a hybrid\n",
            "neuro-genetic learning approach for the adaptation a Mamdani fuzzy inference\n",
            "system for the Tactical Air Combat Decision Support System (TACDSS). Some\n",
            "simulation results demonstrating the difference of the learning techniques and\n",
            "are also provided.\n",
            "Actual Title : \n",
            "Adaptation of Mamdani Fuzzy Inference System Using Neuro - Genetic\n",
            "  Approach for Tactical Air Combat Decision Support System\n",
            "Generated Title : \n",
            "fuzzy logic programming for fuzzy logic for fuzzy\n",
            "Abstract : \n",
            "Clothing retrieval is a challenging problem in computer vision. With the\n",
            "advance of Convolutional Neural Networks (CNNs), the accuracy of clothing\n",
            "retrieval has been significantly improved. FashionNet[1], a recent study,\n",
            "proposes to employ a set of artificial features in the form of landmarks for\n",
            "clothing retrieval, which are shown to be helpful for retrieval. However, the\n",
            "landmark detection module is trained with strong supervision which requires\n",
            "considerable efforts to obtain. In this paper, we propose a self-learning\n",
            "Visual Attention Model (VAM) to extract attention maps from clothing images.\n",
            "The VAM is further connected to a global network to form an end-to-end network\n",
            "structure through Impdrop connection which randomly Dropout on the feature maps\n",
            "with the probabilities given by the attention map. Extensive experiments on\n",
            "several widely used benchmark clothing retrieval data sets have demonstrated\n",
            "the promise of the proposed method. We also show that compared to the trivial\n",
            "Product connection, the Impdrop connection makes the network structure more\n",
            "robust when training sets of limited size are used.\n",
            "Actual Title : \n",
            "Clothing Retrieval with Visual Attention Model\n",
            "Generated Title : \n",
            "deep features for visual attention\n",
            "Abstract : \n",
            "We propose a novel tree classification system called Treelogy, that fuses\n",
            "deep representations with hand-crafted features obtained from leaf images to\n",
            "perform leaf-based plant classification. Key to this system are segmentation of\n",
            "the leaf from an untextured background, using convolutional neural networks\n",
            "(CNNs) for learning deep representations, extracting hand-crafted features with\n",
            "a number of image processing techniques, training a linear SVM with feature\n",
            "vectors, merging SVM and CNN results, and identifying the species from a\n",
            "dataset of 57 trees. Our classification results show that fusion of deep\n",
            "representations with hand-crafted features leads to the highest accuracy. The\n",
            "proposed algorithm is embedded in a smart-phone application, which is publicly\n",
            "available. Furthermore, our novel dataset comprised of 5408 leaf images is also\n",
            "made public for use of other researchers.\n",
            "Actual Title : \n",
            "Treelogy: A Novel Tree Classifier Utilizing Deep and Hand-crafted\n",
            "  Representations\n",
            "Generated Title : \n",
            "a deep learning classifier for hand crafted hand crafted hand crafted\n",
            "Abstract : \n",
            "We present a novel detection method using a deep convolutional neural network\n",
            "(CNN), named AttentionNet. We cast an object detection problem as an iterative\n",
            "classification problem, which is the most suitable form of a CNN. AttentionNet\n",
            "provides quantized weak directions pointing a target object and the ensemble of\n",
            "iterative predictions from AttentionNet converges to an accurate object\n",
            "boundary box. Since AttentionNet is a unified network for object detection, it\n",
            "detects objects without any separated models from the object proposal to the\n",
            "post bounding-box regression. We evaluate AttentionNet by a human detection\n",
            "task and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC\n",
            "2007/2012 with an 8-layered architecture only.\n",
            "Actual Title : \n",
            "AttentionNet: Aggregating Weak Directions for Accurate Object Detection\n",
            "Generated Title : \n",
            "aggregating weak network for object object detection\n",
            "Abstract : \n",
            "Recent advances have enabled \"oracle\" classifiers that can classify across\n",
            "many classes and input distributions with high accuracy without retraining.\n",
            "However, these classifiers are relatively heavyweight, so that applying them to\n",
            "classify video is costly. We show that day-to-day video exhibits highly skewed\n",
            "class distributions over the short term, and that these distributions can be\n",
            "classified by much simpler models. We formulate the problem of detecting the\n",
            "short-term skews online and exploiting models based on it as a new sequential\n",
            "decision making problem dubbed the Online Bandit Problem, and present a new\n",
            "algorithm to solve it. When applied to recognizing faces in TV shows and\n",
            "movies, we realize end-to-end classification speedups of 2.4-7.8x/2.6-11.2x (on\n",
            "GPU/CPU) relative to a state-of-the-art convolutional neural network, at\n",
            "competitive accuracy.\n",
            "Actual Title : \n",
            "Fast Video Classification via Adaptive Cascading of Deep Models\n",
            "Generated Title : \n",
            "video classification via adaptive cascading of deep networks\n",
            "Abstract : \n",
            "A central challenge in sensory neuroscience is describing how the activity of\n",
            "populations of neurons can represent useful features of the external\n",
            "environment. However, while neurophysiologists have long been able to record\n",
            "the responses of neurons in awake, behaving animals, it is another matter\n",
            "entirely to say what a given neuron does. A key problem is that in many sensory\n",
            "domains, the space of all possible stimuli that one might encounter is\n",
            "effectively infinite; in vision, for instance, natural scenes are\n",
            "combinatorially complex, and an organism will only encounter a tiny fraction of\n",
            "possible stimuli. As a result, even describing the response properties of\n",
            "sensory neurons is difficult, and investigations of neuronal functions are\n",
            "almost always critically limited by the number of stimuli that can be\n",
            "considered. In this paper, we propose a closed-loop, optimization-based\n",
            "experimental framework for characterizing the response properties of sensory\n",
            "neurons, building on past efforts in closed-loop experimental methods, and\n",
            "leveraging recent advances in artificial neural networks to serve as as a\n",
            "proving ground for our techniques. Specifically, using deep convolutional\n",
            "neural networks, we asked whether modern black-box optimization techniques can\n",
            "be used to interrogate the \"tuning landscape\" of an artificial neuron in a\n",
            "deep, nonlinear system, without imposing significant constraints on the space\n",
            "of stimuli under consideration. We introduce a series of measures to quantify\n",
            "the tuning landscapes, and show how these relate to the performances of the\n",
            "networks in an object recognition task. To the extent that deep convolutional\n",
            "neural networks increasingly serve as de facto working hypotheses for\n",
            "biological vision, we argue that developing a unified approach for studying\n",
            "both artificial and biological systems holds great potential to advance both\n",
            "fields together.\n",
            "Actual Title : \n",
            "Measuring and Understanding Sensory Representations within Deep Networks\n",
            "  Using a Numerical Optimization Framework\n",
            "Generated Title : \n",
            "understanding of sensory representations in deep networks\n",
            "Abstract : \n",
            "Optimal transportation distances are a fundamental family of parameterized\n",
            "distances for histograms. Despite their appealing theoretical properties,\n",
            "excellent performance in retrieval tasks and intuitive formulation, their\n",
            "computation involves the resolution of a linear program whose cost is\n",
            "prohibitive whenever the histograms' dimension exceeds a few hundreds. We\n",
            "propose in this work a new family of optimal transportation distances that look\n",
            "at transportation problems from a maximum-entropy perspective. We smooth the\n",
            "classical optimal transportation problem with an entropic regularization term,\n",
            "and show that the resulting optimum is also a distance which can be computed\n",
            "through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several\n",
            "orders of magnitude faster than that of transportation solvers. We also report\n",
            "improved performance over classical optimal transportation distances on the\n",
            "MNIST benchmark problem.\n",
            "Actual Title : \n",
            "Sinkhorn Distances: Lightspeed Computation of Optimal Transportation\n",
            "  Distances\n",
            "Generated Title : \n",
            "distances  computation computation\n",
            "Abstract : \n",
            "Many real-world reinforcement learning problems have a hierarchical nature,\n",
            "and often exhibit some degree of partial observability. While hierarchy and\n",
            "partial observability are usually tackled separately (for instance by combining\n",
            "recurrent neural networks and options), we show that addressing both problems\n",
            "simultaneously is simpler and more efficient in many cases. More specifically,\n",
            "we make the initiation set of options conditional on the previously-executed\n",
            "option, and show that options with such Option-Observation Initiation Sets\n",
            "(OOIs) are at least as expressive as Finite State Controllers (FSCs), a\n",
            "state-of-the-art approach for learning in POMDPs. OOIs are easy to design based\n",
            "on an intuitive description of the task, lead to explainable policies and keep\n",
            "the top-level and option policies memoryless. Our experiments show that OOIs\n",
            "allow agents to learn optimal policies in challenging POMDPs, while being much\n",
            "more sample-efficient than a recurrent neural network over options.\n",
            "Actual Title : \n",
            "Reinforcement Learning in POMDPs with Memoryless Options and\n",
            "  Option-Observation Initiation Sets\n",
            "Generated Title : \n",
            "to reinforcement in reinforcement learning\n",
            "Abstract : \n",
            "Stein kernel has recently shown promising performance on classifying images\n",
            "represented by symmetric positive definite (SPD) matrices. It evaluates the\n",
            "similarity between two SPD matrices through their eigenvalues. In this paper,\n",
            "we argue that directly using the original eigenvalues may be problematic\n",
            "because: i) Eigenvalue estimation becomes biased when the number of samples is\n",
            "inadequate, which may lead to unreliable kernel evaluation; ii) More\n",
            "importantly, eigenvalues only reflect the property of an individual SPD matrix.\n",
            "They are not necessarily optimal for computing Stein kernel when the goal is to\n",
            "discriminate different sets of SPD matrices. To address the two issues in one\n",
            "shot, we propose a discriminative Stein kernel, in which an extra parameter\n",
            "vector is defined to adjust the eigenvalues of the input SPD matrices. The\n",
            "optimal parameter values are sought by optimizing a proxy of classification\n",
            "performance. To show the generality of the proposed method, three different\n",
            "kernel learning criteria that are commonly used in the literature are employed\n",
            "respectively as a proxy. A comprehensive experimental study is conducted on a\n",
            "variety of image classification tasks to compare our proposed discriminative\n",
            "Stein kernel with the original Stein kernel and other commonly used methods for\n",
            "evaluating the similarity between SPD matrices. The experimental results\n",
            "demonstrate that, the discriminative Stein kernel can attain greater\n",
            "discrimination and better align with classification tasks by altering the\n",
            "eigenvalues. This makes it produce higher classification performance than the\n",
            "original Stein kernel and other commonly used methods.\n",
            "Actual Title : \n",
            "Learning Discriminative Stein Kernel for SPD Matrices and Its\n",
            "  Applications\n",
            "Generated Title : \n",
            "discriminative stein kernel for spd classification\n",
            "Abstract : \n",
            "In just three years, Variational Autoencoders (VAEs) have emerged as one of\n",
            "the most popular approaches to unsupervised learning of complicated\n",
            "distributions. VAEs are appealing because they are built on top of standard\n",
            "function approximators (neural networks), and can be trained with stochastic\n",
            "gradient descent. VAEs have already shown promise in generating many kinds of\n",
            "complicated data, including handwritten digits, faces, house numbers, CIFAR\n",
            "images, physical models of scenes, segmentation, and predicting the future from\n",
            "static images. This tutorial introduces the intuitions behind VAEs, explains\n",
            "the mathematics behind them, and describes some empirical behavior. No prior\n",
            "knowledge of variational Bayesian methods is assumed.\n",
            "Actual Title : \n",
            "Tutorial on Variational Autoencoders\n",
            "Generated Title : \n",
            "on variational autoencoders\n",
            "Abstract : \n",
            "While there is currently a lot of enthusiasm about \"big data\", useful data is\n",
            "usually \"small\" and expensive to acquire. In this paper, we present a new\n",
            "paradigm of learning partial differential equations from {\\em small} data. In\n",
            "particular, we introduce \\emph{hidden physics models}, which are essentially\n",
            "data-efficient learning machines capable of leveraging the underlying laws of\n",
            "physics, expressed by time dependent and nonlinear partial differential\n",
            "equations, to extract patterns from high-dimensional data generated from\n",
            "experiments. The proposed methodology may be applied to the problem of\n",
            "learning, system identification, or data-driven discovery of partial\n",
            "differential equations. Our framework relies on Gaussian processes, a powerful\n",
            "tool for probabilistic inference over functions, that enables us to strike a\n",
            "balance between model complexity and data fitting. The effectiveness of the\n",
            "proposed approach is demonstrated through a variety of canonical problems,\n",
            "spanning a number of scientific domains, including the Navier-Stokes,\n",
            "Schr\\\"odinger, Kuramoto-Sivashinsky, and time dependent linear fractional\n",
            "equations. The methodology provides a promising new direction for harnessing\n",
            "the long-standing developments of classical methods in applied mathematics and\n",
            "mathematical physics to design learning machines with the ability to operate in\n",
            "complex domains without requiring large quantities of data.\n",
            "Actual Title : \n",
            "Hidden Physics Models: Machine Learning of Nonlinear Partial\n",
            "  Differential Equations\n",
            "Generated Title : \n",
            "physics models  partial partial partial differential equations\n",
            "Abstract : \n",
            "Effective debugging of ontologies is an important prerequisite for their\n",
            "broad application, especially in areas that rely on everyday users to create\n",
            "and maintain knowledge bases, such as the Semantic Web. In such systems\n",
            "ontologies capture formalized vocabularies of terms shared by its users.\n",
            "However in many cases users have different local views of the domain, i.e. of\n",
            "the context in which a given term is used. Inappropriate usage of terms\n",
            "together with natural complications when formulating and understanding logical\n",
            "descriptions may result in faulty ontologies. Recent ontology debugging\n",
            "approaches use diagnosis methods to identify causes of the faults. In most\n",
            "debugging scenarios these methods return many alternative diagnoses, thus\n",
            "placing the burden of fault localization on the user. This paper demonstrates\n",
            "how the target diagnosis can be identified by performing a sequence of\n",
            "observations, that is, by querying an oracle about entailments of the target\n",
            "ontology. To identify the best query we propose two query selection strategies:\n",
            "a simple \"split-in-half\" strategy and an entropy-based strategy. The latter\n",
            "allows knowledge about typical user errors to be exploited to minimize the\n",
            "number of queries. Our evaluation showed that the entropy-based method\n",
            "significantly reduces the number of required queries compared to the\n",
            "\"split-in-half\" approach. We experimented with different probability\n",
            "distributions of user errors and different qualities of the a-priori\n",
            "probabilities. Our measurements demonstrated the superiority of entropy-based\n",
            "query selection even in cases where all fault probabilities are equal, i.e.\n",
            "where no information about typical user errors is available.\n",
            "Actual Title : \n",
            "Interactive ontology debugging: two query strategies for efficient fault\n",
            "  localization\n",
            "Generated Title : \n",
            "<unk> approach for finding multiple in in a\n",
            "Abstract : \n",
            "Special technologies need to be used to take advantage of, and overcome, the\n",
            "challenges associated with acquiring, transforming, storing, processing, and\n",
            "distributing spoken language resources in organisations. This paper introduces\n",
            "an application architecture consisting of tools and supporting utilities for\n",
            "indexing and transcription, and describes how these tools, together with\n",
            "downstream processing and distribution systems, can be integrated into a\n",
            "workflow. Two sample applications for this architecture are outlined- the\n",
            "analysis of decision-making processes in organisations and the deployment of\n",
            "systems development methods by designers in the field.\n",
            "Actual Title : \n",
            "Application Architecture for Spoken Language Resources in Organisational\n",
            "  Settings\n",
            "Generated Title : \n",
            "of spoken language resources for organisational settings\n",
            "Abstract : \n",
            "This is full length article (draft version) where problem number of topics in\n",
            "Topic Modeling is discussed. We proposed idea that Renyi and Tsallis entropy\n",
            "can be used for identification of optimal number in large textual collections.\n",
            "We also report results of numerical experiments of Semantic stability for 4\n",
            "topic models, which shows that semantic stability play very important role in\n",
            "problem topic number. The calculation of Renyi and Tsallis entropy based on\n",
            "thermodynamics approach.\n",
            "Actual Title : \n",
            "Application of Rnyi and Tsallis Entropies to Topic Modeling\n",
            "  Optimization\n",
            "Generated Title : \n",
            "entropies and <unk> entropies to topic modeling\n",
            "Abstract : \n",
            "Many efforts have been made to use various forms of domain knowledge in\n",
            "malware detection. Currently there exist two common approaches to malware\n",
            "detection without domain knowledge, namely byte n-grams and strings. In this\n",
            "work we explore the feasibility of applying neural networks to malware\n",
            "detection and feature learning. We do this by restricting ourselves to a\n",
            "minimal amount of domain knowledge in order to extract a portion of the\n",
            "Portable Executable (PE) header. By doing this we show that neural networks can\n",
            "learn from raw bytes without explicit feature construction, and perform even\n",
            "better than a domain knowledge approach that parses the PE header into explicit\n",
            "features.\n",
            "Actual Title : \n",
            "Learning the PE Header, Malware Detection with Minimal Domain Knowledge\n",
            "Generated Title : \n",
            "neural pe malware detection\n",
            "Abstract : \n",
            "We present examples where the use of belief functions provided sound and\n",
            "elegant solutions to real life problems. These are essentially characterized by\n",
            "?missing' information. The examples deal with 1) discriminant analysis using a\n",
            "learning set where classes are only partially known; 2) an information\n",
            "retrieval systems handling inter-documents relationships; 3) the combination of\n",
            "data from sensors competent on partially overlapping frames; 4) the\n",
            "determination of the number of sources in a multi-sensor environment by\n",
            "studying the inter-sensors contradiction. The purpose of the paper is to report\n",
            "on such applications where the use of belief functions provides a convenient\n",
            "tool to handle ?messy' data problems.\n",
            "Actual Title : \n",
            "Practical Uses of Belief Functions\n",
            "Generated Title : \n",
            "uses of belief functions\n",
            "Abstract : \n",
            "We present an efficient method for training slack-rescaled structural SVM.\n",
            "Although finding the most violating label in a margin-rescaled formulation is\n",
            "often easy since the target function decomposes with respect to the structure,\n",
            "this is not the case for a slack-rescaled formulation, and finding the most\n",
            "violated label might be very difficult. Our core contribution is an efficient\n",
            "method for finding the most-violating-label in a slack-rescaled formulation,\n",
            "given an oracle that returns the most-violating-label in a (slightly modified)\n",
            "margin-rescaled formulation. We show that our method enables accurate and\n",
            "scalable training for slack-rescaled SVMs, reducing runtime by an order of\n",
            "magnitude compared to previous approaches to slack-rescaled SVMs.\n",
            "Actual Title : \n",
            "Fast and Scalable Structural SVM with Slack Rescaling\n",
            "Generated Title : \n",
            "and efficient fast scalable svm for scalable structural\n",
            "Abstract : \n",
            "We propose a learning setting in which unlabeled data is free, and the cost\n",
            "of a label depends on its value, which is not known in advance. We study binary\n",
            "classification in an extreme case, where the algorithm only pays for negative\n",
            "labels. Our motivation are applications such as fraud detection, in which\n",
            "investigating an honest transaction should be avoided if possible. We term the\n",
            "setting auditing, and consider the auditing complexity of an algorithm: the\n",
            "number of negative labels the algorithm requires in order to learn a hypothesis\n",
            "with low relative error. We design auditing algorithms for simple hypothesis\n",
            "classes (thresholds and rectangles), and show that with these algorithms, the\n",
            "auditing complexity can be significantly lower than the active label\n",
            "complexity. We also discuss a general competitive approach for auditing and\n",
            "possible modifications to the framework.\n",
            "Actual Title : \n",
            "Auditing: Active Learning with Outcome-Dependent Query Costs\n",
            "Generated Title : \n",
            "active learning via active costs\n",
            "Abstract : \n",
            "After the incredible success of deep learning in the computer vision domain,\n",
            "there has been much interest in applying Convolutional Network (ConvNet)\n",
            "features in robotic fields such as visual navigation and SLAM. Unfortunately,\n",
            "there are fundamental differences and challenges involved. Computer vision\n",
            "datasets are very different in character to robotic camera data, real-time\n",
            "performance is essential, and performance priorities can be different. This\n",
            "paper comprehensively evaluates and compares the utility of three\n",
            "state-of-the-art ConvNets on the problems of particular relevance to navigation\n",
            "for robots; viewpoint-invariance and condition-invariance, and for the first\n",
            "time enables real-time place recognition performance using ConvNets with large\n",
            "maps by integrating a variety of existing (locality-sensitive hashing) and\n",
            "novel (semantic search space partitioning) optimization techniques. We present\n",
            "extensive experiments on four real world datasets cultivated to evaluate each\n",
            "of the specific challenges in place recognition. The results demonstrate that\n",
            "speed-ups of two orders of magnitude can be achieved with minimal accuracy\n",
            "degradation, enabling real-time performance. We confirm that networks trained\n",
            "for semantic place categorization also perform better at (specific) place\n",
            "recognition when faced with severe appearance changes and provide a reference\n",
            "for which networks and layers are optimal for different aspects of the place\n",
            "recognition problem.\n",
            "Actual Title : \n",
            "On the Performance of ConvNet Features for Place Recognition\n",
            "Generated Title : \n",
            "<unk> <unk> self supervised <unk> vision\n",
            "Abstract : \n",
            "In this work, we propose a new segmentation algorithm for images containing\n",
            "convex objects present in multiple shapes with a high degree of overlap. The\n",
            "proposed algorithm is carried out in two steps, first we identify the visible\n",
            "contours, segment them using concave points and finally group the segments\n",
            "belonging to the same object. The next step is to assign a shape identity to\n",
            "these grouped contour segments. For images containing objects in multiple\n",
            "shapes we begin first by identifying shape classes of the contours followed by\n",
            "assigning a shape entity to these classes. We provide a comprehensive\n",
            "experimentation of our algorithm on two crystal image datasets. One dataset\n",
            "comprises of images containing objects in multiple shapes overlapping each\n",
            "other and the other dataset contains standard images with objects present in a\n",
            "single shape. We test our algorithm against two baselines, with our proposed\n",
            "algorithm outperforming both the baselines.\n",
            "Actual Title : \n",
            "Image Segmentation of Multi-Shaped Overlapping Objects\n",
            "Generated Title : \n",
            "a convex approach to segment\n",
            "Abstract : \n",
            "Many logic programming based approaches can be used to describe and solve\n",
            "combinatorial search problems. On the one hand there is constraint logic\n",
            "programming which computes a solution as an answer substitution to a query\n",
            "containing the variables of the constraint satisfaction problem. On the other\n",
            "hand there are systems based on stable model semantics, abductive systems, and\n",
            "first order logic model generators which compute solutions as models of some\n",
            "theory. This paper compares these different approaches from the point of view\n",
            "of knowledge representation (how declarative are the programs) and from the\n",
            "point of view of performance (how good are they at solving typical problems).\n",
            "Actual Title : \n",
            "Logic Programming Approaches for Representing and Solving Constraint\n",
            "  Satisfaction Problems: A Comparison\n",
            "Generated Title : \n",
            "complexity of constraint programming and solving constraint programming\n",
            "Abstract : \n",
            "Unmanned aerial vehicles (UAV) are evolving as an alternative tool to acquire\n",
            "land tenure data. UAVs can capture geospatial data at high quality and\n",
            "resolution in a cost-effective, transparent and flexible manner, from which\n",
            "visible land parcel boundaries, i.e., cadastral boundaries are delineable. This\n",
            "delineation is to no extent automated, even though physical objects\n",
            "automatically retrievable through image analysis methods mark a large portion\n",
            "of cadastral boundaries. This study proposes (i) a workflow that automatically\n",
            "extracts candidate cadastral boundaries from UAV orthoimages and (ii) a tool\n",
            "for their semi-automatic processing to delineate final cadastral boundaries.\n",
            "The workflow consists of two state-of-the-art computer vision methods, namely\n",
            "gPb contour detection and SLIC superpixels that are transferred to remote\n",
            "sensing in this study. The tool combines the two methods, allows a\n",
            "semi-automatic final delineation and is implemented as a publicly available\n",
            "QGIS plugin. The approach does not yet aim to provide a comparable alternative\n",
            "to manual cadastral mapping procedures. However, the methodological development\n",
            "of the tool towards this goal is developed in this paper. A study with 13\n",
            "volunteers investigates the design and implementation of the approach and\n",
            "gathers initial qualitative as well as quantitate results. The study revealed\n",
            "points for improvement, which are prioritized based on the study results and\n",
            "which will be addressed in future work.\n",
            "Actual Title : \n",
            "Towards Automated Cadastral Boundary Delineation from UAV Data\n",
            "Generated Title : \n",
            "automated cadastral boundary delineation from uav data\n",
            "Abstract : \n",
            "One of the most challenging problems in kernel online learning is to bound\n",
            "the model size and to promote the model sparsity. Sparse models not only\n",
            "improve computation and memory usage, but also enhance the generalization\n",
            "capacity, a principle that concurs with the law of parsimony. However,\n",
            "inappropriate sparsity modeling may also significantly degrade the performance.\n",
            "In this paper, we propose Approximation Vector Machine (AVM), a model that can\n",
            "simultaneously encourage the sparsity and safeguard its risk in compromising\n",
            "the performance. When an incoming instance arrives, we approximate this\n",
            "instance by one of its neighbors whose distance to it is less than a predefined\n",
            "threshold. Our key intuition is that since the newly seen instance is expressed\n",
            "by its nearby neighbor the optimal performance can be analytically formulated\n",
            "and maintained. We develop theoretical foundations to support this intuition\n",
            "and further establish an analysis to characterize the gap between the\n",
            "approximation and optimal solutions. This gap crucially depends on the\n",
            "frequency of approximation and the predefined threshold. We perform the\n",
            "convergence analysis for a wide spectrum of loss functions including Hinge,\n",
            "smooth Hinge, and Logistic for classification task, and $l_1$, $l_2$, and\n",
            "$\\epsilon$-insensitive for regression task. We conducted extensive experiments\n",
            "for classification task in batch and online modes, and regression task in\n",
            "online mode over several benchmark datasets. The results show that our proposed\n",
            "AVM achieved a comparable predictive performance with current state-of-the-art\n",
            "methods while simultaneously achieving significant computational speed-up due\n",
            "to the ability of the proposed AVM in maintaining the model size.\n",
            "Actual Title : \n",
            "Approximation Vector Machines for Large-scale Online Learning\n",
            "Generated Title : \n",
            "online for online large scale online learning\n",
            "Abstract : \n",
            "Natural disasters can have catastrophic impacts on the functionality of\n",
            "infrastructure systems and cause severe physical and socio-economic losses.\n",
            "Given budget constraints, it is crucial to optimize decisions regarding\n",
            "mitigation, preparedness, response, and recovery practices for these systems.\n",
            "This requires accurate and efficient means to evaluate the infrastructure\n",
            "system reliability. While numerous research efforts have addressed and\n",
            "quantified the impact of natural disasters on infrastructure systems, typically\n",
            "using the Monte Carlo approach, they still suffer from high computational cost\n",
            "and, thus, are of limited applicability to large systems. This paper presents a\n",
            "deep learning framework for accelerating infrastructure system reliability\n",
            "analysis. In particular, two distinct deep neural network surrogates are\n",
            "constructed and studied: (1) A classifier surrogate which speeds up the\n",
            "connectivity determination of networks, and (2) An end-to-end surrogate that\n",
            "replaces a number of components such as roadway status realization,\n",
            "connectivity determination, and connectivity averaging. The proposed approach\n",
            "is applied to a simulation-based study of the two-terminal connectivity of a\n",
            "California transportation network subject to extreme probabilistic earthquake\n",
            "events. Numerical results highlight the effectiveness of the proposed approach\n",
            "in accelerating the transportation system two-terminal reliability analysis\n",
            "with extremely high prediction accuracy.\n",
            "Actual Title : \n",
            "Deep Learning for Accelerated Reliability Analysis of Infrastructure\n",
            "  Networks\n",
            "Generated Title : \n",
            "learning of deep network reliability analysis\n",
            "Abstract : \n",
            "Scene text recognition has been a hot research topic in computer vision due\n",
            "to its various applications. The state of the art is the attention-based\n",
            "encoder-decoder framework that learns the mapping between input images and\n",
            "output sequences in a purely data-driven way. However, we observe that existing\n",
            "attention-based methods perform poorly on complicated and/or low-quality\n",
            "images. One major reason is that existing methods cannot get accurate\n",
            "alignments between feature areas and targets for such images. We call this\n",
            "phenomenon \"attention drift\". To tackle this problem, in this paper we propose\n",
            "the FAN (the abbreviation of Focusing Attention Network) method that employs a\n",
            "focusing attention mechanism to automatically draw back the drifted attention.\n",
            "FAN consists of two major components: an attention network (AN) that is\n",
            "responsible for recognizing character targets as in the existing methods, and a\n",
            "focusing network (FN) that is responsible for adjusting attention by evaluating\n",
            "whether AN pays attention properly on the target areas in the images.\n",
            "Furthermore, different from the existing methods, we adopt a ResNet-based\n",
            "network to enrich deep representations of scene text images. Extensive\n",
            "experiments on various benchmarks, including the IIIT5k, SVT and ICDAR\n",
            "datasets, show that the FAN method substantially outperforms the existing\n",
            "methods.\n",
            "Actual Title : \n",
            "Focusing Attention: Towards Accurate Text Recognition in Natural Images\n",
            "Generated Title : \n",
            "attention  towards accurate visual recognition in image wild\n",
            "Abstract : \n",
            "Previous works demonstrated that Automatic Text Summarization (ATS) by\n",
            "sentences extraction may be improved using sentence compression. In this work\n",
            "we present a sentence compressions approach guided by level-sentence discourse\n",
            "segmentation and probabilistic language models (LM). The results presented here\n",
            "show that the proposed solution is able to generate coherent summaries with\n",
            "grammatical compressed sentences. The approach is simple enough to be\n",
            "transposed into other languages.\n",
            "Actual Title : \n",
            "Sentence Compression in Spanish driven by Discourse Segmentation and\n",
            "  Language Models\n",
            "Generated Title : \n",
            "sentence compression for discourse discourse\n",
            "Abstract : \n",
            "We present an unusual algorithm involving classification trees where two\n",
            "trees are grown in opposite directions so that they are matched at their\n",
            "leaves. This approach finds application in a new data mining task we formulate,\n",
            "called \"redescription mining\". A redescription is a shift-of-vocabulary, or a\n",
            "different way of communicating information about a given subset of data; the\n",
            "goal of redescription mining is to find subsets of data that afford multiple\n",
            "descriptions. We highlight the importance of this problem in domains such as\n",
            "bioinformatics, which exhibit an underlying richness and diversity of data\n",
            "descriptors (e.g., genes can be studied in a variety of ways). Our approach\n",
            "helps integrate multiple forms of characterizing datasets, situates the\n",
            "knowledge gained from one dataset in the context of others, and harnesses\n",
            "high-level abstractions for uncovering cryptic and subtle features of data.\n",
            "Algorithm design decisions, implementation details, and experimental results\n",
            "are presented.\n",
            "Actual Title : \n",
            "Turning CARTwheels: An Alternating Algorithm for Mining Redescriptions\n",
            "Generated Title : \n",
            "<unk> alternating alternating with\n",
            "Abstract : \n",
            "Multiresolution analysis and matrix factorization are foundational tools in\n",
            "computer vision. In this work, we study the interface between these two\n",
            "distinct topics and obtain techniques to uncover hierarchical block structure\n",
            "in symmetric matrices -- an important aspect in the success of many vision\n",
            "problems. Our new algorithm, the incremental multiresolution matrix\n",
            "factorization, uncovers such structure one feature at a time, and hence scales\n",
            "well to large matrices. We describe how this multiscale analysis goes much\n",
            "farther than what a direct global factorization of the data can identify. We\n",
            "evaluate the efficacy of the resulting factorizations for relative leveraging\n",
            "within regression tasks using medical imaging data. We also use the\n",
            "factorization on representations learned by popular deep networks, providing\n",
            "evidence of their ability to infer semantic relationships even when they are\n",
            "not explicitly trained to do so. We show that this algorithm can be used as an\n",
            "exploratory tool to improve the network architecture, and within numerous other\n",
            "settings in vision.\n",
            "Actual Title : \n",
            "The Incremental Multiresolution Matrix Factorization Algorithm\n",
            "Generated Title : \n",
            "matrix factorization with non negative matrix factorization\n",
            "Abstract : \n",
            "We exploit the versatile framework of Riemannian optimization on quotient\n",
            "manifolds to develop R3MC, a nonlinear conjugate-gradient method for low-rank\n",
            "matrix completion. The underlying search space of fixed-rank matrices is\n",
            "endowed with a novel Riemannian metric that is tailored to the least-squares\n",
            "cost. Numerical comparisons suggest that R3MC robustly outperforms\n",
            "state-of-the-art algorithms across different problem instances, especially\n",
            "those that combine scarcely sampled and ill-conditioned data.\n",
            "Actual Title : \n",
            "R3MC: A Riemannian three-factor algorithm for low-rank matrix completion\n",
            "Generated Title : \n",
            "a riemannian <unk> framework for low rank matrix completion\n",
            "Abstract : \n",
            "This paper describes algorithms for nonnegative matrix factorization (NMF)\n",
            "with the beta-divergence (beta-NMF). The beta-divergence is a family of cost\n",
            "functions parametrized by a single shape parameter beta that takes the\n",
            "Euclidean distance, the Kullback-Leibler divergence and the Itakura-Saito\n",
            "divergence as special cases (beta = 2,1,0, respectively). The proposed\n",
            "algorithms are based on a surrogate auxiliary function (a local majorization of\n",
            "the criterion function). We first describe a majorization-minimization (MM)\n",
            "algorithm that leads to multiplicative updates, which differ from standard\n",
            "heuristic multiplicative updates by a beta-dependent power exponent. The\n",
            "monotonicity of the heuristic algorithm can however be proven for beta in (0,1)\n",
            "using the proposed auxiliary function. Then we introduce the concept of\n",
            "majorization-equalization (ME) algorithm which produces updates that move along\n",
            "constant level sets of the auxiliary function and lead to larger steps than MM.\n",
            "Simulations on synthetic and real data illustrate the faster convergence of the\n",
            "ME approach. The paper also describes how the proposed algorithms can be\n",
            "adapted to two common variants of NMF : penalized NMF (i.e., when a penalty\n",
            "function of the factors is added to the criterion function) and convex-NMF\n",
            "(when the dictionary is assumed to belong to a known subspace).\n",
            "Actual Title : \n",
            "Algorithms for nonnegative matrix factorization with the beta-divergence\n",
            "Generated Title : \n",
            "algorithms for the beta divergence\n",
            "Abstract : \n",
            "Dealing with datasets of very high dimension is a major challenge in machine\n",
            "learning. In this paper, we consider the problem of feature selection in\n",
            "applications where the memory is not large enough to contain all features. In\n",
            "this setting, we propose a novel tree-based feature selection approach that\n",
            "builds a sequence of randomized trees on small subsamples of variables mixing\n",
            "both variables already identified as relevant by previous models and variables\n",
            "randomly selected among the other variables. As our main contribution, we\n",
            "provide an in-depth theoretical analysis of this method in infinite sample\n",
            "setting. In particular, we study its soundness with respect to common\n",
            "definitions of feature relevance and its convergence speed under various\n",
            "variable dependance scenarios. We also provide some preliminary empirical\n",
            "results highlighting the potential of the approach.\n",
            "Actual Title : \n",
            "Random Subspace with Trees for Feature Selection Under Memory\n",
            "  Constraints\n",
            "Generated Title : \n",
            "selection with trees\n",
            "Abstract : \n",
            "Collective intelligence, which aggregates the shared information from large\n",
            "crowds, is often negatively impacted by unreliable information sources with the\n",
            "low quality data. This becomes a barrier to the effective use of collective\n",
            "intelligence in a variety of applications. In order to address this issue, we\n",
            "propose a probabilistic model to jointly assess the reliability of sources and\n",
            "find the true data. We observe that different sources are often not independent\n",
            "of each other. Instead, sources are prone to be mutually influenced, which\n",
            "makes them dependent when sharing information with each other. High dependency\n",
            "between sources makes collective intelligence vulnerable to the overuse of\n",
            "redundant (and possibly incorrect) information from the dependent sources.\n",
            "Thus, we reveal the latent group structure among dependent sources, and\n",
            "aggregate the information at the group level rather than from individual\n",
            "sources directly. This can prevent the collective intelligence from being\n",
            "inappropriately dominated by dependent sources. We will also explicitly reveal\n",
            "the reliability of groups, and minimize the negative impacts of unreliable\n",
            "groups. Experimental results on real-world data sets show the effectiveness of\n",
            "the proposed approach with respect to existing algorithms.\n",
            "Actual Title : \n",
            "Learning from Collective Intelligence in Groups\n",
            "Generated Title : \n",
            "<unk> collective intelligence\n",
            "Abstract : \n",
            "This is the second part of a paper on Conscious Intelligent Systems. We use\n",
            "the understanding gained in the first part (Conscious Intelligent Systems Part\n",
            "1: IXI (arxiv id cs.AI/0612056)) to look at understanding. We see how the\n",
            "presence of mind affects understanding and intelligent systems; we see that the\n",
            "presence of mind necessitates language. The rise of language in turn has\n",
            "important effects on understanding. We discuss the humanoid question and how\n",
            "the question of self-consciousness (and by association mind/thought/language)\n",
            "would affect humanoids too.\n",
            "Actual Title : \n",
            "Conscious Intelligent Systems - Part II - Mind, Thought, Language and\n",
            "  Understanding\n",
            "Generated Title : \n",
            "intelligent systems   part ii   mind  thought \n",
            "Abstract : \n",
            "Current methods for automatically evaluating grammatical error correction\n",
            "(GEC) systems rely on gold-standard references. However, these methods suffer\n",
            "from penalizing grammatical edits that are correct but not in the gold\n",
            "standard. We show that reference-less grammaticality metrics correlate very\n",
            "strongly with human judgments and are competitive with the leading\n",
            "reference-based evaluation metrics. By interpolating both methods, we achieve\n",
            "state-of-the-art correlation with human judgments. Finally, we show that GEC\n",
            "metrics are much more reliable when they are calculated at the sentence level\n",
            "instead of the corpus level. We have set up a CodaLab site for benchmarking GEC\n",
            "output using a common dataset and different evaluation metrics.\n",
            "Actual Title : \n",
            "There's No Comparison: Reference-less Evaluation Metrics in Grammatical\n",
            "  Error Correction\n",
            "Generated Title : \n",
            "and comparison  metrics metrics metrics for grammatical error\n",
            "Abstract : \n",
            "This paper presented our work on applying Recurrent Deep Stacking Networks\n",
            "(RDSNs) to Robust Automatic Speech Recognition (ASR) tasks. In the paper, we\n",
            "also proposed a more efficient yet comparable substitute to RDSN, Bi- Pass\n",
            "Stacking Network (BPSN). The main idea of these two models is to add\n",
            "phoneme-level information into acoustic models, transforming an acoustic model\n",
            "to the combination of an acoustic model and a phoneme-level N-gram model.\n",
            "Experiments showed that RDSN and BPsn can substantially improve the\n",
            "performances over conventional DNNs.\n",
            "Actual Title : \n",
            "Recurrent Deep Stacking Networks for Speech Recognition\n",
            "Generated Title : \n",
            "deep stacking with <unk> speech recognition\n",
            "Abstract : \n",
            "In a recent paper, we presented an intelligent evolutionary search technique\n",
            "through genetic programming (GP) for finding new analytical expressions of\n",
            "nonlinear dynamical systems, similar to the classical Lorenz attractor's which\n",
            "also exhibit chaotic behaviour in the phase space. In this paper, we extend our\n",
            "previous finding to explore yet another gallery of new chaotic attractors which\n",
            "are derived from the original Lorenz system of equations. Compared to the\n",
            "previous exploration with sinusoidal type transcendental nonlinearity, here we\n",
            "focus on only cross-product and higher-power type nonlinearities in the three\n",
            "state equations. We here report over 150 different structures of chaotic\n",
            "attractors along with their one set of parameter values, phase space dynamics\n",
            "and the Largest Lyapunov Exponents (LLE). The expressions of these new\n",
            "Lorenz-like nonlinear dynamical systems have been automatically evolved through\n",
            "multi-gene genetic programming (MGGP). In the past two decades, there have been\n",
            "many claims of designing new chaotic attractors as an incremental extension of\n",
            "the Lorenz family. We provide here a large family of chaotic systems whose\n",
            "structure closely resemble the original Lorenz system but with drastically\n",
            "different phase space dynamics. This advances the state of the art knowledge of\n",
            "discovering new chaotic systems which can find application in many real-world\n",
            "problems. This work may also find its archival value in future in the domain of\n",
            "new chaotic system discovery.\n",
            "Actual Title : \n",
            "Evolving Chaos: Identifying New Attractors of the Generalised Lorenz\n",
            "  Family\n",
            "Generated Title : \n",
            "chaos  in a new system system\n",
            "Abstract : \n",
            "Trust is a fundamental concept in many real-world applications such as\n",
            "e-commerce and peer-to-peer networks. In these applications, users can generate\n",
            "local opinions about the counterparts based on direct experiences, and these\n",
            "opinions can then be aggregated to build trust among unknown users. The\n",
            "mechanism to build new trust relationships based on existing ones is referred\n",
            "to as trust inference. State-of-the-art trust inference approaches employ the\n",
            "transitivity property of trust by propagating trust along connected users. In\n",
            "this paper, we propose a novel trust inference model (MaTrust) by exploring an\n",
            "equally important property of trust, i.e., the multi-aspect property. MaTrust\n",
            "directly characterizes multiple latent factors for each trustor and trustee\n",
            "from the locally-generated trust relationships. Furthermore, it can naturally\n",
            "incorporate prior knowledge as specified factors. These factors in turn serve\n",
            "as the basis to infer the unseen trustworthiness scores. Experimental\n",
            "evaluations on real data sets show that the proposed MaTrust significantly\n",
            "outperforms several benchmark trust inference models in both effectiveness and\n",
            "efficiency.\n",
            "Actual Title : \n",
            "MaTrust: An Effective Multi-Aspect Trust Inference Model\n",
            "Generated Title : \n",
            "a reliable multi aspect trust inference\n",
            "Abstract : \n",
            "In this paper, we are concerned with obtaining distribution-free\n",
            "concentration inequalities for mixture of independent Bernoulli variables that\n",
            "incorporate a notion of variance. Missing mass is the total probability mass\n",
            "associated to the outcomes that have not been seen in a given sample which is\n",
            "an important quantity that connects density estimates obtained from a sample to\n",
            "the population for discrete distributions. Therefore, we are specifically\n",
            "motivated to apply our method to study the concentration of missing mass -\n",
            "which can be expressed as a mixture of Bernoulli - in a novel way.\n",
            "  We not only derive - for the first time - Bernstein-like large deviation\n",
            "bounds for the missing mass whose exponents behave almost linearly with respect\n",
            "to deviation size, but also sharpen McAllester and Ortiz (2003) and Berend and\n",
            "Kontorovich (2013) for large sample sizes in the case of small deviations which\n",
            "is the most interesting case in learning theory. In the meantime, our approach\n",
            "shows that the heterogeneity issue introduced in McAllester and Ortiz (2003) is\n",
            "resolvable in the case of missing mass in the sense that one can use standard\n",
            "inequalities but it may not lead to strong results. Thus, we postulate that our\n",
            "results are general and can be applied to provide potentially sharp\n",
            "Bernstein-like bounds under some constraints.\n",
            "Actual Title : \n",
            "Novel Deviation Bounds for Mixture of Independent Bernoulli Variables\n",
            "  with Application to the Missing Mass\n",
            "Generated Title : \n",
            "deviation of the mixture of mixture of the\n",
            "Abstract : \n",
            "In this paper, an online adaptive model-free tracker is proposed to track\n",
            "single objects in video sequences to deal with real-world tracking challenges\n",
            "like low-resolution, object deformation, occlusion and motion blur. The novelty\n",
            "lies in the construction of a strong appearance model that captures features\n",
            "from the initialized bounding box and then are assembled into anchor-point\n",
            "features. These features memorize the global pattern of the object and have an\n",
            "internal star graph-like structure. These features are unique and flexible and\n",
            "helps tracking generic and deformable objects with no limitation on specific\n",
            "objects. In addition, the relevance of each feature is evaluated online using\n",
            "short-term consistency and long-term consistency. These parameters are adapted\n",
            "to retain consistent features that vote for the object location and that deal\n",
            "with outliers for long-term tracking scenarios. Additionally, voting in a\n",
            "Gaussian manner helps in tackling inherent noise of the tracking system and in\n",
            "accurate object localization. Furthermore, the proposed tracker uses pairwise\n",
            "distance measure to cope with scale variations and combines pixel-level binary\n",
            "features and global weighted color features for model update. Finally,\n",
            "experimental results on a visual tracking benchmark dataset are presented to\n",
            "demonstrate the effectiveness and competitiveness of the proposed tracker.\n",
            "Actual Title : \n",
            "Tracking using Numerous Anchor points\n",
            "Generated Title : \n",
            "tracking tracking with anchor points\n",
            "Abstract : \n",
            "Many algorithms formulate graph matching as an optimization of an objective\n",
            "function of pairwise quantification of nodes and edges of two graphs to be\n",
            "matched. Pairwise measurements usually consider local attributes but disregard\n",
            "contextual information involved in graph structures. We address this issue by\n",
            "proposing contextual similarities between pairs of nodes. This is done by\n",
            "considering the tensor product graph (TPG) of two graphs to be matched, where\n",
            "each node is an ordered pair of nodes of the operand graphs. Contextual\n",
            "similarities between a pair of nodes are computed by accumulating weighted\n",
            "walks (normalized pairwise similarities) terminating at the corresponding\n",
            "paired node in TPG. Once the contextual similarities are obtained, we formulate\n",
            "subgraph matching as a node and edge selection problem in TPG. We use\n",
            "contextual similarities to construct an objective function and optimize it with\n",
            "a linear programming approach. Since random walk formulation through TPG takes\n",
            "into account higher order information, it is not a surprise that we obtain more\n",
            "reliable similarities and better discrimination among the nodes and edges.\n",
            "Experimental results shown on synthetic as well as real benchmarks illustrate\n",
            "that higher order contextual similarities add discriminating power and allow\n",
            "one to find approximate solutions to the subgraph matching problem.\n",
            "Actual Title : \n",
            "Product Graph-based Higher Order Contextual Similarities for Inexact\n",
            "  Subgraph Matching\n",
            "Generated Title : \n",
            "matching matching\n",
            "Abstract : \n",
            "Echo state networks (ESN), a type of reservoir computing (RC) architecture,\n",
            "are efficient and accurate artificial neural systems for time series processing\n",
            "and learning. An ESN consists of a core of recurrent neural networks, called a\n",
            "reservoir, with a small number of tunable parameters to generate a\n",
            "high-dimensional representation of an input, and a readout layer which is\n",
            "easily trained using regression to produce a desired output from the reservoir\n",
            "states. Certain computational tasks involve real-time calculation of high-order\n",
            "time correlations, which requires nonlinear transformation either in the\n",
            "reservoir or the readout layer. Traditional ESN employs a reservoir with\n",
            "sigmoid or tanh function neurons. In contrast, some types of biological neurons\n",
            "obey response curves that can be described as a product unit rather than a sum\n",
            "and threshold. Inspired by this class of neurons, we introduce a RC\n",
            "architecture with a reservoir of product nodes for time series computation. We\n",
            "find that the product RC shows many properties of standard ESN such as\n",
            "short-term memory and nonlinear capacity. On standard benchmarks for chaotic\n",
            "prediction tasks, the product RC maintains the performance of a standard\n",
            "nonlinear ESN while being more amenable to mathematical analysis. Our study\n",
            "provides evidence that such networks are powerful in highly nonlinear tasks\n",
            "owing to high-order statistics generated by the recurrent product node\n",
            "reservoir.\n",
            "Actual Title : \n",
            "Product Reservoir Computing: Time-Series Computation with Multiplicative\n",
            "  Neurons\n",
            "Generated Title : \n",
            "on the state networks\n",
            "Abstract : \n",
            "We present a novel deep architecture termed templateNet for depth based\n",
            "object instance recognition. Using an intermediate template layer we exploit\n",
            "prior knowledge of an object's shape to sparsify the feature maps. This has\n",
            "three advantages: (i) the network is better regularised resulting in structured\n",
            "filters; (ii) the sparse feature maps results in intuitive features been learnt\n",
            "which can be visualized as the output of the template layer and (iii) the\n",
            "resulting network achieves state-of-the-art performance. The network benefits\n",
            "from this without any additional parametrization from the template layer. We\n",
            "derive the weight updates needed to efficiently train this network in an\n",
            "end-to-end manner. We benchmark the templateNet for depth based object instance\n",
            "recognition using two publicly available datasets. The datasets present\n",
            "multiple challenges of clutter, large pose variations and similar looking\n",
            "distractors. Through our experiments we show that with the addition of a\n",
            "template layer, a depth based CNN is able to outperform existing\n",
            "state-of-the-art methods in the field.\n",
            "Actual Title : \n",
            "TemplateNet for Depth-Based Object Instance Recognition\n",
            "Generated Title : \n",
            "for depth based object instance recognition\n",
            "Abstract : \n",
            "This article proposes to auto-encode text at byte-level using convolutional\n",
            "networks with a recursive architecture. The motivation is to explore whether it\n",
            "is possible to have scalable and homogeneous text generation at byte-level in a\n",
            "non-sequential fashion through the simple task of auto-encoding. We show that\n",
            "non-sequential text generation from a fixed-length representation is not only\n",
            "possible, but also achieved much better auto-encoding results than recurrent\n",
            "networks. The proposed model is a multi-stage deep convolutional\n",
            "encoder-decoder framework using residual connections, containing up to 160\n",
            "parameterized layers. Each encoder or decoder contains a shared group of\n",
            "modules that consists of either pooling or upsampling layers, making the\n",
            "network recursive in terms of abstraction levels in representation. Results for\n",
            "6 large-scale paragraph datasets are reported, in 3 languages including Arabic,\n",
            "Chinese and English. Analyses are conducted to study several properties of the\n",
            "proposed model.\n",
            "Actual Title : \n",
            "Byte-Level Recursive Convolutional Auto-Encoder for Text\n",
            "Generated Title : \n",
            "recursive auto encoder auto encoder\n",
            "Abstract : \n",
            "Efficient modeling on uncertain information plays an important role in\n",
            "estimating the risk of contaminant intrusion in water distribution networks.\n",
            "Dempster-Shafer evidence theory is one of the most commonly used methods.\n",
            "However, the Dempster-Shafer evidence theory has some hypotheses including the\n",
            "exclusive property of the elements in the frame of discernment, which may not\n",
            "be consistent with the real world. In this paper, based on a more effective\n",
            "representation of uncertainty, called D numbers, a new method that allows the\n",
            "elements in the frame of discernment to be non-exclusive is proposed. To\n",
            "demonstrate the efficiency of the proposed method, we apply it to the water\n",
            "distribution networks to estimate the risk of contaminant intrusion.\n",
            "Actual Title : \n",
            "Modeling contaminant intrusion in water distribution networks based on D\n",
            "  numbers\n",
            "Generated Title : \n",
            "contaminant intrusion in water distribution networks by <unk>\n",
            "Abstract : \n",
            "We work out a classification scheme for quantum modeling in Hilbert space of\n",
            "any kind of composite entity violating Bell's inequalities and exhibiting\n",
            "entanglement. Our theoretical framework includes situations with entangled\n",
            "states and product measurements ('customary quantum situation'), and also\n",
            "situations with both entangled states and entangled measurements ('nonlocal box\n",
            "situation', 'nonlocal non-marginal box situation'). We show that entanglement\n",
            "is structurally a joint property of states and measurements. Furthermore,\n",
            "entangled measurements enable quantum modeling of situations that are usually\n",
            "believed to be 'beyond quantum'. Our results are also extended from pure states\n",
            "to quantum mixtures.\n",
            "Actual Title : \n",
            "General Quantum Hilbert Space Modeling Scheme for Entanglement\n",
            "Generated Title : \n",
            "quantum hilbert space with quantum hilbert space\n",
            "Abstract : \n",
            "Existing neural machine translation (NMT) models generally translate\n",
            "sentences in isolation, missing the opportunity to take advantage of\n",
            "document-level information. In this work, we propose to augment NMT models with\n",
            "a very light-weight cache-like memory network, which stores recent hidden\n",
            "representations as translation history. The probability distribution over\n",
            "generated words is updated online depending on the translation history\n",
            "retrieved from the memory, endowing NMT models with the capability to\n",
            "dynamically adapt over time. Experiments on multiple domains with different\n",
            "topics and styles show the effectiveness of the proposed approach with\n",
            "negligible impact on the computational cost.\n",
            "Actual Title : \n",
            "Learning to Remember Translation History with a Continuous Cache\n",
            "Generated Title : \n",
            "to remember translation history with a cache\n",
            "Abstract : \n",
            "The goal of this paper is to discover a set of discriminative patches which\n",
            "can serve as a fully unsupervised mid-level visual representation. The desired\n",
            "patches need to satisfy two requirements: 1) to be representative, they need to\n",
            "occur frequently enough in the visual world; 2) to be discriminative, they need\n",
            "to be different enough from the rest of the visual world. The patches could\n",
            "correspond to parts, objects, \"visual phrases\", etc. but are not restricted to\n",
            "be any one of them. We pose this as an unsupervised discriminative clustering\n",
            "problem on a huge dataset of image patches. We use an iterative procedure which\n",
            "alternates between clustering and training discriminative classifiers, while\n",
            "applying careful cross-validation at each step to prevent overfitting. The\n",
            "paper experimentally demonstrates the effectiveness of discriminative patches\n",
            "as an unsupervised mid-level visual representation, suggesting that it could be\n",
            "used in place of visual words for many tasks. Furthermore, discriminative\n",
            "patches can also be used in a supervised regime, such as scene classification,\n",
            "where they demonstrate state-of-the-art performance on the MIT Indoor-67\n",
            "dataset.\n",
            "Actual Title : \n",
            "Unsupervised Discovery of Mid-Level Discriminative Patches\n",
            "Generated Title : \n",
            "discovery of mid level discriminative patches\n",
            "Abstract : \n",
            "Classification is one of the most important tasks of machine learning.\n",
            "Although the most well studied model is the two-class problem, in many\n",
            "scenarios there is the opportunity to label critical items for manual revision,\n",
            "instead of trying to automatically classify every item. In this paper we adapt\n",
            "a paradigm initially proposed for the classification of ordinal data to address\n",
            "the classification problem with reject option. The technique reduces the\n",
            "problem of classifying with reject option to the standard two-class problem.\n",
            "The introduced method is then mapped into support vector machines and neural\n",
            "networks. Finally, the framework is extended to multiclass ordinal data with\n",
            "reject option. An experimental study with synthetic and real data sets,\n",
            "verifies the usefulness of the proposed approach.\n",
            "Actual Title : \n",
            "The Data Replication Method for the Classification with Reject Option\n",
            "Generated Title : \n",
            "data replication classification classification classification with reject reject\n",
            "Abstract : \n",
            "The aim of this paper is to propose an application of mutual\n",
            "information-based ensemble methods to the analysis and classification of heart\n",
            "beats associated with different types of Arrhythmia. Models of multilayer\n",
            "perceptrons, support vector machines, and radial basis function neural networks\n",
            "were trained and tested using the MIT-BIH arrhythmia database. This research\n",
            "brings a focus to an ensemble method that, to our knowledge, is a novel\n",
            "application in the area of ECG Arrhythmia detection. The proposed classifier\n",
            "ensemble method showed improved performance, relative to either majority voting\n",
            "classifier integration or to individual classifier performance. The overall\n",
            "ensemble accuracy was 98.25%.\n",
            "Actual Title : \n",
            "Arrhythmia Detection using Mutual Information-Based Integration Method\n",
            "Generated Title : \n",
            "detection using using arrhythmia detection and arrhythmia detection\n",
            "Abstract : \n",
            "The Outilex software platform, which will be made available to research,\n",
            "development and industry, comprises software components implementing all the\n",
            "fundamental operations of written text processing: processing without lexicons,\n",
            "exploitation of lexicons and grammars, language resource management. All data\n",
            "are structured in XML formats, and also in more compact formats, either\n",
            "readable or binary, whenever necessary; the required format converters are\n",
            "included in the platform; the grammar formats allow for combining statistical\n",
            "approaches with resource-based approaches. Manually constructed lexicons for\n",
            "French and English, originating from the LADL, and of substantial coverage,\n",
            "will be distributed with the platform under LGPL-LR license.\n",
            "Actual Title : \n",
            "Outilex, plate-forme logicielle de traitement de textes crits\n",
            "Generated Title : \n",
            "<unk> <unk> <unk> de textes <unk>\n",
            "Abstract : \n",
            "Detecting multiple planes in images is a challenging problem, but one with\n",
            "many applications. Recent work such as J-Linkage and Ordered Residual Kernels\n",
            "have focussed on developing a domain independent approach to detect multiple\n",
            "structures. These multiple structure detection methods are then used for\n",
            "estimating multiple homographies given feature matches between two images.\n",
            "Features participating in the multiple homographies detected, provide us the\n",
            "multiple scene planes. We show that these methods provide locally optimal\n",
            "results and fail to merge detected planar patches to the true scene planes.\n",
            "These methods use only residues obtained on applying homography of one plane to\n",
            "another as cue for merging. In this paper, we develop additional cues such as\n",
            "local consistency of planes, local normals, texture etc. to perform better\n",
            "classification and merging . We formulate the classification as an MRF problem\n",
            "and use TRWS message passing algorithm to solve non metric energy terms and\n",
            "complex sparse graph structure. We show results on challenging dataset common\n",
            "in robotics navigation scenarios where our method shows accuracy of more than\n",
            "85 percent on average while being close or same as the actual number of scene\n",
            "planes.\n",
            "Actual Title : \n",
            "Top Down Approach to Multiple Plane Detection\n",
            "Generated Title : \n",
            "<unk> multiple <unk> <unk>\n",
            "Abstract : \n",
            "One popular approach for blind deconvolution is to formulate a maximum a\n",
            "posteriori (MAP) problem with sparsity priors on the gradients of the latent\n",
            "image, and then alternatingly estimate the blur kernel and the latent image.\n",
            "While several successful MAP based methods have been proposed, there has been\n",
            "much controversy and confusion about their convergence, because sparsity priors\n",
            "have been shown to prefer blurry images to sharp natural images. In this paper,\n",
            "we revisit this problem and provide an analysis on the convergence of MAP based\n",
            "approaches. We first introduce a slight modification to a conventional joint\n",
            "energy function for blind deconvolution. The reformulated energy function\n",
            "yields the same alternating estimation process, but more clearly reveals how\n",
            "blind deconvolution works. We then show the energy function can actually favor\n",
            "the right solution instead of the no-blur solution under certain conditions,\n",
            "which explains the success of previous MAP based approaches. The reformulated\n",
            "energy function and our conditions for the convergence also provide a way to\n",
            "compare the qualities of different blur kernels, and we demonstrate its\n",
            "applicability to automatic blur kernel size selection, blur kernel estimation\n",
            "using light streaks, and defocus estimation.\n",
            "Actual Title : \n",
            "Convergence Analysis of MAP based Blur Kernel Estimation\n",
            "Generated Title : \n",
            "image estimation via kernel blur\n",
            "Abstract : \n",
            "In this paper, we take a new look at the possibilistic c-means (PCM) and\n",
            "adaptive PCM (APCM) clustering algorithms from the perspective of uncertainty.\n",
            "This new perspective offers us insights into the clustering process, and also\n",
            "provides us greater degree of flexibility. We analyze the clustering behavior\n",
            "of PCM-based algorithms and introduce parameters $\\sigma_v$ and $\\alpha$ to\n",
            "characterize uncertainty of estimated bandwidth and noise level of the dataset\n",
            "respectively. Then uncertainty (fuzziness) of membership values caused by\n",
            "uncertainty of the estimated bandwidth parameter is modeled by a conditional\n",
            "fuzzy set, which is a new formulation of the type-2 fuzzy set. Experiments show\n",
            "that parameters $\\sigma_v$ and $\\alpha$ make the clustering process more easy\n",
            "to control, and main features of PCM and APCM are unified in this new\n",
            "clustering framework (UPCM). More specifically, UPCM reduces to PCM when we set\n",
            "a small $\\alpha$ or a large $\\sigma_v$, and UPCM reduces to APCM when clusters\n",
            "are confined in their physical clusters and possible cluster elimination are\n",
            "ensured. Finally we present further researches of this paper.\n",
            "Actual Title : \n",
            "PCM and APCM Revisited: An Uncertainty Perspective\n",
            "Generated Title : \n",
            "clustering\n",
            "Abstract : \n",
            "This article demonstrates a new conceptor network based classifier in\n",
            "classifying images. Mathematical descriptions and analysis are presented.\n",
            "Various tests are experimented using three benchmark datasets: MNIST, CIFAR-10\n",
            "and CIFAR-100. The experiments displayed that conceptor network can offer\n",
            "superior results and flexible configurations than conventional classifiers such\n",
            "as Softmax Regression and Support Vector Machine (SVM).\n",
            "Actual Title : \n",
            "Classify Images with Conceptor Network\n",
            "Generated Title : \n",
            "images with conceptor network\n",
            "Abstract : \n",
            "In this work we consider the problem of anomaly detection in heterogeneous,\n",
            "multivariate, variable-length time series datasets. Our focus is on the\n",
            "aviation safety domain, where data objects are flights and time series are\n",
            "sensor readings and pilot switches. In this context the goal is to detect\n",
            "anomalous flight segments, due to mechanical, environmental, or human factors\n",
            "in order to identifying operationally significant events and provide insights\n",
            "into the flight operations and highlight otherwise unavailable potential safety\n",
            "risks and precursors to accidents. For this purpose, we propose a framework\n",
            "which represents each flight using a semi-Markov switching vector\n",
            "autoregressive (SMS-VAR) model. Detection of anomalies is then based on\n",
            "measuring dissimilarities between the model's prediction and data observation.\n",
            "The framework is scalable, due to the inherent parallel nature of most\n",
            "computations, and can be used to perform online anomaly detection. Extensive\n",
            "experimental results on simulated and real datasets illustrate that the\n",
            "framework can detect various types of anomalies along with the key parameters\n",
            "involved.\n",
            "Actual Title : \n",
            "Semi-Markov Switching Vector Autoregressive Model-based Anomaly\n",
            "  Detection in Aviation Systems\n",
            "Generated Title : \n",
            "switching in aviation systems\n",
            "Abstract : \n",
            "This paper derives a formula for computing the conditional probability of a\n",
            "set of candidates, where a candidate is a set of disorders that explain a given\n",
            "set of positive findings. Such candidate sets are produced by a recent method\n",
            "for multidisorder diagnosis called symptom clustering. A symptom clustering\n",
            "represents a set of candidates compactly as a cartesian product of differential\n",
            "diagnoses. By evaluating the probability of a candidate set, then, a large set\n",
            "of candidates can be validated or pruned simultaneously. The probability of a\n",
            "candidate set is then specialized to obtain the probability of a single\n",
            "candidate. Unlike earlier results, the equation derived here allows the\n",
            "specification of positive, negative, and unknown symptoms and does not make\n",
            "assumptions about disorders not in the candidate.\n",
            "Actual Title : \n",
            "Probabilistic Evaluation of Candidates and Symptom Clustering for\n",
            "  Multidisorder Diagnosis\n",
            "Generated Title : \n",
            "probabilistic <unk> for the clustering of <unk>\n",
            "Abstract : \n",
            "This paper presents a novel framework for generating texture mosaics with\n",
            "convolutional neural networks. Our method is called GANosaic and performs\n",
            "optimization in the latent noise space of a generative texture model, which\n",
            "allows the transformation of a content image into a mosaic exhibiting the\n",
            "visual properties of the underlying texture manifold. To represent that\n",
            "manifold, we use a state-of-the-art generative adversarial method for texture\n",
            "synthesis, which can learn expressive texture representations from data and\n",
            "produce mosaic images with very high resolution. This fully convolutional model\n",
            "generates smooth (without any visible borders) mosaic images which morph and\n",
            "blend different textures locally. In addition, we develop a new type of\n",
            "differentiable statistical regularization appropriate for optimization over the\n",
            "prior noise space of the PSGAN model.\n",
            "Actual Title : \n",
            "GANosaic: Mosaic Creation with Generative Texture Manifolds\n",
            "Generated Title : \n",
            "mosaic creation with generative texture\n",
            "Abstract : \n",
            "Detection and learning based appearance feature play the central role in data\n",
            "association based multiple object tracking (MOT), but most recent MOT works\n",
            "usually ignore them and only focus on the hand-crafted feature and association\n",
            "algorithms. In this paper, we explore the high-performance detection and deep\n",
            "learning based appearance feature, and show that they lead to significantly\n",
            "better MOT results in both online and offline setting. We make our detection\n",
            "and appearance feature publicly available. In the following part, we first\n",
            "summarize the detection and appearance feature, and then introduce our tracker\n",
            "named Person of Interest (POI), which has both online and offline version.\n",
            "Actual Title : \n",
            "POI: Multiple Object Tracking with High Performance Detection and\n",
            "  Appearance Feature\n",
            "Generated Title : \n",
            "feature learning for multiple object tracking\n",
            "Abstract : \n",
            "K-Nearest Neighbours (k-NN) is a popular classification and regression\n",
            "algorithm, yet one of its main limitations is the difficulty in choosing the\n",
            "number of neighbours. We present a Bayesian algorithm to compute the posterior\n",
            "probability distribution for k given a target point within a data-set,\n",
            "efficiently and without the use of Markov Chain Monte Carlo (MCMC) methods or\n",
            "simulation - alongside an exact solution for distributions within the\n",
            "exponential family. The central idea is that data points around our target are\n",
            "generated by the same probability distribution, extending outwards over the\n",
            "appropriate, though unknown, number of neighbours. Once the data is projected\n",
            "onto a distance metric of choice, we can transform the choice of k into a\n",
            "change-point detection problem, for which there is an efficient solution: we\n",
            "recursively compute the probability of the last change-point as we move towards\n",
            "our target, and thus de facto compute the posterior probability distribution\n",
            "over k. Applying this approach to both a classification and a regression UCI\n",
            "data-sets, we compare favourably and, most importantly, by removing the need\n",
            "for simulation, we are able to compute the posterior probability of k exactly\n",
            "and rapidly. As an example, the computational time for the Ripley data-set is a\n",
            "few milliseconds compared to a few hours when using a MCMC approach.\n",
            "Actual Title : \n",
            "An Efficient Algorithm for Bayesian Nearest Neighbours\n",
            "Generated Title : \n",
            "model for nearest neighbour classification\n",
            "Abstract : \n",
            "Transductive learning considers situations when a learner observes $m$\n",
            "labelled training points and $u$ unlabelled test points with the final goal of\n",
            "giving correct answers for the test points. This paper introduces a new\n",
            "complexity measure for transductive learning called Permutational Rademacher\n",
            "Complexity (PRC) and studies its properties. A novel symmetrization inequality\n",
            "is proved, which shows that PRC provides a tighter control over expected\n",
            "suprema of empirical processes compared to what happens in the standard i.i.d.\n",
            "setting. A number of comparison results are also provided, which show the\n",
            "relation between PRC and other popular complexity measures used in statistical\n",
            "learning theory, including Rademacher complexity and Transductive Rademacher\n",
            "Complexity (TRC). We argue that PRC is a more suitable complexity measure for\n",
            "transductive learning. Finally, these results are combined with a standard\n",
            "concentration argument to provide novel data-dependent risk bounds for\n",
            "transductive learning.\n",
            "Actual Title : \n",
            "Permutational Rademacher Complexity: a New Complexity Measure for\n",
            "  Transductive Learning\n",
            "Generated Title : \n",
            "rademacher complexity complexity complexity measure for transductive learning\n",
            "Abstract : \n",
            "Exemplar-based face sketch synthesis plays an important role in both digital\n",
            "entertainment and law enforcement. It generally consists of two parts: neighbor\n",
            "selection and reconstruction weight representation. The most time-consuming or\n",
            "main computation complexity for exemplar-based face sketch synthesis methods\n",
            "lies in the neighbor selection process. State-of-the-art face sketch synthesis\n",
            "methods perform neighbor selection online in a data-driven manner by $K$\n",
            "nearest neighbor ($K$-NN) searching. Actually, the online search increases the\n",
            "time consuming for synthesis. Moreover, since these methods need to traverse\n",
            "the whole training dataset for neighbor selection, the computational complexity\n",
            "increases with the scale of the training database and hence these methods have\n",
            "limited scalability. In this paper, we proposed a simple but effective offline\n",
            "random sampling in place of online $K$-NN search to improve the synthesis\n",
            "efficiency. Extensive experiments on public face sketch databases demonstrate\n",
            "the superiority of the proposed method in comparison to state-of-the-art\n",
            "methods, in terms of both synthesis quality and time consumption. The proposed\n",
            "method could be extended to other heterogeneous face image transformation\n",
            "problems such as face hallucination. We release the source codes of our\n",
            "proposed methods and the evaluation metrics for future study online:\n",
            "http://www.ihitworld.com/RSLCR.html.\n",
            "Actual Title : \n",
            "Random Sampling for Fast Face Sketch Synthesis\n",
            "Generated Title : \n",
            "sampling sampling for face sketch synthesis\n",
            "Abstract : \n",
            "Backdoors of answer-set programs are sets of atoms that represent clever\n",
            "reasoning shortcuts through the search space. Assignments to backdoor atoms\n",
            "reduce the given program to several programs that belong to a tractable target\n",
            "class. Previous research has considered target classes based on notions of\n",
            "acyclicity where various types of cycles (good and bad cycles) are excluded\n",
            "from graph representations of programs. We generalize the target classes by\n",
            "taking the parity of the number of negative edges on bad cycles into account\n",
            "and consider backdoors for such classes. We establish new hardness results and\n",
            "non-uniform polynomial-time tractability relative to directed or undirected\n",
            "cycles.\n",
            "Actual Title : \n",
            "The Good, the Bad, and the Odd: Cycles in Answer-Set Programs\n",
            "Generated Title : \n",
            "good  the bad  and <unk> programs\n",
            "Abstract : \n",
            "A recent theoretical analysis shows the equivalence between non-negative\n",
            "matrix factorization (NMF) and spectral clustering based approach to subspace\n",
            "clustering. As NMF and many of its variants are essentially linear, we\n",
            "introduce a nonlinear NMF with explicit orthogonality and derive general\n",
            "kernel-based orthogonal multiplicative update rules to solve the subspace\n",
            "clustering problem. In nonlinear orthogonal NMF framework, we propose two\n",
            "subspace clustering algorithms, named kernel-based non-negative subspace\n",
            "clustering KNSC-Ncut and KNSC-Rcut and establish their connection with spectral\n",
            "normalized cut and ratio cut clustering. We further extend the nonlinear\n",
            "orthogonal NMF framework and introduce a graph regularization to obtain a\n",
            "factorization that respects a local geometric structure of the data after the\n",
            "nonlinear mapping. The proposed NMF-based approach to subspace clustering takes\n",
            "into account the nonlinear nature of the manifold, as well as its intrinsic\n",
            "local geometry, which considerably improves the clustering performance when\n",
            "compared to the several recently proposed state-of-the-art methods.\n",
            "Actual Title : \n",
            "A Nonlinear Orthogonal Non-Negative Matrix Factorization Approach to\n",
            "  Subspace Clustering\n",
            "Generated Title : \n",
            "matrix non negative matrix factorization\n",
            "Abstract : \n",
            "We develop a natural language interface for human robot interaction that\n",
            "implements reasoning about deep semantics in natural language. To realize the\n",
            "required deep analysis, we employ methods from cognitive linguistics, namely\n",
            "the modular and compositional framework of Embodied Construction Grammar (ECG)\n",
            "[Feldman, 2009]. Using ECG, robots are able to solve fine-grained reference\n",
            "resolution problems and other issues related to deep semantics and\n",
            "compositionality of natural language. This also includes verbal interaction\n",
            "with humans to clarify commands and queries that are too ambiguous to be\n",
            "executed safely. We implement our NLU framework as a ROS package and present\n",
            "proof-of-concept scenarios with different robots, as well as a survey on the\n",
            "state of the art.\n",
            "Actual Title : \n",
            "Exploiting Deep Semantics and Compositionality of Natural Language for\n",
            "  Human-Robot-Interaction\n",
            "Generated Title : \n",
            "semantics of deep deep semantics for deep and\n",
            "Abstract : \n",
            "This paper describes a method for identification of the informative variables\n",
            "in the information system with discrete decision variables. It is targeted\n",
            "specifically towards discovery of the variables that are non-informative when\n",
            "considered alone, but are informative when the synergistic interactions between\n",
            "multiple variables are considered. To this end, the mutual entropy of all\n",
            "possible k-tuples of variables with decision variable is computed. Then, for\n",
            "each variable the maximal information gain due to interactions with other\n",
            "variables is obtained. For non-informative variables this quantity conforms to\n",
            "the well known statistical distributions. This allows for discerning truly\n",
            "informative variables from non-informative ones. For demonstration of the\n",
            "approach, the method is applied to several synthetic datasets that involve\n",
            "complex multidimensional interactions between variables. It is capable of\n",
            "identifying most important informative variables, even in the case when the\n",
            "dimensionality of the analysis is smaller than the true dimensionality of the\n",
            "problem. What is more, the high sensitivity of the algorithm allows for\n",
            "detection of the influence of nuisance variables on the response variable.\n",
            "Actual Title : \n",
            "All-relevant feature selection using multidimensional filters with\n",
            "  exhaustive search\n",
            "Generated Title : \n",
            "identification of <unk> variables with latent variables\n",
            "Abstract : \n",
            "Given two subsets A and B of nodes in a directed graph, the conduciveness of\n",
            "the graph from A to B is the ratio representing how many of the edges outgoing\n",
            "from nodes in A are incoming to nodes in B. When the graph's nodes stand for\n",
            "the possible solutions to certain problems of combinatorial optimization,\n",
            "choosing its edges appropriately has been shown to lead to conduciveness\n",
            "properties that provide useful insight into the performance of algorithms to\n",
            "solve those problems. Here we study the conduciveness of CA-rule graphs, that\n",
            "is, graphs whose node set is the set of all CA rules given a cell's number of\n",
            "possible states and neighborhood size. We consider several different edge sets\n",
            "interconnecting these nodes, both deterministic and random ones, and derive\n",
            "analytical expressions for the resulting graph's conduciveness toward rules\n",
            "having a fixed number of non-quiescent entries. We demonstrate that one of the\n",
            "random edge sets, characterized by allowing nodes to be sparsely interconnected\n",
            "across any Hamming distance between the corresponding rules, has the potential\n",
            "of providing reasonable conduciveness toward the desired rules. We conjecture\n",
            "that this may lie at the bottom of the best strategies known to date for\n",
            "discovering complex rules to solve specific problems, all of an evolutionary\n",
            "nature.\n",
            "Actual Title : \n",
            "The conduciveness of CA-rule graphs\n",
            "Generated Title : \n",
            "conduciveness of <unk> graphs\n",
            "Abstract : \n",
            "Artificial reinforcement learning (RL) is a widely used technique in\n",
            "artificial intelligence that provides a general method for training agents to\n",
            "perform a wide variety of behaviours. RL as used in computer science has\n",
            "striking parallels to reward and punishment learning in animal and human\n",
            "brains. I argue that present-day artificial RL agents have a very small but\n",
            "nonzero degree of ethical importance. This is particularly plausible for views\n",
            "according to which sentience comes in degrees based on the abilities and\n",
            "complexities of minds, but even binary views on consciousness should assign\n",
            "nonzero probability to RL programs having morally relevant experiences. While\n",
            "RL programs are not a top ethical priority today, they may become more\n",
            "significant in the coming decades as RL is increasingly applied to industry,\n",
            "robotics, video games, and other areas. I encourage scientists, philosophers,\n",
            "and citizens to begin a conversation about our ethical duties to reduce the\n",
            "harm that we inflict on powerless, voiceless RL agents.\n",
            "Actual Title : \n",
            "Do Artificial Reinforcement-Learning Agents Matter Morally?\n",
            "Generated Title : \n",
            "artificial reinforcement learning agents\n",
            "Abstract : \n",
            "Commercial head-mounted eye trackers provide useful features to customers in\n",
            "industry and research but are expensive and rely on closed source hardware and\n",
            "software. This limits the application areas and use of mobile eye tracking to\n",
            "expert users and inhibits user-driven development, customisation, and\n",
            "extension. In this paper we present Pupil -- an accessible, affordable, and\n",
            "extensible open source platform for mobile eye tracking and gaze-based\n",
            "interaction. Pupil comprises 1) a light-weight headset with high-resolution\n",
            "cameras, 2) an open source software framework for mobile eye tracking, as well\n",
            "as 3) a graphical user interface (GUI) to playback and visualize video and gaze\n",
            "data. Pupil features high-resolution scene and eye cameras for monocular and\n",
            "binocular gaze estimation. The software and GUI are platform-independent and\n",
            "include state-of-the-art algorithms for real-time pupil detection and tracking,\n",
            "calibration, and accurate gaze estimation. Results of a performance evaluation\n",
            "show that Pupil can provide an average gaze estimation accuracy of 0.6 degree\n",
            "of visual angle (0.08 degree precision) with a latency of the processing\n",
            "pipeline of only 0.045 seconds.\n",
            "Actual Title : \n",
            "Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile\n",
            "  Gaze-based Interaction\n",
            "Generated Title : \n",
            "a open source for for eye tracking\n",
            "Abstract : \n",
            "We propose a novel method for semi-supervised learning (SSL) based on\n",
            "data-driven distributionally robust optimization (DRO) using optimal transport\n",
            "metrics. Our proposed method enhances generalization error by using the\n",
            "unlabeled data to restrict the support of the worst case distribution in our\n",
            "DRO formulation. We enable the implementation of our DRO formulation by\n",
            "proposing a stochastic gradient descent algorithm which allows to easily\n",
            "implement the training procedure. We demonstrate that our Semi-supervised DRO\n",
            "method is able to improve the generalization error over natural supervised\n",
            "procedures and state-of-the-art SSL estimators. Finally, we include a\n",
            "discussion on the large sample behavior of the optimal uncertainty region in\n",
            "the DRO formulation. Our discussion exposes important aspects such as the role\n",
            "of dimension reduction in SSL.\n",
            "Actual Title : \n",
            "Semi-supervised Learning based on Distributionally Robust Optimization\n",
            "Generated Title : \n",
            "robust learning based on distributionally robust\n",
            "Abstract : \n",
            "Unsupervised models of dependency parsing typically require large amounts of\n",
            "clean, unlabeled data plus gold-standard part-of-speech tags. Adding indirect\n",
            "supervision (e.g. language universals and rules) can help, but we show that\n",
            "obtaining small amounts of direct supervision - here, partial dependency\n",
            "annotations - provides a strong balance between zero and full supervision. We\n",
            "adapt the unsupervised ConvexMST dependency parser to learn from partial\n",
            "dependencies expressed in the Graph Fragment Language. With less than 24 hours\n",
            "of total annotation, we obtain 7% and 17% absolute improvement in unlabeled\n",
            "dependency scores for English and Spanish, respectively, compared to the same\n",
            "parser using only universal grammar constraints.\n",
            "Actual Title : \n",
            "Fill it up: Exploiting partial dependency annotations in a minimum\n",
            "  spanning tree parser\n",
            "Generated Title : \n",
            "  a dependency parser with <unk> minimum spanning\n",
            "Abstract : \n",
            "This paper introduces a simple but highly efficient ensemble for robust\n",
            "texture classification, which can effectively deal with translation, scale and\n",
            "changes of significant viewpoint problems. The proposed method first inherits\n",
            "the spirit of spatial pyramid matching model (SPM), which is popular for\n",
            "encoding spatial distribution of local features, but in a flexible way,\n",
            "partitioning the original image into different levels and incorporating\n",
            "different overlapping patterns of each level. This flexible setup helps capture\n",
            "the informative features and produces sufficient local feature codes by some\n",
            "well-chosen aggregation statistics or pooling operations within each\n",
            "partitioned region, even when only a few sample images are available for\n",
            "training. Then each texture image is represented by several orderless feature\n",
            "codes and thereby all the training data form a reliable feature pond. Finally,\n",
            "to take full advantage of this feature pond, we develop a collaborative\n",
            "representation-based strategy with locality constraint (LC-CRC) for the final\n",
            "classification, and experimental results on three well-known public texture\n",
            "datasets demonstrate the proposed approach is very competitive and even\n",
            "outperforms several state-of-the-art methods. Particularly, when only a few\n",
            "samples of each category are available for training, our approach still\n",
            "achieves very high classification performance.\n",
            "Actual Title : \n",
            "Multi-Level Feature Descriptor for Robust Texture Classification via\n",
            "  Locality-Constrained Collaborative Strategy\n",
            "Generated Title : \n",
            "feature descriptor for texture classification\n",
            "Abstract : \n",
            "This paper describes the method of visualization of periodic constituents and\n",
            "instability areas in series of measurements, being based on the algorithm of\n",
            "smoothing out and concept of one-dimensional cellular automata. A method can be\n",
            "used at the analysis of temporal series, related to the volumes of thematic\n",
            "publications in web-space.\n",
            "Actual Title : \n",
            "Visualization of features of a series of measurements with\n",
            "  one-dimensional cellular structure\n",
            "Generated Title : \n",
            "of of measurements of measurements one dimensional one dimensional cellular\n",
            "Abstract : \n",
            "The superposition of temporal point processes has been studied for many\n",
            "years, although the usefulness of such models for practical applications has\n",
            "not be fully developed. We investigate superposed Hawkes process as an\n",
            "important class of such models, with properties studied in the framework of\n",
            "least squares estimation. The superposition of Hawkes processes is demonstrated\n",
            "to be beneficial for tightening the upper bound of excess risk under certain\n",
            "conditions, and we show the feasibility of the benefit in typical situations.\n",
            "The usefulness of superposed Hawkes processes is verified on synthetic data,\n",
            "and its potential to solve the cold-start problem of recommendation systems is\n",
            "demonstrated on real-world data.\n",
            "Actual Title : \n",
            "Benefits from Superposed Hawkes Processes\n",
            "Generated Title : \n",
            "from superposed hawkes processes\n",
            "Abstract : \n",
            "Understanding physical phenomena is a key competence that enables humans and\n",
            "animals to act and interact under uncertain perception in previously unseen\n",
            "environments containing novel object and their configurations. Developmental\n",
            "psychology has shown that such skills are acquired by infants from observations\n",
            "at a very early stage.\n",
            "  In this paper, we contrast a more traditional approach of taking a\n",
            "model-based route with explicit 3D representations and physical simulation by\n",
            "an end-to-end approach that directly predicts stability and related quantities\n",
            "from appearance. We ask the question if and to what extent and quality such a\n",
            "skill can directly be acquired in a data-driven way bypassing the need for an\n",
            "explicit simulation.\n",
            "  We present a learning-based approach based on simulated data that predicts\n",
            "stability of towers comprised of wooden blocks under different conditions and\n",
            "quantities related to the potential fall of the towers. The evaluation is\n",
            "carried out on synthetic data and compared to human judgments on the same\n",
            "stimuli.\n",
            "Actual Title : \n",
            "To Fall Or Not To Fall: A Visual Approach to Physical Stability\n",
            "  Prediction\n",
            "Generated Title : \n",
            "and stability prediction with a stability of stability\n",
            "Abstract : \n",
            "Standing at the paradigm shift towards data-intensive science, machine\n",
            "learning techniques are becoming increasingly important. In particular, as a\n",
            "major breakthrough in the field, deep learning has proven as an extremely\n",
            "powerful tool in many fields. Shall we embrace deep learning as the key to all?\n",
            "Or, should we resist a 'black-box' solution? There are controversial opinions\n",
            "in the remote sensing community. In this article, we analyze the challenges of\n",
            "using deep learning for remote sensing data analysis, review the recent\n",
            "advances, and provide resources to make deep learning in remote sensing\n",
            "ridiculously simple to start with. More importantly, we advocate remote sensing\n",
            "scientists to bring their expertise into deep learning, and use it as an\n",
            "implicit general model to tackle unprecedented large-scale influential\n",
            "challenges, such as climate change and urbanization.\n",
            "Actual Title : \n",
            "Deep learning in remote sensing: a review\n",
            "Generated Title : \n",
            "a critical <unk> remote sensing benchmark\n",
            "Abstract : \n",
            "Commute Time Distance (CTD) is a random walk based metric on graphs. CTD has\n",
            "found widespread applications in many domains including personalized search,\n",
            "collaborative filtering and making search engines robust against manipulation.\n",
            "Our interest is inspired by the use of CTD as a metric for anomaly detection.\n",
            "It has been shown that CTD can be used to simultaneously identify both global\n",
            "and local anomalies. Here we propose an accurate and efficient approximation\n",
            "for computing the CTD in an incremental fashion in order to facilitate\n",
            "real-time applications. An online anomaly detection algorithm is designed where\n",
            "the CTD of each new arriving data point to any point in the current graph can\n",
            "be estimated in constant time ensuring a real-time response. Moreover, the\n",
            "proposed approach can also be applied in many other applications that utilize\n",
            "commute time distance.\n",
            "Actual Title : \n",
            "Online Anomaly Detection Systems Using Incremental Commute Time\n",
            "Generated Title : \n",
            "anomaly detection using online commute commute commute\n",
            "Abstract : \n",
            "The objective of the paper is to design an agent which provides efficient\n",
            "response to the caller when a call goes unanswered in smartphones. The agent\n",
            "provides responses through text messages, email etc stating the most likely\n",
            "reason as to why the callee is unable to answer a call. Responses are composed\n",
            "taking into consideration the importance of the present call and the situation\n",
            "the callee is in at the moment like driving, sleeping, at work etc. The agent\n",
            "makes decisons in the compostion of response messages based on the patterns it\n",
            "has come across in the learning environment. Initially the user helps the agent\n",
            "to compose response messages. The agent associates this message to the percept\n",
            "it recieves with respect to the environment the callee is in. The user may\n",
            "thereafter either choose to make to response system automatic or choose to\n",
            "recieve suggestions from the agent for responses messages and confirm what is\n",
            "to be sent to the caller.\n",
            "Actual Title : \n",
            "Design of an Agent for Answering Back in Smart Phones\n",
            "Generated Title : \n",
            "<unk> response response response response response response\n",
            "Abstract : \n",
            "We consider the problem of bottom-up compilation of knowledge bases, which is\n",
            "usually predicated on the existence of a polytime function for combining\n",
            "compilations using Boolean operators (usually called an Apply function). While\n",
            "such a polytime Apply function is known to exist for certain languages (e.g.,\n",
            "OBDDs) and not exist for others (e.g., DNNF), its existence for certain\n",
            "languages remains unknown. Among the latter is the recently introduced language\n",
            "of Sentential Decision Diagrams (SDDs), for which a polytime Apply function\n",
            "exists for unreduced SDDs, but remains unknown for reduced ones (i.e. canonical\n",
            "SDDs). We resolve this open question in this paper and consider some of its\n",
            "theoretical and practical implications. Some of the findings we report question\n",
            "the common wisdom on the relationship between bottom-up compilation, language\n",
            "canonicity and the complexity of the Apply function.\n",
            "Actual Title : \n",
            "On the Role of Canonicity in Bottom-up Knowledge Compilation\n",
            "Generated Title : \n",
            "the role of <unk> in <unk>\n",
            "Abstract : \n",
            "Visual representation is crucial for a visual tracking method's performances.\n",
            "Conventionally, visual representations adopted in visual tracking rely on\n",
            "hand-crafted computer vision descriptors. These descriptors were developed\n",
            "generically without considering tracking-specific information. In this paper,\n",
            "we propose to learn complex-valued invariant representations from tracked\n",
            "sequential image patches, via strong temporal slowness constraint and stacked\n",
            "convolutional autoencoders. The deep slow local representations are learned\n",
            "offline on unlabeled data and transferred to the observational model of our\n",
            "proposed tracker. The proposed observational model retains old training samples\n",
            "to alleviate drift, and collect negative samples which are coherent with\n",
            "target's motion pattern for better discriminative tracking. With the learned\n",
            "representation and online training samples, a logistic regression classifier is\n",
            "adopted to distinguish target from background, and retrained online to adapt to\n",
            "appearance changes. Subsequently, the observational model is integrated into a\n",
            "particle filter framework to peform visual tracking. Experimental results on\n",
            "various challenging benchmark sequences demonstrate that the proposed tracker\n",
            "performs favourably against several state-of-the-art trackers.\n",
            "Actual Title : \n",
            "Self-taught learning of a deep invariant representation for visual\n",
            "  tracking via temporal slowness principle\n",
            "Generated Title : \n",
            "learning for representation learning for representation learning\n",
            "Abstract : \n",
            "Diversity is an important aspect of highly efficient multi-agent teams. We\n",
            "introduce the main factors that drive a multi-agent system in either direction\n",
            "along the diversity scale. A metric for diversity is described, and we\n",
            "speculate on the concept of transient diversity. Finally, an experiment on\n",
            "social entropy using a RoboCup simulated soccer team is presented.\n",
            "Actual Title : \n",
            "Transient Diversity in Multi-Agent Systems\n",
            "Generated Title : \n",
            "diversity\n",
            "Abstract : \n",
            "Intelligent agents offer a new and exciting way of understanding the world of\n",
            "work. In this paper we apply agent-based modeling and simulation to investigate\n",
            "a set of problems in a retail context. Specifically, we are working to\n",
            "understand the relationship between human resource management practices and\n",
            "retail productivity. Despite the fact we are working within a relatively novel\n",
            "and complex domain, it is clear that intelligent agents could offer potential\n",
            "for fostering sustainable organizational capabilities in the future. The\n",
            "project is still at an early stage. So far we have conducted a case study in a\n",
            "UK department store to collect data and capture impressions about operations\n",
            "and actors within departments. Furthermore, based on our case study we have\n",
            "built and tested our first version of a retail branch simulator which we will\n",
            "present in this paper.\n",
            "Actual Title : \n",
            "Using Intelligent Agents to Understand Management Practices and Retail\n",
            "  Productivity\n",
            "Generated Title : \n",
            "retail productivity to understand management practise\n",
            "Abstract : \n",
            "In parallel with the success of CNNs to solve vision problems, there is a\n",
            "growing interest in developing methodologies to understand and visualize the\n",
            "internal representations of these networks. How the responses of a trained CNN\n",
            "encode the visual information is a fundamental question both for computer and\n",
            "human vision research. Image representations provided by the first\n",
            "convolutional layer as well as the resolution change provided by the\n",
            "max-polling operation are easy to understand, however, as soon as a second and\n",
            "further convolutional layers are added in the representation, any intuition is\n",
            "lost. A usual way to deal with this problem has been to define deconvolutional\n",
            "networks that somehow allow to explore the internal representations of the most\n",
            "important activations towards the image space, where deconvolution is assumed\n",
            "as a convolution with the transposed filter. However, this assumption is not\n",
            "the best approximation of an inverse convolution. In this paper we propose a\n",
            "new assumption based on filter substitution to reverse the encoding of a\n",
            "convolutional layer. This provides us with a new tool to directly visualize any\n",
            "CNN single neuron as a filter in the first layer, this is in terms of the image\n",
            "space.\n",
            "Actual Title : \n",
            "Understanding learned CNN features through Filter Decoding with\n",
            "  Substitution\n",
            "Generated Title : \n",
            "learned cnn features through learned filter\n",
            "Abstract : \n",
            "We present a new model-based integrative method for clustering objects given\n",
            "both vectorial data, which describes the feature of each object, and network\n",
            "data, which indicates the similarity of connected objects. The proposed general\n",
            "model is able to cluster the two types of data simultaneously within one\n",
            "integrative probabilistic model, while traditional methods can only handle one\n",
            "data type or depend on transforming one data type to another. Bayesian\n",
            "inference of the clustering is conducted based on a Markov chain Monte Carlo\n",
            "algorithm. A special case of the general model combining the Gaussian mixture\n",
            "model and the stochastic block model is extensively studied. We used both\n",
            "synthetic data and real data to evaluate this new method and compare it with\n",
            "alternative methods. The results show that our simultaneous clustering method\n",
            "performs much better. This improvement is due to the power of the model-based\n",
            "probabilistic approach for efficiently integrating information.\n",
            "Actual Title : \n",
            "A Bayesian Method for Joint Clustering of Vectorial Data and Network\n",
            "  Data\n",
            "Generated Title : \n",
            "bayesian clustering method for joint data data\n",
            "Abstract : \n",
            "Hand pose estimation is to predict the pose parameters representing a 3D hand\n",
            "model, such as locations of hand joints. This problem is very challenging due\n",
            "to large changes in viewpoints and articulations, and intense self-occlusions,\n",
            "etc. Many researchers have investigated the problem from both aspects of input\n",
            "feature learning and output prediction modelling. Though effective, most of the\n",
            "existing discriminative methods only give a deterministic estimation of target\n",
            "poses. Also, due to their single-value mapping intrinsic, they fail to\n",
            "adequately handle self-occlusion problems, where occluded joints present\n",
            "multiple modes. In this paper, we tackle the self-occlusion issue and provide a\n",
            "complete description of observed poses given an input depth image through a\n",
            "hierarchical mixture density network (HMDN) framework. In particular, HMDN\n",
            "leverages the state-of-the-art CNN module to facilitate feature learning, while\n",
            "proposes a density in a two-level hierarchy to reconcile single-valued and\n",
            "multi-valued mapping in the output. The whole framework is naturally end-to-end\n",
            "trainable with a mixture of two differentiable density functions. HMDN produces\n",
            "interpretable and diverse candidate samples, and significantly outperforms the\n",
            "state-of-the-art algorithms on benchmarks that exhibit occlusions.\n",
            "Actual Title : \n",
            "Occlusion-aware Hand Pose Estimation Using Hierarchical Mixture Density\n",
            "  Network\n",
            "Generated Title : \n",
            "mixture density network for for person re identification\n",
            "Abstract : \n",
            "This paper addresses the optimal control problem known as the Linear\n",
            "Quadratic Regulator in the case when the dynamics are unknown. We propose a\n",
            "multi-stage procedure, called Coarse-ID control, that estimates a model from a\n",
            "few experimental trials, estimates the error in that model with respect to the\n",
            "truth, and then designs a controller using both the model and uncertainty\n",
            "estimate. Our technique uses contemporary tools from random matrix theory to\n",
            "bound the error in the estimation procedure. We also employ a recently\n",
            "developed approach to control synthesis called System Level Synthesis that\n",
            "enables robust control design by solving a convex optimization problem. We\n",
            "provide end-to-end bounds on the relative error in control cost that are nearly\n",
            "optimal in the number of parameters and that highlight salient properties of\n",
            "the system to be controlled such as closed-loop sensitivity and optimal control\n",
            "magnitude. We show experimentally that the Coarse-ID approach enables efficient\n",
            "computation of a stabilizing controller in regimes where simple control schemes\n",
            "that do not take the model uncertainty into account fail to stabilize the true\n",
            "system.\n",
            "Actual Title : \n",
            "On the Sample Complexity of the Linear Quadratic Regulator\n",
            "Generated Title : \n",
            "linear sample complexity of linear quadratic regulator\n",
            "Abstract : \n",
            "BDeu marginal likelihood score is a popular model selection criterion for\n",
            "selecting a Bayesian network structure based on sample data. This\n",
            "non-informative scoring criterion assigns same score for network structures\n",
            "that encode same independence statements. However, before applying the BDeu\n",
            "score, one must determine a single parameter, the equivalent sample size alpha.\n",
            "Unfortunately no generally accepted rule for determining the alpha parameter\n",
            "has been suggested. This is disturbing, since in this paper we show through a\n",
            "series of concrete experiments that the solution of the network structure\n",
            "optimization problem is highly sensitive to the chosen alpha parameter value.\n",
            "Based on these results, we are able to give explanations for how and why this\n",
            "phenomenon happens, and discuss ideas for solving this problem.\n",
            "Actual Title : \n",
            "On Sensitivity of the MAP Bayesian Network Structure to the Equivalent\n",
            "  Sample Size Parameter\n",
            "Generated Title : \n",
            "sensitivity of bayesian network map\n",
            "Abstract : \n",
            "We study the task of cleaning scanned text documents that are strongly\n",
            "corrupted by dirt such as manual line strokes, spilled ink etc. We aim at\n",
            "autonomously removing dirt from a single letter-size page based only on the\n",
            "information the page contains. Our approach, therefore, has to learn character\n",
            "representations without supervision and requires a mechanism to distinguish\n",
            "learned representations from irregular patterns. To learn character\n",
            "representations, we use a probabilistic generative model parameterizing pattern\n",
            "features, feature variances, the features' planar arrangements, and pattern\n",
            "frequencies. The latent variables of the model describe pattern class, pattern\n",
            "position, and the presence or absence of individual pattern features. The model\n",
            "parameters are optimized using a novel variational EM approximation. After\n",
            "learning, the parameters represent, independently of their absolute position,\n",
            "planar feature arrangements and their variances. A quality measure defined\n",
            "based on the learned representation then allows for an autonomous\n",
            "discrimination between regular character patterns and the irregular patterns\n",
            "making up the dirt. The irregular patterns can thus be removed to clean the\n",
            "document. For a full Latin alphabet we found that a single page does not\n",
            "contain sufficiently many character examples. However, even if heavily\n",
            "corrupted by dirt, we show that a page containing a lower number of character\n",
            "types can efficiently and autonomously be cleaned solely based on the\n",
            "structural regularity of the characters it contains. In different examples\n",
            "using characters from different alphabets, we demonstrate generality of the\n",
            "approach and discuss its implications for future developments.\n",
            "Actual Title : \n",
            "Autonomous Cleaning of Corrupted Scanned Documents - A Generative\n",
            "  Modeling Approach\n",
            "Generated Title : \n",
            "text representations from text and text\n",
            "Abstract : \n",
            "Factored neural machine translation (FNMT) is founded on the idea of using\n",
            "the morphological and grammatical decomposition of the words (factors) at the\n",
            "output side of the neural network. This architecture addresses two well-known\n",
            "problems occurring in MT, namely the size of target language vocabulary and the\n",
            "number of unknown tokens produced in the translation. FNMT system is designed\n",
            "to manage larger vocabulary and reduce the training time (for systems with\n",
            "equivalent target language vocabulary size). Moreover, we can produce\n",
            "grammatically correct words that are not part of the vocabulary. FNMT model is\n",
            "evaluated on IWSLT'15 English to French task and compared to the baseline\n",
            "word-based and BPE-based NMT systems. Promising qualitative and quantitative\n",
            "results (in terms of BLEU and METEOR) are reported.\n",
            "Actual Title : \n",
            "Neural Machine Translation by Generating Multiple Linguistic Factors\n",
            "Generated Title : \n",
            "machine translation by neural factors factors factors factors\n",
            "Abstract : \n",
            "Algorithmic image-based diagnosis and prognosis of neurodegenerative diseases\n",
            "on longitudinal data has drawn great interest from computer vision researchers.\n",
            "The current state-of-the-art models for many image classification tasks are\n",
            "based on the Convolutional Neural Networks (CNN). However, a key challenge in\n",
            "applying CNN to biological problems is that the available labeled training\n",
            "samples are very limited. Another issue for CNN to be applied in computer aided\n",
            "diagnosis applications is that to achieve better diagnosis and prognosis\n",
            "accuracy, one usually has to deal with the longitudinal dataset, i.e., the\n",
            "dataset of images scanned at different time points. Here we argue that an\n",
            "enhanced CNN model with transfer learning for the joint analysis of tasks from\n",
            "multiple time points or regions of interests may have a potential to improve\n",
            "the accuracy of computer aided diagnosis. To reach this goal, we innovate a CNN\n",
            "based deep learning multi-task dictionary learning framework to address the\n",
            "above challenges. Firstly, we pre-train CNN on the ImageNet dataset and\n",
            "transfer the knowledge from the pre-trained model to the medical imaging\n",
            "progression representation, generating the features for different tasks. Then,\n",
            "we propose a novel unsupervised learning method, termed Multi-task Stochastic\n",
            "Coordinate Coding (MSCC), for learning different tasks by using shared and\n",
            "individual dictionaries and generating the sparse features required to predict\n",
            "the future cognitive clinical scores. We apply our new model in a publicly\n",
            "available neuroimaging cohort to predict clinical measures with two different\n",
            "feature sets and compare them with seven other state-of-the-art methods. The\n",
            "experimental results show our proposed method achieved superior results.\n",
            "Actual Title : \n",
            "Multi-task Dictionary Learning based Convolutional Neural Network for\n",
            "  Computer aided Diagnosis with Longitudinal Images\n",
            "Generated Title : \n",
            "feature learning for visual classification of longitudinal imaging\n",
            "Abstract : \n",
            "Mechanical learning is a computing system that is based on a set of simple\n",
            "and fixed rules, and can learn from incoming data. A learning machine is a\n",
            "system that realizes mechanical learning. Importantly, we emphasis that it is\n",
            "based on a set of simple and fixed rules, contrasting to often called machine\n",
            "learning that is sophisticated software based on very complicated mathematical\n",
            "theory, and often needs human intervene for software fine tune and manual\n",
            "adjustments. Here, we discuss some basic facts and principles of such system,\n",
            "and try to lay down a framework for further study. We propose 2 directions to\n",
            "approach mechanical learning, just like Church-Turing pair: one is trying to\n",
            "realize a learning machine, another is trying to well describe the mechanical\n",
            "learning.\n",
            "Actual Title : \n",
            "Discussion on Mechanical Learning and Learning Machine\n",
            "Generated Title : \n",
            "discussion on learning\n",
            "Abstract : \n",
            "This work introduces a novel framework for quantifying the presence and\n",
            "strength of recurrent dynamics in video data. Specifically, we provide\n",
            "continuous measures of periodicity (perfect repetition) and quasiperiodicity\n",
            "(superposition of periodic modes with non-commensurate periods), in a way which\n",
            "does not require segmentation, training, object tracking or 1-dimensional\n",
            "surrogate signals. Our methodology operates directly on video data. The\n",
            "approach combines ideas from nonlinear time series analysis (delay embeddings)\n",
            "and computational topology (persistent homology), by translating the problem of\n",
            "finding recurrent dynamics in video data, into the problem of determining the\n",
            "circularity or toroidality of an associated geometric space. Through extensive\n",
            "testing, we show the robustness of our scores with respect to several noise\n",
            "models/levels, we show that our periodicity score is superior to other methods\n",
            "when compared to human-generated periodicity rankings, and furthermore, we show\n",
            "that our quasiperiodicity score clearly indicates the presence of biphonation\n",
            "in videos of vibrating vocal folds, which has never before been accomplished\n",
            "end to end quantitatively.\n",
            "Actual Title : \n",
            "(Quasi)Periodicity Quantification in Video Data, Using Topology\n",
            "Generated Title : \n",
            "quantification in in video data  using recurrent neural\n",
            "Abstract : \n",
            "We propose an image representation and matching approach that substantially\n",
            "improves visual-based location estimation for images. The main novelty of the\n",
            "approach, called distinctive visual element matching (DVEM), is its use of\n",
            "representations that are specific to the query image whose location is being\n",
            "predicted. These representations are based on visual element clouds, which\n",
            "robustly capture the connection between the query and visual evidence from\n",
            "candidate locations. We then maximize the influence of visual elements that are\n",
            "geo-distinctive because they do not occur in images taken at many other\n",
            "locations. We carry out experiments and analysis for both geo-constrained and\n",
            "geo-unconstrained location estimation cases using two large-scale,\n",
            "publicly-available datasets: the San Francisco Landmark dataset with $1.06$\n",
            "million street-view images and the MediaEval '15 Placing Task dataset with\n",
            "$5.6$ million geo-tagged images from Flickr. We present examples that\n",
            "illustrate the highly-transparent mechanics of the approach, which are based on\n",
            "common sense observations about the visual patterns in image collections. Our\n",
            "results show that the proposed method delivers a considerable performance\n",
            "improvement compared to the state of the art.\n",
            "Actual Title : \n",
            "Geo-distinctive Visual Element Matching for Location Estimation of\n",
            "  Images\n",
            "Generated Title : \n",
            "visual element matching for location estimation\n",
            "Abstract : \n",
            "In this doctoral thesis, we apply premises of cognitive linguistics to\n",
            "terminological definitions and present a proposal called the flexible\n",
            "terminological definition. This consists of a set of definitions of the same\n",
            "concept made up of a general definition (in this case, one encompassing the\n",
            "entire environmental domain) along with additional definitions describing the\n",
            "concept from the perspective of the subdomains in which it is relevant. Since\n",
            "context is a determining factor in the construction of the meaning of lexical\n",
            "units (including terms), we assume that terminological definitions can, and\n",
            "should, reflect the effects of context, even though definitions have\n",
            "traditionally been treated as the expression of meaning void of any contextual\n",
            "effect. The main objective of this thesis is to analyze the effects of\n",
            "contextual variation on specialized environmental concepts with a view to their\n",
            "representation in terminological definitions. Specifically, we focused on\n",
            "contextual variation based on thematic restrictions. To accomplish the\n",
            "objectives of this doctoral thesis, we conducted an empirical study consisting\n",
            "of the analysis of a set of contextually variable concepts and the creation of\n",
            "a flexible definition for two of them. As a result of the first part of our\n",
            "empirical study, we divided our notion of domain-dependent contextual variation\n",
            "into three different phenomena: modulation, perspectivization and\n",
            "subconceptualization. These phenomena are additive in that all concepts\n",
            "experience modulation, some concepts also undergo perspectivization, and\n",
            "finally, a small number of concepts are additionally subjected to\n",
            "subconceptualization. In the second part, we applied these notions to\n",
            "terminological definitions and we presented we presented guidelines on how to\n",
            "build flexible definitions, from the extraction of knowledge to the actual\n",
            "writing of the definition.\n",
            "Actual Title : \n",
            "La representacin de la variacin contextual mediante definiciones\n",
            "  terminolgicas flexibles\n",
            "Generated Title : \n",
            "<unk> contextual contextual contextual <unk>\n",
            "Abstract : \n",
            "The inherent noise in the observed (e.g., scanned) binary document image\n",
            "degrades the image quality and harms the compression ratio through breaking the\n",
            "pattern repentance and adding entropy to the document images. In this paper, we\n",
            "design a cost function in Bayesian framework with dictionary learning.\n",
            "Minimizing our cost function produces a restored image which has better quality\n",
            "than that of the observed noisy image, and a dictionary for representing and\n",
            "encoding the image. After the restoration, we use this dictionary (from the\n",
            "same cost function) to encode the restored image following the\n",
            "symbol-dictionary framework by JBIG2 standard with the lossless mode.\n",
            "Experimental results with a variety of document images demonstrate that our\n",
            "method improves the image quality compared with the observed image, and\n",
            "simultaneously improves the compression ratio. For the test images with\n",
            "synthetic noise, our method reduces the number of flipped pixels by 48.2% and\n",
            "improves the compression ratio by 36.36% as compared with the best encoding\n",
            "methods. For the test images with real noise, our method visually improves the\n",
            "image quality, and outperforms the cutting-edge method by 28.27% in terms of\n",
            "the compression ratio.\n",
            "Actual Title : \n",
            "Model-based Iterative Restoration for Binary Document Image Compression\n",
            "  with Dictionary Learning\n",
            "Generated Title : \n",
            "iterative dictionary learning for document image compression\n",
            "Abstract : \n",
            "This paper proposes an image dehazing model built with a convolutional neural\n",
            "network (CNN), called All-in-One Dehazing Network (AOD-Net). It is designed\n",
            "based on a re-formulated atmospheric scattering model. Instead of estimating\n",
            "the transmission matrix and the atmospheric light separately as most previous\n",
            "models did, AOD-Net directly generates the clean image through a light-weight\n",
            "CNN. Such a novel end-to-end design makes it easy to embed AOD-Net into other\n",
            "deep models, e.g., Faster R-CNN, for improving high-level task performance on\n",
            "hazy images. Experimental results on both synthesized and natural hazy image\n",
            "datasets demonstrate our superior performance than the state-of-the-art in\n",
            "terms of PSNR, SSIM and the subjective visual quality. Furthermore, when\n",
            "concatenating AOD-Net with Faster R-CNN and training the joint pipeline from\n",
            "end to end, we witness a large improvement of the object detection performance\n",
            "on hazy images.\n",
            "Actual Title : \n",
            "An All-in-One Network for Dehazing and Beyond\n",
            "Generated Title : \n",
            "all in one network for dehazing\n",
            "Abstract : \n",
            "Our team won the second prize of the Safe Aging with SPHERE Challenge\n",
            "organized by SPHERE, in conjunction with ECML-PKDD and Driven Data. The goal of\n",
            "the competition was to recognize activities performed by humans, using sensor\n",
            "data. This paper presents our solution. It is based on a rich pre-processing\n",
            "and state of the art machine learning methods. From the raw train data, we\n",
            "generate a synthetic train set with the same statistical characteristics as the\n",
            "test set. We then perform feature engineering. The machine learning modeling\n",
            "part is based on stacking weak learners through a grid searched XGBoost\n",
            "algorithm. Finally, we use post-processing to smooth our predictions over time.\n",
            "Actual Title : \n",
            "Dataiku's Solution to SPHERE's Activity Recognition Challenge\n",
            "Generated Title : \n",
            "<unk> solution to activity recognition\n",
            "Abstract : \n",
            "We discuss the evolution of aspects of nonmonotonic reasoning towards the\n",
            "computational paradigm of answer-set programming (ASP). We give a general\n",
            "overview of the roots of ASP and follow up with the personal perspective on\n",
            "research developments that helped verbalize the main principles of ASP and\n",
            "differentiated it from the classical logic programming.\n",
            "Actual Title : \n",
            "Origins of Answer-Set Programming - Some Background And Two Personal\n",
            "  Accounts\n",
            "Generated Title : \n",
            "of answer set programming   some background answer set programming\n",
            "Abstract : \n",
            "In this paper, we show how absolute orientation measurements provided by\n",
            "low-cost but high-fidelity IMU sensors can be integrated into the KinectFusion\n",
            "pipeline. We show that integration improves both runtime, robustness and\n",
            "quality of the 3D reconstruction. In particular, we use this orientation data\n",
            "to seed and regularize the ICP registration technique. We also present a\n",
            "technique to filter the pairs of 3D matched points based on the distribution of\n",
            "their distances. This filter is implemented efficiently on the GPU. Estimating\n",
            "the distribution of the distances helps control the number of iterations\n",
            "necessary for the convergence of the ICP algorithm. Finally, we show\n",
            "experimental results that highlight improvements in robustness, a speed-up of\n",
            "almost 12%, and a gain in tracking quality of 53% for the ATE metric on the\n",
            "Freiburg benchmark.\n",
            "Actual Title : \n",
            "Integration of Absolute Orientation Measurements in the KinectFusion\n",
            "  Reconstruction pipeline\n",
            "Generated Title : \n",
            "the orientation of orientation and orientation\n",
            "Abstract : \n",
            "We propose a general class of language models that treat reference as an\n",
            "explicit stochastic latent variable. This architecture allows models to create\n",
            "mentions of entities and their attributes by accessing external databases\n",
            "(required by, e.g., dialogue generation and recipe generation) and internal\n",
            "state (required by, e.g. language models which are aware of coreference). This\n",
            "facilitates the incorporation of information that can be accessed in\n",
            "predictable locations in databases or discourse context, even when the targets\n",
            "of the reference may be rare words. Experiments on three tasks shows our model\n",
            "variants based on deterministic attention.\n",
            "Actual Title : \n",
            "Reference-Aware Language Models\n",
            "Generated Title : \n",
            "language models\n",
            "Abstract : \n",
            "The most direct way to express arbitrary dependencies in datasets is to\n",
            "estimate the joint distribution and to apply afterwards the argmax-function to\n",
            "obtain the mode of the corresponding conditional distribution. This method is\n",
            "in practice difficult, because it requires a global optimization of a\n",
            "complicated function, the joint distribution by fixed input variables. This\n",
            "article proposes a method for finding global maxima if the joint distribution\n",
            "is modeled by a kernel density estimation. Some experiments show advantages and\n",
            "shortcomings of the resulting regression method in comparison to the standard\n",
            "Nadaraya-Watson regression technique, which approximates the optimum by the\n",
            "expectation value.\n",
            "Actual Title : \n",
            "Kernel Regression by Mode Calculation of the Conditional Probability\n",
            "  Distribution\n",
            "Generated Title : \n",
            "<unk> and mode calculation of bridge regression\n",
            "Abstract : \n",
            "This paper presents a renewed overview of photosensor oculography (PSOG), an\n",
            "eye-tracking technique based on the principle of using simple photosensors to\n",
            "measure the amount of reflected (usually infrared) light when the eye rotates.\n",
            "Photosensor oculography can provide measurements with high precision, low\n",
            "latency and reduced power consumption, and thus it appears as an attractive\n",
            "option for performing eye-tracking in the emerging head-mounted interaction\n",
            "devices, e.g. augmented and virtual reality (AR/VR) headsets. In our current\n",
            "work we employ an adjustable simulation framework as a common basis for\n",
            "performing an exploratory study of the eye-tracking behavior of different\n",
            "photosensor oculography designs. With the performed experiments we explore the\n",
            "effects from the variation of some basic parameters of the designs on the\n",
            "resulting accuracy and cross-talk, which are crucial characteristics for the\n",
            "seamless operation of human-computer interaction applications based on\n",
            "eye-tracking. Our experimental results reveal the design trade-offs that need\n",
            "to be adopted to tackle the competing conditions that lead to optimum\n",
            "performance of different eye-tracking characteristics. We also present the\n",
            "transformations that arise in the eye-tracking output when sensor shifts occur,\n",
            "and assess the resulting degradation in accuracy for different combinations of\n",
            "eye movements and sensor shifts.\n",
            "Actual Title : \n",
            "Photosensor Oculography: Survey and Parametric Analysis of Designs using\n",
            "  Model-Based Simulation\n",
            "Generated Title : \n",
            "<unk> survey and parametric analysis of designs\n",
            "Abstract : \n",
            "Many models of interest in the natural and social sciences have no\n",
            "closed-form likelihood function, which means that they cannot be treated using\n",
            "the usual techniques of statistical inference. In the case where such models\n",
            "can be efficiently simulated, Bayesian inference is still possible thanks to\n",
            "the Approximate Bayesian Computation (ABC) algorithm. Although many refinements\n",
            "have been suggested, ABC inference is still far from routine. ABC is often\n",
            "excruciatingly slow due to very low acceptance rates. In addition, ABC requires\n",
            "introducing a vector of \"summary statistics\", the choice of which is relatively\n",
            "arbitrary, and often require some trial and error, making the whole process\n",
            "quite laborious for the user.\n",
            "  We introduce in this work the EP-ABC algorithm, which is an adaptation to the\n",
            "likelihood-free context of the variational approximation algorithm known as\n",
            "Expectation Propagation (Minka, 2001). The main advantage of EP-ABC is that it\n",
            "is faster by a few orders of magnitude than standard algorithms, while\n",
            "producing an overall approximation error which is typically negligible. A\n",
            "second advantage of EP-ABC is that it replaces the usual global ABC constraint\n",
            "on the vector of summary statistics computed on the whole dataset, by n local\n",
            "constraints of the form that apply separately to each data-point. As a\n",
            "consequence, it is often possible to do away with summary statistics entirely.\n",
            "In that case, EP-ABC approximates directly the evidence (marginal likelihood)\n",
            "of the model.\n",
            "  Comparisons are performed in three real-world applications which are typical\n",
            "of likelihood-free inference, including one application in neuroscience which\n",
            "is novel, and possibly too challenging for standard ABC techniques.\n",
            "Actual Title : \n",
            "Expectation-Propagation for Likelihood-Free Inference\n",
            "Generated Title : \n",
            "for likelihood free inference\n",
            "Abstract : \n",
            "In this paper we develop a dynamic form of Bayesian optimization for machine\n",
            "learning models with the goal of rapidly finding good hyperparameter settings.\n",
            "Our method uses the partial information gained during the training of a machine\n",
            "learning model in order to decide whether to pause training and start a new\n",
            "model, or resume the training of a previously-considered model. We specifically\n",
            "tailor our method to machine learning problems by developing a novel\n",
            "positive-definite covariance kernel to capture a variety of training curves.\n",
            "Furthermore, we develop a Gaussian process prior that scales gracefully with\n",
            "additional temporal observations. Finally, we provide an information-theoretic\n",
            "framework to automate the decision process. Experiments on several common\n",
            "machine learning models show that our approach is extremely effective in\n",
            "practice.\n",
            "Actual Title : \n",
            "Freeze-Thaw Bayesian Optimization\n",
            "Generated Title : \n",
            "process learning using bayesian optimization\n",
            "Abstract : \n",
            "We are presenting work on recognising acronyms of the form Long-Form\n",
            "(Short-Form) such as \"International Monetary Fund (IMF)\" in millions of news\n",
            "articles in twenty-two languages, as part of our more general effort to\n",
            "recognise entities and their variants in news text and to use them for the\n",
            "automatic analysis of the news, including the linking of related news across\n",
            "languages. We show how the acronym recognition patterns, initially developed\n",
            "for medical terms, needed to be adapted to the more general news domain and we\n",
            "present evaluation results. We describe our effort to automatically merge the\n",
            "numerous long-form variants referring to the same short-form, while keeping\n",
            "non-related long-forms separate. Finally, we provide extensive statistics on\n",
            "the frequency and the distribution of short-form/long-form pairs across\n",
            "languages.\n",
            "Actual Title : \n",
            "Acronym recognition and processing in 22 languages\n",
            "Generated Title : \n",
            "recognition and processing\n",
            "Abstract : \n",
            "The recent, remarkable growth of machine learning has led to intense interest\n",
            "in the privacy of the data on which machine learning relies, and to new\n",
            "techniques for preserving privacy. However, older ideas about privacy may well\n",
            "remain valid and useful. This note reviews two recent works on privacy in the\n",
            "light of the wisdom of some of the early literature, in particular the\n",
            "principles distilled by Saltzer and Schroeder in the 1970s.\n",
            "Actual Title : \n",
            "On the Protection of Private Information in Machine Learning Systems:\n",
            "  Two Recent Approaches\n",
            "Generated Title : \n",
            "of the machine learning\n",
            "Abstract : \n",
            "The field of Automatic Facial Expression Analysis has grown rapidly in recent\n",
            "years. However, despite progress in new approaches as well as benchmarking\n",
            "efforts, most evaluations still focus on either posed expressions, near-frontal\n",
            "recordings, or both. This makes it hard to tell how existing expression\n",
            "recognition approaches perform under conditions where faces appear in a wide\n",
            "range of poses (or camera views), displaying ecologically valid expressions.\n",
            "The main obstacle for assessing this is the availability of suitable data, and\n",
            "the challenge proposed here addresses this limitation. The FG 2017 Facial\n",
            "Expression Recognition and Analysis challenge (FERA 2017) extends FERA 2015 to\n",
            "the estimation of Action Units occurrence and intensity under different camera\n",
            "views. In this paper we present the third challenge in automatic recognition of\n",
            "facial expressions, to be held in conjunction with the 12th IEEE conference on\n",
            "Face and Gesture Recognition, May 2017, in Washington, United States. Two\n",
            "sub-challenges are defined: the detection of AU occurrence, and the estimation\n",
            "of AU intensity. In this work we outline the evaluation protocol, the data\n",
            "used, and the results of a baseline method for both sub-challenges.\n",
            "Actual Title : \n",
            "FERA 2017 - Addressing Head Pose in the Third Facial Expression\n",
            "  Recognition and Analysis Challenge\n",
            "Generated Title : \n",
            "facial expression recognition recognition recognition\n",
            "Abstract : \n",
            "We present a loss function for neural networks that encompasses an idea of\n",
            "trivial versus non-trivial predictions, such that the network jointly\n",
            "determines its own prediction goals and learns to satisfy them. This permits\n",
            "the network to choose sub-sets of a problem which are most amenable to its\n",
            "abilities to focus on solving, while discarding 'distracting' elements that\n",
            "interfere with its learning. To do this, the network first transforms the raw\n",
            "data into a higher-level categorical representation, and then trains a\n",
            "predictor from that new time series to its future. To prevent a trivial\n",
            "solution of mapping the signal to zero, we introduce a measure of\n",
            "non-triviality via a contrast between the prediction error of the learned model\n",
            "with a naive model of the overall signal statistics. The transform can learn to\n",
            "discard uninformative and unpredictable components of the signal in favor of\n",
            "the features which are both highly predictive and highly predictable. This\n",
            "creates a coarse-grained model of the time-series dynamics, focusing on\n",
            "predicting the slowly varying latent parameters which control the statistics of\n",
            "the time-series, rather than predicting the fast details directly. The result\n",
            "is a semi-supervised algorithm which is capable of extracting latent\n",
            "parameters, segmenting sections of time-series with differing statistics, and\n",
            "building a higher-level representation of the underlying dynamics from\n",
            "unlabeled data.\n",
            "Actual Title : \n",
            "Neural Coarse-Graining: Extracting slowly-varying latent degrees of\n",
            "  freedom with neural networks\n",
            "Generated Title : \n",
            "neural network that are <unk> networks  a <unk>\n",
            "Abstract : \n",
            "Information on the web is prodigious; searching relevant information is\n",
            "difficult making web users to rely on search engines for finding relevant\n",
            "information on the web. Search engines index and categorize web pages according\n",
            "to their contents using crawlers and rank them accordingly. For given user\n",
            "query they retrieve millions of webpages and display them to users according to\n",
            "web-page rank. Every search engine has their own algorithms based on certain\n",
            "parameters for ranking web-pages. Search Engine Optimization (SEO) is that\n",
            "technique by which webmasters try to improve ranking of their websites by\n",
            "optimizing it according to search engines ranking parameters. It is the aim of\n",
            "this research to identify the most popular SEO techniques used by search\n",
            "engines for ranking web-pages and to establish their importance for indexing\n",
            "and categorizing web data. The research tries to establish that using more SEO\n",
            "parameters in ranking algorithms helps in retrieving better search results thus\n",
            "increasing user satisfaction.\n",
            "  In the accomplished research, a web based Meta search engine is proposed to\n",
            "aggregates search results from different search engines and rank web-pages\n",
            "based on new page ranking algorithm which will assign heuristic page rank to\n",
            "web-pages based on SEO parameters such as title tag, Meta description, sitemap\n",
            "etc. The research also provides insight into techniques which webmasters can\n",
            "use for better ranking their websites in Google and Bing.\n",
            "  Initial results has shown that using certain SEO parameters in present\n",
            "ranking algorithm helps in retrieving more useful results for user queries.\n",
            "These results generated from Meta search engine outperformed existing search\n",
            "engines in terms of better retrieved search results and high precision.\n",
            "Actual Title : \n",
            "Intelligent Search Optimization using Artificial Fuzzy Logics\n",
            "Generated Title : \n",
            "optimization optimization optimization using fuzzy search and searching\n",
            "Abstract : \n",
            "Discussion forums are an important source of information. They are often used\n",
            "to answer specific questions a user might have and to discover more about a\n",
            "topic of interest. Discussions in these forums may evolve in intricate ways,\n",
            "making it difficult for users to follow the flow of ideas. We propose a novel\n",
            "approach for automatically identifying the underlying thread structure of a\n",
            "forum discussion. Our approach is based on a neural model that computes\n",
            "coherence scores of possible reconstructions and then selects the highest\n",
            "scoring, i.e., the most coherent one. Preliminary experiments demonstrate\n",
            "promising results outperforming a number of strong baseline methods.\n",
            "Actual Title : \n",
            "Thread Reconstruction in Conversational Data using Neural Coherence\n",
            "  Models\n",
            "Generated Title : \n",
            "reconstruction in conversational data using neural neural\n",
            "Abstract : \n",
            "PARAFAC2 has demonstrated success in modeling irregular tensors, where the\n",
            "tensor dimensions vary across one of the modes. An example scenario is jointly\n",
            "modeling treatments across a set of patients with varying number of medical\n",
            "encounters, where the alignment of events in time bears no clinical meaning,\n",
            "and it may also be impossible to align them due to their varying length.\n",
            "Despite recent improvements on scaling up unconstrained PARAFAC2, its model\n",
            "factors are usually dense and sensitive to noise which limits their\n",
            "interpretability. As a result, the following open challenges remain: a) various\n",
            "modeling constraints, such as temporal smoothness, sparsity and non-negativity,\n",
            "are needed to be imposed for interpretable temporal modeling and b) a scalable\n",
            "approach is required to support those constraints efficiently for large\n",
            "datasets. To tackle these challenges, we propose a COnstrained PARAFAC2 (COPA)\n",
            "method, which carefully incorporates optimization constraints such as temporal\n",
            "smoothness, sparsity, and non-negativity in the resulting factors. To\n",
            "efficiently support all those constraints, COPA adopts a hybrid optimization\n",
            "framework using alternating optimization and alternating direction method of\n",
            "multiplier (AO-ADMM). As evaluated on large electronic health record (EHR)\n",
            "datasets with hundreds of thousands of patients, COPA achieves significant\n",
            "speedups (up to 36x faster) over prior PARAFAC2 approaches that only attempt to\n",
            "handle a subset of the constraints that COPA enables. Overall, our method\n",
            "outperforms all the baselines attempting to handle a subset of the constraints\n",
            "in terms of speed, while achieving the same level of accuracy.\n",
            "Actual Title : \n",
            "COPA: Constrained PARAFAC2 for Sparse & Large Datasets\n",
            "Generated Title : \n",
            "constrained constrained <unk> with constrained temporal datasets\n",
            "Abstract : \n",
            "Although face recognition has been improved much as the development of Deep\n",
            "Neural Networks, SIPP(Single Image Per Person) problem in face recognition has\n",
            "not been better solved, especially in practical applications where searching\n",
            "over complicated database. In this paper, a combination of modified mean search\n",
            "and LSH method would be introduced orderly to improve the precision and recall\n",
            "of SIPP face recognition without retrain of the DNN model. First, a modified\n",
            "SVD based augmentation method would be introduced to get more intra-class\n",
            "variations even for person with only one image. Second, an unique rule based\n",
            "combination of modified mean search and LSH method was proposed the first time\n",
            "to help get the most similar personID in a complicated dataset, and some\n",
            "theoretical explaining followed. Third, we would like to emphasize, no need to\n",
            "retrain of the DNN model and would easy to be extended without much efforts. We\n",
            "do some practical testing in competition of Msceleb challenge-2 2017 which was\n",
            "hold by Microsoft Research, great improvement of coverage from 13.39% to\n",
            "19.25%, 29.94%, 42.11%, 47.52% at precision 99%(P99) would be shown latter,\n",
            "coverage reach 94.2% and 100% at precision 97%(P97) and 95%(P95) respectively.\n",
            "As far as we known, this is the only paper who do not fine-tuning on\n",
            "competition dataset and ranked top-10. A similar test on CASIA WebFace dataset\n",
            "also demonstrated the same improvements on both precision and recall.\n",
            "Actual Title : \n",
            "Improving precision and recall of face recognition in SIPP with\n",
            "  combination of modified mean search and LSH\n",
            "Generated Title : \n",
            "precision recognition using modified mean and and robust\n",
            "Abstract : \n",
            "We propose a novel word embedding pre-training approach that exploits writing\n",
            "errors in learners' scripts. We compare our method to previous models that tune\n",
            "the embeddings based on script scores and the discrimination between correct\n",
            "and corrupt word contexts in addition to the generic commonly-used embeddings\n",
            "pre-trained on large corpora. The comparison is achieved by using the\n",
            "aforementioned models to bootstrap a neural network that learns to predict a\n",
            "holistic score for scripts. Furthermore, we investigate augmenting our model\n",
            "with error corrections and monitor the impact on performance. Our results show\n",
            "that our error-oriented approach outperforms other comparable ones which is\n",
            "further demonstrated when training on more data. Additionally, extending the\n",
            "model with corrections provides further performance gains when data sparsity is\n",
            "an issue.\n",
            "Actual Title : \n",
            "An Error-Oriented Approach to Word Embedding Pre-Training\n",
            "Generated Title : \n",
            "word embeddings for word expansion\n",
            "Abstract : \n",
            "A standard approach to approximate inference in state-space models isto apply\n",
            "a particle filter, e.g., the Condensation Algorithm.However, the performance of\n",
            "particle filters often varies significantlydue to their stochastic nature.We\n",
            "present a class of algorithms, called lattice particle filters, thatcircumvent\n",
            "this difficulty by placing the particles deterministicallyaccording to a\n",
            "Quasi-Monte Carlo integration rule.We describe a practical realization of this\n",
            "idea, discuss itstheoretical properties, and its efficiency.Experimental\n",
            "results with a synthetic 2D tracking problem show that thelattice particle\n",
            "filter is equivalent to a conventional particle filterthat has between 10 and\n",
            "60% more particles, depending ontheir \"sparsity\" in the state-space.We also\n",
            "present results on inferring 3D human motion frommoving light displays.\n",
            "Actual Title : \n",
            "Lattice Particle Filters\n",
            "Generated Title : \n",
            "particle particle particle filter\n",
            "Abstract : \n",
            "One of the most famous drawings by Leonardo da Vinci is a self-portrait in\n",
            "red chalk, where he looks quite old. In fact, there is a sketch in one of his\n",
            "notebooks, partially covered by written notes, that can be a self-portrait of\n",
            "the artist when he was young. The use of image processing, to remove the\n",
            "handwritten text and improve the image, allows a comparison of the two\n",
            "portraits.\n",
            "Actual Title : \n",
            "A self-portrait of young Leonardo\n",
            "Generated Title : \n",
            "self portrait of leonardo\n",
            "Abstract : \n",
            "Recovering matrices from compressive and grossly corrupted observations is a\n",
            "fundamental problem in robust statistics, with rich applications in computer\n",
            "vision and machine learning. In theory, under certain conditions, this problem\n",
            "can be solved in polynomial time via a natural convex relaxation, known as\n",
            "Compressive Principal Component Pursuit (CPCP). However, all existing provable\n",
            "algorithms for CPCP suffer from superlinear per-iteration cost, which severely\n",
            "limits their applicability to large scale problems. In this paper, we propose\n",
            "provable, scalable and efficient methods to solve CPCP with (essentially)\n",
            "linear per-iteration cost. Our method combines classical ideas from Frank-Wolfe\n",
            "and proximal methods. In each iteration, we mainly exploit Frank-Wolfe to\n",
            "update the low-rank component with rank-one SVD and exploit the proximal step\n",
            "for the sparse term. Convergence results and implementation details are also\n",
            "discussed. We demonstrate the scalability of the proposed approach with\n",
            "promising numerical experiments on visual data.\n",
            "Actual Title : \n",
            "Scalable Robust Matrix Recovery: Frank-Wolfe Meets Proximal Methods\n",
            "Generated Title : \n",
            "low rank low rank matrix recovery  frank wolfe methods\n",
            "Abstract : \n",
            "A semi-parametric, non-linear regression model in the presence of latent\n",
            "variables is introduced. These latent variables can correspond to unmodeled\n",
            "phenomena or unmeasured agents in a complex networked system. This new\n",
            "formulation allows joint estimation of certain non-linearities in the system,\n",
            "the direct interactions between measured variables, and the effects of\n",
            "unmodeled elements on the observed system. The particular form of the model\n",
            "adopted is justified, and learning is posed as a regularized maximum likelihood\n",
            "estimation. This leads to classes of structured convex optimization problems\n",
            "with a \"sparse plus low-rank\" flavor. Relations between the proposed model and\n",
            "several common model paradigms, such as those of Robust Principal Component\n",
            "Analysis (PCA) and Vector Autoregression (VAR), are established. Particularly\n",
            "in the VAR setting, the low-rank contributions can come from broad trends\n",
            "exhibited in the time series. Details of the algorithm for learning the model\n",
            "are presented. Experiments demonstrate the performance of the model and the\n",
            "estimation algorithm on simulated and real data.\n",
            "Actual Title : \n",
            "SILVar: Single Index Latent Variable Models\n",
            "Generated Title : \n",
            "estimation from latent variable models\n",
            "Abstract : \n",
            "An inductive probabilistic classification rule must generally obey the\n",
            "principles of Bayesian predictive inference, such that all observed and\n",
            "unobserved stochastic quantities are jointly modeled and the parameter\n",
            "uncertainty is fully acknowledged through the posterior predictive\n",
            "distribution. Several such rules have been recently considered and their\n",
            "asymptotic behavior has been characterized under the assumption that the\n",
            "observed features or variables used for building a classifier are conditionally\n",
            "independent given a simultaneous labeling of both the training samples and\n",
            "those from an unknown origin. Here we extend the theoretical results to\n",
            "predictive classifiers acknowledging feature dependencies either through\n",
            "graphical models or sparser alternatives defined as stratified graphical\n",
            "models. We also show through experimentation with both synthetic and real data\n",
            "that the predictive classifiers based on stratified graphical models have\n",
            "consistently best accuracy compared with the predictive classifiers based on\n",
            "either conditionally independent features or on ordinary graphical models.\n",
            "Actual Title : \n",
            "Marginal and simultaneous predictive classification using stratified\n",
            "  graphical models\n",
            "Generated Title : \n",
            "feature learning for predictive predictive models predictive predictive\n",
            "Abstract : \n",
            "Learning by children and animals occurs effortlessly and largely without\n",
            "obvious supervision. Successes in automating supervised learning have not\n",
            "translated to the more ambiguous realm of unsupervised learning where goals and\n",
            "labels are not provided. Barlow (1961) suggested that the signal that brains\n",
            "leverage for unsupervised learning is dependence, or redundancy, in the sensory\n",
            "environment. Dependence can be characterized using the information-theoretic\n",
            "multivariate mutual information measure called total correlation. The principle\n",
            "of Total Cor-relation Ex-planation (CorEx) is to learn representations of data\n",
            "that \"explain\" as much dependence in the data as possible. We review some\n",
            "manifestations of this principle along with successes in unsupervised learning\n",
            "problems across diverse domains including human behavior, biology, and\n",
            "language.\n",
            "Actual Title : \n",
            "Unsupervised Learning via Total Correlation Explanation\n",
            "Generated Title : \n",
            "from <unk> unsupervised learning from unsupervised learning\n",
            "Abstract : \n",
            "The heuristic identification of peaks from noisy complex spectra often leads\n",
            "to misunderstanding of the physical and chemical properties of matter. In this\n",
            "paper, we propose a framework based on Bayesian inference, which enables us to\n",
            "separate multipeak spectra into single peaks statistically and consists of two\n",
            "steps. The first step is estimating both the noise variance and the number of\n",
            "peaks as hyperparameters based on Bayes free energy, which generally is not\n",
            "analytically tractable. The second step is fitting the parameters of each peak\n",
            "function to the given spectrum by calculating the posterior density, which has\n",
            "a problem of local minima and saddles since multipeak models are nonlinear and\n",
            "hierarchical. Our framework enables the escape from local minima or saddles by\n",
            "using the exchange Monte Carlo method and calculates Bayes free energy via the\n",
            "multiple histogram method. We discuss a simulation demonstrating how efficient\n",
            "our framework is and show that estimating both the noise variance and the\n",
            "number of peaks prevents overfitting, overpenalizing, and misunderstanding the\n",
            "precision of parameter estimation.\n",
            "Actual Title : \n",
            "Simultaneous Estimation of Noise Variance and Number of Peaks in\n",
            "  Bayesian Spectral Deconvolution\n",
            "Generated Title : \n",
            "inference for model based variance reduction of peaks in\n",
            "Abstract : \n",
            "This paper proposes an unsupervised learning technique by using Multi-layer\n",
            "Mirroring Neural Network and Forgy's clustering algorithm. Multi-layer\n",
            "Mirroring Neural Network is a neural network that can be trained with\n",
            "generalized data inputs (different categories of image patterns) to perform\n",
            "non-linear dimensionality reduction and the resultant low-dimensional code is\n",
            "used for unsupervised pattern classification using Forgy's algorithm. By\n",
            "adapting the non-linear activation function (modified sigmoidal function) and\n",
            "initializing the weights and bias terms to small random values, mirroring of\n",
            "the input pattern is initiated. In training, the weights and bias terms are\n",
            "changed in such a way that the input presented is reproduced at the output by\n",
            "back propagating the error. The mirroring neural network is capable of reducing\n",
            "the input vector to a great degree (approximately 1/30th the original size) and\n",
            "also able to reconstruct the input pattern at the output layer from this\n",
            "reduced code units. The feature set (output of central hidden layer) extracted\n",
            "from this network is fed to Forgy's algorithm, which classify input data\n",
            "patterns into distinguishable classes. In the implementation of Forgy's\n",
            "algorithm, initial seed points are selected in such a way that they are distant\n",
            "enough to be perfectly grouped into different categories. Thus a new method of\n",
            "unsupervised learning is formulated and demonstrated in this paper. This method\n",
            "gave impressive results when applied to classification of different image\n",
            "patterns.\n",
            "Actual Title : \n",
            "Automatic Pattern Classification by Unsupervised Learning Using\n",
            "  Dimensionality Reduction of Data with Mirroring Neural Networks\n",
            "Generated Title : \n",
            "learning with unsupervised time series\n",
            "Abstract : \n",
            "Authorship attribution mainly deals with undecided authorship of literary\n",
            "texts. Authorship attribution is useful in resolving issues like uncertain\n",
            "authorship, recognize authorship of unknown texts, spot plagiarism so on.\n",
            "Statistical methods can be used to set apart the approach of an author\n",
            "numerically. The basic methodologies that are made use in computational\n",
            "stylometry are word length, sentence length, vocabulary affluence, frequencies\n",
            "etc. Each author has an inborn style of writing, which is particular to\n",
            "himself. Statistical quantitative techniques can be used to differentiate the\n",
            "approach of an author in a numerical way. The problem can be broken down into\n",
            "three sub problems as author identification, author characterization and\n",
            "similarity detection. The steps involved are pre-processing, extracting\n",
            "features, classification and author identification. For this different\n",
            "classifiers can be used. Here fuzzy learning classifier and SVM are used. After\n",
            "author identification the SVM was found to have more accuracy than Fuzzy\n",
            "classifier. Later combined the classifiers to obtain a better accuracy when\n",
            "compared to individual SVM and fuzzy classifier.\n",
            "Actual Title : \n",
            "Text Classification For Authorship Attribution Analysis\n",
            "Generated Title : \n",
            "approach for text classification\n",
            "Abstract : \n",
            "POMDPs are standard models for probabilistic planning problems, where an\n",
            "agent interacts with an uncertain environment. We study the problem of\n",
            "almost-sure reachability, where given a set of target states, the question is\n",
            "to decide whether there is a policy to ensure that the target set is reached\n",
            "with probability 1 (almost-surely). While in general the problem is\n",
            "EXPTIME-complete, in many practical cases policies with a small amount of\n",
            "memory suffice. Moreover, the existing solution to the problem is explicit,\n",
            "which first requires to construct explicitly an exponential reduction to a\n",
            "belief-support MDP. In this work, we first study the existence of\n",
            "observation-stationary strategies, which is NP-complete, and then small-memory\n",
            "strategies. We present a symbolic algorithm by an efficient encoding to SAT and\n",
            "using a SAT solver for the problem. We report experimental results\n",
            "demonstrating the scalability of our symbolic (SAT-based) approach.\n",
            "Actual Title : \n",
            "A Symbolic SAT-based Algorithm for Almost-sure Reachability with Small\n",
            "  Strategies in POMDPs\n",
            "Generated Title : \n",
            "the use of solving symbolic problem solving the\n",
            "Abstract : \n",
            "While deep reinforcement learning (RL) methods have achieved unprecedented\n",
            "successes in a range of challenging problems, their applicability has been\n",
            "mainly limited to simulation or game domains due to the high sample complexity\n",
            "of the trial-and-error learning process. However, real-world robotic\n",
            "applications often need a data-efficient learning process with safety-critical\n",
            "constraints. In this paper, we consider the challenging problem of learning\n",
            "unmanned aerial vehicle (UAV) control for tracking a moving target. To acquire\n",
            "a strategy that combines perception and control, we represent the policy by a\n",
            "convolutional neural network. We develop a hierarchical approach that combines\n",
            "a model-free policy gradient method with a conventional feedback\n",
            "proportional-integral-derivative (PID) controller to enable stable learning\n",
            "without catastrophic failure. The neural network is trained by a combination of\n",
            "supervised learning from raw images and reinforcement learning from games of\n",
            "self-play. We show that the proposed approach can learn a target following\n",
            "policy in a simulator efficiently and the learned behavior can be successfully\n",
            "transferred to the DJI quadrotor platform for real-world UAV control.\n",
            "Actual Title : \n",
            "Learning Unmanned Aerial Vehicle Control for Autonomous Target Following\n",
            "Generated Title : \n",
            "to navigate for real time control of deep policies\n",
            "Abstract : \n",
            "When designing systems that are complex, dynamic and stochastic in nature,\n",
            "simulation is generally recognised as one of the best design support\n",
            "technologies, and a valuable aid in the strategic and tactical decision making\n",
            "process. A simulation model consists of a set of rules that define how a system\n",
            "changes over time, given its current state. Unlike analytical models, a\n",
            "simulation model is not solved but is run and the changes of system states can\n",
            "be observed at any point in time. This provides an insight into system dynamics\n",
            "rather than just predicting the output of a system based on specific inputs.\n",
            "Simulation is not a decision making tool but a decision support tool, allowing\n",
            "better informed decisions to be made. Due to the complexity of the real world,\n",
            "a simulation model can only be an approximation of the target system. The\n",
            "essence of the art of simulation modelling is abstraction and simplification.\n",
            "Only those characteristics that are important for the study and analysis of the\n",
            "target system should be included in the simulation model.\n",
            "Actual Title : \n",
            "Introduction to Multi-Agent Simulation\n",
            "Generated Title : \n",
            "multi agent simulation\n",
            "Abstract : \n",
            "One of the important requirements in image retrieval, indexing,\n",
            "classification, clustering and etc. is extracting efficient features from\n",
            "images. The color feature is one of the most widely used visual features. Use\n",
            "of color histogram is the most common way for representing color feature. One\n",
            "of disadvantage of the color histogram is that it does not take the color\n",
            "spatial distribution into consideration. In this paper dynamic color\n",
            "distribution entropy of neighborhoods method based on color distribution\n",
            "entropy is presented, which effectively describes the spatial information of\n",
            "colors. The image retrieval results in compare to improved color distribution\n",
            "entropy show the acceptable efficiency of this approach.\n",
            "Actual Title : \n",
            "A New Color Feature Extraction Method Based on Dynamic Color\n",
            "  Distribution Entropy of Neighborhoods\n",
            "Generated Title : \n",
            "feature selection using color feature and color\n",
            "Abstract : \n",
            "Recent studies have highlighted the vulnerability of deep neural networks\n",
            "(DNNs) to adversarial examples - a visually indistinguishable adversarial image\n",
            "can easily be crafted to cause a well-trained model to misclassify. Existing\n",
            "methods for crafting adversarial examples are based on $L_2$ and $L_\\infty$\n",
            "distortion metrics. However, despite the fact that $L_1$ distortion accounts\n",
            "for the total variation and encourages sparsity in the perturbation, little has\n",
            "been developed for crafting $L_1$-based adversarial examples. In this paper, we\n",
            "formulate the process of attacking DNNs via adversarial examples as an\n",
            "elastic-net regularized optimization problem. Our elastic-net attacks to DNNs\n",
            "(EAD) feature $L_1$-oriented adversarial examples and include the\n",
            "state-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,\n",
            "CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial\n",
            "examples with small $L_1$ distortion and attains similar attack performance to\n",
            "the state-of-the-art methods in different attack scenarios. More importantly,\n",
            "EAD leads to improved attack transferability and complements adversarial\n",
            "training for DNNs, suggesting novel insights on leveraging $L_1$ distortion in\n",
            "adversarial machine learning and security implications of DNNs.\n",
            "Actual Title : \n",
            "EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial\n",
            "  Examples\n",
            "Generated Title : \n",
            "<unk> neural networks\n",
            "Abstract : \n",
            "Two types of probabilistic maps are popular in the mobile robotics\n",
            "literature: occupancy grids and geometric maps. Occupancy grids have the\n",
            "advantages of simplicity and speed, but they represent only a restricted class\n",
            "of maps and they make incorrect independence assumptions. On the other hand,\n",
            "current geometric approaches, which characterize the environment by features\n",
            "such as line segments, can represent complex environments compactly. However,\n",
            "they do not reason explicitly about occupancy, a necessity for motion planning;\n",
            "and, they lack a complete probability model over environmental structures. In\n",
            "this paper we present a probabilistic mapping technique based on polygonal\n",
            "random fields (PRF), which combines the advantages of both approaches. Our\n",
            "approach explicitly represents occupancy using a geometric representation, and\n",
            "it is based upon a consistent probability distribution over environments which\n",
            "avoids the incorrect independence assumptions made by occupancy grids. We show\n",
            "how sampling techniques for PRFs can be applied to localized laser and sonar\n",
            "data, and we demonstrate significant improvements in mapping performance over\n",
            "occupancy grids.\n",
            "Actual Title : \n",
            "Robotic Mapping with Polygonal Random Fields\n",
            "Generated Title : \n",
            "mapping robotic mapping with a random projection approach\n",
            "Abstract : \n",
            "Unlike traditional programs (such as operating systems or word processors)\n",
            "which have large amounts of code, machine learning tasks use programs with\n",
            "relatively small amounts of code (written in machine learning libraries), but\n",
            "voluminous amounts of data. Just like developers of traditional programs debug\n",
            "errors in their code, developers of machine learning tasks debug and fix errors\n",
            "in their data. However, algorithms and tools for debugging and fixing errors in\n",
            "data are less common, when compared to their counterparts for detecting and\n",
            "fixing errors in code. In this paper, we consider classification tasks where\n",
            "errors in training data lead to misclassifications in test points, and propose\n",
            "an automated method to find the root causes of such misclassifications. Our\n",
            "root cause analysis is based on Pearl's theory of causation, and uses Pearl's\n",
            "PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,\n",
            "encodes the computation of PS as a probabilistic program, and uses recent work\n",
            "on probabilistic programs and transformations on probabilistic programs (along\n",
            "with gray-box models of machine learning algorithms) to efficiently compute PS.\n",
            "Psi is able to identify root causes of data errors in interesting data sets.\n",
            "Actual Title : \n",
            "Debugging Machine Learning Tasks\n",
            "Generated Title : \n",
            "machine learning\n",
            "Abstract : \n",
            "The present study is focused on the automatic identification and description\n",
            "of frozen similes in British and French novels written between the 19 th\n",
            "century and the beginning of the 20 th century. Two main patterns of frozen\n",
            "similes were considered: adjectival ground + simile marker + nominal vehicle\n",
            "(e.g. happy as a lark) and eventuality + simile marker + nominal vehicle (e.g.\n",
            "sleep like a top). All potential similes and their components were first\n",
            "extracted using a rule-based algorithm. Then, frozen similes were identified\n",
            "based on reference lists of existing similes and semantic distance between the\n",
            "tenor and the vehicle. The results obtained tend to confirm the fact that\n",
            "frozen similes are not used haphazardly in literary texts. In addition,\n",
            "contrary to how they are often presented, frozen similes often go beyond the\n",
            "ground or the eventuality and the vehicle to also include the tenor.\n",
            "Actual Title : \n",
            "\"Pale as death\" or \"ple comme la mort\" : Frozen similes used as\n",
            "  literary clichs\n",
            "Generated Title : \n",
            "<unk> death  or <unk> comme la <unk> <unk>\n",
            "Abstract : \n",
            "Symbol detection techniques in online handwritten graphics (e.g. diagrams and\n",
            "mathematical expressions) consist of methods specifically designed for a single\n",
            "graphic type. In this work, we evaluate the Faster R-CNN object detection\n",
            "algorithm as a general method for detection of symbols in handwritten graphics.\n",
            "We evaluate different configurations of the Faster R-CNN method, and point out\n",
            "issues relative to the handwritten nature of the data. Considering the online\n",
            "recognition context, we evaluate efficiency and accuracy trade-offs of using\n",
            "Deep Neural Networks of different complexities as feature extractors. We\n",
            "evaluate the method on publicly available flowchart and mathematical expression\n",
            "(CROHME-2016) datasets. Results show that Faster R-CNN can be effectively used\n",
            "on both datasets, enabling the possibility of developing general methods for\n",
            "symbol detection, and furthermore, general graphic understanding methods that\n",
            "could be built on top of the algorithm.\n",
            "Actual Title : \n",
            "Symbol detection in online handwritten graphics using Faster R-CNN\n",
            "Generated Title : \n",
            "detection in online handwritten graphics\n",
            "Abstract : \n",
            "Our study applies statistical methods to French and Italian corpora to\n",
            "examine the phenomenon of multi-word term reduction in specialty languages.\n",
            "There are two kinds of reduction: anaphoric and lexical. We show that anaphoric\n",
            "reduction depends on the discourse type (vulgarization, pedagogical,\n",
            "specialized) but is independent of both domain and language; that lexical\n",
            "reduction depends on domain and is more frequent in technical, rapidly evolving\n",
            "domains; and that anaphoric reductions tend to follow full terms rather than\n",
            "precede them. We define the notion of the anaphoric tree of the term and study\n",
            "its properties. Concerning lexical reduction, we attempt to prove statistically\n",
            "that there is a notion of term lifecycle, where the full form is progressively\n",
            "replaced by a lexical reduction. ----- Nous \\'etudions par des m\\'ethodes\n",
            "statistiques sur des corpus fran\\c{c}ais et italiens, le ph\\'enom\\`ene de\n",
            "r\\'eduction des termes complexes dans les langues de sp\\'ecialit\\'e. Il existe\n",
            "deux types de r\\'eductions : anaphorique et lexicale. Nous montrons que la\n",
            "r\\'eduction anaphorique d\\'epend du type de discours (de vulgarisation,\n",
            "p\\'edagogique, sp\\'ecialis\\'e) mais ne d\\'epend ni du domaine, ni de la langue,\n",
            "alors que la r\\'eduction lexicale d\\'epend du domaine et est plus fr\\'equente\n",
            "dans les domaines techniques \\`a \\'evolution rapide. D'autre part, nous\n",
            "montrons que la r\\'eduction anaphorique a tendance \\`a suivre la forme pleine\n",
            "du terme, nous d\\'efinissons une notion d'arbre anaphorique de terme et nous\n",
            "\\'etudions ses propri\\'et\\'es. Concernant la r\\'eduction lexicale, nous tentons\n",
            "de d\\'emontrer statistiquement qu'il existe une notion de cycle de vie de\n",
            "terme, o\\`u la forme pleine est progressivement remplac\\'ee par une r\\'eduction\n",
            "lexicale.\n",
            "Actual Title : \n",
            "La rduction de termes complexes dans les langues de spcialit\n",
            "Generated Title : \n",
            "<unk> de <unk> de de <unk> <unk> de\n",
            "Abstract : \n",
            "Distance metric learning is of fundamental interest in machine learning\n",
            "because the distance metric employed can significantly affect the performance\n",
            "of many learning methods. Quadratic Mahalanobis metric learning is a popular\n",
            "approach to the problem, but typically requires solving a semidefinite\n",
            "programming (SDP) problem, which is computationally expensive. Standard\n",
            "interior-point SDP solvers typically have a complexity of $O(D^{6.5})$ (with\n",
            "$D$ the dimension of input data), and can thus only practically solve problems\n",
            "exhibiting less than a few thousand variables. Since the number of variables is\n",
            "$D (D+1) / 2 $, this implies a limit upon the size of problem that can\n",
            "practically be solved of around a few hundred dimensions. The complexity of the\n",
            "popular quadratic Mahalanobis metric learning approach thus limits the size of\n",
            "problem to which metric learning can be applied. Here we propose a\n",
            "significantly more efficient approach to the metric learning problem based on\n",
            "the Lagrange dual formulation of the problem. The proposed formulation is much\n",
            "simpler to implement, and therefore allows much larger Mahalanobis metric\n",
            "learning problems to be solved. The time complexity of the proposed method is\n",
            "$O (D ^ 3) $, which is significantly lower than that of the SDP approach.\n",
            "Experiments on a variety of datasets demonstrate that the proposed method\n",
            "achieves an accuracy comparable to the state-of-the-art, but is applicable to\n",
            "significantly larger problems. We also show that the proposed method can be\n",
            "applied to solve more general Frobenius-norm regularized SDP problems\n",
            "approximately.\n",
            "Actual Title : \n",
            "An Efficient Dual Approach to Distance Metric Learning\n",
            "Generated Title : \n",
            "dual dual metric learning\n",
            "Abstract : \n",
            "Bounds on the log partition function are important in a variety of contexts,\n",
            "including approximate inference, model fitting, decision theory, and large\n",
            "deviations analysis. We introduce a new class of upper bounds on the log\n",
            "partition function, based on convex combinations of distributions in the\n",
            "exponential domain, that is applicable to an arbitrary undirected graphical\n",
            "model. In the special case of convex combinations of tree-structured\n",
            "distributions, we obtain a family of variational problems, similar to the Bethe\n",
            "free energy, but distinguished by the following desirable properties: i. they\n",
            "are cnvex, and have a unique global minimum; and ii. the global minimum gives\n",
            "an upper bound on the log partition function. The global minimum is defined by\n",
            "stationary conditions very similar to those defining fixed points of belief\n",
            "propagation or tree-based reparameterization Wainwright et al., 2001. As with\n",
            "BP fixed points, the elements of the minimizing argument can be used as\n",
            "approximations to the marginals of the original model. The analysis described\n",
            "here can be extended to structures of higher treewidth e.g., hypertrees,\n",
            "thereby making connections with more advanced approximations e.g., Kikuchi and\n",
            "variants Yedidia et al., 2001; Minka, 2001.\n",
            "Actual Title : \n",
            "A New Class of Upper Bounds on the Log Partition Function\n",
            "Generated Title : \n",
            "of upper bounds on the propagation\n",
            "Abstract : \n",
            "In the last decade, special purpose computing systems, such as Neuromorphic\n",
            "computing, have become very popular in the field of computer vision and machine\n",
            "learning for classification tasks. In 2015, IBM's released the TrueNorth\n",
            "Neuromorphic system, kick-starting a new era of Neuromorphic computing.\n",
            "Alternatively, Deep Learning approaches such as Deep Convolutional Neural\n",
            "Networks (DCNN) show almost human-level accuracies for detection and\n",
            "classification tasks. IBM's 2016 release of a deep learning framework for\n",
            "DCNNs, called Energy Efficient Deep Neuromorphic Networks (Eedn). Eedn shows\n",
            "promise for delivering high accuracies across a number of different benchmarks,\n",
            "while consuming very low power, using IBM's TrueNorth chip. However, there are\n",
            "many things that remained undiscovered using the Eedn framework for\n",
            "classification tasks on a Neuromorphic system. In this paper, we have\n",
            "empirically evaluated the performance of different DCNN architectures\n",
            "implemented within the Eedn framework. The goal of this work was discover the\n",
            "most efficient way to implement DCNN models for object classification tasks\n",
            "using the TrueNorth system. We performed our experiments using benchmark data\n",
            "sets such as MNIST, COIL 20, and COIL 100. The experimental results show very\n",
            "promising classification accuracies with very low power consumption on IBM's\n",
            "NS1e Neurosynaptic system. The results show that for datasets with large\n",
            "numbers of classes, wider networks perform better when compared to deep\n",
            "networks comprised of nearly the same core complexity on IBM's TrueNorth\n",
            "system.\n",
            "Actual Title : \n",
            "Deep Versus Wide Convolutional Neural Networks for Object Recognition on\n",
            "  Neuromorphic System\n",
            "Generated Title : \n",
            "deep learning networks for <unk> and <unk> object\n",
            "Abstract : \n",
            "Recent advances in AI and robotics have claimed many incredible results with\n",
            "deep learning, yet no work to date has applied deep learning to the problem of\n",
            "liquid perception and reasoning. In this paper, we apply fully-convolutional\n",
            "deep neural networks to the tasks of detecting and tracking liquids. We\n",
            "evaluate three models: a single-frame network, multi-frame network, and a LSTM\n",
            "recurrent network. Our results show that the best liquid detection results are\n",
            "achieved when aggregating data over multiple frames and that the LSTM network\n",
            "outperforms the other two in both tasks. This suggests that LSTM-based neural\n",
            "networks have the potential to be a key component for enabling robots to handle\n",
            "liquids using robust, closed-loop controllers.\n",
            "Actual Title : \n",
            "Towards Learning to Perceive and Reason About Liquids\n",
            "Generated Title : \n",
            "<unk> to perceive and reason about liquids\n",
            "Abstract : \n",
            "Let $G=(V,E,w)$ be a finite, connected graph with weighted edges. We are\n",
            "interested in the problem of finding a subset $W \\subset V$ of vertices and\n",
            "weights $a_w$ such that $$ \\frac{1}{|V|}\\sum_{v \\in V}^{}{f(v)} \\sim \\sum_{w\n",
            "\\in W}{a_w f(w)}$$ for functions $f:V \\rightarrow \\mathbb{R}$ that are `smooth'\n",
            "with respect to the geometry of the graph. The main application are problems\n",
            "where $f$ is known to somehow depend on the underlying graph but is expensive\n",
            "to evaluate on even a single vertex. We prove an inequality showing that the\n",
            "integration problem can be rewritten as a geometric problem (`the optimal\n",
            "packing of heat balls'). We discuss how one would construct approximate\n",
            "solutions of the heat ball packing problem; numerical examples demonstrate the\n",
            "efficiency of the method.\n",
            "Actual Title : \n",
            "Numerical Integration on Graphs: where to sample and how to weigh\n",
            "Generated Title : \n",
            "completion on graphs  and <unk>\n",
            "Abstract : \n",
            "The implicit bias of gradient descent is not fully understood even in simple\n",
            "linear classification tasks (e.g., logistic regression). Soudry et al. (2018)\n",
            "studied this bias on separable data, where there are multiple solutions that\n",
            "correctly classify the data. It was found that, when optimizing monotonically\n",
            "decreasing loss functions with exponential tails using gradient descent, the\n",
            "linear classifier specified by the gradient descent iterates converge to the\n",
            "$L_2$ max margin separator. However, the convergence rate to the maximum margin\n",
            "solution with fixed step size was found to be extremely slow: $1/\\log(t)$.\n",
            "  Here we examine how the convergence is influenced by using different loss\n",
            "functions and by using variable step sizes. First, we calculate the convergence\n",
            "rate for loss functions with poly-exponential tails near $\\exp(-u^{\\nu})$. We\n",
            "prove that $\\nu=1$ yields the optimal convergence rate in the range $\\nu>0.25$.\n",
            "Based on further analysis we conjecture that this remains the optimal rate for\n",
            "$\\nu \\leq 0.25$, and even for sub-poly-exponential tails --- until loss\n",
            "functions with polynomial tails no longer converge to the max margin. Second,\n",
            "we prove the convergence rate could be improved to $(\\log t) /\\sqrt{t}$ for the\n",
            "exponential loss, by using aggressive step sizes which compensate for the\n",
            "rapidly vanishing gradients.\n",
            "Actual Title : \n",
            "Convergence of Gradient Descent on Separable Data\n",
            "Generated Title : \n",
            "convergence of gradient descent with gradient and gradient\n",
            "Abstract : \n",
            "Artificial bee colony (ABC), an optimization algorithm is a recent addition\n",
            "to the family of population based search algorithm. ABC has taken its\n",
            "inspiration from the collective intelligent foraging behavior of honey bees. In\n",
            "this study we have incorporated golden section search mechanism in the\n",
            "structure of basic ABC to improve the global convergence and prevent to stick\n",
            "on a local solution. The proposed variant is termed as ILS-ABC. Comparative\n",
            "numerical results with the state-of-art algorithms show the performance of the\n",
            "proposal when applied to the set of unconstrained engineering design problems.\n",
            "The simulated results show that the proposed variant can be successfully\n",
            "applied to solve real life problems.\n",
            "Actual Title : \n",
            "Improved Local Search in Artificial Bee Colony using Golden Section\n",
            "  Search\n",
            "Generated Title : \n",
            "search in <unk> bee colony optimization\n",
            "Abstract : \n",
            "In this paper, we propose a new network architecture for Chinese typography\n",
            "transformation based on deep learning. The architecture consists of two\n",
            "sub-networks: (1)a fully convolutional network(FCN) aiming at transferring\n",
            "specified typography style to another in condition of preserving structure\n",
            "information; (2)an adversarial network aiming at generating more realistic\n",
            "strokes in some details. Unlike models proposed before 2012 relying on the\n",
            "complex segmentation of Chinese components or strokes, our model treats every\n",
            "Chinese character as an inseparable image, so pre-processing or\n",
            "post-preprocessing are abandoned. Besides, our model adopts end-to-end training\n",
            "without pre-trained used in other deep models. The experiments demonstrates\n",
            "that our model can synthesize realistic-looking target typography from any\n",
            "source typography both on printed style and handwriting style.\n",
            "Actual Title : \n",
            "Chinese Typography Transfer\n",
            "Generated Title : \n",
            "typography transfer\n",
            "Abstract : \n",
            "Deep neural networks have been shown to achieve state-of-the-art performance\n",
            "in several machine learning tasks. Stochastic Gradient Descent (SGD) is the\n",
            "preferred optimization algorithm for training these networks and asynchronous\n",
            "SGD (ASGD) has been widely adopted for accelerating the training of large-scale\n",
            "deep networks in a distributed computing environment. However, in practice it\n",
            "is quite challenging to tune the training hyperparameters (such as learning\n",
            "rate) when using ASGD so as achieve convergence and linear speedup, since the\n",
            "stability of the optimization algorithm is strongly influenced by the\n",
            "asynchronous nature of parameter updates. In this paper, we propose a variant\n",
            "of the ASGD algorithm in which the learning rate is modulated according to the\n",
            "gradient staleness and provide theoretical guarantees for convergence of this\n",
            "algorithm. Experimental verification is performed on commonly-used image\n",
            "classification benchmarks: CIFAR10 and Imagenet to demonstrate the superior\n",
            "effectiveness of the proposed approach, compared to SSGD (Synchronous SGD) and\n",
            "the conventional ASGD algorithm.\n",
            "Actual Title : \n",
            "Staleness-aware Async-SGD for Distributed Deep Learning\n",
            "Generated Title : \n",
            "<unk> distributed deep distributed deep learning\n",
            "Abstract : \n",
            "In this paper, we propose an automated evaluation metric for text entry. We\n",
            "also consider possible improvements to existing text entry evaluation metrics,\n",
            "such as the minimum string distance error rate, keystrokes per character, cost\n",
            "per correction, and a unified approach proposed by MacKenzie, so they can\n",
            "accommodate the special characteristics of Chinese text. Current methods lack\n",
            "an integrated concern about both typing speed and accuracy for Chinese text\n",
            "entry evaluation. Our goal is to remove the bias that arises due to human\n",
            "factors. First, we propose a new metric, called the correction penalty (P),\n",
            "based on Fitts' law and Hick's law. Next, we transform it into the approximate\n",
            "amortized cost (AAC) of information theory. An analysis of the AAC of Chinese\n",
            "text input methods with different context lengths is also presented.\n",
            "Actual Title : \n",
            "An Automated Evaluation Metric for Chinese Text Entry\n",
            "Generated Title : \n",
            "automated evaluation metric for chinese text entry\n",
            "Abstract : \n",
            "Recent literature on online learning has focused on developing adaptive\n",
            "algorithms that take advantage of a regularity of the sequence of observations,\n",
            "yet retain worst-case performance guarantees. A complementary direction is to\n",
            "develop prediction methods that perform well against complex benchmarks. In\n",
            "this paper, we address these two directions together. We present a fully\n",
            "adaptive method that competes with dynamic benchmarks in which regret guarantee\n",
            "scales with regularity of the sequence of cost functions and comparators.\n",
            "Notably, the regret bound adapts to the smaller complexity measure in the\n",
            "problem environment. Finally, we apply our results to drifting zero-sum,\n",
            "two-player games where both players achieve no regret guarantees against best\n",
            "sequences of actions in hindsight.\n",
            "Actual Title : \n",
            "Online Optimization : Competing with Dynamic Comparators\n",
            "Generated Title : \n",
            "learning with dynamic dynamic dynamic dynamic dynamic dynamic\n",
            "Abstract : \n",
            "While the research on convolutional neural networks (CNNs) is progressing\n",
            "quickly, the real-world deployment of these models is often limited by\n",
            "computing resources and memory constraints. In this paper, we address this\n",
            "issue by proposing a novel filter pruning method to compress and accelerate\n",
            "CNNs. Our work is based on the linear relationship identified in different\n",
            "feature map subspaces via visualization of feature maps. Such linear\n",
            "relationship implies that the information in CNNs is redundant. Our method\n",
            "eliminates the redundancy in convolutional filters by applying subspace\n",
            "clustering to feature maps. In this way, most of the representative information\n",
            "in the network can be retained in each cluster. Therefore, our method provides\n",
            "an effective solution to filter pruning for which most existing methods\n",
            "directly remove filters based on simple heuristics. The proposed method is\n",
            "independent of the network structure, thus it can be adopted by any\n",
            "off-the-shelf deep learning libraries. Experiments on different networks and\n",
            "tasks show that our method outperforms existing techniques before fine-tuning,\n",
            "and achieves the state-of-the-art results after fine-tuning.\n",
            "Actual Title : \n",
            "Exploring Linear Relationship in Feature Map Subspace for ConvNets\n",
            "  Compression\n",
            "Generated Title : \n",
            "linear <unk> with linear linear networks\n",
            "Abstract : \n",
            "We review the problem of defining and inferring a \"state\" for a control\n",
            "system based on complex, high-dimensional, highly uncertain measurement streams\n",
            "such as videos. Such a state, or representation, should contain all and only\n",
            "the information needed for control, and discount nuisance variability in the\n",
            "data. It should also have finite complexity, ideally modulated depending on\n",
            "available resources. This representation is what we want to store in memory in\n",
            "lieu of the data, as it \"separates\" the control task from the measurement\n",
            "process. For the trivial case with no dynamics, a representation can be\n",
            "inferred by minimizing the Information Bottleneck Lagrangian in a function\n",
            "class realized by deep neural networks. The resulting representation has much\n",
            "higher dimension than the data, already in the millions, but it is smaller in\n",
            "the sense of information content, retaining only what is needed for the task.\n",
            "This process also yields representations that are invariant to nuisance factors\n",
            "and having maximally independent components. We extend these ideas to the\n",
            "dynamic case, where the representation is the posterior density of the task\n",
            "variable given the measurements up to the current time, which is in general\n",
            "much simpler than the prediction density maintained by the classical Bayesian\n",
            "filter. Again this can be finitely-parametrized using a deep neural network,\n",
            "and already some applications are beginning to emerge. No explicit assumption\n",
            "of Markovianity is needed; instead, complexity trades off approximation of an\n",
            "optimal representation, including the degree of Markovianity.\n",
            "Actual Title : \n",
            "A Separation Principle for Control in the Age of Deep Learning\n",
            "Generated Title : \n",
            "bayesian deep learning in deep learning\n",
            "Abstract : \n",
            "Metaheuristic algorithms are becoming an important part of modern\n",
            "optimization. A wide range of metaheuristic algorithms have emerged over the\n",
            "last two decades, and many metaheuristics such as particle swarm optimization\n",
            "are becoming increasingly popular. Despite their popularity, mathematical\n",
            "analysis of these algorithms lacks behind. Convergence analysis still remains\n",
            "unsolved for the majority of metaheuristic algorithms, while efficiency\n",
            "analysis is equally challenging. In this paper, we intend to provide an\n",
            "overview of convergence and efficiency studies of metaheuristics, and try to\n",
            "provide a framework for analyzing metaheuristics in terms of convergence and\n",
            "efficiency. This can form a basis for analyzing other algorithms. We also\n",
            "outline some open questions as further research topics.\n",
            "Actual Title : \n",
            "Metaheuristic Optimization: Algorithm Analysis and Open Problems\n",
            "Generated Title : \n",
            "of strategies for combinatorial optimization\n",
            "Abstract : \n",
            "We consider the exploration-exploitation tradeoff in linear quadratic (LQ)\n",
            "control problems, where the state dynamics is linear and the cost function is\n",
            "quadratic in states and controls. We analyze the regret of Thompson sampling\n",
            "(TS) (a.k.a. posterior-sampling for reinforcement learning) in the frequentist\n",
            "setting, i.e., when the parameters characterizing the LQ dynamics are fixed.\n",
            "Despite the empirical and theoretical success in a wide range of problems from\n",
            "multi-armed bandit to linear bandit, we show that when studying the frequentist\n",
            "regret TS in control problems, we need to trade-off the frequency of sampling\n",
            "optimistic parameters and the frequency of switches in the control policy. This\n",
            "results in an overall regret of $O(T^{2/3})$, which is significantly worse than\n",
            "the regret $O(\\sqrt{T})$ achieved by the optimism-in-face-of-uncertainty\n",
            "algorithm in LQ control problems.\n",
            "Actual Title : \n",
            "Thompson Sampling for Linear-Quadratic Control Problems\n",
            "Generated Title : \n",
            "sampling for reinforcement control\n",
            "Abstract : \n",
            "For analysis of a high-dimensional dataset, a common approach is to test a\n",
            "null hypothesis of statistical independence on all variable pairs using a\n",
            "non-parametric measure of dependence. However, because this approach attempts\n",
            "to identify any non-trivial relationship no matter how weak, it often\n",
            "identifies too many relationships to be useful. What is needed is a way of\n",
            "identifying a smaller set of relationships that merit detailed further\n",
            "analysis.\n",
            "  Here we formally present and characterize equitability, a property of\n",
            "measures of dependence that aims to overcome this challenge. Notionally, an\n",
            "equitable statistic is a statistic that, given some measure of noise, assigns\n",
            "similar scores to equally noisy relationships of different types [Reshef et al.\n",
            "2011]. We begin by formalizing this idea via a new object called the\n",
            "interpretable interval, which functions as an interval estimate of the amount\n",
            "of noise in a relationship of unknown type. We define an equitable statistic as\n",
            "one with small interpretable intervals.\n",
            "  We then draw on the equivalence of interval estimation and hypothesis testing\n",
            "to show that under moderate assumptions an equitable statistic is one that\n",
            "yields well powered tests for distinguishing not only between trivial and\n",
            "non-trivial relationships of all kinds but also between non-trivial\n",
            "relationships of different strengths. This means that equitability allows us to\n",
            "specify a threshold relationship strength $x_0$ and to search for relationships\n",
            "of all kinds with strength greater than $x_0$. Thus, equitability can be\n",
            "thought of as a strengthening of power against independence that enables\n",
            "fruitful analysis of data sets with a small number of strong, interesting\n",
            "relationships and a large number of weaker ones. We conclude with a\n",
            "demonstration of how our two equivalent characterizations of equitability can\n",
            "be used to evaluate the equitability of a statistic in practice.\n",
            "Actual Title : \n",
            "Equitability, interval estimation, and statistical power\n",
            "Generated Title : \n",
            "interval interval\n",
            "Abstract : \n",
            "We consider the problem of distributed statistical machine learning in\n",
            "adversarial settings, where some unknown and time-varying subset of working\n",
            "machines may be compromised and behave arbitrarily to prevent an accurate model\n",
            "from being learned. This setting captures the potential adversarial attacks\n",
            "faced by Federated Learning -- a modern machine learning paradigm that is\n",
            "proposed by Google researchers and has been intensively studied for ensuring\n",
            "user privacy. Formally, we focus on a distributed system consisting of a\n",
            "parameter server and $m$ working machines. Each working machine keeps $N/m$\n",
            "data samples, where $N$ is the total number of samples. The goal is to\n",
            "collectively learn the underlying true model parameter of dimension $d$.\n",
            "  In classical batch gradient descent methods, the gradients reported to the\n",
            "server by the working machines are aggregated via simple averaging, which is\n",
            "vulnerable to a single Byzantine failure. In this paper, we propose a Byzantine\n",
            "gradient descent method based on the geometric median of means of the\n",
            "gradients. We show that our method can tolerate $q \\le (m-1)/2$ Byzantine\n",
            "failures, and the parameter estimate converges in $O(\\log N)$ rounds with an\n",
            "estimation error of $\\sqrt{d(2q+1)/N}$, hence approaching the optimal error\n",
            "rate $\\sqrt{d/N}$ in the centralized and failure-free setting. The total\n",
            "computational complexity of our algorithm is of $O((Nd/m) \\log N)$ at each\n",
            "working machine and $O(md + kd \\log^3 N)$ at the central server, and the total\n",
            "communication cost is of $O(m d \\log N)$. We further provide an application of\n",
            "our general results to the linear regression problem.\n",
            "  A key challenge arises in the above problem is that Byzantine failures create\n",
            "arbitrary and unspecified dependency among the iterations and the aggregated\n",
            "gradients. We prove that the aggregated gradient converges uniformly to the\n",
            "true gradient function.\n",
            "Actual Title : \n",
            "Distributed Statistical Machine Learning in Adversarial Settings:\n",
            "  Byzantine Gradient Descent\n",
            "Generated Title : \n",
            "distributed machine learning\n",
            "Abstract : \n",
            "Seasonality is a distinctive characteristic which is often observed in many\n",
            "practical time series. Artificial Neural Networks (ANNs) are a class of\n",
            "promising models for efficiently recognizing and forecasting seasonal patterns.\n",
            "In this paper, the Particle Swarm Optimization (PSO) approach is used to\n",
            "enhance the forecasting strengths of feedforward ANN (FANN) as well as Elman\n",
            "ANN (EANN) models for seasonal data. Three widely popular versions of the basic\n",
            "PSO algorithm, viz. Trelea-I, Trelea-II and Clerc-Type1 are considered here.\n",
            "The empirical analysis is conducted on three real-world seasonal time series.\n",
            "Results clearly show that each version of the PSO algorithm achieves notably\n",
            "better forecasting accuracies than the standard Backpropagation (BP) training\n",
            "method for both FANN and EANN models. The neural network forecasting results\n",
            "are also compared with those from the three traditional statistical models,\n",
            "viz. Seasonal Autoregressive Integrated Moving Average (SARIMA), Holt-Winters\n",
            "(HW) and Support Vector Machine (SVM). The comparison demonstrates that both\n",
            "PSO and BP based neural networks outperform SARIMA, HW and SVM models for all\n",
            "three time series datasets. The forecasting performances of ANNs are further\n",
            "improved through combining the outputs from the three PSO based models.\n",
            "Actual Title : \n",
            "PSO based Neural Networks vs. Traditional Statistical Models for\n",
            "  Seasonal Time Series Forecasting\n",
            "Generated Title : \n",
            "neural network based neural networks for seasonal forecasting\n",
            "Abstract : \n",
            "Word embeddings are now a standard technique for inducing meaning\n",
            "representations for words. For getting good representations, it is important to\n",
            "take into account different senses of a word. In this paper, we propose a\n",
            "mixture model for learning multi-sense word embeddings. Our model generalizes\n",
            "the previous works in that it allows to induce different weights of different\n",
            "senses of a word. The experimental results show that our model outperforms\n",
            "previous models on standard evaluation tasks.\n",
            "Actual Title : \n",
            "A Mixture Model for Learning Multi-Sense Word Embeddings\n",
            "Generated Title : \n",
            "mixture model for multi sense multi sense word embeddings\n",
            "Abstract : \n",
            "Symmetric matrices are widely used in machine learning problems such as\n",
            "kernel machines and manifold learning. Using large datasets often requires\n",
            "computing low-rank approximations of these symmetric matrices so that they fit\n",
            "in memory. In this paper, we present a novel method based on biharmonic\n",
            "interpolation for low-rank matrix approximation. The method exploits knowledge\n",
            "of the data manifold to learn an interpolation operator that approximates\n",
            "values using a subset of randomly selected landmark points. This operator is\n",
            "readily sparsified, reducing memory requirements by at least two orders of\n",
            "magnitude without significant loss in accuracy. We show that our method can\n",
            "approximate very large datasets using twenty times more landmarks than other\n",
            "methods. Further, numerical results suggest that our method is stable even when\n",
            "numerical difficulties arise for other methods.\n",
            "Actual Title : \n",
            "Sparse and low-rank approximations of large symmetric matrices using\n",
            "  biharmonic interpolation\n",
            "Generated Title : \n",
            "sparse low rank approximations to symmetric interpolation\n",
            "Abstract : \n",
            "Globally, in 2016, one out of eleven adults suffered from Diabetes Mellitus.\n",
            "Diabetic Foot Ulcers (DFU) are a major complication of this disease, which if\n",
            "not managed properly can lead to amputation. Current clinical approaches to DFU\n",
            "treatment rely on patient and clinician vigilance, which has significant\n",
            "limitations such as the high cost involved in the diagnosis, treatment and\n",
            "lengthy care of the DFU. We collected an extensive dataset of foot images,\n",
            "which contain DFU from different patients. In this paper, we have proposed the\n",
            "use of traditional computer vision features for detecting foot ulcers among\n",
            "diabetic patients, which represent a cost-effective, remote and convenient\n",
            "healthcare solution. Furthermore, we used Convolutional Neural Networks (CNNs)\n",
            "for the first time in DFU classification. We have proposed a novel\n",
            "convolutional neural network architecture, DFUNet, with better feature\n",
            "extraction to identify the feature differences between healthy skin and the\n",
            "DFU. Using 10-fold cross-validation, DFUNet achieved an AUC score of 0.962.\n",
            "This outperformed both the machine learning and deep learning classifiers we\n",
            "have tested. Here we present the development of a novel and highly sensitive\n",
            "DFUNet for objectively detecting the presence of DFUs. This novel approach has\n",
            "the potential to deliver a paradigm shift in diabetic foot care.\n",
            "Actual Title : \n",
            "DFUNet: Convolutional Neural Networks for Diabetic Foot Ulcer\n",
            "  Classification\n",
            "Generated Title : \n",
            "a deep neural network for diabetic diabetic ulcer\n",
            "Abstract : \n",
            "We propose a simple and fast algorithm called PatchLift for computing\n",
            "distances between patches (contiguous block of samples) extracted from a given\n",
            "one-dimensional signal. PatchLift is based on the observation that the patch\n",
            "distances can be efficiently computed from a matrix that is derived from the\n",
            "one-dimensional signal using lifting; importantly, the number of operations\n",
            "required to compute the patch distances using this approach does not scale with\n",
            "the patch length. We next demonstrate how PatchLift can be used for patch-based\n",
            "denoising of images corrupted with Gaussian noise. In particular, we propose a\n",
            "separable formulation of the classical Non-Local Means (NLM) algorithm that can\n",
            "be implemented using PatchLift. We demonstrate that the PatchLift-based\n",
            "implementation of separable NLM is few orders faster than standard NLM, and is\n",
            "competitive with existing fast implementations of NLM. Moreover, its denoising\n",
            "performance is shown to be consistently superior to that of NLM and some of its\n",
            "variants, both in terms of PSNR/SSIM and visual quality.\n",
            "Actual Title : \n",
            "Fast Separable Non-Local Means\n",
            "Generated Title : \n",
            "separable separable non local separable non local separable non local\n",
            "Abstract : \n",
            "Generative Adversarial Networks (GANs) have been shown to be able to sample\n",
            "impressively realistic images. GAN training consists of a saddle point\n",
            "optimization problem that can be thought of as an adversarial game between a\n",
            "generator which produces the images, and a discriminator, which judges if the\n",
            "images are real. Both the generator and the discriminator are commonly\n",
            "parametrized as deep convolutional neural networks. The goal of this paper is\n",
            "to disentangle the contribution of the optimization procedure and the network\n",
            "parametrization to the success of GANs. To this end we introduce and study\n",
            "Generative Latent Optimization (GLO), a framework to train a generator without\n",
            "the need to learn a discriminator, thus avoiding challenging adversarial\n",
            "optimization problems. We show experimentally that GLO enjoys many of the\n",
            "desirable properties of GANs: learning from large data, synthesizing\n",
            "visually-appealing samples, interpolating meaningfully between samples, and\n",
            "performing linear arithmetic with noise vectors.\n",
            "Actual Title : \n",
            "Optimizing the Latent Space of Generative Networks\n",
            "Generated Title : \n",
            "the training of deep generative models\n",
            "Abstract : \n",
            "In this work a new way to calculate the multivariate joint entropy is\n",
            "presented. This measure is the basis for a fast information-theoretic based\n",
            "evaluation of gene relevance in a Microarray Gene Expression data context. Its\n",
            "low complexity is based on the reuse of previous computations to calculate\n",
            "current feature relevance. The mu-TAFS algorithm --named as such to\n",
            "differentiate it from previous TAFS algorithms-- implements a simulated\n",
            "annealing technique specially designed for feature subset selection. The\n",
            "algorithm is applied to the maximization of gene subset relevance in several\n",
            "public-domain microarray data sets. The experimental results show a notoriously\n",
            "high classification performance and low size subsets formed by biologically\n",
            "meaningful genes.\n",
            "Actual Title : \n",
            "Feature Selection for Microarray Gene Expression Data using Simulated\n",
            "  Annealing guided by the Multivariate Joint Entropy\n",
            "Generated Title : \n",
            "expression based using entropy annealing\n",
            "Abstract : \n",
            "We propose the first fully-adaptive algorithm for pure exploration in linear\n",
            "bandits---the task to find the arm with the largest expected reward, which\n",
            "depends on an unknown parameter linearly. While existing methods partially or\n",
            "entirely fix sequences of arm selections before observing rewards, our method\n",
            "adaptively changes the arm selection strategy based on past observations at\n",
            "each round. We show our sample complexity matches the achievable lower bound up\n",
            "to a constant factor in an extreme case. Furthermore, we evaluate the\n",
            "performance of the methods by simulations based on both synthetic setting and\n",
            "real-world data, in which our method shows vast improvement over existing\n",
            "methods.\n",
            "Actual Title : \n",
            "Fully adaptive algorithm for pure exploration in linear bandits\n",
            "Generated Title : \n",
            "algorithms for linear linear in linear bandits\n",
            "Abstract : \n",
            "Existing learning-based atmospheric particle-removal approaches such as those\n",
            "used for rainy and hazy images are designed with strong assumptions regarding\n",
            "spatial frequency, trajectory, and translucency. However, the removal of snow\n",
            "particles is more complicated because it possess the additional attributes of\n",
            "particle size and shape, and these attributes may vary within a single image.\n",
            "Currently, hand-crafted features are still the mainstream for snow removal,\n",
            "making significant generalization difficult to achieve. In response, we have\n",
            "designed a multistage network codenamed DesnowNet to in turn deal with the\n",
            "removal of translucent and opaque snow particles. We also differentiate snow\n",
            "into attributes of translucency and chromatic aberration for accurate\n",
            "estimation. Moreover, our approach individually estimates residual complements\n",
            "of the snow-free images to recover details obscured by opaque snow.\n",
            "Additionally, a multi-scale design is utilized throughout the entire network to\n",
            "model the diversity of snow. As demonstrated in experimental results, our\n",
            "approach outperforms state-of-the-art learning-based atmospheric phenomena\n",
            "removal methods and one semantic segmentation baseline on the proposed Snow100K\n",
            "dataset in both qualitative and quantitative comparisons. The results indicate\n",
            "our network would benefit applications involving computer vision and graphics.\n",
            "Actual Title : \n",
            "DesnowNet: Context-Aware Deep Network for Snow Removal\n",
            "Generated Title : \n",
            "context aware context aware deep network for snow removal\n",
            "Abstract : \n",
            "A good clustering algorithm should not only be able to discover clusters of\n",
            "arbitrary shapes (global view) but also provide additional information, which\n",
            "can be used to gain more meaningful insights into the internal structure of the\n",
            "clusters (local view). In this work we use the mathematical framework of factor\n",
            "graphs and message passing algorithms to optimize a pairwise similarity based\n",
            "cost function, in the same spirit as was done in Affinity Propagation. Using\n",
            "this framework we develop two variants of a new clustering algorithm, EAP and\n",
            "SHAPE. EAP/SHAPE can not only discover clusters of arbitrary shapes but also\n",
            "provide a rich local view in the form of meaningful local representatives\n",
            "(exemplars) and connections between these local exemplars. We discuss how this\n",
            "local information can be used to gain various insights about the clusters\n",
            "including varying relative cluster densities and indication of local strength\n",
            "in different regions of a cluster . We also discuss how this can help an\n",
            "analyst in discovering and resolving potential inconsistencies in the results.\n",
            "The efficacy of EAP/SHAPE is shown by applying it to various synthetic and real\n",
            "world benchmark datasets.\n",
            "Actual Title : \n",
            "Clustering with Simultaneous Local and Global View of Data: A message\n",
            "  passing based approach\n",
            "Generated Title : \n",
            "clustering\n",
            "Abstract : \n",
            "We present a neural encoder-decoder model to convert images into\n",
            "presentational markup based on a scalable coarse-to-fine attention mechanism.\n",
            "Our method is evaluated in the context of image-to-LaTeX generation, and we\n",
            "introduce a new dataset of real-world rendered mathematical expressions paired\n",
            "with LaTeX markup. We show that unlike neural OCR techniques using CTC-based\n",
            "models, attention-based approaches can tackle this non-standard OCR task. Our\n",
            "approach outperforms classical mathematical OCR systems by a large margin on\n",
            "in-domain rendered data, and, with pretraining, also performs well on\n",
            "out-of-domain handwritten data. To reduce the inference complexity associated\n",
            "with the attention-based approaches, we introduce a new coarse-to-fine\n",
            "attention layer that selects a support region before applying attention.\n",
            "Actual Title : \n",
            "Image-to-Markup Generation with Coarse-to-Fine Attention\n",
            "Generated Title : \n",
            "coarse to fine generation\n",
            "Abstract : \n",
            "Deep Web databases contain more than 90% of pertinent information of the Web.\n",
            "Despite their importance, users don't profit of this treasury. Many deep web\n",
            "services are offering competitive services in term of prices, quality of\n",
            "service, and facilities. As the number of services is growing rapidly, users\n",
            "have difficulty to ask many web services in the same time. In this paper, we\n",
            "imagine a system where users have the possibility to formulate one query using\n",
            "one query interface and then the system translates query to the rest of query\n",
            "interfaces. However, interfaces are created by designers in order to be\n",
            "interpreted visually by users, machines can not interpret query from a given\n",
            "interface. We propose a new approach which emulates capacity of interpretation\n",
            "of users and extracts query from deep web query interfaces. Our approach has\n",
            "proved good performances on two standard datasets.\n",
            "Actual Title : \n",
            "VIQI: A New Approach for Visual Interpretation of Deep Web Query\n",
            "  Interfaces\n",
            "Generated Title : \n",
            "a new approach to visual reasoning through deep\n",
            "Abstract : \n",
            "We propose a deep neural network for the prediction of future frames in\n",
            "natural video sequences. To effectively handle complex evolution of pixels in\n",
            "videos, we propose to decompose the motion and content, two key components\n",
            "generating dynamics in videos. Our model is built upon the Encoder-Decoder\n",
            "Convolutional Neural Network and Convolutional LSTM for pixel-level prediction,\n",
            "which independently capture the spatial layout of an image and the\n",
            "corresponding temporal dynamics. By independently modeling motion and content,\n",
            "predicting the next frame reduces to converting the extracted content features\n",
            "into the next frame content by the identified motion features, which simplifies\n",
            "the task of prediction. Our model is end-to-end trainable over multiple time\n",
            "steps, and naturally learns to decompose motion and content without separate\n",
            "training. We evaluate the proposed network architecture on human activity\n",
            "videos using KTH, Weizmann action, and UCF-101 datasets. We show\n",
            "state-of-the-art performance in comparison to recent approaches. To the best of\n",
            "our knowledge, this is the first end-to-end trainable network architecture with\n",
            "motion and content separation to model the spatiotemporal dynamics for\n",
            "pixel-level future prediction in natural videos.\n",
            "Actual Title : \n",
            "Decomposing Motion and Content for Natural Video Sequence Prediction\n",
            "Generated Title : \n",
            "motion and video prediction\n",
            "Abstract : \n",
            "We present an approach towards convex optimization that relies on a novel\n",
            "scheme which converts online adaptive algorithms into offline methods. In the\n",
            "offline optimization setting, our derived methods are shown to obtain\n",
            "favourable adaptive guarantees which depend on the harmonic sum of the queried\n",
            "gradients. We further show that our methods implicitly adapt to the objective's\n",
            "structure: in the smooth case fast convergence rates are ensured without any\n",
            "prior knowledge of the smoothness parameter, while still maintaining guarantees\n",
            "in the non-smooth setting. Our approach has a natural extension to the\n",
            "stochastic setting, resulting in a lazy version of SGD (stochastic GD), where\n",
            "minibathces are chosen \\emph{adaptively} depending on the magnitude of the\n",
            "gradients. Thus providing a principled approach towards choosing minibatch\n",
            "sizes.\n",
            "Actual Title : \n",
            "Online to Offline Conversions, Universality and Adaptive Minibatch Sizes\n",
            "Generated Title : \n",
            "to offline to universality to universality adaptive universality\n",
            "Abstract : \n",
            "We introduce LAMBADA, a dataset to evaluate the capabilities of computational\n",
            "models for text understanding by means of a word prediction task. LAMBADA is a\n",
            "collection of narrative passages sharing the characteristic that human subjects\n",
            "are able to guess their last word if they are exposed to the whole passage, but\n",
            "not if they only see the last sentence preceding the target word. To succeed on\n",
            "LAMBADA, computational models cannot simply rely on local context, but must be\n",
            "able to keep track of information in the broader discourse. We show that\n",
            "LAMBADA exemplifies a wide range of linguistic phenomena, and that none of\n",
            "several state-of-the-art language models reaches accuracy above 1% on this\n",
            "novel benchmark. We thus propose LAMBADA as a challenging test set, meant to\n",
            "encourage the development of new models capable of genuine understanding of\n",
            "broad context in natural language text.\n",
            "Actual Title : \n",
            "The LAMBADA dataset: Word prediction requiring a broad discourse context\n",
            "Generated Title : \n",
            "lambada dataset  language for broad broad discourse corpus\n",
            "Abstract : \n",
            "Dominance-based Rough Set Approach (DRSA), as the extension of Pawlak's Rough\n",
            "Set theory, is effective and fundamentally important in Multiple Criteria\n",
            "Decision Analysis (MCDA). In previous DRSA models, the definitions of the upper\n",
            "and lower approximations are preserving the class unions rather than the\n",
            "singleton class. In this paper, we propose a new Class-based Rough\n",
            "Approximation with respect to a series of previous DRSA models, including\n",
            "Classical DRSA model, VC-DRSA model and VP-DRSA model. In addition, the new\n",
            "class-based reducts are investigated.\n",
            "Actual Title : \n",
            "Class-based Rough Approximation with Dominance Principle\n",
            "Generated Title : \n",
            "rough rough set for\n",
            "Abstract : \n",
            "We address the issue of adapting optical images-based edge detection\n",
            "techniques for use in Polarimetric Synthetic Aperture Radar (PolSAR) imagery.\n",
            "We modify the gravitational edge detection technique (inspired by the Law of\n",
            "Universal Gravity) proposed by Lopez-Molina et al, using the non-standard\n",
            "neighbourhood configuration proposed by Fu et al, to reduce the speckle noise\n",
            "in polarimetric SAR imagery. We compare the modified and unmodified versions of\n",
            "the gravitational edge detection technique with the well-established one\n",
            "proposed by Canny, as well as with a recent multiscale fuzzy-based technique\n",
            "proposed by Lopez-Molina et Alejandro We also address the issues of aggregation\n",
            "of gray level images before and after edge detection and of filtering. All\n",
            "techniques addressed here are applied to a mosaic built using class\n",
            "distributions obtained from a real scene, as well as to the true PolSAR image;\n",
            "the mosaic results are assessed using Baddeley's Delta Metric. Our experiments\n",
            "show that modifying the gravitational edge detection technique with a\n",
            "non-standard neighbourhood configuration produces better results than the\n",
            "original technique, as well as the other techniques used for comparison. The\n",
            "experiments show that adapting edge detection methods from Computational\n",
            "Intelligence for use in PolSAR imagery is a new field worthy of exploration.\n",
            "Actual Title : \n",
            "Optical images-based edge detection in Synthetic Aperture Radar images\n",
            "Generated Title : \n",
            "images based edge detection in optical synthetic images\n",
            "Abstract : \n",
            "A problem faced by many instructors is that of designing exams that\n",
            "accurately assess the abilities of the students. Typically these exams are\n",
            "prepared several days in advance, and generic question scores are used based on\n",
            "rough approximation of the question difficulty and length. For example, for a\n",
            "recent class taught by the author, there were 30 multiple choice questions\n",
            "worth 3 points, 15 true/false with explanation questions worth 4 points, and 5\n",
            "analytical exercises worth 10 points. We describe a novel framework where\n",
            "algorithms from machine learning are used to modify the exam question weights\n",
            "in order to optimize the exam scores, using the overall class grade as a proxy\n",
            "for a student's true ability. We show that significant error reduction can be\n",
            "obtained by our approach over standard weighting schemes, and we make several\n",
            "new observations regarding the properties of the \"good\" and \"bad\" exam\n",
            "questions that can have impact on the design of improved future evaluation\n",
            "methods.\n",
            "Actual Title : \n",
            "Optimal Weighting for Exam Composition\n",
            "Generated Title : \n",
            "optimal weighting of optimal weighting in the <unk>\n",
            "Abstract : \n",
            "Visual representation is crucial for a visual tracking method's performances.\n",
            "Conventionally, visual representations adopted in visual tracking rely on\n",
            "hand-crafted computer vision descriptors. These descriptors were developed\n",
            "generically without considering tracking-specific information. In this paper,\n",
            "we propose to learn complex-valued invariant representations from tracked\n",
            "sequential image patches, via strong temporal slowness constraint and stacked\n",
            "convolutional autoencoders. The deep slow local representations are learned\n",
            "offline on unlabeled data and transferred to the observational model of our\n",
            "proposed tracker. The proposed observational model retains old training samples\n",
            "to alleviate drift, and collect negative samples which are coherent with\n",
            "target's motion pattern for better discriminative tracking. With the learned\n",
            "representation and online training samples, a logistic regression classifier is\n",
            "adopted to distinguish target from background, and retrained online to adapt to\n",
            "appearance changes. Subsequently, the observational model is integrated into a\n",
            "particle filter framework to peform visual tracking. Experimental results on\n",
            "various challenging benchmark sequences demonstrate that the proposed tracker\n",
            "performs favourably against several state-of-the-art trackers.\n",
            "Actual Title : \n",
            "Self-taught learning of a deep invariant representation for visual\n",
            "  tracking via temporal slowness principle\n",
            "Generated Title : \n",
            "learning for representation learning for representation learning\n",
            "Abstract : \n",
            "To recover the three dimensional (3D) volumetric distribution of matter in an\n",
            "object, images of the object are captured from multiple directions and\n",
            "locations. Using these images tomographic computations extract the\n",
            "distribution. In highly scattering media and constrained, natural irradiance,\n",
            "tomography must explicitly account for off-axis scattering. Furthermore, the\n",
            "tomographic model and recovery must function when imaging is done in-situ, as\n",
            "occurs in medical imaging and ground-based atmospheric sensing. We formulate\n",
            "tomography that handles arbitrary orders of scattering, using a monte-carlo\n",
            "model. Moreover, the model is highly parallelizable in our formulation. This\n",
            "enables large scale rendering and recovery of volumetric scenes having a large\n",
            "number of variables. We solve stability and conditioning problems that stem\n",
            "from radiative transfer (RT) modeling in-situ.\n",
            "Actual Title : \n",
            "In-situ multi-scattering tomography\n",
            "Generated Title : \n",
            "image reconstruction from x ray measurements using variational priors\n",
            "Abstract : \n",
            "We describe our system for SemEval-2018 Shared Task on Semantic Relation\n",
            "Extraction and Classification in Scientific Papers where we focus on the\n",
            "Classification task. Our simple piecewise convolution neural encoder performs\n",
            "decently in an end to end manner. A simple inter-task data augmentation\n",
            "signifi- cantly boosts the performance of the model. Our best-performing\n",
            "systems stood 8th out of 20 teams on the classification task on noisy data and\n",
            "12th out of 28 teams on the classification task on clean data.\n",
            "Actual Title : \n",
            "OhioState at SemEval-2018 Task 7: Exploiting Data Augmentation for\n",
            "  Relation Classification in Scientific Papers using Piecewise Convolutional\n",
            "  Neural Networks\n",
            "Generated Title : \n",
            "a semeval #### task #  exploiting convolutional neural network\n",
            "Abstract : \n",
            "We present a method of training a differentiable function approximator for a\n",
            "regression task using negative examples. We effect this training using negative\n",
            "learning rates. We also show how this method can be used to perform direct\n",
            "policy learning in a reinforcement learning setting.\n",
            "Actual Title : \n",
            "Negative Learning Rates and P-Learning\n",
            "Generated Title : \n",
            "learning with <unk>\n",
            "Abstract : \n",
            "Many high dimensional sparse learning problems are formulated as nonconvex\n",
            "optimization. A popular approach to solve these nonconvex optimization problems\n",
            "is through convex relaxations such as linear and semidefinite programming. In\n",
            "this paper, we study the statistical limits of convex relaxations.\n",
            "Particularly, we consider two problems: Mean estimation for sparse principal\n",
            "submatrix and edge probability estimation for stochastic block model. We\n",
            "exploit the sum-of-squares relaxation hierarchy to sharply characterize the\n",
            "limits of a broad class of convex relaxations. Our result shows statistical\n",
            "optimality needs to be compromised for achieving computational tractability\n",
            "using convex relaxations. Compared with existing results on computational lower\n",
            "bounds for statistical problems, which consider general polynomial-time\n",
            "algorithms and rely on computational hardness hypotheses on problems like\n",
            "planted clique detection, our theory focuses on a broad class of convex\n",
            "relaxations and does not rely on unproven hypotheses.\n",
            "Actual Title : \n",
            "Statistical Limits of Convex Relaxations\n",
            "Generated Title : \n",
            "convex relaxations of convex relaxations\n",
            "Abstract : \n",
            "We propose RoBiRank, a ranking algorithm that is motivated by observing a\n",
            "close connection between evaluation metrics for learning to rank and loss\n",
            "functions for robust classification. The algorithm shows a very competitive\n",
            "performance on standard benchmark datasets against other representative\n",
            "algorithms in the literature. On the other hand, in large scale problems where\n",
            "explicit feature vectors and scores are not given, our algorithm can be\n",
            "efficiently parallelized across a large number of machines; for a task that\n",
            "requires 386,133 x 49,824,519 pairwise interactions between items to be ranked,\n",
            "our algorithm finds solutions that are of dramatically higher quality than that\n",
            "can be found by a state-of-the-art competitor algorithm, given the same amount\n",
            "of wall-clock time for computation.\n",
            "Actual Title : \n",
            "Ranking via Robust Binary Classification and Parallel Parameter\n",
            "  Estimation in Large-Scale Data\n",
            "Generated Title : \n",
            "ranking via pairwise ranking of pairwise comparisons\n",
            "Abstract : \n",
            "We investigate a novel cluster-of-bandit algorithm CAB for collaborative\n",
            "recommendation tasks that implements the underlying feedback sharing mechanism\n",
            "by estimating the neighborhood of users in a context-dependent manner. CAB\n",
            "makes sharp departures from the state of the art by incorporating collaborative\n",
            "effects into inference as well as learning processes in a manner that\n",
            "seamlessly interleaving explore-exploit tradeoffs and collaborative steps. We\n",
            "prove regret bounds under various assumptions on the data, which exhibit a\n",
            "crisp dependence on the expected number of clusters over the users, a natural\n",
            "measure of the statistical difficulty of the learning task. Experiments on\n",
            "production and real-world datasets show that CAB offers significantly increased\n",
            "prediction performance against a representative pool of state-of-the-art\n",
            "methods.\n",
            "Actual Title : \n",
            "On Context-Dependent Clustering of Bandits\n",
            "Generated Title : \n",
            "bandits\n",
            "Abstract : \n",
            "How universal is human conceptual structure? The way concepts are organized\n",
            "in the human brain may reflect distinct features of cultural, historical, and\n",
            "environmental background in addition to properties universal to human\n",
            "cognition. Semantics, or meaning expressed through language, provides direct\n",
            "access to the underlying conceptual structure, but meaning is notoriously\n",
            "difficult to measure, let alone parameterize. Here we provide an empirical\n",
            "measure of semantic proximity between concepts using cross-linguistic\n",
            "dictionaries. Across languages carefully selected from a phylogenetically and\n",
            "geographically stratified sample of genera, translations of words reveal cases\n",
            "where a particular language uses a single polysemous word to express concepts\n",
            "represented by distinct words in another. We use the frequency of polysemies\n",
            "linking two concepts as a measure of their semantic proximity, and represent\n",
            "the pattern of such linkages by a weighted network. This network is highly\n",
            "uneven and fragmented: certain concepts are far more prone to polysemy than\n",
            "others, and there emerge naturally interpretable clusters loosely connected to\n",
            "each other. Statistical analysis shows such structural properties are\n",
            "consistent across different language groups, largely independent of geography,\n",
            "environment, and literacy. It is therefore possible to conclude the conceptual\n",
            "structure connecting basic vocabulary studied is primarily due to universal\n",
            "features of human cognition and language use.\n",
            "Actual Title : \n",
            "On the universal structure of human lexical semantics\n",
            "Generated Title : \n",
            "the <unk> network structure human semantic of a\n",
            "Abstract : \n",
            "We propose an end-to-end learning framework for segmenting generic objects in\n",
            "videos. Our method learns to combine appearance and motion information to\n",
            "produce pixel level segmentation masks for all prominent objects in videos. We\n",
            "formulate this task as a structured prediction problem and design a two-stream\n",
            "fully convolutional neural network which fuses together motion and appearance\n",
            "in a unified framework. Since large-scale video datasets with pixel level\n",
            "segmentations are problematic, we show how to bootstrap weakly annotated videos\n",
            "together with existing image recognition datasets for training. Through\n",
            "experiments on three challenging video segmentation benchmarks, our method\n",
            "substantially improves the state-of-the-art for segmenting generic (unseen)\n",
            "objects. Code and pre-trained models are available on the project website.\n",
            "Actual Title : \n",
            "FusionSeg: Learning to combine motion and appearance for fully automatic\n",
            "  segmention of generic objects in videos\n",
            "Generated Title : \n",
            "video and appearance learning for appearance video using\n",
            "Abstract : \n",
            "Principal component analysis (PCA) is largely adopted for chemical process\n",
            "monitoring and numerous PCA-based systems have been developed to solve various\n",
            "fault detection and diagnosis problems. Since PCA-based methods assume that the\n",
            "monitored process is linear, nonlinear PCA models, such as autoencoder models\n",
            "and kernel principal component analysis (KPCA), has been proposed and applied\n",
            "to nonlinear process monitoring. However, KPCA-based methods need to perform\n",
            "eigen-decomposition (ED) on the kernel Gram matrix whose dimensions depend on\n",
            "the number of training data. Moreover, prefixed kernel parameters cannot be\n",
            "most effective for different faults which may need different parameters to\n",
            "maximize their respective detection performances. Autoencoder models lack the\n",
            "consideration of orthogonal constraints which is crucial for PCA-based\n",
            "algorithms. To address these problems, this paper proposes a novel nonlinear\n",
            "method, called neural component analysis (NCA), which intends to train a\n",
            "feedforward neural work with orthogonal constraints such as those used in PCA.\n",
            "NCA can adaptively learn its parameters through backpropagation and the\n",
            "dimensionality of the nonlinear features has no relationship with the number of\n",
            "training samples. Extensive experimental results on the Tennessee Eastman (TE)\n",
            "benchmark process show the superiority of NCA in terms of missed detection rate\n",
            "(MDR) and false alarm rate (FAR). The source code of NCA can be found in\n",
            "https://github.com/haitaozhao/Neural-Component-Analysis.git.\n",
            "Actual Title : \n",
            "Neural Component Analysis for Fault Detection\n",
            "Generated Title : \n",
            "component analysis for for fault detection\n",
            "Abstract : \n",
            "This paper contributes to the human-machine interface community in two ways:\n",
            "as a critique of the closed-loop AC (augmented cognition) approach, and as a\n",
            "way to introduce concepts from complex systems and systems physiology into the\n",
            "field. Of particular relevance is a comparison of the inverted-U (or Gaussian)\n",
            "model of optimal performance and multidimensional fitness landscape model.\n",
            "Hypothetical examples will be given from human physiology and learning and\n",
            "memory. In particular, a four-step model will be introduced that is proposed as\n",
            "a better means to characterize multivariate systems during behavioral processes\n",
            "with complex dynamics such as learning. Finally, the alternate approach\n",
            "presented herein is considered as a preferable design alternate in\n",
            "human-machine systems. It is within this context that future directions are\n",
            "discussed.\n",
            "Actual Title : \n",
            "The adaptability of physiological systems optimizes performance: new\n",
            "  directions in augmentation\n",
            "Generated Title : \n",
            "in in physiological systems optimizes performance  the augmentation\n",
            "Abstract : \n",
            "After data selection, pre-processing, transformation, and feature extraction,\n",
            "knowledge extraction is not the final step in a data mining process. It is then\n",
            "necessary to understand this knowledge in order to apply it efficiently and\n",
            "effectively. Up to now, there is a lack of appropriate techniques that support\n",
            "this significant step. This is partly due to the fact that the assessment of\n",
            "knowledge is often highly subjective, e.g., regarding aspects such as novelty\n",
            "or usefulness. These aspects depend on the specific knowledge and requirements\n",
            "of the data miner. There are, however, a number of aspects that are objective\n",
            "and for which it is possible to provide appropriate measures. In this article\n",
            "we focus on classification problems and use probabilistic generative\n",
            "classifiers based on mixture density models that are quite common in data\n",
            "mining applications. We define objective measures to assess the\n",
            "informativeness, uniqueness, importance, discrimination, representativity,\n",
            "uncertainty, and distinguishability of rules contained in these classifiers\n",
            "numerically. These measures not only support a data miner in evaluating results\n",
            "of a data mining process based on such classifiers. As we will see in\n",
            "illustrative case studies, they may also be used to improve the data mining\n",
            "process itself or to support the later application of the extracted knowledge.\n",
            "Actual Title : \n",
            "Towards Automation of Knowledge Understanding: An Approach for\n",
            "  Probabilistic Generative Classifiers\n",
            "Generated Title : \n",
            "mining for probabilistic classifiers\n",
            "Abstract : \n",
            "We consider a team of reinforcement learning agents that concurrently learn\n",
            "to operate in a common environment. We identify three properties - adaptivity,\n",
            "commitment, and diversity - which are necessary for efficient coordinated\n",
            "exploration and demonstrate that straightforward extensions to single-agent\n",
            "optimistic and posterior sampling approaches fail to satisfy them. As an\n",
            "alternative, we propose seed sampling, which extends posterior sampling in a\n",
            "manner that meets these requirements. Simulation results investigate how\n",
            "per-agent regret decreases as the number of agents grows, establishing\n",
            "substantial advantages of seed sampling over alternative exploration schemes.\n",
            "Actual Title : \n",
            "Coordinated Exploration in Concurrent Reinforcement Learning\n",
            "Generated Title : \n",
            "coordinated exploration via reinforcement reinforcement learning\n",
            "Abstract : \n",
            "We consider the problem of learning Bayesian network classifiers that\n",
            "maximize the marginover a set of classification variables. We find that this\n",
            "problem is harder for Bayesian networks than for undirected graphical models\n",
            "like maximum margin Markov networks. The main difficulty is that the parameters\n",
            "in a Bayesian network must satisfy additional normalization constraints that an\n",
            "undirected graphical model need not respect. These additional constraints\n",
            "complicate the optimization task. Nevertheless, we derive an effective training\n",
            "algorithm that solves the maximum margin training problem for a range of\n",
            "Bayesian network topologies, and converges to an approximate solution for\n",
            "arbitrary network topologies. Experimental results show that the method can\n",
            "demonstrate improved generalization performance over Markov networks when the\n",
            "directed graphical structure encodes relevant knowledge. In practice, the\n",
            "training technique allows one to combine prior knowledge expressed as a\n",
            "directed (causal) model with state of the art discriminative learning methods.\n",
            "Actual Title : \n",
            "Maximum Margin Bayesian Networks\n",
            "Generated Title : \n",
            "bayesian networks in bayesian networks\n",
            "Abstract : \n",
            "This paper addresses the problem of correlation estimation in sets of\n",
            "compressed images. We consider a framework where images are represented under\n",
            "the form of linear measurements due to low complexity sensing or security\n",
            "requirements. We assume that the images are correlated through the displacement\n",
            "of visual objects due to motion or viewpoint change and the correlation is\n",
            "effectively represented by optical flow or motion field models. The correlation\n",
            "is estimated in the compressed domain by jointly processing the linear\n",
            "measurements. We first show that the correlated images can be efficiently\n",
            "related using a linear operator. Using this linear relationship we then\n",
            "describe the dependencies between images in the compressed domain. We further\n",
            "cast a regularized optimization problem where the correlation is estimated in\n",
            "order to satisfy both data consistency and motion smoothness objectives with a\n",
            "Graph Cut algorithm. We analyze in detail the correlation estimation\n",
            "performance and quantify the penalty due to image compression. Extensive\n",
            "experiments in stereo and video imaging applications show that our novel\n",
            "solution stays competitive with methods that implement complex image\n",
            "reconstruction steps prior to correlation estimation. We finally use the\n",
            "estimated correlation in a novel joint image reconstruction scheme that is\n",
            "based on an optimization problem with sparsity priors on the reconstructed\n",
            "images. Additional experiments show that our correlation estimation algorithm\n",
            "leads to an effective reconstruction of pairs of images in distributed image\n",
            "coding schemes that outperform independent reconstruction algorithms by 2 to 4\n",
            "dB.\n",
            "Actual Title : \n",
            "Correlation Estimation from Compressed Images\n",
            "Generated Title : \n",
            "estimation from compressed images\n",
            "Abstract : \n",
            "We consider the problem of maximizing submodular functions; while this\n",
            "problem is known to be NP-hard, several numerically efficient local search\n",
            "techniques with approximation guarantees are available. In this paper, we\n",
            "propose a novel convex relaxation which is based on the relationship between\n",
            "submodular functions, entropies and probabilistic graphical models. In a\n",
            "graphical model, the entropy of the joint distribution decomposes as a sum of\n",
            "marginal entropies of subsets of variables; moreover, for any distribution, the\n",
            "entropy of the closest distribution factorizing in the graphical model provides\n",
            "an bound on the entropy. For directed graphical models, this last property\n",
            "turns out to be a direct consequence of the submodularity of the entropy\n",
            "function, and allows the generalization of graphical-model-based upper bounds\n",
            "to any submodular functions. These upper bounds may then be jointly maximized\n",
            "with respect to a set, while minimized with respect to the graph, leading to a\n",
            "convex variational inference scheme for maximizing submodular functions, based\n",
            "on outer approximations of the marginal polytope and maximum likelihood bounded\n",
            "treewidth structures. By considering graphs of increasing treewidths, we may\n",
            "then explore the trade-off between computational complexity and tightness of\n",
            "the relaxation. We also present extensions to constrained problems and\n",
            "maximizing the difference of submodular functions, which include all possible\n",
            "set functions.\n",
            "Actual Title : \n",
            "Maximizing submodular functions using probabilistic graphical models\n",
            "Generated Title : \n",
            "submodular maximization in graphical graphical models\n",
            "Abstract : \n",
            "In a recent article we described a new type of deep neural network - a\n",
            "Perpetual Learning Machine (PLM) - which is capable of learning 'on the fly'\n",
            "like a brain by existing in a state of Perpetual Stochastic Gradient Descent\n",
            "(PSGD). Here, by simulating the process of practice, we demonstrate both\n",
            "selective memory and selective forgetting when we introduce statistical recall\n",
            "biases during PSGD. Frequently recalled memories are remembered, whilst\n",
            "memories recalled rarely are forgotten. This results in a 'use it or lose it'\n",
            "stimulus driven memory process that is similar to human memory.\n",
            "Actual Title : \n",
            "Use it or Lose it: Selective Memory and Forgetting in a Perpetual\n",
            "  Learning Machine\n",
            "Generated Title : \n",
            "memory of selective memory forgetting in selective selective\n",
            "Abstract : \n",
            "Skin cancer, the most common human malignancy, is primarily diagnosed\n",
            "visually by physicians [1]. Classification with an automated method like CNN\n",
            "[2, 3] shows potential for challenging tasks [1]. By now, the deep\n",
            "convolutional neural networks are on par with human dermatologist [1]. This\n",
            "abstract is dedicated on developing a Deep Learning method for ISIC [5] 2017\n",
            "Skin Lesion Detection Competition hosted at [6] to classify the dermatology\n",
            "pictures, which is aimed at improving the diagnostic accuracy rate and general\n",
            "level of the human health. The challenge falls into three sub-challenges,\n",
            "including Lesion Segmentation, Lesion Dermoscopic Feature Extraction and Lesion\n",
            "Classification. This project only participates in the Lesion Classification\n",
            "part. This algorithm is comprised of three steps: (1) original images\n",
            "preprocessing, (2) modelling the processed images using CNN [2, 3] in Caffe [4]\n",
            "framework, (3) predicting the test images and calculating the scores that\n",
            "represent the likelihood of corresponding classification. The models are built\n",
            "on the source images are using the Caffe [4] framework. The scores in\n",
            "prediction step are obtained by two different models from the source images.\n",
            "Actual Title : \n",
            "Using Deep Learning Method for Classification: A Proposed Algorithm for\n",
            "  the ISIC 2017 Skin Lesion Classification Challenge\n",
            "Generated Title : \n",
            "learning for skin lesion classification in isic ####\n",
            "Abstract : \n",
            "Neural networks are analogous in many ways to spin glasses, systems which are\n",
            "known for their rich set of dynamics and equally complex phase diagrams. We\n",
            "apply well-known techniques in the study of spin glasses to a convolutional\n",
            "sparsely encoding neural network and observe power law finite-size scaling\n",
            "behavior in the sparsity and reconstruction error as the network denoises\n",
            "32$\\times$32 RGB CIFAR-10 images. This finite-size scaling indicates the\n",
            "presence of a continuous phase transition at a critical value of this sparsity.\n",
            "By using the power law scaling relations inherent to finite-size scaling, we\n",
            "can determine the optimal value of sparsity for any network size by tuning the\n",
            "system to the critical point and operate the system at the minimum denoising\n",
            "error.\n",
            "Actual Title : \n",
            "Phase Transitions in Image Denoising via Sparsely Coding Convolutional\n",
            "  Neural Networks\n",
            "Generated Title : \n",
            "image image denoising via sparsely coding\n",
            "Abstract : \n",
            "Automatic segmentation of medical images is an important task for many\n",
            "clinical applications. In practice, a wide range of anatomical structures are\n",
            "visualised using different imaging modalities. In this paper, we investigate\n",
            "whether a single convolutional neural network (CNN) can be trained to perform\n",
            "different segmentation tasks.\n",
            "  A single CNN is trained to segment six tissues in MR brain images, the\n",
            "pectoral muscle in MR breast images, and the coronary arteries in cardiac CTA.\n",
            "The CNN therefore learns to identify the imaging modality, the visualised\n",
            "anatomical structures, and the tissue classes.\n",
            "  For each of the three tasks (brain MRI, breast MRI and cardiac CTA), this\n",
            "combined training procedure resulted in a segmentation performance equivalent\n",
            "to that of a CNN trained specifically for that task, demonstrating the high\n",
            "capacity of CNN architectures. Hence, a single system could be used in clinical\n",
            "practice to automatically perform diverse segmentation tasks without\n",
            "task-specific training.\n",
            "Actual Title : \n",
            "Deep Learning for Multi-Task Medical Image Segmentation in Multiple\n",
            "  Modalities\n",
            "Generated Title : \n",
            "convolutional neural networks for segmenting and segmentation in\n",
            "Abstract : \n",
            "In this paper we present a unified framework for modeling multi-relational\n",
            "representations, scoring, and learning, and conduct an empirical study of\n",
            "several recent multi-relational embedding models under the framework. We\n",
            "investigate the different choices of relation operators based on linear and\n",
            "bilinear transformations, and also the effects of entity representations by\n",
            "incorporating unsupervised vectors pre-trained on extra textual resources. Our\n",
            "results show several interesting findings, enabling the design of a simple\n",
            "embedding model that achieves the new state-of-the-art performance on a popular\n",
            "knowledge base completion task evaluated on Freebase.\n",
            "Actual Title : \n",
            "Learning Multi-Relational Semantics Using Neural-Embedding Models\n",
            "Generated Title : \n",
            "embedding embedding for learning learning of entity embeddings\n",
            "Abstract : \n",
            "The classical mixture of Gaussians model is related to K-means via\n",
            "small-variance asymptotics: as the covariances of the Gaussians tend to zero,\n",
            "the negative log-likelihood of the mixture of Gaussians model approaches the\n",
            "K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis\n",
            "& Jordan (2012) used this observation to obtain a novel K-means-like algorithm\n",
            "from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead\n",
            "consider applying small-variance asymptotics directly to the posterior in\n",
            "Bayesian nonparametric models. This framework is independent of any specific\n",
            "Bayesian inference algorithm, and it has the major advantage that it\n",
            "generalizes immediately to a range of models beyond the DP mixture. To\n",
            "illustrate, we apply our framework to the feature learning setting, where the\n",
            "beta process and Indian buffet process provide an appropriate Bayesian\n",
            "nonparametric prior. We obtain a novel objective function that goes beyond\n",
            "clustering to learn (and penalize new) groupings for which we relax the mutual\n",
            "exclusivity and exhaustivity assumptions of clustering. We demonstrate several\n",
            "other algorithms, all of which are scalable and simple to implement. Empirical\n",
            "results demonstrate the benefits of the new framework.\n",
            "Actual Title : \n",
            "MAD-Bayes: MAP-based Asymptotic Derivations from Bayes\n",
            "Generated Title : \n",
            "inference with a mixture mixture mixture models\n",
            "Abstract : \n",
            "Conventional dual-frequency fringe projection algorithm often suffers from\n",
            "phase unwrapping failure when the frequency ratio between the high frequency\n",
            "and the low one is too large. Zhang et.al. proposed an enhanced two-frequency\n",
            "phase-shifting method to use geometric constraints of digital fringe\n",
            "projection(DFP) to reduce the noise impact due to the large frequency ratio.\n",
            "However, this method needs to calibrate the DFP system and calculate the\n",
            "minimum phase map at the nearest position from the camera perspective, these\n",
            "procedures are are relatively complex and more time-cosuming. In this paper, we\n",
            "proposed an improved method, which eliminates the system calibration and\n",
            "determination in Zhang's method,meanwhile does not need to use the low\n",
            "frequency fringe pattern. In the proposed method,we only need a set of high\n",
            "frequency fringe patterns to measure the object after the high frequency is\n",
            "directly estimated by the experiment. Thus the proposed method can simplify the\n",
            "procedure and improve the speed. Finally, the experimental evaluation is\n",
            "conducted to prove the validity of the proposed method.The results demonstrate\n",
            "that the proposed method can overcome the main disadvantages encountered by\n",
            "Zhang's method.\n",
            "Actual Title : \n",
            "Improved phase-unwrapping method using geometric constraints\n",
            "Generated Title : \n",
            "<unk> approach to improved geometric constraints\n",
            "Abstract : \n",
            "In this work, we present a new dataset for computational humor, specifically\n",
            "comparative humor ranking, which attempts to eschew the ubiquitous binary\n",
            "approach to humor detection. The dataset consists of tweets that are humorous\n",
            "responses to a given hashtag. We describe the motivation for this new dataset,\n",
            "as well as the collection process, which includes a description of our\n",
            "semi-automated system for data collection. We also present initial experiments\n",
            "for this dataset using both unsupervised and supervised approaches. Our best\n",
            "supervised system achieved 63.7% accuracy, suggesting that this task is much\n",
            "more difficult than comparable humor detection tasks. Initial experiments\n",
            "indicate that a character-level model is more suitable for this task than a\n",
            "token-level model, likely due to a large amount of puns that can be captured by\n",
            "a character-level model.\n",
            "Actual Title : \n",
            "#HashtagWars: Learning a Sense of Humor\n",
            "Generated Title : \n",
            "a large sense of humor\n",
            "Abstract : \n",
            "Over the past century, personality theory and research has successfully\n",
            "identified core sets of characteristics that consistently describe and explain\n",
            "fundamental differences in the way people think, feel and behave. Such\n",
            "characteristics were derived through theory, dictionary analyses, and survey\n",
            "research using explicit self-reports. The availability of social media data\n",
            "spanning millions of users now makes it possible to automatically derive\n",
            "characteristics from language use -- at large scale. Taking advantage of\n",
            "linguistic information available through Facebook, we study the process of\n",
            "inferring a new set of potential human traits based on unprompted language use.\n",
            "We subject these new traits to a comprehensive set of evaluations and compare\n",
            "them with a popular five factor model of personality. We find that our\n",
            "language-based trait construct is often more generalizable in that it often\n",
            "predicts non-questionnaire-based outcomes better than questionnaire-based\n",
            "traits (e.g. entities someone likes, income and intelligence quotient), while\n",
            "the factors remain nearly as stable as traditional factors. Our approach\n",
            "suggests a value in new constructs of personality derived from everyday human\n",
            "language use.\n",
            "Actual Title : \n",
            "Latent Human Traits in the Language of Social Media: An Open-Vocabulary\n",
            "  Approach\n",
            "Generated Title : \n",
            "latent traits of social media  a language\n",
            "Abstract : \n",
            "We first present our work in machine translation, during which we used\n",
            "aligned sentences to train a neural network to embed n-grams of different\n",
            "languages into an $d$-dimensional space, such that n-grams that are the\n",
            "translation of each other are close with respect to some metric. Good n-grams\n",
            "to n-grams translation results were achieved, but full sentences translation is\n",
            "still problematic. We realized that learning semantics of sentences and\n",
            "documents was the key for solving a lot of natural language processing\n",
            "problems, and thus moved to the second part of our work: sentence compression.\n",
            "We introduce a flexible neural network architecture for learning embeddings of\n",
            "words and sentences that extract their semantics, propose an efficient\n",
            "implementation in the Torch framework and present embedding results comparable\n",
            "to the ones obtained with classical neural language models, while being more\n",
            "powerful.\n",
            "Actual Title : \n",
            "Semantic Vector Machines\n",
            "Generated Title : \n",
            "models for text processing\n",
            "Abstract : \n",
            "This paper describes a new kind of knowledge representation and mining system\n",
            "which we are calling the Semantic Knowledge Graph. At its heart, the Semantic\n",
            "Knowledge Graph leverages an inverted index, along with a complementary\n",
            "uninverted index, to represent nodes (terms) and edges (the documents within\n",
            "intersecting postings lists for multiple terms/nodes). This provides a layer of\n",
            "indirection between each pair of nodes and their corresponding edge, enabling\n",
            "edges to materialize dynamically from underlying corpus statistics. As a\n",
            "result, any combination of nodes can have edges to any other nodes materialize\n",
            "and be scored to reveal latent relationships between the nodes. This provides\n",
            "numerous benefits: the knowledge graph can be built automatically from a\n",
            "real-world corpus of data, new nodes - along with their combined edges - can be\n",
            "instantly materialized from any arbitrary combination of preexisting nodes\n",
            "(using set operations), and a full model of the semantic relationships between\n",
            "all entities within a domain can be represented and dynamically traversed using\n",
            "a highly compact representation of the graph. Such a system has widespread\n",
            "applications in areas as diverse as knowledge modeling and reasoning, natural\n",
            "language processing, anomaly detection, data cleansing, semantic search,\n",
            "analytics, data classification, root cause analysis, and recommendations\n",
            "systems. The main contribution of this paper is the introduction of a novel\n",
            "system - the Semantic Knowledge Graph - which is able to dynamically discover\n",
            "and score interesting relationships between any arbitrary combination of\n",
            "entities (words, phrases, or extracted concepts) through dynamically\n",
            "materializing nodes and edges from a compact graphical representation built\n",
            "automatically from a corpus of data representative of a knowledge domain.\n",
            "Actual Title : \n",
            "The Semantic Knowledge Graph: A compact, auto-generated model for\n",
            "  real-time traversal and ranking of any relationship within a domain\n",
            "Generated Title : \n",
            "semantic knowledge graph  a compact  auto generated model for\n",
            "Abstract : \n",
            "In previous work we developed a method of learning Bayesian Network models\n",
            "from raw data. This method relies on the well known minimal description length\n",
            "(MDL) principle. The MDL principle is particularly well suited to this task as\n",
            "it allows us to tradeoff, in a principled way, the accuracy of the learned\n",
            "network against its practical usefulness. In this paper we present some new\n",
            "results that have arisen from our work. In particular, we present a new local\n",
            "way of computing the description length. This allows us to make significant\n",
            "improvements in our search algorithm. In addition, we modify our algorithm so\n",
            "that it can take into account partial domain information that might be provided\n",
            "by a domain expert. The local computation of description length also opens the\n",
            "door for local refinement of an existent network. The feasibility of our\n",
            "approach is demonstrated by experiments involving networks of a practical size.\n",
            "Actual Title : \n",
            "Using Causal Information and Local Measures to Learn Bayesian Networks\n",
            "Generated Title : \n",
            "the bayesian to local measures\n",
            "Abstract : \n",
            "Non-negative Matrix Factorization (NMF) has already been applied to learn\n",
            "speaker characterizations from single or non-simultaneous speech for speaker\n",
            "recognition applications. It is also known for its good performance in (blind)\n",
            "source separation for simultaneous speech. This paper explains how NMF can be\n",
            "used to jointly solve the two problems in a multichannel speaker recognizer for\n",
            "simultaneous speech. It is shown how state-of-the-art multichannel NMF for\n",
            "blind source separation can be easily extended to incorporate speaker\n",
            "recognition. Experiments on the CHiME corpus show that this method outperforms\n",
            "the sequential approach of first applying source separation, followed by\n",
            "speaker recognition that uses state-of-the-art i-vector techniques.\n",
            "Actual Title : \n",
            "Joint Sound Source Separation and Speaker Recognition\n",
            "Generated Title : \n",
            "blind source separation with non negative non negative matrix factorization\n",
            "Abstract : \n",
            "The majority of online reviews consist of plain-text feedback together with a\n",
            "single numeric score. However, there are multiple dimensions to products and\n",
            "opinions, and understanding the `aspects' that contribute to users' ratings may\n",
            "help us to better understand their individual preferences. For example, a\n",
            "user's impression of an audiobook presumably depends on aspects such as the\n",
            "story and the narrator, and knowing their opinions on these aspects may help us\n",
            "to recommend better products. In this paper, we build models for rating systems\n",
            "in which such dimensions are explicit, in the sense that users leave separate\n",
            "ratings for each aspect of a product. By introducing new corpora consisting of\n",
            "five million reviews, rated with between three and six aspects, we evaluate our\n",
            "models on three prediction tasks: First, we use our model to uncover which\n",
            "parts of a review discuss which of the rated aspects. Second, we use our model\n",
            "to summarize reviews, which for us means finding the sentences that best\n",
            "explain a user's rating. Finally, since aspect ratings are optional in many of\n",
            "the datasets we consider, we use our model to recover those ratings that are\n",
            "missing from a user's evaluation. Our model matches state-of-the-art approaches\n",
            "on existing small-scale datasets, while scaling to the real-world datasets we\n",
            "introduce. Moreover, our model is able to `disentangle' content and sentiment\n",
            "words: we automatically learn content words that are indicative of a particular\n",
            "aspect as well as the aspect-specific sentiment words that are indicative of a\n",
            "particular rating.\n",
            "Actual Title : \n",
            "Learning Attitudes and Attributes from Multi-Aspect Reviews\n",
            "Generated Title : \n",
            "attitudes and attributes from multi aspect reviews\n",
            "Abstract : \n",
            "Deep generative models (DGMs) are effective on learning multilayered\n",
            "representations of complex data and performing inference of input data by\n",
            "exploring the generative ability. However, it is relatively insufficient to\n",
            "empower the discriminative ability of DGMs on making accurate predictions. This\n",
            "paper presents max-margin deep generative models (mmDGMs) and a\n",
            "class-conditional variant (mmDCGMs), which explore the strongly discriminative\n",
            "principle of max-margin learning to improve the predictive performance of DGMs\n",
            "in both supervised and semi-supervised learning, while retaining the generative\n",
            "capability. In semi-supervised learning, we use the predictions of a max-margin\n",
            "classifier as the missing labels instead of performing full posterior inference\n",
            "for efficiency; we also introduce additional max-margin and label-balance\n",
            "regularization terms of unlabeled data for effectiveness. We develop an\n",
            "efficient doubly stochastic subgradient algorithm for the piecewise linear\n",
            "objectives in different settings. Empirical results on various datasets\n",
            "demonstrate that: (1) max-margin learning can significantly improve the\n",
            "prediction performance of DGMs and meanwhile retain the generative ability; (2)\n",
            "in supervised learning, mmDGMs are competitive to the best fully discriminative\n",
            "networks when employing convolutional neural networks as the generative and\n",
            "recognition models; and (3) in semi-supervised learning, mmDCGMs can perform\n",
            "efficient inference and achieve state-of-the-art classification results on\n",
            "several benchmarks.\n",
            "Actual Title : \n",
            "Max-Margin Deep Generative Models for (Semi-)Supervised Learning\n",
            "Generated Title : \n",
            "deep generative deep generative models\n",
            "Abstract : \n",
            "Positron Emission Tomography (PET) scan images are one of the bio medical\n",
            "imaging techniques similar to that of MRI scan images but PET scan images are\n",
            "helpful in finding the development of tumors.The PET scan images requires\n",
            "expertise in the segmentation where clustering plays an important role in the\n",
            "automation process.The segmentation of such images is manual to automate the\n",
            "process clustering is used.Clustering is commonly known as unsupervised\n",
            "learning process of n dimensional data sets are clustered into k groups so as\n",
            "to maximize the inter cluster similarity and to minimize the intra cluster\n",
            "similarity.This paper is proposed to implement the commonly used K Means and\n",
            "Fuzzy CMeans (FCM) clustering algorithm.This work is implemented using MATrix\n",
            "LABoratory (MATLAB) and tested with sample PET scan image. The sample data is\n",
            "collected from Alzheimers Disease Neuro imaging Initiative ADNI. Medical Image\n",
            "Processing and Visualization Tool (MIPAV) are used to compare the resultant\n",
            "images.\n",
            "Actual Title : \n",
            "Segmentation of Alzheimers Disease in PET scan datasets using MATLAB\n",
            "Generated Title : \n",
            "of alzheimers disease in pet scan datasets using\n",
            "Abstract : \n",
            "This paper describes the results of some experiments exploring statistical\n",
            "methods to infer syntactic behavior of words and morphemes from a raw corpus in\n",
            "an unsupervised fashion. It shares certain points in common with Brown et al\n",
            "(1992) and work that has grown out of that: it employs statistical techniques\n",
            "to analyze syntactic behavior based on what words occur adjacent to a given\n",
            "word. However, we use an eigenvector decomposition of a nearest-neighbor graph\n",
            "to produce a two-dimensional rendering of the words of a corpus in which words\n",
            "of the same syntactic category tend to form neighborhoods. We exploit this\n",
            "technique for extending the value of automatic learning of morphology. In\n",
            "particular, we look at the suffixes derived from a corpus by unsupervised\n",
            "learning of morphology, and we ask which of these suffixes have a consistent\n",
            "syntactic function (e.g., in English, -tion is primarily a mark of nouns, but\n",
            "-s marks both noun plurals and 3rd person present on verbs), and we determine\n",
            "that this method works well for this task.\n",
            "Actual Title : \n",
            "Using eigenvectors of the bigram graph to infer morpheme identity\n",
            "Generated Title : \n",
            "of graph graph in natural language graph\n",
            "Abstract : \n",
            "Filters in convolutional networks are typically parameterized in a pixel\n",
            "basis, that does not take prior knowledge about the visual world into account.\n",
            "We investigate the generalized notion of frames designed with image properties\n",
            "in mind, as alternatives to this parametrization. We show that frame-based\n",
            "ResNets and Densenets can improve performance on Cifar-10+ consistently, while\n",
            "having additional pleasant properties like steerability. By exploiting these\n",
            "transformation properties explicitly, we arrive at dynamic steerable blocks.\n",
            "They are an extension of residual blocks, that are able to seamlessly transform\n",
            "filters under pre-defined transformations, conditioned on the input at training\n",
            "and inference time. Dynamic steerable blocks learn the degree of invariance\n",
            "from data and locally adapt filters, allowing them to apply a different\n",
            "geometrical variant of the same filter to each location of the feature map.\n",
            "When evaluated on the Berkeley Segmentation contour detection dataset, our\n",
            "approach outperforms all competing approaches that do not utilize pre-training.\n",
            "Our results highlight the benefits of image-based regularization to deep\n",
            "networks.\n",
            "Actual Title : \n",
            "Dynamic Steerable Blocks in Deep Residual Networks\n",
            "Generated Title : \n",
            "steerable deep networks\n",
            "Abstract : \n",
            "Machine learning and computer vision have driven many of the greatest\n",
            "advances in the modeling of Deep Convolutional Neural Networks (DCNNs).\n",
            "Nowadays, most of the research has been focused on improving recognition\n",
            "accuracy with better DCNN models and learning approaches. The recurrent\n",
            "convolutional approach is not applied very much, other than in a few DCNN\n",
            "architectures. On the other hand, Inception-v4 and Residual networks have\n",
            "promptly become popular among computer the vision community. In this paper, we\n",
            "introduce a new DCNN model called the Inception Recurrent Residual\n",
            "Convolutional Neural Network (IRRCNN), which utilizes the power of the\n",
            "Recurrent Convolutional Neural Network (RCNN), the Inception network, and the\n",
            "Residual network. This approach improves the recognition accuracy of the\n",
            "Inception-residual network with same number of network parameters. In addition,\n",
            "this proposed architecture generalizes the Inception network, the RCNN, and the\n",
            "Residual network with significantly improved training accuracy. We have\n",
            "empirically evaluated the performance of the IRRCNN model on different\n",
            "benchmarks including CIFAR-10, CIFAR-100, TinyImageNet-200, and CU3D-100. The\n",
            "experimental results show higher recognition accuracy against most of the\n",
            "popular DCNN models including the RCNN. We have also investigated the\n",
            "performance of the IRRCNN approach against the Equivalent Inception Network\n",
            "(EIN) and the Equivalent Inception Residual Network (EIRN) counterpart on the\n",
            "CIFAR-100 dataset. We report around 4.53%, 4.49% and 3.56% improvement in\n",
            "classification accuracy compared with the RCNN, EIN, and EIRN on the CIFAR-100\n",
            "dataset respectively. Furthermore, the experiment has been conducted on the\n",
            "TinyImageNet-200 and CU3D-100 datasets where the IRRCNN provides better testing\n",
            "accuracy compared to the Inception Recurrent CNN (IRCNN), the EIN, and the\n",
            "EIRN.\n",
            "Actual Title : \n",
            "Improved Inception-Residual Convolutional Neural Network for Object\n",
            "  Recognition\n",
            "Generated Title : \n",
            "inception residual convolutional neural network for object recognition\n",
            "Abstract : \n",
            "We consider the problem of computing a lightest derivation of a global\n",
            "structure using a set of weighted rules. A large variety of inference problems\n",
            "in AI can be formulated in this framework. We generalize A* search and\n",
            "heuristics derived from abstractions to a broad class of lightest derivation\n",
            "problems. We also describe a new algorithm that searches for lightest\n",
            "derivations using a hierarchy of abstractions. Our generalization of A* gives a\n",
            "new algorithm for searching AND/OR graphs in a bottom-up fashion. We discuss\n",
            "how the algorithms described here provide a general architecture for addressing\n",
            "the pipeline problem --- the problem of passing information back and forth\n",
            "between various stages of processing in a perceptual system. We consider\n",
            "examples in computer vision and natural language processing. We apply the\n",
            "hierarchical search algorithm to the problem of estimating the boundaries of\n",
            "convex objects in grayscale images and compare it to other search methods. A\n",
            "second set of experiments demonstrate the use of a new compositional model for\n",
            "finding salient curves in images.\n",
            "Actual Title : \n",
            "The Generalized A* Architecture\n",
            "Generated Title : \n",
            "generalized a \n",
            "Abstract : \n",
            "Often, when dealing with real-world recognition problems, we do not need, and\n",
            "often cannot have, knowledge of the entire set of possible classes that might\n",
            "appear during operational testing. Moreover, sometimes some of these classes\n",
            "may be ill-sampled, not sampled at all or undefined. In such cases, we need to\n",
            "think of robust classification methods able to deal with the \"unknown\" and\n",
            "properly reject samples belonging to classes never seen during training.\n",
            "Notwithstanding, almost all existing classifiers to date were mostly developed\n",
            "for the closed-set scenario, i.e., the classification setup in which it is\n",
            "assumed that all test samples belong to one of the classes with which the\n",
            "classifier was trained. In the open-set scenario, however, a test sample can\n",
            "belong to none of the known classes and the classifier must properly reject it\n",
            "by classifying it as unknown. In this work, we extend upon the well-known\n",
            "Support Vector Machines (SVM) classifier and introduce the Specialized Support\n",
            "Vector Machines (SSVM), which is suitable for recognition in open-set setups.\n",
            "SSVM balances the empirical risk and the risk of the unknown and ensures that\n",
            "the region of the feature space in which a test sample would be classified as\n",
            "known (one of the known classes) is always bounded, ensuring a finite risk of\n",
            "the unknown. The same cannot be guaranteed by the traditional SVM formulation,\n",
            "even when using the Radial Basis Function (RBF) kernel. In this work, we also\n",
            "highlight the properties of the SVM classifier related to the open-set\n",
            "scenario, and provide necessary and sufficient conditions for an RBF SVM to\n",
            "have bounded open-space risk. An extensive set of experiments compares the\n",
            "proposed method with existing solutions in the literature for open-set\n",
            "recognition and the reported results show its effectiveness.\n",
            "Actual Title : \n",
            "Specialized Support Vector Machines for open-set recognition\n",
            "Generated Title : \n",
            "vector machines for open set support vector machines\n",
            "Abstract : \n",
            "In this paper, we propose a recurrent framework for Joint Unsupervised\n",
            "LEarning (JULE) of deep representations and image clusters. In our framework,\n",
            "successive operations in a clustering algorithm are expressed as steps in a\n",
            "recurrent process, stacked on top of representations output by a Convolutional\n",
            "Neural Network (CNN). During training, image clusters and representations are\n",
            "updated jointly: image clustering is conducted in the forward pass, while\n",
            "representation learning in the backward pass. Our key idea behind this\n",
            "framework is that good representations are beneficial to image clustering and\n",
            "clustering results provide supervisory signals to representation learning. By\n",
            "integrating two processes into a single model with a unified weighted triplet\n",
            "loss and optimizing it end-to-end, we can obtain not only more powerful\n",
            "representations, but also more precise image clusters. Extensive experiments\n",
            "show that our method outperforms the state-of-the-art on image clustering\n",
            "across a variety of image datasets. Moreover, the learned representations\n",
            "generalize well when transferred to other tasks.\n",
            "Actual Title : \n",
            "Joint Unsupervised Learning of Deep Representations and Image Clusters\n",
            "Generated Title : \n",
            "unsupervised learning of joint and and clusters\n",
            "Abstract : \n",
            "A database of objects discovered in houses in the Roman city of Pompeii\n",
            "provides a unique view of ordinary life in an ancient city. Experts have used\n",
            "this collection to study the structure of Roman households, exploring the\n",
            "distribution and variability of tasks in architectural spaces, but such\n",
            "approaches are necessarily affected by modern cultural assumptions. In this\n",
            "study we present a data-driven approach to household archeology, treating it as\n",
            "an unsupervised labeling problem. This approach scales to large data sets and\n",
            "provides a more objective complement to human interpretation.\n",
            "Actual Title : \n",
            "Reconstructing Pompeian Households\n",
            "Generated Title : \n",
            "<unk> households\n",
            "Abstract : \n",
            "Image orientation detection requires high-level scene understanding. Humans\n",
            "use object recognition and contextual scene information to correctly orient\n",
            "images. In literature, the problem of image orientation detection is mostly\n",
            "confronted by using low-level vision features, while some approaches\n",
            "incorporate few easily detectable semantic cues to gain minor improvements. The\n",
            "vast amount of semantic content in images makes orientation detection\n",
            "challenging, and therefore there is a large semantic gap between existing\n",
            "methods and human behavior. Also, existing methods in literature report highly\n",
            "discrepant detection rates, which is mainly due to large differences in\n",
            "datasets and limited variety of test images used for evaluation. In this work,\n",
            "for the first time, we leverage the power of deep learning and adapt\n",
            "pre-trained convolutional neural networks using largest training dataset\n",
            "to-date for the image orientation detection task. An extensive evaluation of\n",
            "our model on different public datasets shows that it remarkably generalizes to\n",
            "correctly orient a large set of unconstrained images; it also significantly\n",
            "outperforms the state-of-the-art and achieves accuracy very close to that of\n",
            "humans.\n",
            "Actual Title : \n",
            "Why my photos look sideways or upside down? Detecting Canonical\n",
            "  Orientation of Images using Convolutional Neural Networks\n",
            "Generated Title : \n",
            "my photos look <unk> a <unk> detecting gender\n",
            "Abstract : \n",
            "Conditional random fields (CRFs) are commonly employed as a post-processing\n",
            "tool for image segmentation tasks. The unary potentials of the CRF are often\n",
            "learnt independently by a classifier, thereby decoupling the inference in CRF\n",
            "from the training of classifier. Such a scheme works effectively, when\n",
            "pixel-level labelling is available for all the images. However, in absence of\n",
            "pixel-level labels, the classifier is faced with the uphill task of selectively\n",
            "assigning the image-level labels to the pixels of the image. Prior work often\n",
            "relied on localization cues, such as saliency maps, objectness priors, bounding\n",
            "boxes etc., to address this challenging problem. In contrast, we model the\n",
            "labels of the pixels as latent variables of a CRF. The pixels and the\n",
            "image-level labels are the observed variables of the latent CRF. We amortize\n",
            "the cost of inference in the latent CRF over the entire dataset, by training an\n",
            "inference network to approximate the posterior distribution of the latent\n",
            "variables given the observed variables. The inference network can be trained in\n",
            "an end-to-end fashion, and requires no localization cues for training.\n",
            "Moreover, unlike other approaches for weakly-supervised segmentation, the\n",
            "proposed model doesn't require further post-processing. The proposed model\n",
            "achieves performance comparable with other approaches that employ saliency\n",
            "masks for the task of weakly-supervised semantic image segmentation on the\n",
            "challenging VOC 2012 dataset.\n",
            "Actual Title : \n",
            "Amortized Inference and Learning in Latent Conditional Random Fields for\n",
            "  Weakly-Supervised Semantic Image Segmentation\n",
            "Generated Title : \n",
            "inference of latent latent latent for latent segmentation\n",
            "Abstract : \n",
            "Millions of hearing impaired people around the world routinely use some\n",
            "variants of sign languages to communicate, thus the automatic translation of a\n",
            "sign language is meaningful and important. Currently, there are two\n",
            "sub-problems in Sign Language Recognition (SLR), i.e., isolated SLR that\n",
            "recognizes word by word and continuous SLR that translates entire sentences.\n",
            "Existing continuous SLR methods typically utilize isolated SLRs as building\n",
            "blocks, with an extra layer of preprocessing (temporal segmentation) and\n",
            "another layer of post-processing (sentence synthesis). Unfortunately, temporal\n",
            "segmentation itself is non-trivial and inevitably propagates errors into\n",
            "subsequent steps. Worse still, isolated SLR methods typically require strenuous\n",
            "labeling of each word separately in a sentence, severely limiting the amount of\n",
            "attainable training data. To address these challenges, we propose a novel\n",
            "continuous sign recognition framework, the Hierarchical Attention Network with\n",
            "Latent Space (LS-HAN), which eliminates the preprocessing of temporal\n",
            "segmentation. The proposed LS-HAN consists of three components: a two-stream\n",
            "Convolutional Neural Network (CNN) for video feature representation generation,\n",
            "a Latent Space (LS) for semantic gap bridging, and a Hierarchical Attention\n",
            "Network (HAN) for latent space based recognition. Experiments are carried out\n",
            "on two large scale datasets. Experimental results demonstrate the effectiveness\n",
            "of the proposed framework.\n",
            "Actual Title : \n",
            "Video-based Sign Language Recognition without Temporal Segmentation\n",
            "Generated Title : \n",
            "sign recognition recognition with hierarchical deep lstm\n",
            "Abstract : \n",
            "Robust foreground object segmentation via background modelling is a difficult\n",
            "problem in cluttered environments, where obtaining a clear view of the\n",
            "background to model is almost impossible. In this paper, we propose a method\n",
            "capable of robustly estimating the background and detecting regions of interest\n",
            "in such environments. In particular, we propose to extend the background\n",
            "initialisation component of a recent patch-based foreground detection algorithm\n",
            "with an elaborate technique based on Markov Random Fields, where the optimal\n",
            "labelling solution is computed using iterated conditional modes. Rather than\n",
            "relying purely on local temporal statistics, the proposed technique takes into\n",
            "account the spatial continuity of the entire background. Experiments with\n",
            "several tracking algorithms on the CAVIAR dataset indicate that the proposed\n",
            "method leads to considerable improvements in object tracking accuracy, when\n",
            "compared to methods based on Gaussian mixture models and feature histograms.\n",
            "Actual Title : \n",
            "MRF-based Background Initialisation for Improved Foreground Detection in\n",
            "  Cluttered Surveillance Videos\n",
            "Generated Title : \n",
            "foreground tracking via robust foreground and robust object\n",
            "Abstract : \n",
            "Canonical correlation analysis (CCA) is a multivariate statistical technique\n",
            "for finding the linear relationship between two sets of variables. The kernel\n",
            "generalization of CCA named kernel CCA has been proposed to find nonlinear\n",
            "relations between datasets. Despite their wide usage, they have one common\n",
            "limitation that is the lack of sparsity in their solution. In this paper, we\n",
            "consider sparse kernel CCA and propose a novel sparse kernel CCA algorithm\n",
            "(SKCCA). Our algorithm is based on a relationship between kernel CCA and least\n",
            "squares. Sparsity of the dual transformations is introduced by penalizing the\n",
            "$\\ell_{1}$-norm of dual vectors. Experiments demonstrate that our algorithm not\n",
            "only performs well in computing sparse dual transformations but also can\n",
            "alleviate the over-fitting problem of kernel CCA.\n",
            "Actual Title : \n",
            "Sparse Kernel Canonical Correlation Analysis via $\\ell_1$-regularization\n",
            "Generated Title : \n",
            "kernel canonical correlation analysis\n",
            "Abstract : \n",
            "Learning acoustic models directly from the raw waveform data with minimal\n",
            "processing is challenging. Current waveform-based models have generally used\n",
            "very few (~2) convolutional layers, which might be insufficient for building\n",
            "high-level discriminative features. In this work, we propose very deep\n",
            "convolutional neural networks (CNNs) that directly use time-domain waveforms as\n",
            "inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over\n",
            "very long sequences (e.g., vector of size 32000), necessary for processing\n",
            "acoustic waveforms. This is achieved through batch normalization, residual\n",
            "learning, and a careful design of down-sampling in the initial layers. Our\n",
            "networks are fully convolutional, without the use of fully connected layers and\n",
            "dropout, to maximize representation learning. We use a large receptive field in\n",
            "the first convolutional layer to mimic bandpass filters, but very small\n",
            "receptive fields subsequently to control the model capacity. We demonstrate the\n",
            "performance gains with the deeper models. Our evaluation shows that the CNN\n",
            "with 18 weight layers outperform the CNN with 3 weight layers by over 15% in\n",
            "absolute accuracy for an environmental sound recognition task and matches the\n",
            "performance of models using log-mel features.\n",
            "Actual Title : \n",
            "Very Deep Convolutional Neural Networks for Raw Waveforms\n",
            "Generated Title : \n",
            "deep convolutional neural networks for <unk> recognition\n",
            "Abstract : \n",
            "We study here the well-known propagation rules for Boolean constraints. First\n",
            "we propose a simple notion of completeness for sets of such rules and establish\n",
            "a completeness result. Then we show an equivalence in an appropriate sense\n",
            "between Boolean constraint propagation and unit propagation, a form of\n",
            "resolution for propositional logic.\n",
            "  Subsequently we characterize one set of such rules by means of the notion of\n",
            "hyper-arc consistency introduced in (Mohr and Masini 1988). Also, we clarify\n",
            "the status of a similar, though different, set of rules introduced in (Simonis\n",
            "1989a) and more fully in (Codognet and Diaz 1996).\n",
            "Actual Title : \n",
            "Some Remarks on Boolean Constraint Propagation\n",
            "Generated Title : \n",
            "propagation on boolean propagation\n",
            "Abstract : \n",
            "We propose a clustering-based iterative algorithm to solve certain\n",
            "optimization problems in machine learning, where we start the algorithm by\n",
            "aggregating the original data, solving the problem on aggregated data, and then\n",
            "in subsequent steps gradually disaggregate the aggregated data. We apply the\n",
            "algorithm to common machine learning problems such as the least absolute\n",
            "deviation regression problem, support vector machines, and semi-supervised\n",
            "support vector machines. We derive model-specific data aggregation and\n",
            "disaggregation procedures. We also show optimality, convergence, and the\n",
            "optimality gap of the approximated solution in each iteration. A computational\n",
            "study is provided.\n",
            "Actual Title : \n",
            "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality\n",
            "  in Machine Learning\n",
            "Generated Title : \n",
            "aggregate optimality in machine learning\n",
            "Abstract : \n",
            "As with articles and journals, the customary methods for measuring books'\n",
            "academic impact mainly involve citations, which is easy but limited to\n",
            "interrogating traditional citation databases and scholarly book reviews,\n",
            "Researchers have attempted to use other metrics, such as Google Books,\n",
            "libcitation, and publisher prestige. However, these approaches lack\n",
            "content-level information and cannot determine the citation intentions of\n",
            "users. Meanwhile, the abundant online review resources concerning academic\n",
            "books can be used to mine deeper information and content utilizing altmetric\n",
            "perspectives. In this study, we measure the impacts of academic books by\n",
            "multi-granularity mining online reviews, and we identify factors that affect a\n",
            "book's impact. First, online reviews of a sample of academic books on Amazon.cn\n",
            "are crawled and processed. Then, multi-granularity review mining is conducted\n",
            "to identify review sentiment polarities and aspects' sentiment values. Lastly,\n",
            "the numbers of positive reviews and negative reviews, aspect sentiment values,\n",
            "star values, and information regarding helpfulness are integrated via the\n",
            "entropy method, and lead to the calculation of the final book impact scores.\n",
            "The results of a correlation analysis of book impact scores obtained via our\n",
            "method versus traditional book citations show that, although there are\n",
            "substantial differences between subject areas, online book reviews tend to\n",
            "reflect the academic impact. Thus, we infer that online reviews represent a\n",
            "promising source for mining book impact within the altmetric perspective and at\n",
            "the multi-granularity content level. Moreover, our proposed method might also\n",
            "be a means by which to measure other books besides academic publications.\n",
            "Actual Title : \n",
            "Measuring Book Impact Based on the Multi-granularity Online Review\n",
            "  Mining\n",
            "Generated Title : \n",
            "book impact on a review\n",
            "Abstract : \n",
            "In many applications of finance, biology and sociology, complex systems\n",
            "involve entities interacting with each other. These processes have the\n",
            "peculiarity of evolving over time and of comprising latent factors, which\n",
            "influence the system without being explicitly measured. In this work we present\n",
            "latent variable time-varying graphical lasso (LTGL), a method for multivariate\n",
            "time-series graphical modelling that considers the influence of hidden or\n",
            "unmeasurable factors. The estimation of the contribution of the latent factors\n",
            "is embedded in the model which produces both sparse and low-rank components for\n",
            "each time point. In particular, the first component represents the connectivity\n",
            "structure of observable variables of the system, while the second represents\n",
            "the influence of hidden factors, assumed to be few with respect to the observed\n",
            "variables. Our model includes temporal consistency on both components,\n",
            "providing an accurate evolutionary pattern of the system. We derive a tractable\n",
            "optimisation algorithm based on alternating direction method of multipliers,\n",
            "and develop a scalable and efficient implementation which exploits proximity\n",
            "operators in closed form. LTGL is extensively validated on synthetic data,\n",
            "achieving optimal performance in terms of accuracy, structure learning and\n",
            "scalability with respect to ground truth and state-of-the-art methods for\n",
            "graphical inference. We conclude with the application of LTGL to real case\n",
            "studies, from biology and finance, to illustrate how our method can be\n",
            "successfully employed to gain insights on multivariate time-series data.\n",
            "Actual Title : \n",
            "Latent variable time-varying network inference\n",
            "Generated Title : \n",
            "factor framework for network inference\n",
            "Abstract : \n",
            "Community detection is a fundamental problem in network analysis which is\n",
            "made more challenging by overlaps between communities which often occur in\n",
            "practice. Here we propose a general, flexible, and interpretable generative\n",
            "model for overlapping communities, which can be thought of as a generalization\n",
            "of the degree-corrected stochastic block model. We develop an efficient\n",
            "spectral algorithm for estimating the community memberships, which deals with\n",
            "the overlaps by employing the K-medians algorithm rather than the usual K-means\n",
            "for clustering in the spectral domain. We show that the algorithm is\n",
            "asymptotically consistent when networks are not too sparse and the overlaps\n",
            "between communities not too large. Numerical experiments on both simulated\n",
            "networks and many real social networks demonstrate that our method performs\n",
            "very well compared to a number of benchmark methods for overlapping community\n",
            "detection.\n",
            "Actual Title : \n",
            "Detecting Overlapping Communities in Networks Using Spectral Methods\n",
            "Generated Title : \n",
            "overlapping communities in large labeled data\n",
            "Abstract : \n",
            "Often multiple instances of an object occur in the same scene, for example in\n",
            "a warehouse. Unsupervised multi-instance object discovery algorithms are able\n",
            "to detect and identify such objects. We use such an algorithm to provide object\n",
            "proposals to a convolutional neural network (CNN) based classifier. This\n",
            "results in fewer regions to evaluate, compared to traditional region proposal\n",
            "algorithms. Additionally, it enables using the joint probability of multiple\n",
            "instances of an object, resulting in improved classification accuracy. The\n",
            "proposed technique can also split a single class into multiple sub-classes\n",
            "corresponding to the different object types, enabling hierarchical\n",
            "classification.\n",
            "Actual Title : \n",
            "Detecting and Grouping Identical Objects for Region Proposal and\n",
            "  Classification\n",
            "Generated Title : \n",
            "object discovery with multiple object proposal\n",
            "Abstract : \n",
            "In this work, we present a new Vector Space Model (VSM) of speech utterances\n",
            "for the task of spoken dialect identification. Generally, DID systems are built\n",
            "using two sets of features that are extracted from speech utterances; acoustic\n",
            "and phonetic. The acoustic and phonetic features are used to form vector\n",
            "representations of speech utterances in an attempt to encode information about\n",
            "the spoken dialects. The Phonotactic and Acoustic VSMs, thus formed, are used\n",
            "for the task of DID. The aim of this paper is to construct a single VSM that\n",
            "encodes information about spoken dialects from both the Phonotactic and\n",
            "Acoustic VSMs. Given the two views of the data, we make use of a well known\n",
            "multi-view dimensionality reduction technique known as Canonical Correlation\n",
            "Analysis (CCA), to form a single vector representation for each speech\n",
            "utterance that encodes dialect specific discriminative information from both\n",
            "the phonetic and acoustic representations. We refer to this approach as feature\n",
            "space combination approach and show that our CCA based feature vector\n",
            "representation performs better on the Arabic DID task than the phonetic and\n",
            "acoustic feature representations used alone. We also present the feature space\n",
            "combination approach as a viable alternative to the model based combination\n",
            "approach, where two DID systems are built using the two VSMs (Phonotactic and\n",
            "Acoustic) and the final prediction score is the output score combination from\n",
            "the two systems.\n",
            "Actual Title : \n",
            "Multi-view Dimensionality Reduction for Dialect Identification of Arabic\n",
            "  Broadcast Speech\n",
            "Generated Title : \n",
            "dimensionality reduction for dialect identification in speech broadcast\n",
            "Abstract : \n",
            "Detecting outliers which are grossly different from or inconsistent with the\n",
            "remaining dataset is a major challenge in real-world KDD applications. Existing\n",
            "outlier detection methods are ineffective on scattered real-world datasets due\n",
            "to implicit data patterns and parameter setting issues. We define a novel\n",
            "\"Local Distance-based Outlier Factor\" (LDOF) to measure the {outlier-ness} of\n",
            "objects in scattered datasets which addresses these issues. LDOF uses the\n",
            "relative location of an object to its neighbours to determine the degree to\n",
            "which the object deviates from its neighbourhood. Properties of LDOF are\n",
            "theoretically analysed including LDOF's lower bound and its false-detection\n",
            "probability, as well as parameter settings. In order to facilitate parameter\n",
            "settings in real-world applications, we employ a top-n technique in our outlier\n",
            "detection approach, where only the objects with the highest LDOF values are\n",
            "regarded as outliers. Compared to conventional approaches (such as top-n KNN\n",
            "and top-n LOF), our method top-n LDOF is more effective at detecting outliers\n",
            "in scattered data. It is also easier to set parameters, since its performance\n",
            "is relatively stable over a large range of parameter values, as illustrated by\n",
            "experimental results on both real-world and synthetic datasets.\n",
            "Actual Title : \n",
            "A New Local Distance-Based Outlier Detection Approach for Scattered\n",
            "  Real-World Data\n",
            "Generated Title : \n",
            "distance based local distance based local for scattered real world data\n",
            "Abstract : \n",
            "Recurrent Neural Networks (RNNs) have been widely used in natural language\n",
            "processing and computer vision. Among them, the Hierarchical Multi-scale RNN\n",
            "(HM-RNN), a kind of multi-scale hierarchical RNN proposed recently, can learn\n",
            "the hierarchical temporal structure from data automatically. In this paper, we\n",
            "extend the work to solve the computer vision task of action recognition.\n",
            "However, in sequence-to-sequence models like RNN, it is normally very hard to\n",
            "discover the relationships between inputs and outputs given static inputs. As a\n",
            "solution, attention mechanism could be applied to extract the relevant\n",
            "information from input thus facilitating the modeling of input-output\n",
            "relationships. Based on these considerations, we propose a novel attention\n",
            "network, namely Hierarchical Multi-scale Attention Network (HM-AN), by\n",
            "combining the HM-RNN and the attention mechanism and apply it to action\n",
            "recognition. A newly proposed gradient estimation method for stochastic\n",
            "neurons, namely Gumbel-softmax, is exploited to implement the temporal boundary\n",
            "detectors and the stochastic hard attention mechanism. To amealiate the\n",
            "negative effect of sensitive temperature of the Gumbel-softmax, an adaptive\n",
            "temperature training method is applied to better the system performance. The\n",
            "experimental results demonstrate the improved effect of HM-AN over LSTM with\n",
            "attention on the vision task. Through visualization of what have been learnt by\n",
            "the networks, it can be observed that both the attention regions of images and\n",
            "the hierarchical temporal structure can be captured by HM-AN.\n",
            "Actual Title : \n",
            "Hierarchical Multi-scale Attention Networks for Action Recognition\n",
            "Generated Title : \n",
            "hierarchical attention for action recognition\n",
            "Abstract : \n",
            "Person Re-Identification (re-id) is a challenging task in computer vision,\n",
            "especially when there are limited training data from multiple camera views. In\n",
            "this paper, we pro- pose a deep learning based person re-identification method\n",
            "by transferring knowledge of mid-level attribute features and high-level\n",
            "classification features. Building on the idea that identity classification,\n",
            "attribute recognition and re- identification share the same mid-level semantic\n",
            "representations, they can be trained sequentially by fine-tuning one based on\n",
            "another. In our framework, we train identity classification and attribute\n",
            "recognition tasks from deep Convolutional Neural Network (dCNN) to learn person\n",
            "information. The information can be transferred to the person re-id task and\n",
            "improves its accuracy by a large margin. Further- more, a Long Short Term\n",
            "Memory(LSTM) based Recurrent Neural Network (RNN) component is extended by a\n",
            "spacial gate. This component is used in the re-id model to pay attention to\n",
            "certain spacial parts in each recurrent unit. Experimental results show that\n",
            "our method achieves 78.3% of rank-1 recognition accuracy on the CUHK03\n",
            "benchmark.\n",
            "Actual Title : \n",
            "Cross Domain Knowledge Transfer for Person Re-identification\n",
            "Generated Title : \n",
            "person transfer learning for person re identification\n",
            "Abstract : \n",
            "Policy optimization methods have shown great promise in solving complex\n",
            "reinforcement and imitation learning tasks. While model-free methods are\n",
            "broadly applicable, they often require many samples to optimize complex\n",
            "policies. Model-based methods greatly improve sample-efficiency but at the cost\n",
            "of poor generalization, requiring a carefully handcrafted model of the system\n",
            "dynamics for each task. Recently, hybrid methods have been successful in\n",
            "trading off applicability for improved sample-complexity. However, these have\n",
            "been limited to continuous action spaces. In this work, we present a new hybrid\n",
            "method based on an approximation of the dynamics as an expectation over the\n",
            "next state under the current policy. This relaxation allows us to derive a\n",
            "novel hybrid policy gradient estimator, combining score function and pathwise\n",
            "derivative estimators, that is applicable to discrete action spaces. We show\n",
            "significant gains in sample complexity, ranging between $1.7$ and $25\\times$,\n",
            "when learning parameterized policies on Cart Pole, Acrobot, Mountain Car and\n",
            "Hand Mass. Our method is applicable to both discrete and continuous action\n",
            "spaces, when competing pathwise methods are limited to the latter.\n",
            "Actual Title : \n",
            "Deterministic Policy Optimization by Combining Pathwise and Score\n",
            "  Function Estimators for Discrete Action Spaces\n",
            "Generated Title : \n",
            "linear approximation approach to based on a hybrid\n",
            "Abstract : \n",
            "Possibilistic answer set programming (PASP) extends answer set programming\n",
            "(ASP) by attaching to each rule a degree of certainty. While such an extension\n",
            "is important from an application point of view, existing semantics are not\n",
            "well-motivated, and do not always yield intuitive results. To develop a more\n",
            "suitable semantics, we first introduce a characterization of answer sets of\n",
            "classical ASP programs in terms of possibilistic logic where an ASP program\n",
            "specifies a set of constraints on possibility distributions. This\n",
            "characterization is then naturally generalized to define answer sets of PASP\n",
            "programs. We furthermore provide a syntactic counterpart, leading to a\n",
            "possibilistic generalization of the well-known Gelfond-Lifschitz reduct, and we\n",
            "show how our framework can readily be implemented using standard ASP solvers.\n",
            "Actual Title : \n",
            "Possibilistic Answer Set Programming Revisited\n",
            "Generated Title : \n",
            "answer set programming revisited\n",
            "Abstract : \n",
            "We extend the work of Narasimhan and Bilmes [30] for minimizing set functions\n",
            "representable as a difference between submodular functions. Similar to [30],\n",
            "our new algorithms are guaranteed to monotonically reduce the objective\n",
            "function at every step. We empirically and theoretically show that the\n",
            "per-iteration cost of our algorithms is much less than [30], and our algorithms\n",
            "can be used to efficiently minimize a difference between submodular functions\n",
            "under various combinatorial constraints, a problem not previously addressed. We\n",
            "provide computational bounds and a hardness result on the mul- tiplicative\n",
            "inapproximability of minimizing the difference between submodular functions. We\n",
            "show, however, that it is possible to give worst-case additive bounds by\n",
            "providing a polynomial time computable lower-bound on the minima. Finally we\n",
            "show how a number of machine learning problems can be modeled as minimizing the\n",
            "difference between submodular functions. We experimentally show the validity of\n",
            "our algorithms by testing them on the problem of feature selection with\n",
            "submodular cost features.\n",
            "Actual Title : \n",
            "Algorithms for Approximate Minimization of the Difference Between\n",
            "  Submodular Functions, with Applications\n",
            "Generated Title : \n",
            "algorithms for <unk> algorithms\n",
            "Abstract : \n",
            "We interpret HyperNetworks within the framework of variational inference\n",
            "within implicit distributions. Our method, Bayes by Hypernet, is able to model\n",
            "a richer variational distribution than previous methods. Experiments show that\n",
            "it achieves comparable predictive performance on the MNIST classification task\n",
            "while providing higher predictive uncertainties compared to MC-Dropout and\n",
            "regular maximum likelihood training.\n",
            "Actual Title : \n",
            "Implicit Weight Uncertainty in Neural Networks\n",
            "Generated Title : \n",
            "weight uncertainty in neural networks\n",
            "Abstract : \n",
            "We propose an automatic method to infer high dynamic range illumination from\n",
            "a single, limited field-of-view, low dynamic range photograph of an indoor\n",
            "scene. In contrast to previous work that relies on specialized image capture,\n",
            "user input, and/or simple scene models, we train an end-to-end deep neural\n",
            "network that directly regresses a limited field-of-view photo to HDR\n",
            "illumination, without strong assumptions on scene geometry, material\n",
            "properties, or lighting. We show that this can be accomplished in a three step\n",
            "process: 1) we train a robust lighting classifier to automatically annotate the\n",
            "location of light sources in a large dataset of LDR environment maps, 2) we use\n",
            "these annotations to train a deep neural network that predicts the location of\n",
            "lights in a scene from a single limited field-of-view photo, and 3) we\n",
            "fine-tune this network using a small dataset of HDR environment maps to predict\n",
            "light intensities. This allows us to automatically recover high-quality HDR\n",
            "illumination estimates that significantly outperform previous state-of-the-art\n",
            "methods. Consequently, using our illumination estimates for applications like\n",
            "3D object insertion, we can achieve results that are photo-realistic, which is\n",
            "validated via a perceptual user study.\n",
            "Actual Title : \n",
            "Learning to Predict Indoor Illumination from a Single Image\n",
            "Generated Title : \n",
            "light field capture from a single image\n",
            "Abstract : \n",
            "This paper describes a novel storyboarding scheme that uses a model trained\n",
            "on pairwise image comparisons to identify images likely to be of interest to a\n",
            "mobile robot user. Traditional storyboarding schemes typically attempt to\n",
            "summarise robot observations using predefined novelty or image quality\n",
            "objectives, but we propose a user training stage that allows the incorporation\n",
            "of user interest when storyboarding. Our approach dramatically reduces the\n",
            "number of image comparisons required to infer image interest by applying a\n",
            "Gaussian process smoothing algorithm on image features extracted using a\n",
            "pre-trained convolutional neural network. As a particularly valuable\n",
            "by-product, the proposed approach allows the generation of user-specific\n",
            "saliency or attention maps.\n",
            "Actual Title : \n",
            "User-driven mobile robot storyboarding: Learning image interest and\n",
            "  saliency from pairwise image comparisons\n",
            "Generated Title : \n",
            "mobile robot image generation using ground image saliency\n",
            "Abstract : \n",
            "Change detection is one of the central problems in earth observation and was\n",
            "extensively investigated over recent decades. In this paper, we propose a novel\n",
            "recurrent convolutional neural network (ReCNN) architecture, which is trained\n",
            "to learn a joint spectral-spatial-temporal feature representation in a unified\n",
            "framework for change detection in multispectral images. To this end, we bring\n",
            "together a convolutional neural network (CNN) and a recurrent neural network\n",
            "(RNN) into one end-to-end network. The former is able to generate rich\n",
            "spectral-spatial feature representations, while the latter effectively analyzes\n",
            "temporal dependency in bi-temporal images. In comparison with previous\n",
            "approaches to change detection, the proposed network architecture possesses\n",
            "three distinctive properties: 1) It is end-to-end trainable, in contrast to\n",
            "most existing methods whose components are separately trained or computed; 2)\n",
            "it naturally harnesses spatial information that has been proven to be\n",
            "beneficial to change detection task; 3) it is capable of adaptively learning\n",
            "the temporal dependency between multitemporal images, unlike most of algorithms\n",
            "that use fairly simple operation like image differencing or stacking. As far as\n",
            "we know, this is the first time that a recurrent convolutional network\n",
            "architecture has been proposed for multitemporal remote sensing image analysis.\n",
            "The proposed network is validated on real multispectral data sets. Both visual\n",
            "and quantitative analysis of experimental results demonstrates competitive\n",
            "performance in the proposed mode.\n",
            "Actual Title : \n",
            "Learning Spectral-Spatial-Temporal Features via a Recurrent\n",
            "  Convolutional Neural Network for Change Detection in Multispectral Imagery\n",
            "Generated Title : \n",
            "recurrent neural networks for <unk> and change detection\n",
            "Abstract : \n",
            "Feature representations, both hand-designed and learned ones, are often hard\n",
            "to analyze and interpret, even when they are extracted from visual data. We\n",
            "propose a new approach to study image representations by inverting them with an\n",
            "up-convolutional neural network. We apply the method to shallow representations\n",
            "(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our\n",
            "approach provides significantly better reconstructions than existing methods,\n",
            "revealing that there is surprisingly rich information contained in these\n",
            "features. Inverting a deep network trained on ImageNet provides several\n",
            "insights into the properties of the feature representation learned by the\n",
            "network. Most strikingly, the colors and the rough contours of an image can be\n",
            "reconstructed from activations in higher network layers and even from the\n",
            "predicted class probabilities.\n",
            "Actual Title : \n",
            "Inverting Visual Representations with Convolutional Networks\n",
            "Generated Title : \n",
            "visual representations with visual\n",
            "Abstract : \n",
            "We investigate neural techniques for end-to-end computational argumentation\n",
            "mining (AM). We frame AM both as a token-based dependency parsing and as a\n",
            "token-based sequence tagging problem, including a multi-task learning setup.\n",
            "Contrary to models that operate on the argument component level, we find that\n",
            "framing AM as dependency parsing leads to subpar performance results. In\n",
            "contrast, less complex (local) tagging models based on BiLSTMs perform robustly\n",
            "across classification scenarios, being able to catch long-range dependencies\n",
            "inherent to the AM problem. Moreover, we find that jointly learning 'natural'\n",
            "subtasks, in a multi-task learning setup, improves performance.\n",
            "Actual Title : \n",
            "Neural End-to-End Learning for Computational Argumentation Mining\n",
            "Generated Title : \n",
            "neural neural for neural networks for <unk>\n",
            "Abstract : \n",
            "Face alignment is a classic problem in the computer vision field. Previous\n",
            "works mostly focus on sparse alignment with a limited number of facial landmark\n",
            "points, i.e., facial landmark detection. In this paper, for the first time, we\n",
            "aim at providing a very dense 3D alignment for large-pose face images. To\n",
            "achieve this, we train a CNN to estimate the 3D face shape, which not only\n",
            "aligns limited facial landmarks but also fits face contours and SIFT feature\n",
            "points. Moreover, we also address the bottleneck of training CNN with multiple\n",
            "datasets, due to different landmark markups on different datasets, such as 5,\n",
            "34, 68. Experimental results show our method not only provides high-quality,\n",
            "dense 3D face fitting but also outperforms the state-of-the-art facial landmark\n",
            "detection methods on the challenging datasets. Our model can run at real time\n",
            "during testing.\n",
            "Actual Title : \n",
            "Dense Face Alignment\n",
            "Generated Title : \n",
            "face alignment\n",
            "Abstract : \n",
            "In the domain of image processing, often real-time constraints are required.\n",
            "In particular, in safety-critical applications, such as X-ray computed\n",
            "tomography in medical imaging or advanced driver assistance systems in the\n",
            "automotive domain, timing is of utmost importance. A common approach to\n",
            "maintain real-time capabilities of compute-intensive applications is to offload\n",
            "those computations to dedicated accelerator hardware, such as Field\n",
            "Programmable Gate Arrays (FPGAs). Programming such architectures is a\n",
            "challenging task, with respect to the typical FPGA-specific design criteria:\n",
            "Achievable overall algorithm latency and resource usage of FPGA primitives\n",
            "(BRAM, FF, LUT, and DSP). High-Level Synthesis (HLS) dramatically simplifies\n",
            "this task by enabling the description of algorithms in well-known higher\n",
            "languages (C/C++) and its automatic synthesis that can be accomplished by HLS\n",
            "tools. However, algorithm developers still need expert knowledge about the\n",
            "target architecture, in order to achieve satisfying results. Therefore, in\n",
            "previous work, we have shown that elevating the description of image algorithms\n",
            "to an even higher abstraction level, by using a Domain-Specific Language (DSL),\n",
            "can significantly cut down the complexity for designing such algorithms for\n",
            "FPGAs. To give the developer even more control over the common trade-off,\n",
            "latency vs. resource usage, we will present an automatic optimization process\n",
            "where these criteria are analyzed and fed back to the DSL compiler, in order to\n",
            "generate code that is closer to the desired design specifications. Finally, we\n",
            "generate code for stereo block matching algorithms and compare it with\n",
            "handwritten implementations to quantify the quality of our results.\n",
            "Actual Title : \n",
            "Automatic Optimization of Hardware Accelerators for Image Processing\n",
            "Generated Title : \n",
            "an architecture and hardware accelerators for image processing\n",
            "Abstract : \n",
            "We study the problem of inducing interpretability in KG embeddings.\n",
            "Specifically, we explore the Universal Schema (Riedel et al., 2013) and propose\n",
            "a method to induce interpretability. There have been many vector space models\n",
            "proposed for the problem, however, most of these methods don't address the\n",
            "interpretability (semantics) of individual dimensions. In this work, we study\n",
            "this problem and propose a method for inducing interpretability in KG\n",
            "embeddings using entity co-occurrence statistics. The proposed method\n",
            "significantly improves the interpretability, while maintaining comparable\n",
            "performance in other KG tasks.\n",
            "Actual Title : \n",
            "Inducing Interpretability in Knowledge Graph Embeddings\n",
            "Generated Title : \n",
            "interpretability in knowledge graph embeddings\n",
            "Abstract : \n",
            "Deep learning has significantly advanced the state of the art in artificial\n",
            "intelligence, gaining wide popularity from both industry and academia. Special\n",
            "interest is around Convolutional Neural Networks (CNN), which take inspiration\n",
            "from the hierarchical structure of the visual cortex, to form deep layers of\n",
            "convolutional operations, along with fully connected classifiers. Hardware\n",
            "implementations of these deep CNN architectures are challenged with memory\n",
            "bottlenecks that require many convolution and fully-connected layers demanding\n",
            "large amount of communication for parallel computation. Multi-core CPU based\n",
            "solutions have demonstrated their inadequacy for this problem due to the memory\n",
            "wall and low parallelism. Many-core GPU architectures show superior performance\n",
            "but they consume high power and also have memory constraints due to\n",
            "inconsistencies between cache and main memory. FPGA design solutions are also\n",
            "actively being explored, which allow implementing the memory hierarchy using\n",
            "embedded BlockRAM. This boosts the parallel use of shared memory elements\n",
            "between multiple processing units, avoiding data replicability and\n",
            "inconsistencies. This makes FPGAs potentially powerful solutions for real-time\n",
            "classification of CNNs. Both Altera and Xilinx have adopted OpenCL co-design\n",
            "framework from GPU for FPGA designs as a pseudo-automatic development solution.\n",
            "In this paper, a comprehensive evaluation and comparison of Altera and Xilinx\n",
            "OpenCL frameworks for a 5-layer deep CNN is presented. Hardware resources,\n",
            "temporal performance and the OpenCL architecture for CNNs are discussed. Xilinx\n",
            "demonstrates faster synthesis, better FPGA resource utilization and more\n",
            "compact boards. Altera provides multi-platforms tools, mature design community\n",
            "and better execution times.\n",
            "Actual Title : \n",
            "Comprehensive Evaluation of OpenCL-based Convolutional Neural Network\n",
            "  Accelerators in Xilinx and Altera FPGAs\n",
            "Generated Title : \n",
            "  a deep and framework for deep convolutional\n",
            "Abstract : \n",
            "We introduce GAMSEL (Generalized Additive Model Selection), a penalized\n",
            "likelihood approach for fitting sparse generalized additive models in high\n",
            "dimension. Our method interpolates between null, linear and additive models by\n",
            "allowing the effect of each variable to be estimated as being either zero,\n",
            "linear, or a low-complexity curve, as determined by the data. We present a\n",
            "blockwise coordinate descent procedure for efficiently optimizing the penalized\n",
            "likelihood objective over a dense grid of the tuning parameter, producing a\n",
            "regularization path of additive models. We demonstrate the performance of our\n",
            "method on both real and simulated data examples, and compare it with existing\n",
            "techniques for additive model selection.\n",
            "Actual Title : \n",
            "Generalized Additive Model Selection\n",
            "Generated Title : \n",
            "additive additive model selection\n",
            "Abstract : \n",
            "There have been intensive research interests in ship detection and\n",
            "segmentation due to high demands on a wide range of civil applications in the\n",
            "last two decades. However, existing approaches, which are mainly based on\n",
            "statistical properties of images, fail to detect smaller ships and boats.\n",
            "Specifically, known techniques are not robust enough in view of inevitable\n",
            "small geometric and photometric changes in images consisting of ships. In this\n",
            "paper a novel approach for ship detection is proposed based on correlation of\n",
            "maritime images. The idea comes from the observation that a fine pattern of the\n",
            "sea surface changes considerably from time to time whereas the ship appearance\n",
            "basically keeps unchanged. We want to examine whether the images have a common\n",
            "unaltered part, a ship in this case. To this end, we developed a method -\n",
            "Focused Correlation (FC) to achieve robustness to geometric distortions of the\n",
            "image content. Various experiments have been conducted to evaluate the\n",
            "effectiveness of the proposed approach.\n",
            "Actual Title : \n",
            "Ship Detection and Segmentation using Image Correlation\n",
            "Generated Title : \n",
            "detection and correlation image correlation\n",
            "Abstract : \n",
            "Close-range Photogrammetry is widely used in many industries because of the\n",
            "cost effectiveness and efficiency of the technique. In this research, we\n",
            "introduce an automated coded target detection method which can be used to\n",
            "enhance the efficiency of the Photogrammetry.\n",
            "Actual Title : \n",
            "Automatic Detection and Decoding of Photogrammetric Coded Targets\n",
            "Generated Title : \n",
            "detection of <unk> <unk>\n",
            "Abstract : \n",
            "Developers often wonder how to implement a certain functionality (e.g., how\n",
            "to parse XML files) using APIs. Obtaining an API usage sequence based on an\n",
            "API-related natural language query is very helpful in this regard. Given a\n",
            "query, existing approaches utilize information retrieval models to search for\n",
            "matching API sequences. These approaches treat queries and APIs as bag-of-words\n",
            "(i.e., keyword matching or word-to-word alignment) and lack a deep\n",
            "understanding of the semantics of the query.\n",
            "  We propose DeepAPI, a deep learning based approach to generate API usage\n",
            "sequences for a given natural language query. Instead of a bags-of-words\n",
            "assumption, it learns the sequence of words in a query and the sequence of\n",
            "associated APIs. DeepAPI adapts a neural language model named RNN\n",
            "Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length\n",
            "context vector, and generates an API sequence based on the context vector. We\n",
            "also augment the RNN Encoder-Decoder by considering the importance of\n",
            "individual APIs. We empirically evaluate our approach with more than 7 million\n",
            "annotated code snippets collected from GitHub. The results show that our\n",
            "approach generates largely accurate API sequences and outperforms the related\n",
            "approaches.\n",
            "Actual Title : \n",
            "Deep API Learning\n",
            "Generated Title : \n",
            "a deep learning for natural language queries\n",
            "Abstract : \n",
            "Maximally stable component detection is a very popular method for feature\n",
            "analysis in images, mainly due to its low computation cost and high\n",
            "repeatability. With the recent advance of feature-based methods in geometric\n",
            "shape analysis, there is significant interest in finding analogous approaches\n",
            "in the 3D world. In this paper, we formulate a diffusion-geometric framework\n",
            "for stable component detection in non-rigid 3D shapes, which can be used for\n",
            "geometric feature detection and description. A quantitative evaluation of our\n",
            "method on the SHREC'10 feature detection benchmark shows its potential as a\n",
            "source of high-quality features.\n",
            "Actual Title : \n",
            "Diffusion-geometric maximally stable component detection in deformable\n",
            "  shapes\n",
            "Generated Title : \n",
            "maximally stable deformable deformable deformable deformable shapes\n",
            "Abstract : \n",
            "In recent years genetic algorithms have emerged as a useful tool for the\n",
            "heuristic solution of complex discrete optimisation problems. In particular\n",
            "there has been considerable interest in their use in tackling problems arising\n",
            "in the areas of scheduling and timetabling. However, the classical genetic\n",
            "algorithm paradigm is not well equipped to handle constraints and successful\n",
            "implementations usually require some sort of modification to enable the search\n",
            "to exploit problem specific knowledge in order to overcome this shortcoming.\n",
            "This paper is concerned with the development of a family of genetic algorithms\n",
            "for the solution of a nurse rostering problem at a major UK hospital. The\n",
            "hospital is made up of wards of up to 30 nurses. Each ward has its own group of\n",
            "nurses whose shifts have to be scheduled on a weekly basis. In addition to\n",
            "fulfilling the minimum demand for staff over three daily shifts, nurses' wishes\n",
            "and qualifications have to be taken into account. The schedules must also be\n",
            "seen to be fair, in that unpopular shifts have to be spread evenly amongst all\n",
            "nurses, and other restrictions, such as team nursing and special conditions for\n",
            "senior staff, have to be satisfied. The basis of the family of genetic\n",
            "algorithms is a classical genetic algorithm consisting of n-point crossover,\n",
            "single-bit mutation and a rank-based selection. The solution space consists of\n",
            "all schedules in which each nurse works the required number of shifts, but the\n",
            "remaining constraints, both hard and soft, are relaxed and penalised in the\n",
            "fitness function. The talk will start with a detailed description of the\n",
            "problem and the initial implementation and will go on to highlight the\n",
            "shortcomings of such an approach, in terms of the key element of balancing\n",
            "feasibility, i.e. covering the demand and work regulations, and quality, as\n",
            "measured by the nurses' preferences. A series of experiments involving\n",
            "parameter adaptation, niching, intelligent weights, delta coding, local hill\n",
            "climbing, migration and special selection rules will then be outlined and it\n",
            "will be shown how a series of these enhancements were able to eradicate these\n",
            "difficulties. Results based on several months' real data will be used to\n",
            "measure the impact of each modification, and to show that the final algorithm\n",
            "is able to compete with a tabu search approach currently employed at the\n",
            "hospital. The talk will conclude with some observations as to the overall\n",
            "quality of this approach to this and similar problems.\n",
            "Actual Title : \n",
            "Nurse Rostering with Genetic Algorithms\n",
            "Generated Title : \n",
            "rostering rostering rostering genetic algorithms\n",
            "Abstract : \n",
            "Automatic continuous speech recognition (CSR) is sufficiently mature that a\n",
            "variety of real world applications are now possible including large vocabulary\n",
            "transcription and interactive spoken dialogues. This paper reviews the\n",
            "evolution of the statistical modelling techniques which underlie current-day\n",
            "systems, specifically hidden Markov models (HMMs) and N-grams. Starting from a\n",
            "description of the speech signal and its parameterisation, the various\n",
            "modelling assumptions and their consequences are discussed. It then describes\n",
            "various techniques by which the effects of these assumptions can be mitigated.\n",
            "Despite the progress that has been made, the limitations of current modelling\n",
            "techniques are still evident. The paper therefore concludes with a brief review\n",
            "of some of the more fundamental modelling work now in progress.\n",
            "Actual Title : \n",
            "Statistical Modeling in Continuous Speech Recognition (CSR)(Invited\n",
            "  Talk)\n",
            "Generated Title : \n",
            "modeling for speech modeling\n",
            "Abstract : \n",
            "This paper deals with the revision of partially ordered beliefs. It proposes\n",
            "a semantic representation of epistemic states by partial pre-orders on\n",
            "interpretations and a syntactic representation by partially ordered belief\n",
            "bases. Two revision operations, the revision stemming from the history of\n",
            "observations and the possibilistic revision, defined when the epistemic state\n",
            "is represented by a total pre-order, are generalized, at a semantic level, to\n",
            "the case of a partial pre-order on interpretations, and at a syntactic level,\n",
            "to the case of a partially ordered belief base. The equivalence between the two\n",
            "representations is shown for the two revision operations.\n",
            "Actual Title : \n",
            "Revising Partially Ordered Beliefs\n",
            "Generated Title : \n",
            "partially ordered beliefs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0vqLDIYSLpd",
        "outputId": "007b4531-a32e-45e4-acca-4c055404638a"
      },
      "source": [
        "len(df1['Abstract'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBeWyGU8SLpg"
      },
      "source": [
        "df2 = pd.DataFrame(df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A51Jm9IOSLpg"
      },
      "source": [
        "df2.to_csv('./drive/MyDrive/generated_titles_4.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f6BpwHUUr7d"
      },
      "source": [
        "### Experiment 5 - Simple attention, encoder layer = 1, Hidden_dim = 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3HS1nFRU1bR"
      },
      "source": [
        "INPUT_DIM = len(SUM.vocab)\n",
        "OUTPUT_DIM = len(TITLE.vocab)\n",
        "ENC_EMB_DIM = 100\n",
        "DEC_EMB_DIM = 100\n",
        "HID_DIM = 128\n",
        "N_LAYERS = 1\n",
        "ENC_DROPOUT = 0\n",
        "DEC_DROPOUT = 0\n",
        "SPLIT = 100\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "con = ControlLayer(HID_DIM,HID_DIM)\n",
        "'''trying with complex attention first'''\n",
        "attention = SimpleAttention(HID_DIM,HID_DIM,HID_DIM,SPLIT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,HID_DIM,HID_DIM,attention,'simple')\n",
        "\n",
        "model = Seq2Seq(enc,con, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVfFmtCgU1bT",
        "outputId": "37ac5cce-1443-419c-a3e3-3d2117a87858"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "      #print(name)\n",
        "      nn.init.uniform_(param.data, -0.1, 0.1)  \n",
        "model.apply(init_weights)\n",
        "pretrained_embeddings = SUM.vocab.vectors\n",
        "model.encoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "model.decoder.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ_VIVz6U1bV"
      },
      "source": [
        "def checkpoint_and_save(model, min_loss, epoch, optimizer):\n",
        "    print()\n",
        "    state = {'model': model,'min_loss': min_loss,'epoch': epoch,'model_state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),'train_iterator':train_iterator.state_dict(),'valid_iterator':valid_iterator.state_dict()}\n",
        "    path =  './drive/MyDrive/Colab Notebooks/final_net_5.pt'\n",
        "    torch.save(state, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AknTYGCCU1bX"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYJzZxboU1bY",
        "outputId": "f58e590b-ed03-49e4-a51b-7580c364200c"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch} - Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 6.501\n",
            "\tTest Loss: 6.345\n",
            "Epoch 0 - Time taken : 2.972mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.800\n",
            "\tTest Loss: 6.123\n",
            "Epoch 1 - Time taken : 2.960mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.406\n",
            "\tTest Loss: 5.982\n",
            "Epoch 2 - Time taken : 2.960mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 5.114\n",
            "\tTest Loss: 5.951\n",
            "Epoch 3 - Time taken : 2.966mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.878\n",
            "\tTest Loss: 5.868\n",
            "Epoch 4 - Time taken : 2.954mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.638\n",
            "\tTest Loss: 5.849\n",
            "Epoch 5 - Time taken : 2.962mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.431\n",
            "\tTest Loss: 5.868\n",
            "Epoch 6 - Time taken : 2.969mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.223\n",
            "\tTest Loss: 5.908\n",
            "Epoch 7 - Time taken : 2.970mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 4.053\n",
            "\tTest Loss: 5.912\n",
            "Epoch 8 - Time taken : 2.976mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.871\n",
            "\tTest Loss: 5.967\n",
            "Epoch 9 - Time taken : 2.970mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osefEj6yjp7-"
      },
      "source": [
        "#### Resume Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BFF7xtg9TVL"
      },
      "source": [
        "path =  './drive/MyDrive/Colab Notebooks/final_net_5.pt'\n",
        "checkpoint = torch.load(path)\n",
        "#print(checkpoint)\n",
        "model = checkpoint['model']\n",
        "model.load_state_dict( checkpoint['model_state_dict'])\n",
        "min_loss = checkpoint['min_loss']\n",
        "epoch = checkpoint['epoch']\n",
        "optimizer1 = checkpoint['optimizer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDuCBu_B9TVO"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "optimizer.load_state_dict(optimizer1)\n",
        "PAD_IDX = TITLE.vocab.stoi[TITLE.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-qw3bEUjnQ8",
        "outputId": "caf60c32-6545-4ee6-ca7e-c72fe62b7cd1"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(10,N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch} - Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 3.716\n",
            "\tTest Loss: 6.044\n",
            "Epoch 10 - Time taken : 2.947mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.554\n",
            "\tTest Loss: 6.077\n",
            "Epoch 11 - Time taken : 2.968mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.398\n",
            "\tTest Loss: 6.143\n",
            "Epoch 12 - Time taken : 2.952mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.264\n",
            "\tTest Loss: 6.199\n",
            "Epoch 13 - Time taken : 2.965mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.134\n",
            "\tTest Loss: 6.276\n",
            "Epoch 14 - Time taken : 2.969mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 3.002\n",
            "\tTest Loss: 6.372\n",
            "Epoch 15 - Time taken : 2.972mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.883\n",
            "\tTest Loss: 6.448\n",
            "Epoch 16 - Time taken : 2.960mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.772\n",
            "\tTest Loss: 6.518\n",
            "Epoch 17 - Time taken : 2.962mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.657\n",
            "\tTest Loss: 6.604\n",
            "Epoch 18 - Time taken : 2.971mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.541\n",
            "\tTest Loss: 6.674\n",
            "Epoch 19 - Time taken : 2.972mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnv3O_D_FbX3"
      },
      "source": [
        "#### Run the model for 10 more epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voK692bm95W2",
        "outputId": "23d4e2f5-4745-4919-a420-ef63e265bf38"
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "min_loss = 1000000\n",
        "min_epoch = -1\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "for epoch in range(10,N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = test(model,valid_iterator,criterion)\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
        "    #print(\"After epoch {} , generated title is {}\".format(epoch,translate(model,demo_sentence,10)))\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch} - Time taken : {(end_time-start_time)/60:.3f}mins')\n",
        "    if(train_loss < min_loss):\n",
        "      min_loss=train_loss\n",
        "      min_epoch = epoch\n",
        "      print(\"Saving the new checkpoint....\")\n",
        "      checkpoint_and_save(model,min_loss,epoch,optimizer)\n",
        "    if(epoch-min_epoch >= 10):\n",
        "      print(\"NO further improvement over 10 epochs. Terminating...\")\n",
        "      break\n",
        "    \n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 2.460\n",
            "\tTest Loss: 6.773\n",
            "Epoch 10 - Time taken : 2.893mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.366\n",
            "\tTest Loss: 6.848\n",
            "Epoch 11 - Time taken : 2.911mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.272\n",
            "\tTest Loss: 6.957\n",
            "Epoch 12 - Time taken : 2.899mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.178\n",
            "\tTest Loss: 7.015\n",
            "Epoch 13 - Time taken : 2.901mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.087\n",
            "\tTest Loss: 7.127\n",
            "Epoch 14 - Time taken : 2.909mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 2.035\n",
            "\tTest Loss: 7.195\n",
            "Epoch 15 - Time taken : 2.900mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.947\n",
            "\tTest Loss: 7.300\n",
            "Epoch 16 - Time taken : 2.891mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.861\n",
            "\tTest Loss: 7.396\n",
            "Epoch 17 - Time taken : 2.903mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.804\n",
            "\tTest Loss: 7.463\n",
            "Epoch 18 - Time taken : 2.898mins\n",
            "Saving the new checkpoint....\n",
            "\n",
            "\tTrain Loss: 1.735\n",
            "\tTest Loss: 7.582\n",
            "Epoch 19 - Time taken : 2.895mins\n",
            "Saving the new checkpoint....\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXMtwYUX_bsN"
      },
      "source": [
        "#### Call Translate(final_model_5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U84Wg7VrEsuC"
      },
      "source": [
        "df1={'Abstract':[],'Title':[],'Generated Title':[]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mytSHmisEsuE",
        "outputId": "bb141961-86b4-4aac-d2aa-b433c9a9cf02"
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Abstract': [], 'Generated Title': [], 'Title': []}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqf5ses_EsuJ"
      },
      "source": [
        "ID = data.Field(use_vocab=False,sequential=False,preprocessing=int)\n",
        "fields = [('Id',ID),('Abstract',None),('Title',TITLE),('sum1',SUM),('sum2',SUM),('sum3',SUM),('sum4',SUM),('sum5',SUM),('sum6',SUM),('sum7',SUM)]\n",
        "ran_dataset = data.TabularDataset(path='./drive/MyDrive/test_data.csv',format='csv', fields=fields,skip_header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qCLM0i7EsuL"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "BATCH_SIZE =1\n",
        "iterator=data.Iterator(\n",
        "    ran_dataset,\n",
        "    batch_size = BATCH_SIZE,shuffle=False,device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC701uP3EsuM"
      },
      "source": [
        "path =  './drive/MyDrive/Colab Notebooks/final_net_5.pt'\n",
        "checkpoint = torch.load(path)\n",
        "#print(checkpoint)\n",
        "model1 = checkpoint['model']\n",
        "model1.load_state_dict( checkpoint['model_state_dict'])\n",
        "min_loss = checkpoint['min_loss']\n",
        "epoch = checkpoint['epoch']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPFG2P5HEsuN",
        "outputId": "36b32c10-ae7a-4e8f-92be-5bdd4acf08b0"
      },
      "source": [
        "for i,batch in enumerate(iterator):\n",
        "  id1 = batch.Id.item()\n",
        "  #print(batch)\n",
        "  print(\"Abstract : \")\n",
        "  print(\"\".join(list(df.loc[df['Id'] == id1]['Abstract'])))\n",
        "  print(\"Actual Title : \")\n",
        "  print(\"\".join(list(df.loc[df['Id'] == id1]['Title'])))\n",
        "  print(\"Generated Title : \")\n",
        "  print(\" \".join(translate(model1,batch,10)))\n",
        "  abs= \"\".join(list(df.loc[df['Id'] == id1]['Abstract']))\n",
        "  title= \"\".join(list(df.loc[df['Id'] == id1]['Title']))\n",
        "  df1['Abstract'].append(abs)\n",
        "  df1['Title'].append(title)\n",
        "  df1['Generated Title'].append(\" \".join(translate(model1,batch,10)))\n",
        "  #df1['Generated_Title'][id1]=translate(model1,batch,15))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "information, we developed a classification scheme based on neuro-fuzzy modeling\n",
            "of the AU intensity, which is robust to intensity variations, 2) using both\n",
            "geometric and appearance-based features, and applying efficient dimension\n",
            "reduction techniques, our system is robust to illumination changes and it can\n",
            "represent the subtle changes as well as temporal information involved in\n",
            "formation of the facial expressions, and 3) by continuous values of intensity\n",
            "and employing top-down hierarchical rule-based classifiers, we can develop\n",
            "accurate human-interpretable AU-to-expression converters. Extensive experiments\n",
            "on Cohn-Kanade database show the superiority of the proposed method, in\n",
            "comparison with support vector machines, hidden Markov models, and neural\n",
            "network classifiers. Keywords: biased discriminant analysis (BDA), classifier\n",
            "design and evaluation, facial action units (AUs), hybrid learning, neuro-fuzzy\n",
            "modeling.\n",
            "Actual Title : \n",
            "Analysis, Interpretation, and Recognition of Facial Action Units and\n",
            "  Expressions Using Neuro-Fuzzy Modeling\n",
            "Generated Title : \n",
            "interpretation  and recognition and recognition of facial expressions\n",
            "Abstract : \n",
            "Recent years have witnessed amazing progress in AI related fields such as\n",
            "computer vision, machine learning and autonomous vehicles. As with any rapidly\n",
            "growing field, however, it becomes increasingly difficult to stay up-to-date or\n",
            "enter the field as a beginner. While several topic specific survey papers have\n",
            "been written, to date no general survey on problems, datasets and methods in\n",
            "computer vision for autonomous vehicles exists. This paper attempts to narrow\n",
            "this gap by providing a state-of-the-art survey on this topic. Our survey\n",
            "includes both the historically most relevant literature as well as the current\n",
            "state-of-the-art on several specific topics, including recognition,\n",
            "reconstruction, motion estimation, tracking, scene understanding and end-to-end\n",
            "learning. Towards this goal, we first provide a taxonomy to classify each\n",
            "approach and then analyze the performance of the state-of-the-art on several\n",
            "challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes.\n",
            "Besides, we discuss open problems and current research challenges. To ease\n",
            "accessibility and accommodate missing references, we will also provide an\n",
            "interactive platform which allows to navigate topics and methods, and provides\n",
            "additional information and project links for each paper.\n",
            "Actual Title : \n",
            "Computer Vision for Autonomous Vehicles: Problems, Datasets and\n",
            "  State-of-the-Art\n",
            "Generated Title : \n",
            "vision research in virtual technology\n",
            "Abstract : \n",
            "We present a new similarity measure based on information theoretic measures\n",
            "which is superior than Normalized Compression Distance for clustering problems\n",
            "and inherits the useful properties of conditional Kolmogorov complexity. We\n",
            "show that Normalized Compression Dictionary Size and Normalized Compression\n",
            "Dictionary Entropy are computationally more efficient, as the need to perform\n",
            "the compression itself is eliminated. Also they scale linearly with exponential\n",
            "vector size growth and are content independent. We show that normalized\n",
            "compression dictionary distance is compressor independent, if limited to\n",
            "lossless compressors, which gives space for optimizations and implementation\n",
            "speed improvement for real-time and big data applications. The introduced\n",
            "measure is applicable for machine learning tasks of parameter-free unsupervised\n",
            "clustering, supervised learning such as classification and regression, feature\n",
            "selection, and is applicable for big data problems with order of magnitude\n",
            "speed increase.\n",
            "Actual Title : \n",
            "Generalized Compression Dictionary Distance as Universal Similarity\n",
            "  Measure\n",
            "Generated Title : \n",
            "hamming distance compression\n",
            "Abstract : \n",
            "With the range and sensitivity of algorithmic decisions expanding at a\n",
            "break-neck speed, it is imperative that we aggressively investigate whether\n",
            "programs are biased. We propose a novel probabilistic program analysis\n",
            "technique and apply it to quantifying bias in decision-making programs.\n",
            "Specifically, we (i) present a sound and complete automated verification\n",
            "technique for proving quantitative properties of probabilistic programs; (ii)\n",
            "show that certain notions of bias, recently proposed in the fairness\n",
            "literature, can be phrased as quantitative correctness properties; and (iii)\n",
            "present FairSquare, the first verification tool for quantifying program bias,\n",
            "and evaluate it on a range of decision-making programs.\n",
            "Actual Title : \n",
            "Quantifying Program Bias\n",
            "Generated Title : \n",
            "program bias in peer grading\n",
            "Abstract : \n",
            "2D face analysis techniques, such as face landmarking, face recognition and\n",
            "face verification, are reasonably dependent on illumination conditions which\n",
            "are usually uncontrolled and unpredictable in the real world. An illumination\n",
            "robust preprocessing method thus remains a significant challenge in reliable\n",
            "face analysis. In this paper we propose a novel approach for improving lighting\n",
            "normalization through building the underlying reflectance model which\n",
            "characterizes interactions between skin surface, lighting source and camera\n",
            "sensor, and elaborates the formation of face color appearance. Specifically,\n",
            "the proposed illumination processing pipeline enables the generation of\n",
            "Chromaticity Intrinsic Image (CII) in a log chromaticity space which is robust\n",
            "to illumination variations. Moreover, as an advantage over most prevailing\n",
            "methods, a photo-realistic color face image is subsequently reconstructed which\n",
            "eliminates a wide variety of shadows whilst retaining the color information and\n",
            "identity details. Experimental results under different scenarios and using\n",
            "various face databases show the effectiveness of the proposed approach to deal\n",
            "with lighting variations, including both soft and hard shadows, in face\n",
            "recognition.\n",
            "Actual Title : \n",
            "Improving Shadow Suppression for Illumination Robust Face Recognition\n",
            "Generated Title : \n",
            "shadow suppression for face face recognition\n",
            "Abstract : \n",
            "We present Deeply Supervised Object Detector (DSOD), a framework that can\n",
            "learn object detectors from scratch. State-of-the-art object objectors rely\n",
            "heavily on the off-the-shelf networks pre-trained on large-scale classification\n",
            "datasets like ImageNet, which incurs learning bias due to the difference on\n",
            "both the loss functions and the category distributions between classification\n",
            "and detection tasks. Model fine-tuning for the detection task could alleviate\n",
            "this bias to some extent but not fundamentally. Besides, transferring\n",
            "pre-trained models from classification to detection between discrepant domains\n",
            "is even more difficult (e.g. RGB to depth images). A better solution to tackle\n",
            "these two critical problems is to train object detectors from scratch, which\n",
            "motivates our proposed DSOD. Previous efforts in this direction mostly failed\n",
            "due to much more complicated loss functions and limited training data in object\n",
            "detection. In DSOD, we contribute a set of design principles for training\n",
            "object detectors from scratch. One of the key findings is that deep\n",
            "supervision, enabled by dense layer-wise connections, plays a critical role in\n",
            "learning a good detector. Combining with several other principles, we develop\n",
            "DSOD following the single-shot detection (SSD) framework. Experiments on PASCAL\n",
            "VOC 2007, 2012 and MS COCO datasets demonstrate that DSOD can achieve better\n",
            "results than the state-of-the-art solutions with much more compact models. For\n",
            "instance, DSOD outperforms SSD on all three benchmarks with real-time detection\n",
            "speed, while requires only 1/2 parameters to SSD and 1/10 parameters to Faster\n",
            "RCNN. Our code and models are available at: https://github.com/szq0214/DSOD .\n",
            "Actual Title : \n",
            "DSOD: Learning Deeply Supervised Object Detectors from Scratch\n",
            "Generated Title : \n",
            "deeply supervised object detectors from scratch\n",
            "Abstract : \n",
            "We present an approach for real-time, robust and accurate hand pose\n",
            "estimation from moving egocentric RGB-D cameras in cluttered real environments.\n",
            "Existing methods typically fail for hand-object interactions in cluttered\n",
            "scenes imaged from egocentric viewpoints, common for virtual or augmented\n",
            "reality applications. Our approach uses two subsequently applied Convolutional\n",
            "Neural Networks (CNNs) to localize the hand and regress 3D joint locations.\n",
            "Hand localization is achieved by using a CNN to estimate the 2D position of the\n",
            "hand center in the input, even in the presence of clutter and occlusions. The\n",
            "localized hand position, together with the corresponding input depth value, is\n",
            "used to generate a normalized cropped image that is fed into a second CNN to\n",
            "regress relative 3D hand joint locations in real time. For added accuracy,\n",
            "robustness and temporal stability, we refine the pose estimates using a\n",
            "kinematic pose tracking energy. To train the CNNs, we introduce a new\n",
            "photorealistic dataset that uses a merged reality approach to capture and\n",
            "synthesize large amounts of annotated data of natural hand interaction in\n",
            "cluttered scenes. Through quantitative and qualitative evaluation, we show that\n",
            "our method is robust to self-occlusion and occlusions by objects, particularly\n",
            "in moving egocentric perspectives.\n",
            "Actual Title : \n",
            "Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor\n",
            "Generated Title : \n",
            "hand hand tracking using egocentric rgb d\n",
            "Abstract : \n",
            "Accurate segmentation of the heart is an important step towards evaluating\n",
            "cardiac function. In this paper, we present a fully automated framework for\n",
            "segmentation of the left (LV) and right (RV) ventricular cavities and the\n",
            "myocardium (Myo) on short-axis cardiac MR images. We investigate various 2D and\n",
            "3D convolutional neural network architectures for this task. We investigate the\n",
            "suitability of various state-of-the art 2D and 3D convolutional neural network\n",
            "architectures, as well as slight modifications thereof, for this task.\n",
            "Experiments were performed on the ACDC 2017 challenge training dataset\n",
            "comprising cardiac MR images of 100 patients, where manual reference\n",
            "segmentations were made available for end-diastolic (ED) and end-systolic (ES)\n",
            "frames. We find that processing the images in a slice-by-slice fashion using 2D\n",
            "networks is beneficial due to a relatively large slice thickness. However, the\n",
            "exact network architecture only plays a minor role. We report mean Dice\n",
            "coefficients of $0.950$ (LV), $0.893$ (RV), and $0.899$ (Myo), respectively\n",
            "with an average evaluation time of 1.1 seconds per volume on a modern GPU.\n",
            "Actual Title : \n",
            "An Exploration of 2D and 3D Deep Learning Techniques for Cardiac MR\n",
            "  Image Segmentation\n",
            "Generated Title : \n",
            "architectures for #d #d of #d cardiac mr\n",
            "Abstract : \n",
            "Tensor decompositions are invaluable tools in analyzing multimodal datasets.\n",
            "In many real-world scenarios, such datasets are far from being static, to the\n",
            "contrary they tend to grow over time. For instance, in an online social network\n",
            "setting, as we observe new interactions over time, our dataset gets updated in\n",
            "its \"time\" mode. How can we maintain a valid and accurate tensor decomposition\n",
            "of such a dynamically evolving multimodal dataset, without having to re-compute\n",
            "the entire decomposition after every single update? In this paper we introduce\n",
            "SaMbaTen, a Sampling-based Batch Incremental Tensor Decomposition algorithm,\n",
            "which incrementally maintains the decomposition given new updates to the tensor\n",
            "dataset. SaMbaTen is able to scale to datasets that the state-of-the-art in\n",
            "incremental tensor decomposition is unable to operate on, due to its ability to\n",
            "effectively summarize the existing tensor and the incoming updates, and perform\n",
            "all computations in the reduced summary space. We extensively evaluate SaMbaTen\n",
            "using synthetic and real datasets. Indicatively, SaMbaTen achieves comparable\n",
            "accuracy to state-of-the-art incremental and non-incremental techniques, while\n",
            "being 25-30 times faster. Furthermore, SaMbaTen scales to very large sparse and\n",
            "dense dynamically evolving tensors of dimensions up to 100K x 100K x 100K where\n",
            "state-of-the-art incremental approaches were not able to operate.\n",
            "Actual Title : \n",
            "SamBaTen: Sampling-based Batch Incremental Tensor Decomposition\n",
            "Generated Title : \n",
            "sampling based batch incremental tensor decomposition\n",
            "Abstract : \n",
            "Normally a decision support system is build to solve problem where\n",
            "multi-criteria decisions are involved. The knowledge base is the vital part of\n",
            "the decision support containing the information or data that is used in\n",
            "decision-making process. This is the field where engineers and scientists have\n",
            "applied several intelligent techniques and heuristics to obtain optimal\n",
            "decisions from imprecise information. In this paper, we present a hybrid\n",
            "neuro-genetic learning approach for the adaptation a Mamdani fuzzy inference\n",
            "system for the Tactical Air Combat Decision Support System (TACDSS). Some\n",
            "simulation results demonstrating the difference of the learning techniques and\n",
            "are also provided.\n",
            "Actual Title : \n",
            "Adaptation of Mamdani Fuzzy Inference System Using Neuro - Genetic\n",
            "  Approach for Tactical Air Combat Decision Support System\n",
            "Generated Title : \n",
            "hybrid <unk> modeling for for fuzzy inference systems\n",
            "Abstract : \n",
            "Clothing retrieval is a challenging problem in computer vision. With the\n",
            "advance of Convolutional Neural Networks (CNNs), the accuracy of clothing\n",
            "retrieval has been significantly improved. FashionNet[1], a recent study,\n",
            "proposes to employ a set of artificial features in the form of landmarks for\n",
            "clothing retrieval, which are shown to be helpful for retrieval. However, the\n",
            "landmark detection module is trained with strong supervision which requires\n",
            "considerable efforts to obtain. In this paper, we propose a self-learning\n",
            "Visual Attention Model (VAM) to extract attention maps from clothing images.\n",
            "The VAM is further connected to a global network to form an end-to-end network\n",
            "structure through Impdrop connection which randomly Dropout on the feature maps\n",
            "with the probabilities given by the attention map. Extensive experiments on\n",
            "several widely used benchmark clothing retrieval data sets have demonstrated\n",
            "the promise of the proposed method. We also show that compared to the trivial\n",
            "Product connection, the Impdrop connection makes the network structure more\n",
            "robust when training sets of limited size are used.\n",
            "Actual Title : \n",
            "Clothing Retrieval with Visual Attention Model\n",
            "Generated Title : \n",
            "retrieval in visual attention\n",
            "Abstract : \n",
            "We propose a novel tree classification system called Treelogy, that fuses\n",
            "deep representations with hand-crafted features obtained from leaf images to\n",
            "perform leaf-based plant classification. Key to this system are segmentation of\n",
            "the leaf from an untextured background, using convolutional neural networks\n",
            "(CNNs) for learning deep representations, extracting hand-crafted features with\n",
            "a number of image processing techniques, training a linear SVM with feature\n",
            "vectors, merging SVM and CNN results, and identifying the species from a\n",
            "dataset of 57 trees. Our classification results show that fusion of deep\n",
            "representations with hand-crafted features leads to the highest accuracy. The\n",
            "proposed algorithm is embedded in a smart-phone application, which is publicly\n",
            "available. Furthermore, our novel dataset comprised of 5408 leaf images is also\n",
            "made public for use of other researchers.\n",
            "Actual Title : \n",
            "Treelogy: A Novel Tree Classifier Utilizing Deep and Hand-crafted\n",
            "  Representations\n",
            "Generated Title : \n",
            "learning based deep deep deep deep representations for\n",
            "Abstract : \n",
            "We present a novel detection method using a deep convolutional neural network\n",
            "(CNN), named AttentionNet. We cast an object detection problem as an iterative\n",
            "classification problem, which is the most suitable form of a CNN. AttentionNet\n",
            "provides quantized weak directions pointing a target object and the ensemble of\n",
            "iterative predictions from AttentionNet converges to an accurate object\n",
            "boundary box. Since AttentionNet is a unified network for object detection, it\n",
            "detects objects without any separated models from the object proposal to the\n",
            "post bounding-box regression. We evaluate AttentionNet by a human detection\n",
            "task and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC\n",
            "2007/2012 with an 8-layered architecture only.\n",
            "Actual Title : \n",
            "AttentionNet: Aggregating Weak Directions for Accurate Object Detection\n",
            "Generated Title : \n",
            "aggregating object object detection with object detection\n",
            "Abstract : \n",
            "Recent advances have enabled \"oracle\" classifiers that can classify across\n",
            "many classes and input distributions with high accuracy without retraining.\n",
            "However, these classifiers are relatively heavyweight, so that applying them to\n",
            "classify video is costly. We show that day-to-day video exhibits highly skewed\n",
            "class distributions over the short term, and that these distributions can be\n",
            "classified by much simpler models. We formulate the problem of detecting the\n",
            "short-term skews online and exploiting models based on it as a new sequential\n",
            "decision making problem dubbed the Online Bandit Problem, and present a new\n",
            "algorithm to solve it. When applied to recognizing faces in TV shows and\n",
            "movies, we realize end-to-end classification speedups of 2.4-7.8x/2.6-11.2x (on\n",
            "GPU/CPU) relative to a state-of-the-art convolutional neural network, at\n",
            "competitive accuracy.\n",
            "Actual Title : \n",
            "Fast Video Classification via Adaptive Cascading of Deep Models\n",
            "Generated Title : \n",
            "video classification via <unk> adaptive cascading\n",
            "Abstract : \n",
            "A central challenge in sensory neuroscience is describing how the activity of\n",
            "populations of neurons can represent useful features of the external\n",
            "environment. However, while neurophysiologists have long been able to record\n",
            "the responses of neurons in awake, behaving animals, it is another matter\n",
            "entirely to say what a given neuron does. A key problem is that in many sensory\n",
            "domains, the space of all possible stimuli that one might encounter is\n",
            "effectively infinite; in vision, for instance, natural scenes are\n",
            "combinatorially complex, and an organism will only encounter a tiny fraction of\n",
            "possible stimuli. As a result, even describing the response properties of\n",
            "sensory neurons is difficult, and investigations of neuronal functions are\n",
            "almost always critically limited by the number of stimuli that can be\n",
            "considered. In this paper, we propose a closed-loop, optimization-based\n",
            "experimental framework for characterizing the response properties of sensory\n",
            "neurons, building on past efforts in closed-loop experimental methods, and\n",
            "leveraging recent advances in artificial neural networks to serve as as a\n",
            "proving ground for our techniques. Specifically, using deep convolutional\n",
            "neural networks, we asked whether modern black-box optimization techniques can\n",
            "be used to interrogate the \"tuning landscape\" of an artificial neuron in a\n",
            "deep, nonlinear system, without imposing significant constraints on the space\n",
            "of stimuli under consideration. We introduce a series of measures to quantify\n",
            "the tuning landscapes, and show how these relate to the performances of the\n",
            "networks in an object recognition task. To the extent that deep convolutional\n",
            "neural networks increasingly serve as de facto working hypotheses for\n",
            "biological vision, we argue that developing a unified approach for studying\n",
            "both artificial and biological systems holds great potential to advance both\n",
            "fields together.\n",
            "Actual Title : \n",
            "Measuring and Understanding Sensory Representations within Deep Networks\n",
            "  Using a Numerical Optimization Framework\n",
            "Generated Title : \n",
            "sensory representations in the deep of deep learning\n",
            "Abstract : \n",
            "Optimal transportation distances are a fundamental family of parameterized\n",
            "distances for histograms. Despite their appealing theoretical properties,\n",
            "excellent performance in retrieval tasks and intuitive formulation, their\n",
            "computation involves the resolution of a linear program whose cost is\n",
            "prohibitive whenever the histograms' dimension exceeds a few hundreds. We\n",
            "propose in this work a new family of optimal transportation distances that look\n",
            "at transportation problems from a maximum-entropy perspective. We smooth the\n",
            "classical optimal transportation problem with an entropic regularization term,\n",
            "and show that the resulting optimum is also a distance which can be computed\n",
            "through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several\n",
            "orders of magnitude faster than that of transportation solvers. We also report\n",
            "improved performance over classical optimal transportation distances on the\n",
            "MNIST benchmark problem.\n",
            "Actual Title : \n",
            "Sinkhorn Distances: Lightspeed Computation of Optimal Transportation\n",
            "  Distances\n",
            "Generated Title : \n",
            "distances  <unk> optimal transportation distances\n",
            "Abstract : \n",
            "Many real-world reinforcement learning problems have a hierarchical nature,\n",
            "and often exhibit some degree of partial observability. While hierarchy and\n",
            "partial observability are usually tackled separately (for instance by combining\n",
            "recurrent neural networks and options), we show that addressing both problems\n",
            "simultaneously is simpler and more efficient in many cases. More specifically,\n",
            "we make the initiation set of options conditional on the previously-executed\n",
            "option, and show that options with such Option-Observation Initiation Sets\n",
            "(OOIs) are at least as expressive as Finite State Controllers (FSCs), a\n",
            "state-of-the-art approach for learning in POMDPs. OOIs are easy to design based\n",
            "on an intuitive description of the task, lead to explainable policies and keep\n",
            "the top-level and option policies memoryless. Our experiments show that OOIs\n",
            "allow agents to learn optimal policies in challenging POMDPs, while being much\n",
            "more sample-efficient than a recurrent neural network over options.\n",
            "Actual Title : \n",
            "Reinforcement Learning in POMDPs with Memoryless Options and\n",
            "  Option-Observation Initiation Sets\n",
            "Generated Title : \n",
            "learning with <unk> initiation sets\n",
            "Abstract : \n",
            "Stein kernel has recently shown promising performance on classifying images\n",
            "represented by symmetric positive definite (SPD) matrices. It evaluates the\n",
            "similarity between two SPD matrices through their eigenvalues. In this paper,\n",
            "we argue that directly using the original eigenvalues may be problematic\n",
            "because: i) Eigenvalue estimation becomes biased when the number of samples is\n",
            "inadequate, which may lead to unreliable kernel evaluation; ii) More\n",
            "importantly, eigenvalues only reflect the property of an individual SPD matrix.\n",
            "They are not necessarily optimal for computing Stein kernel when the goal is to\n",
            "discriminate different sets of SPD matrices. To address the two issues in one\n",
            "shot, we propose a discriminative Stein kernel, in which an extra parameter\n",
            "vector is defined to adjust the eigenvalues of the input SPD matrices. The\n",
            "optimal parameter values are sought by optimizing a proxy of classification\n",
            "performance. To show the generality of the proposed method, three different\n",
            "kernel learning criteria that are commonly used in the literature are employed\n",
            "respectively as a proxy. A comprehensive experimental study is conducted on a\n",
            "variety of image classification tasks to compare our proposed discriminative\n",
            "Stein kernel with the original Stein kernel and other commonly used methods for\n",
            "evaluating the similarity between SPD matrices. The experimental results\n",
            "demonstrate that, the discriminative Stein kernel can attain greater\n",
            "discrimination and better align with classification tasks by altering the\n",
            "eigenvalues. This makes it produce higher classification performance than the\n",
            "original Stein kernel and other commonly used methods.\n",
            "Actual Title : \n",
            "Learning Discriminative Stein Kernel for SPD Matrices and Its\n",
            "  Applications\n",
            "Generated Title : \n",
            "discriminative stein kernel learning spd matrices\n",
            "Abstract : \n",
            "In just three years, Variational Autoencoders (VAEs) have emerged as one of\n",
            "the most popular approaches to unsupervised learning of complicated\n",
            "distributions. VAEs are appealing because they are built on top of standard\n",
            "function approximators (neural networks), and can be trained with stochastic\n",
            "gradient descent. VAEs have already shown promise in generating many kinds of\n",
            "complicated data, including handwritten digits, faces, house numbers, CIFAR\n",
            "images, physical models of scenes, segmentation, and predicting the future from\n",
            "static images. This tutorial introduces the intuitions behind VAEs, explains\n",
            "the mathematics behind them, and describes some empirical behavior. No prior\n",
            "knowledge of variational Bayesian methods is assumed.\n",
            "Actual Title : \n",
            "Tutorial on Variational Autoencoders\n",
            "Generated Title : \n",
            "autoencoders\n",
            "Abstract : \n",
            "While there is currently a lot of enthusiasm about \"big data\", useful data is\n",
            "usually \"small\" and expensive to acquire. In this paper, we present a new\n",
            "paradigm of learning partial differential equations from {\\em small} data. In\n",
            "particular, we introduce \\emph{hidden physics models}, which are essentially\n",
            "data-efficient learning machines capable of leveraging the underlying laws of\n",
            "physics, expressed by time dependent and nonlinear partial differential\n",
            "equations, to extract patterns from high-dimensional data generated from\n",
            "experiments. The proposed methodology may be applied to the problem of\n",
            "learning, system identification, or data-driven discovery of partial\n",
            "differential equations. Our framework relies on Gaussian processes, a powerful\n",
            "tool for probabilistic inference over functions, that enables us to strike a\n",
            "balance between model complexity and data fitting. The effectiveness of the\n",
            "proposed approach is demonstrated through a variety of canonical problems,\n",
            "spanning a number of scientific domains, including the Navier-Stokes,\n",
            "Schr\\\"odinger, Kuramoto-Sivashinsky, and time dependent linear fractional\n",
            "equations. The methodology provides a promising new direction for harnessing\n",
            "the long-standing developments of classical methods in applied mathematics and\n",
            "mathematical physics to design learning machines with the ability to operate in\n",
            "complex domains without requiring large quantities of data.\n",
            "Actual Title : \n",
            "Hidden Physics Models: Machine Learning of Nonlinear Partial\n",
            "  Differential Equations\n",
            "Generated Title : \n",
            "learning learning for nonlinear partial differential equations\n",
            "Abstract : \n",
            "Effective debugging of ontologies is an important prerequisite for their\n",
            "broad application, especially in areas that rely on everyday users to create\n",
            "and maintain knowledge bases, such as the Semantic Web. In such systems\n",
            "ontologies capture formalized vocabularies of terms shared by its users.\n",
            "However in many cases users have different local views of the domain, i.e. of\n",
            "the context in which a given term is used. Inappropriate usage of terms\n",
            "together with natural complications when formulating and understanding logical\n",
            "descriptions may result in faulty ontologies. Recent ontology debugging\n",
            "approaches use diagnosis methods to identify causes of the faults. In most\n",
            "debugging scenarios these methods return many alternative diagnoses, thus\n",
            "placing the burden of fault localization on the user. This paper demonstrates\n",
            "how the target diagnosis can be identified by performing a sequence of\n",
            "observations, that is, by querying an oracle about entailments of the target\n",
            "ontology. To identify the best query we propose two query selection strategies:\n",
            "a simple \"split-in-half\" strategy and an entropy-based strategy. The latter\n",
            "allows knowledge about typical user errors to be exploited to minimize the\n",
            "number of queries. Our evaluation showed that the entropy-based method\n",
            "significantly reduces the number of required queries compared to the\n",
            "\"split-in-half\" approach. We experimented with different probability\n",
            "distributions of user errors and different qualities of the a-priori\n",
            "probabilities. Our measurements demonstrated the superiority of entropy-based\n",
            "query selection even in cases where all fault probabilities are equal, i.e.\n",
            "where no information about typical user errors is available.\n",
            "Actual Title : \n",
            "Interactive ontology debugging: two query strategies for efficient fault\n",
            "  localization\n",
            "Generated Title : \n",
            "search and for for debugging of query\n",
            "Abstract : \n",
            "Special technologies need to be used to take advantage of, and overcome, the\n",
            "challenges associated with acquiring, transforming, storing, processing, and\n",
            "distributing spoken language resources in organisations. This paper introduces\n",
            "an application architecture consisting of tools and supporting utilities for\n",
            "indexing and transcription, and describes how these tools, together with\n",
            "downstream processing and distribution systems, can be integrated into a\n",
            "workflow. Two sample applications for this architecture are outlined- the\n",
            "analysis of decision-making processes in organisations and the deployment of\n",
            "systems development methods by designers in the field.\n",
            "Actual Title : \n",
            "Application Architecture for Spoken Language Resources in Organisational\n",
            "  Settings\n",
            "Generated Title : \n",
            "based spoken language processing in organisational settings\n",
            "Abstract : \n",
            "This is full length article (draft version) where problem number of topics in\n",
            "Topic Modeling is discussed. We proposed idea that Renyi and Tsallis entropy\n",
            "can be used for identification of optimal number in large textual collections.\n",
            "We also report results of numerical experiments of Semantic stability for 4\n",
            "topic models, which shows that semantic stability play very important role in\n",
            "problem topic number. The calculation of Renyi and Tsallis entropy based on\n",
            "thermodynamics approach.\n",
            "Actual Title : \n",
            "Application of Rnyi and Tsallis Entropies to Topic Modeling\n",
            "  Optimization\n",
            "Generated Title : \n",
            "of of tsallis entropies to topic modeling\n",
            "Abstract : \n",
            "Many efforts have been made to use various forms of domain knowledge in\n",
            "malware detection. Currently there exist two common approaches to malware\n",
            "detection without domain knowledge, namely byte n-grams and strings. In this\n",
            "work we explore the feasibility of applying neural networks to malware\n",
            "detection and feature learning. We do this by restricting ourselves to a\n",
            "minimal amount of domain knowledge in order to extract a portion of the\n",
            "Portable Executable (PE) header. By doing this we show that neural networks can\n",
            "learn from raw bytes without explicit feature construction, and perform even\n",
            "better than a domain knowledge approach that parses the PE header into explicit\n",
            "features.\n",
            "Actual Title : \n",
            "Learning the PE Header, Malware Detection with Minimal Domain Knowledge\n",
            "Generated Title : \n",
            "detection using knowledge base for malware detection\n",
            "Abstract : \n",
            "We present examples where the use of belief functions provided sound and\n",
            "elegant solutions to real life problems. These are essentially characterized by\n",
            "?missing' information. The examples deal with 1) discriminant analysis using a\n",
            "learning set where classes are only partially known; 2) an information\n",
            "retrieval systems handling inter-documents relationships; 3) the combination of\n",
            "data from sensors competent on partially overlapping frames; 4) the\n",
            "determination of the number of sources in a multi-sensor environment by\n",
            "studying the inter-sensors contradiction. The purpose of the paper is to report\n",
            "on such applications where the use of belief functions provides a convenient\n",
            "tool to handle ?messy' data problems.\n",
            "Actual Title : \n",
            "Practical Uses of Belief Functions\n",
            "Generated Title : \n",
            "optimization of of functions\n",
            "Abstract : \n",
            "We present an efficient method for training slack-rescaled structural SVM.\n",
            "Although finding the most violating label in a margin-rescaled formulation is\n",
            "often easy since the target function decomposes with respect to the structure,\n",
            "this is not the case for a slack-rescaled formulation, and finding the most\n",
            "violated label might be very difficult. Our core contribution is an efficient\n",
            "method for finding the most-violating-label in a slack-rescaled formulation,\n",
            "given an oracle that returns the most-violating-label in a (slightly modified)\n",
            "margin-rescaled formulation. We show that our method enables accurate and\n",
            "scalable training for slack-rescaled SVMs, reducing runtime by an order of\n",
            "magnitude compared to previous approaches to slack-rescaled SVMs.\n",
            "Actual Title : \n",
            "Fast and Scalable Structural SVM with Slack Rescaling\n",
            "Generated Title : \n",
            "and scalable svm with slack rescaling\n",
            "Abstract : \n",
            "We propose a learning setting in which unlabeled data is free, and the cost\n",
            "of a label depends on its value, which is not known in advance. We study binary\n",
            "classification in an extreme case, where the algorithm only pays for negative\n",
            "labels. Our motivation are applications such as fraud detection, in which\n",
            "investigating an honest transaction should be avoided if possible. We term the\n",
            "setting auditing, and consider the auditing complexity of an algorithm: the\n",
            "number of negative labels the algorithm requires in order to learn a hypothesis\n",
            "with low relative error. We design auditing algorithms for simple hypothesis\n",
            "classes (thresholds and rectangles), and show that with these algorithms, the\n",
            "auditing complexity can be significantly lower than the active label\n",
            "complexity. We also discuss a general competitive approach for auditing and\n",
            "possible modifications to the framework.\n",
            "Actual Title : \n",
            "Auditing: Active Learning with Outcome-Dependent Query Costs\n",
            "Generated Title : \n",
            "learning for <unk>\n",
            "Abstract : \n",
            "After the incredible success of deep learning in the computer vision domain,\n",
            "there has been much interest in applying Convolutional Network (ConvNet)\n",
            "features in robotic fields such as visual navigation and SLAM. Unfortunately,\n",
            "there are fundamental differences and challenges involved. Computer vision\n",
            "datasets are very different in character to robotic camera data, real-time\n",
            "performance is essential, and performance priorities can be different. This\n",
            "paper comprehensively evaluates and compares the utility of three\n",
            "state-of-the-art ConvNets on the problems of particular relevance to navigation\n",
            "for robots; viewpoint-invariance and condition-invariance, and for the first\n",
            "time enables real-time place recognition performance using ConvNets with large\n",
            "maps by integrating a variety of existing (locality-sensitive hashing) and\n",
            "novel (semantic search space partitioning) optimization techniques. We present\n",
            "extensive experiments on four real world datasets cultivated to evaluate each\n",
            "of the specific challenges in place recognition. The results demonstrate that\n",
            "speed-ups of two orders of magnitude can be achieved with minimal accuracy\n",
            "degradation, enabling real-time performance. We confirm that networks trained\n",
            "for semantic place categorization also perform better at (specific) place\n",
            "recognition when faced with severe appearance changes and provide a reference\n",
            "for which networks and layers are optimal for different aspects of the place\n",
            "recognition problem.\n",
            "Actual Title : \n",
            "On the Performance of ConvNet Features for Place Recognition\n",
            "Generated Title : \n",
            "universal semantic and deep learning for benchmark for\n",
            "Abstract : \n",
            "In this work, we propose a new segmentation algorithm for images containing\n",
            "convex objects present in multiple shapes with a high degree of overlap. The\n",
            "proposed algorithm is carried out in two steps, first we identify the visible\n",
            "contours, segment them using concave points and finally group the segments\n",
            "belonging to the same object. The next step is to assign a shape identity to\n",
            "these grouped contour segments. For images containing objects in multiple\n",
            "shapes we begin first by identifying shape classes of the contours followed by\n",
            "assigning a shape entity to these classes. We provide a comprehensive\n",
            "experimentation of our algorithm on two crystal image datasets. One dataset\n",
            "comprises of images containing objects in multiple shapes overlapping each\n",
            "other and the other dataset contains standard images with objects present in a\n",
            "single shape. We test our algorithm against two baselines, with our proposed\n",
            "algorithm outperforming both the baselines.\n",
            "Actual Title : \n",
            "Image Segmentation of Multi-Shaped Overlapping Objects\n",
            "Generated Title : \n",
            "segmentation algorithm for overlapping objects in a <unk>\n",
            "Abstract : \n",
            "Many logic programming based approaches can be used to describe and solve\n",
            "combinatorial search problems. On the one hand there is constraint logic\n",
            "programming which computes a solution as an answer substitution to a query\n",
            "containing the variables of the constraint satisfaction problem. On the other\n",
            "hand there are systems based on stable model semantics, abductive systems, and\n",
            "first order logic model generators which compute solutions as models of some\n",
            "theory. This paper compares these different approaches from the point of view\n",
            "of knowledge representation (how declarative are the programs) and from the\n",
            "point of view of performance (how good are they at solving typical problems).\n",
            "Actual Title : \n",
            "Logic Programming Approaches for Representing and Solving Constraint\n",
            "  Satisfaction Problems: A Comparison\n",
            "Generated Title : \n",
            "programming approaches for logic programming and solving\n",
            "Abstract : \n",
            "Unmanned aerial vehicles (UAV) are evolving as an alternative tool to acquire\n",
            "land tenure data. UAVs can capture geospatial data at high quality and\n",
            "resolution in a cost-effective, transparent and flexible manner, from which\n",
            "visible land parcel boundaries, i.e., cadastral boundaries are delineable. This\n",
            "delineation is to no extent automated, even though physical objects\n",
            "automatically retrievable through image analysis methods mark a large portion\n",
            "of cadastral boundaries. This study proposes (i) a workflow that automatically\n",
            "extracts candidate cadastral boundaries from UAV orthoimages and (ii) a tool\n",
            "for their semi-automatic processing to delineate final cadastral boundaries.\n",
            "The workflow consists of two state-of-the-art computer vision methods, namely\n",
            "gPb contour detection and SLIC superpixels that are transferred to remote\n",
            "sensing in this study. The tool combines the two methods, allows a\n",
            "semi-automatic final delineation and is implemented as a publicly available\n",
            "QGIS plugin. The approach does not yet aim to provide a comparable alternative\n",
            "to manual cadastral mapping procedures. However, the methodological development\n",
            "of the tool towards this goal is developed in this paper. A study with 13\n",
            "volunteers investigates the design and implementation of the approach and\n",
            "gathers initial qualitative as well as quantitate results. The study revealed\n",
            "points for improvement, which are prioritized based on the study results and\n",
            "which will be addressed in future work.\n",
            "Actual Title : \n",
            "Towards Automated Cadastral Boundary Delineation from UAV Data\n",
            "Generated Title : \n",
            "automated cadastral boundary delineation from uav images\n",
            "Abstract : \n",
            "One of the most challenging problems in kernel online learning is to bound\n",
            "the model size and to promote the model sparsity. Sparse models not only\n",
            "improve computation and memory usage, but also enhance the generalization\n",
            "capacity, a principle that concurs with the law of parsimony. However,\n",
            "inappropriate sparsity modeling may also significantly degrade the performance.\n",
            "In this paper, we propose Approximation Vector Machine (AVM), a model that can\n",
            "simultaneously encourage the sparsity and safeguard its risk in compromising\n",
            "the performance. When an incoming instance arrives, we approximate this\n",
            "instance by one of its neighbors whose distance to it is less than a predefined\n",
            "threshold. Our key intuition is that since the newly seen instance is expressed\n",
            "by its nearby neighbor the optimal performance can be analytically formulated\n",
            "and maintained. We develop theoretical foundations to support this intuition\n",
            "and further establish an analysis to characterize the gap between the\n",
            "approximation and optimal solutions. This gap crucially depends on the\n",
            "frequency of approximation and the predefined threshold. We perform the\n",
            "convergence analysis for a wide spectrum of loss functions including Hinge,\n",
            "smooth Hinge, and Logistic for classification task, and $l_1$, $l_2$, and\n",
            "$\\epsilon$-insensitive for regression task. We conducted extensive experiments\n",
            "for classification task in batch and online modes, and regression task in\n",
            "online mode over several benchmark datasets. The results show that our proposed\n",
            "AVM achieved a comparable predictive performance with current state-of-the-art\n",
            "methods while simultaneously achieving significant computational speed-up due\n",
            "to the ability of the proposed AVM in maintaining the model size.\n",
            "Actual Title : \n",
            "Approximation Vector Machines for Large-scale Online Learning\n",
            "Generated Title : \n",
            "support vector regression with online online learning\n",
            "Abstract : \n",
            "Natural disasters can have catastrophic impacts on the functionality of\n",
            "infrastructure systems and cause severe physical and socio-economic losses.\n",
            "Given budget constraints, it is crucial to optimize decisions regarding\n",
            "mitigation, preparedness, response, and recovery practices for these systems.\n",
            "This requires accurate and efficient means to evaluate the infrastructure\n",
            "system reliability. While numerous research efforts have addressed and\n",
            "quantified the impact of natural disasters on infrastructure systems, typically\n",
            "using the Monte Carlo approach, they still suffer from high computational cost\n",
            "and, thus, are of limited applicability to large systems. This paper presents a\n",
            "deep learning framework for accelerating infrastructure system reliability\n",
            "analysis. In particular, two distinct deep neural network surrogates are\n",
            "constructed and studied: (1) A classifier surrogate which speeds up the\n",
            "connectivity determination of networks, and (2) An end-to-end surrogate that\n",
            "replaces a number of components such as roadway status realization,\n",
            "connectivity determination, and connectivity averaging. The proposed approach\n",
            "is applied to a simulation-based study of the two-terminal connectivity of a\n",
            "California transportation network subject to extreme probabilistic earthquake\n",
            "events. Numerical results highlight the effectiveness of the proposed approach\n",
            "in accelerating the transportation system two-terminal reliability analysis\n",
            "with extremely high prediction accuracy.\n",
            "Actual Title : \n",
            "Deep Learning for Accelerated Reliability Analysis of Infrastructure\n",
            "  Networks\n",
            "Generated Title : \n",
            "learning for real time accelerated monitoring data\n",
            "Abstract : \n",
            "Scene text recognition has been a hot research topic in computer vision due\n",
            "to its various applications. The state of the art is the attention-based\n",
            "encoder-decoder framework that learns the mapping between input images and\n",
            "output sequences in a purely data-driven way. However, we observe that existing\n",
            "attention-based methods perform poorly on complicated and/or low-quality\n",
            "images. One major reason is that existing methods cannot get accurate\n",
            "alignments between feature areas and targets for such images. We call this\n",
            "phenomenon \"attention drift\". To tackle this problem, in this paper we propose\n",
            "the FAN (the abbreviation of Focusing Attention Network) method that employs a\n",
            "focusing attention mechanism to automatically draw back the drifted attention.\n",
            "FAN consists of two major components: an attention network (AN) that is\n",
            "responsible for recognizing character targets as in the existing methods, and a\n",
            "focusing network (FN) that is responsible for adjusting attention by evaluating\n",
            "whether AN pays attention properly on the target areas in the images.\n",
            "Furthermore, different from the existing methods, we adopt a ResNet-based\n",
            "network to enrich deep representations of scene text images. Extensive\n",
            "experiments on various benchmarks, including the IIIT5k, SVT and ICDAR\n",
            "datasets, show that the FAN method substantially outperforms the existing\n",
            "methods.\n",
            "Actual Title : \n",
            "Focusing Attention: Towards Accurate Text Recognition in Natural Images\n",
            "Generated Title : \n",
            "the crowd to a review of a wild\n",
            "Abstract : \n",
            "Previous works demonstrated that Automatic Text Summarization (ATS) by\n",
            "sentences extraction may be improved using sentence compression. In this work\n",
            "we present a sentence compressions approach guided by level-sentence discourse\n",
            "segmentation and probabilistic language models (LM). The results presented here\n",
            "show that the proposed solution is able to generate coherent summaries with\n",
            "grammatical compressed sentences. The approach is simple enough to be\n",
            "transposed into other languages.\n",
            "Actual Title : \n",
            "Sentence Compression in Spanish driven by Discourse Segmentation and\n",
            "  Language Models\n",
            "Generated Title : \n",
            "sentence compression for automatic discourse segmentation in spanish\n",
            "Abstract : \n",
            "We present an unusual algorithm involving classification trees where two\n",
            "trees are grown in opposite directions so that they are matched at their\n",
            "leaves. This approach finds application in a new data mining task we formulate,\n",
            "called \"redescription mining\". A redescription is a shift-of-vocabulary, or a\n",
            "different way of communicating information about a given subset of data; the\n",
            "goal of redescription mining is to find subsets of data that afford multiple\n",
            "descriptions. We highlight the importance of this problem in domains such as\n",
            "bioinformatics, which exhibit an underlying richness and diversity of data\n",
            "descriptors (e.g., genes can be studied in a variety of ways). Our approach\n",
            "helps integrate multiple forms of characterizing datasets, situates the\n",
            "knowledge gained from one dataset in the context of others, and harnesses\n",
            "high-level abstractions for uncovering cryptic and subtle features of data.\n",
            "Algorithm design decisions, implementation details, and experimental results\n",
            "are presented.\n",
            "Actual Title : \n",
            "Turning CARTwheels: An Alternating Algorithm for Mining Redescriptions\n",
            "Generated Title : \n",
            "a <unk> algorithm for mining redescriptions\n",
            "Abstract : \n",
            "Multiresolution analysis and matrix factorization are foundational tools in\n",
            "computer vision. In this work, we study the interface between these two\n",
            "distinct topics and obtain techniques to uncover hierarchical block structure\n",
            "in symmetric matrices -- an important aspect in the success of many vision\n",
            "problems. Our new algorithm, the incremental multiresolution matrix\n",
            "factorization, uncovers such structure one feature at a time, and hence scales\n",
            "well to large matrices. We describe how this multiscale analysis goes much\n",
            "farther than what a direct global factorization of the data can identify. We\n",
            "evaluate the efficacy of the resulting factorizations for relative leveraging\n",
            "within regression tasks using medical imaging data. We also use the\n",
            "factorization on representations learned by popular deep networks, providing\n",
            "evidence of their ability to infer semantic relationships even when they are\n",
            "not explicitly trained to do so. We show that this algorithm can be used as an\n",
            "exploratory tool to improve the network architecture, and within numerous other\n",
            "settings in vision.\n",
            "Actual Title : \n",
            "The Incremental Multiresolution Matrix Factorization Algorithm\n",
            "Generated Title : \n",
            "incremental multiresolution matrix factorization algorithm for the basis\n",
            "Abstract : \n",
            "We exploit the versatile framework of Riemannian optimization on quotient\n",
            "manifolds to develop R3MC, a nonlinear conjugate-gradient method for low-rank\n",
            "matrix completion. The underlying search space of fixed-rank matrices is\n",
            "endowed with a novel Riemannian metric that is tailored to the least-squares\n",
            "cost. Numerical comparisons suggest that R3MC robustly outperforms\n",
            "state-of-the-art algorithms across different problem instances, especially\n",
            "those that combine scarcely sampled and ill-conditioned data.\n",
            "Actual Title : \n",
            "R3MC: A Riemannian three-factor algorithm for low-rank matrix completion\n",
            "Generated Title : \n",
            "  a riemannian <unk> for for low rank matrix\n",
            "Abstract : \n",
            "This paper describes algorithms for nonnegative matrix factorization (NMF)\n",
            "with the beta-divergence (beta-NMF). The beta-divergence is a family of cost\n",
            "functions parametrized by a single shape parameter beta that takes the\n",
            "Euclidean distance, the Kullback-Leibler divergence and the Itakura-Saito\n",
            "divergence as special cases (beta = 2,1,0, respectively). The proposed\n",
            "algorithms are based on a surrogate auxiliary function (a local majorization of\n",
            "the criterion function). We first describe a majorization-minimization (MM)\n",
            "algorithm that leads to multiplicative updates, which differ from standard\n",
            "heuristic multiplicative updates by a beta-dependent power exponent. The\n",
            "monotonicity of the heuristic algorithm can however be proven for beta in (0,1)\n",
            "using the proposed auxiliary function. Then we introduce the concept of\n",
            "majorization-equalization (ME) algorithm which produces updates that move along\n",
            "constant level sets of the auxiliary function and lead to larger steps than MM.\n",
            "Simulations on synthetic and real data illustrate the faster convergence of the\n",
            "ME approach. The paper also describes how the proposed algorithms can be\n",
            "adapted to two common variants of NMF : penalized NMF (i.e., when a penalty\n",
            "function of the factors is added to the criterion function) and convex-NMF\n",
            "(when the dictionary is assumed to belong to a known subspace).\n",
            "Actual Title : \n",
            "Algorithms for nonnegative matrix factorization with the beta-divergence\n",
            "Generated Title : \n",
            "nonnegative matrix factorization\n",
            "Abstract : \n",
            "Dealing with datasets of very high dimension is a major challenge in machine\n",
            "learning. In this paper, we consider the problem of feature selection in\n",
            "applications where the memory is not large enough to contain all features. In\n",
            "this setting, we propose a novel tree-based feature selection approach that\n",
            "builds a sequence of randomized trees on small subsamples of variables mixing\n",
            "both variables already identified as relevant by previous models and variables\n",
            "randomly selected among the other variables. As our main contribution, we\n",
            "provide an in-depth theoretical analysis of this method in infinite sample\n",
            "setting. In particular, we study its soundness with respect to common\n",
            "definitions of feature relevance and its convergence speed under various\n",
            "variable dependance scenarios. We also provide some preliminary empirical\n",
            "results highlighting the potential of the approach.\n",
            "Actual Title : \n",
            "Random Subspace with Trees for Feature Selection Under Memory\n",
            "  Constraints\n",
            "Generated Title : \n",
            "selection with random feature selection\n",
            "Abstract : \n",
            "Collective intelligence, which aggregates the shared information from large\n",
            "crowds, is often negatively impacted by unreliable information sources with the\n",
            "low quality data. This becomes a barrier to the effective use of collective\n",
            "intelligence in a variety of applications. In order to address this issue, we\n",
            "propose a probabilistic model to jointly assess the reliability of sources and\n",
            "find the true data. We observe that different sources are often not independent\n",
            "of each other. Instead, sources are prone to be mutually influenced, which\n",
            "makes them dependent when sharing information with each other. High dependency\n",
            "between sources makes collective intelligence vulnerable to the overuse of\n",
            "redundant (and possibly incorrect) information from the dependent sources.\n",
            "Thus, we reveal the latent group structure among dependent sources, and\n",
            "aggregate the information at the group level rather than from individual\n",
            "sources directly. This can prevent the collective intelligence from being\n",
            "inappropriately dominated by dependent sources. We will also explicitly reveal\n",
            "the reliability of groups, and minimize the negative impacts of unreliable\n",
            "groups. Experimental results on real-world data sets show the effectiveness of\n",
            "the proposed approach with respect to existing algorithms.\n",
            "Actual Title : \n",
            "Learning from Collective Intelligence in Groups\n",
            "Generated Title : \n",
            "in collective groups groups\n",
            "Abstract : \n",
            "This is the second part of a paper on Conscious Intelligent Systems. We use\n",
            "the understanding gained in the first part (Conscious Intelligent Systems Part\n",
            "1: IXI (arxiv id cs.AI/0612056)) to look at understanding. We see how the\n",
            "presence of mind affects understanding and intelligent systems; we see that the\n",
            "presence of mind necessitates language. The rise of language in turn has\n",
            "important effects on understanding. We discuss the humanoid question and how\n",
            "the question of self-consciousness (and by association mind/thought/language)\n",
            "would affect humanoids too.\n",
            "Actual Title : \n",
            "Conscious Intelligent Systems - Part II - Mind, Thought, Language and\n",
            "  Understanding\n",
            "Generated Title : \n",
            "intelligent systems   mind  thought  language and its\n",
            "Abstract : \n",
            "Current methods for automatically evaluating grammatical error correction\n",
            "(GEC) systems rely on gold-standard references. However, these methods suffer\n",
            "from penalizing grammatical edits that are correct but not in the gold\n",
            "standard. We show that reference-less grammaticality metrics correlate very\n",
            "strongly with human judgments and are competitive with the leading\n",
            "reference-based evaluation metrics. By interpolating both methods, we achieve\n",
            "state-of-the-art correlation with human judgments. Finally, we show that GEC\n",
            "metrics are much more reliable when they are calculated at the sentence level\n",
            "instead of the corpus level. We have set up a CodaLab site for benchmarking GEC\n",
            "output using a common dataset and different evaluation metrics.\n",
            "Actual Title : \n",
            "There's No Comparison: Reference-less Evaluation Metrics in Grammatical\n",
            "  Error Correction\n",
            "Generated Title : \n",
            "new test for evaluating <unk> evaluation metrics for\n",
            "Abstract : \n",
            "This paper presented our work on applying Recurrent Deep Stacking Networks\n",
            "(RDSNs) to Robust Automatic Speech Recognition (ASR) tasks. In the paper, we\n",
            "also proposed a more efficient yet comparable substitute to RDSN, Bi- Pass\n",
            "Stacking Network (BPSN). The main idea of these two models is to add\n",
            "phoneme-level information into acoustic models, transforming an acoustic model\n",
            "to the combination of an acoustic model and a phoneme-level N-gram model.\n",
            "Experiments showed that RDSN and BPsn can substantially improve the\n",
            "performances over conventional DNNs.\n",
            "Actual Title : \n",
            "Recurrent Deep Stacking Networks for Speech Recognition\n",
            "Generated Title : \n",
            "deep stacking networks with recurrent networks\n",
            "Abstract : \n",
            "In a recent paper, we presented an intelligent evolutionary search technique\n",
            "through genetic programming (GP) for finding new analytical expressions of\n",
            "nonlinear dynamical systems, similar to the classical Lorenz attractor's which\n",
            "also exhibit chaotic behaviour in the phase space. In this paper, we extend our\n",
            "previous finding to explore yet another gallery of new chaotic attractors which\n",
            "are derived from the original Lorenz system of equations. Compared to the\n",
            "previous exploration with sinusoidal type transcendental nonlinearity, here we\n",
            "focus on only cross-product and higher-power type nonlinearities in the three\n",
            "state equations. We here report over 150 different structures of chaotic\n",
            "attractors along with their one set of parameter values, phase space dynamics\n",
            "and the Largest Lyapunov Exponents (LLE). The expressions of these new\n",
            "Lorenz-like nonlinear dynamical systems have been automatically evolved through\n",
            "multi-gene genetic programming (MGGP). In the past two decades, there have been\n",
            "many claims of designing new chaotic attractors as an incremental extension of\n",
            "the Lorenz family. We provide here a large family of chaotic systems whose\n",
            "structure closely resemble the original Lorenz system but with drastically\n",
            "different phase space dynamics. This advances the state of the art knowledge of\n",
            "discovering new chaotic systems which can find application in many real-world\n",
            "problems. This work may also find its archival value in future in the domain of\n",
            "new chaotic system discovery.\n",
            "Actual Title : \n",
            "Evolving Chaos: Identifying New Attractors of the Generalised Lorenz\n",
            "  Family\n",
            "Generated Title : \n",
            "chaos  identifying new attractors for the asymmetric <unk>\n",
            "Abstract : \n",
            "Trust is a fundamental concept in many real-world applications such as\n",
            "e-commerce and peer-to-peer networks. In these applications, users can generate\n",
            "local opinions about the counterparts based on direct experiences, and these\n",
            "opinions can then be aggregated to build trust among unknown users. The\n",
            "mechanism to build new trust relationships based on existing ones is referred\n",
            "to as trust inference. State-of-the-art trust inference approaches employ the\n",
            "transitivity property of trust by propagating trust along connected users. In\n",
            "this paper, we propose a novel trust inference model (MaTrust) by exploring an\n",
            "equally important property of trust, i.e., the multi-aspect property. MaTrust\n",
            "directly characterizes multiple latent factors for each trustor and trustee\n",
            "from the locally-generated trust relationships. Furthermore, it can naturally\n",
            "incorporate prior knowledge as specified factors. These factors in turn serve\n",
            "as the basis to infer the unseen trustworthiness scores. Experimental\n",
            "evaluations on real data sets show that the proposed MaTrust significantly\n",
            "outperforms several benchmark trust inference models in both effectiveness and\n",
            "efficiency.\n",
            "Actual Title : \n",
            "MaTrust: An Effective Multi-Aspect Trust Inference Model\n",
            "Generated Title : \n",
            "<unk> inference inference with external knowledge\n",
            "Abstract : \n",
            "In this paper, we are concerned with obtaining distribution-free\n",
            "concentration inequalities for mixture of independent Bernoulli variables that\n",
            "incorporate a notion of variance. Missing mass is the total probability mass\n",
            "associated to the outcomes that have not been seen in a given sample which is\n",
            "an important quantity that connects density estimates obtained from a sample to\n",
            "the population for discrete distributions. Therefore, we are specifically\n",
            "motivated to apply our method to study the concentration of missing mass -\n",
            "which can be expressed as a mixture of Bernoulli - in a novel way.\n",
            "  We not only derive - for the first time - Bernstein-like large deviation\n",
            "bounds for the missing mass whose exponents behave almost linearly with respect\n",
            "to deviation size, but also sharpen McAllester and Ortiz (2003) and Berend and\n",
            "Kontorovich (2013) for large sample sizes in the case of small deviations which\n",
            "is the most interesting case in learning theory. In the meantime, our approach\n",
            "shows that the heterogeneity issue introduced in McAllester and Ortiz (2003) is\n",
            "resolvable in the case of missing mass in the sense that one can use standard\n",
            "inequalities but it may not lead to strong results. Thus, we postulate that our\n",
            "results are general and can be applied to provide potentially sharp\n",
            "Bernstein-like bounds under some constraints.\n",
            "Actual Title : \n",
            "Novel Deviation Bounds for Mixture of Independent Bernoulli Variables\n",
            "  with Application to the Missing Mass\n",
            "Generated Title : \n",
            "deviation bounds for the dirichlet mixture models\n",
            "Abstract : \n",
            "In this paper, an online adaptive model-free tracker is proposed to track\n",
            "single objects in video sequences to deal with real-world tracking challenges\n",
            "like low-resolution, object deformation, occlusion and motion blur. The novelty\n",
            "lies in the construction of a strong appearance model that captures features\n",
            "from the initialized bounding box and then are assembled into anchor-point\n",
            "features. These features memorize the global pattern of the object and have an\n",
            "internal star graph-like structure. These features are unique and flexible and\n",
            "helps tracking generic and deformable objects with no limitation on specific\n",
            "objects. In addition, the relevance of each feature is evaluated online using\n",
            "short-term consistency and long-term consistency. These parameters are adapted\n",
            "to retain consistent features that vote for the object location and that deal\n",
            "with outliers for long-term tracking scenarios. Additionally, voting in a\n",
            "Gaussian manner helps in tackling inherent noise of the tracking system and in\n",
            "accurate object localization. Furthermore, the proposed tracker uses pairwise\n",
            "distance measure to cope with scale variations and combines pixel-level binary\n",
            "features and global weighted color features for model update. Finally,\n",
            "experimental results on a visual tracking benchmark dataset are presented to\n",
            "demonstrate the effectiveness and competitiveness of the proposed tracker.\n",
            "Actual Title : \n",
            "Tracking using Numerous Anchor points\n",
            "Generated Title : \n",
            "online tracking of adaptive multi target tracking\n",
            "Abstract : \n",
            "Many algorithms formulate graph matching as an optimization of an objective\n",
            "function of pairwise quantification of nodes and edges of two graphs to be\n",
            "matched. Pairwise measurements usually consider local attributes but disregard\n",
            "contextual information involved in graph structures. We address this issue by\n",
            "proposing contextual similarities between pairs of nodes. This is done by\n",
            "considering the tensor product graph (TPG) of two graphs to be matched, where\n",
            "each node is an ordered pair of nodes of the operand graphs. Contextual\n",
            "similarities between a pair of nodes are computed by accumulating weighted\n",
            "walks (normalized pairwise similarities) terminating at the corresponding\n",
            "paired node in TPG. Once the contextual similarities are obtained, we formulate\n",
            "subgraph matching as a node and edge selection problem in TPG. We use\n",
            "contextual similarities to construct an objective function and optimize it with\n",
            "a linear programming approach. Since random walk formulation through TPG takes\n",
            "into account higher order information, it is not a surprise that we obtain more\n",
            "reliable similarities and better discrimination among the nodes and edges.\n",
            "Experimental results shown on synthetic as well as real benchmarks illustrate\n",
            "that higher order contextual similarities add discriminating power and allow\n",
            "one to find approximate solutions to the subgraph matching problem.\n",
            "Actual Title : \n",
            "Product Graph-based Higher Order Contextual Similarities for Inexact\n",
            "  Subgraph Matching\n",
            "Generated Title : \n",
            "laplacian matching for inexact subgraph matching\n",
            "Abstract : \n",
            "Echo state networks (ESN), a type of reservoir computing (RC) architecture,\n",
            "are efficient and accurate artificial neural systems for time series processing\n",
            "and learning. An ESN consists of a core of recurrent neural networks, called a\n",
            "reservoir, with a small number of tunable parameters to generate a\n",
            "high-dimensional representation of an input, and a readout layer which is\n",
            "easily trained using regression to produce a desired output from the reservoir\n",
            "states. Certain computational tasks involve real-time calculation of high-order\n",
            "time correlations, which requires nonlinear transformation either in the\n",
            "reservoir or the readout layer. Traditional ESN employs a reservoir with\n",
            "sigmoid or tanh function neurons. In contrast, some types of biological neurons\n",
            "obey response curves that can be described as a product unit rather than a sum\n",
            "and threshold. Inspired by this class of neurons, we introduce a RC\n",
            "architecture with a reservoir of product nodes for time series computation. We\n",
            "find that the product RC shows many properties of standard ESN such as\n",
            "short-term memory and nonlinear capacity. On standard benchmarks for chaotic\n",
            "prediction tasks, the product RC maintains the performance of a standard\n",
            "nonlinear ESN while being more amenable to mathematical analysis. Our study\n",
            "provides evidence that such networks are powerful in highly nonlinear tasks\n",
            "owing to high-order statistics generated by the recurrent product node\n",
            "reservoir.\n",
            "Actual Title : \n",
            "Product Reservoir Computing: Time-Series Computation with Multiplicative\n",
            "  Neurons\n",
            "Generated Title : \n",
            "computing  time series data\n",
            "Abstract : \n",
            "We present a novel deep architecture termed templateNet for depth based\n",
            "object instance recognition. Using an intermediate template layer we exploit\n",
            "prior knowledge of an object's shape to sparsify the feature maps. This has\n",
            "three advantages: (i) the network is better regularised resulting in structured\n",
            "filters; (ii) the sparse feature maps results in intuitive features been learnt\n",
            "which can be visualized as the output of the template layer and (iii) the\n",
            "resulting network achieves state-of-the-art performance. The network benefits\n",
            "from this without any additional parametrization from the template layer. We\n",
            "derive the weight updates needed to efficiently train this network in an\n",
            "end-to-end manner. We benchmark the templateNet for depth based object instance\n",
            "recognition using two publicly available datasets. The datasets present\n",
            "multiple challenges of clutter, large pose variations and similar looking\n",
            "distractors. Through our experiments we show that with the addition of a\n",
            "template layer, a depth based CNN is able to outperform existing\n",
            "state-of-the-art methods in the field.\n",
            "Actual Title : \n",
            "TemplateNet for Depth-Based Object Instance Recognition\n",
            "Generated Title : \n",
            "instance depth based object instance recognition\n",
            "Abstract : \n",
            "This article proposes to auto-encode text at byte-level using convolutional\n",
            "networks with a recursive architecture. The motivation is to explore whether it\n",
            "is possible to have scalable and homogeneous text generation at byte-level in a\n",
            "non-sequential fashion through the simple task of auto-encoding. We show that\n",
            "non-sequential text generation from a fixed-length representation is not only\n",
            "possible, but also achieved much better auto-encoding results than recurrent\n",
            "networks. The proposed model is a multi-stage deep convolutional\n",
            "encoder-decoder framework using residual connections, containing up to 160\n",
            "parameterized layers. Each encoder or decoder contains a shared group of\n",
            "modules that consists of either pooling or upsampling layers, making the\n",
            "network recursive in terms of abstraction levels in representation. Results for\n",
            "6 large-scale paragraph datasets are reported, in 3 languages including Arabic,\n",
            "Chinese and English. Analyses are conducted to study several properties of the\n",
            "proposed model.\n",
            "Actual Title : \n",
            "Byte-Level Recursive Convolutional Auto-Encoder for Text\n",
            "Generated Title : \n",
            "recursive convolutional auto encoder for text text\n",
            "Abstract : \n",
            "Efficient modeling on uncertain information plays an important role in\n",
            "estimating the risk of contaminant intrusion in water distribution networks.\n",
            "Dempster-Shafer evidence theory is one of the most commonly used methods.\n",
            "However, the Dempster-Shafer evidence theory has some hypotheses including the\n",
            "exclusive property of the elements in the frame of discernment, which may not\n",
            "be consistent with the real world. In this paper, based on a more effective\n",
            "representation of uncertainty, called D numbers, a new method that allows the\n",
            "elements in the frame of discernment to be non-exclusive is proposed. To\n",
            "demonstrate the efficiency of the proposed method, we apply it to the water\n",
            "distribution networks to estimate the risk of contaminant intrusion.\n",
            "Actual Title : \n",
            "Modeling contaminant intrusion in water distribution networks based on D\n",
            "  numbers\n",
            "Generated Title : \n",
            "of <unk> of <unk> based hand gesture recognition\n",
            "Abstract : \n",
            "We work out a classification scheme for quantum modeling in Hilbert space of\n",
            "any kind of composite entity violating Bell's inequalities and exhibiting\n",
            "entanglement. Our theoretical framework includes situations with entangled\n",
            "states and product measurements ('customary quantum situation'), and also\n",
            "situations with both entangled states and entangled measurements ('nonlocal box\n",
            "situation', 'nonlocal non-marginal box situation'). We show that entanglement\n",
            "is structurally a joint property of states and measurements. Furthermore,\n",
            "entangled measurements enable quantum modeling of situations that are usually\n",
            "believed to be 'beyond quantum'. Our results are also extended from pure states\n",
            "to quantum mixtures.\n",
            "Actual Title : \n",
            "General Quantum Hilbert Space Modeling Scheme for Entanglement\n",
            "Generated Title : \n",
            "quantum hilbert space and quantum hilbert space\n",
            "Abstract : \n",
            "Existing neural machine translation (NMT) models generally translate\n",
            "sentences in isolation, missing the opportunity to take advantage of\n",
            "document-level information. In this work, we propose to augment NMT models with\n",
            "a very light-weight cache-like memory network, which stores recent hidden\n",
            "representations as translation history. The probability distribution over\n",
            "generated words is updated online depending on the translation history\n",
            "retrieved from the memory, endowing NMT models with the capability to\n",
            "dynamically adapt over time. Experiments on multiple domains with different\n",
            "topics and styles show the effectiveness of the proposed approach with\n",
            "negligible impact on the computational cost.\n",
            "Actual Title : \n",
            "Learning to Remember Translation History with a Continuous Cache\n",
            "Generated Title : \n",
            "machine translation with neural machine translation\n",
            "Abstract : \n",
            "The goal of this paper is to discover a set of discriminative patches which\n",
            "can serve as a fully unsupervised mid-level visual representation. The desired\n",
            "patches need to satisfy two requirements: 1) to be representative, they need to\n",
            "occur frequently enough in the visual world; 2) to be discriminative, they need\n",
            "to be different enough from the rest of the visual world. The patches could\n",
            "correspond to parts, objects, \"visual phrases\", etc. but are not restricted to\n",
            "be any one of them. We pose this as an unsupervised discriminative clustering\n",
            "problem on a huge dataset of image patches. We use an iterative procedure which\n",
            "alternates between clustering and training discriminative classifiers, while\n",
            "applying careful cross-validation at each step to prevent overfitting. The\n",
            "paper experimentally demonstrates the effectiveness of discriminative patches\n",
            "as an unsupervised mid-level visual representation, suggesting that it could be\n",
            "used in place of visual words for many tasks. Furthermore, discriminative\n",
            "patches can also be used in a supervised regime, such as scene classification,\n",
            "where they demonstrate state-of-the-art performance on the MIT Indoor-67\n",
            "dataset.\n",
            "Actual Title : \n",
            "Unsupervised Discovery of Mid-Level Discriminative Patches\n",
            "Generated Title : \n",
            "mid level visual\n",
            "Abstract : \n",
            "Classification is one of the most important tasks of machine learning.\n",
            "Although the most well studied model is the two-class problem, in many\n",
            "scenarios there is the opportunity to label critical items for manual revision,\n",
            "instead of trying to automatically classify every item. In this paper we adapt\n",
            "a paradigm initially proposed for the classification of ordinal data to address\n",
            "the classification problem with reject option. The technique reduces the\n",
            "problem of classifying with reject option to the standard two-class problem.\n",
            "The introduced method is then mapped into support vector machines and neural\n",
            "networks. Finally, the framework is extended to multiclass ordinal data with\n",
            "reject option. An experimental study with synthetic and real data sets,\n",
            "verifies the usefulness of the proposed approach.\n",
            "Actual Title : \n",
            "The Data Replication Method for the Classification with Reject Option\n",
            "Generated Title : \n",
            "data replication method for reject option\n",
            "Abstract : \n",
            "The aim of this paper is to propose an application of mutual\n",
            "information-based ensemble methods to the analysis and classification of heart\n",
            "beats associated with different types of Arrhythmia. Models of multilayer\n",
            "perceptrons, support vector machines, and radial basis function neural networks\n",
            "were trained and tested using the MIT-BIH arrhythmia database. This research\n",
            "brings a focus to an ensemble method that, to our knowledge, is a novel\n",
            "application in the area of ECG Arrhythmia detection. The proposed classifier\n",
            "ensemble method showed improved performance, relative to either majority voting\n",
            "classifier integration or to individual classifier performance. The overall\n",
            "ensemble accuracy was 98.25%.\n",
            "Actual Title : \n",
            "Arrhythmia Detection using Mutual Information-Based Integration Method\n",
            "Generated Title : \n",
            "detection using multi class regression\n",
            "Abstract : \n",
            "The Outilex software platform, which will be made available to research,\n",
            "development and industry, comprises software components implementing all the\n",
            "fundamental operations of written text processing: processing without lexicons,\n",
            "exploitation of lexicons and grammars, language resource management. All data\n",
            "are structured in XML formats, and also in more compact formats, either\n",
            "readable or binary, whenever necessary; the required format converters are\n",
            "included in the platform; the grammar formats allow for combining statistical\n",
            "approaches with resource-based approaches. Manually constructed lexicons for\n",
            "French and English, originating from the LADL, and of substantial coverage,\n",
            "will be distributed with the platform under LGPL-LR license.\n",
            "Actual Title : \n",
            "Outilex, plate-forme logicielle de traitement de textes crits\n",
            "Generated Title : \n",
            "<unk> <unk> <unk> <unk> <unk> <unk> textes <unk>\n",
            "Abstract : \n",
            "Detecting multiple planes in images is a challenging problem, but one with\n",
            "many applications. Recent work such as J-Linkage and Ordered Residual Kernels\n",
            "have focussed on developing a domain independent approach to detect multiple\n",
            "structures. These multiple structure detection methods are then used for\n",
            "estimating multiple homographies given feature matches between two images.\n",
            "Features participating in the multiple homographies detected, provide us the\n",
            "multiple scene planes. We show that these methods provide locally optimal\n",
            "results and fail to merge detected planar patches to the true scene planes.\n",
            "These methods use only residues obtained on applying homography of one plane to\n",
            "another as cue for merging. In this paper, we develop additional cues such as\n",
            "local consistency of planes, local normals, texture etc. to perform better\n",
            "classification and merging . We formulate the classification as an MRF problem\n",
            "and use TRWS message passing algorithm to solve non metric energy terms and\n",
            "complex sparse graph structure. We show results on challenging dataset common\n",
            "in robotics navigation scenarios where our method shows accuracy of more than\n",
            "85 percent on average while being close or same as the actual number of scene\n",
            "planes.\n",
            "Actual Title : \n",
            "Top Down Approach to Multiple Plane Detection\n",
            "Generated Title : \n",
            "<unk> of multiple objects using multiple maximal correlation\n",
            "Abstract : \n",
            "One popular approach for blind deconvolution is to formulate a maximum a\n",
            "posteriori (MAP) problem with sparsity priors on the gradients of the latent\n",
            "image, and then alternatingly estimate the blur kernel and the latent image.\n",
            "While several successful MAP based methods have been proposed, there has been\n",
            "much controversy and confusion about their convergence, because sparsity priors\n",
            "have been shown to prefer blurry images to sharp natural images. In this paper,\n",
            "we revisit this problem and provide an analysis on the convergence of MAP based\n",
            "approaches. We first introduce a slight modification to a conventional joint\n",
            "energy function for blind deconvolution. The reformulated energy function\n",
            "yields the same alternating estimation process, but more clearly reveals how\n",
            "blind deconvolution works. We then show the energy function can actually favor\n",
            "the right solution instead of the no-blur solution under certain conditions,\n",
            "which explains the success of previous MAP based approaches. The reformulated\n",
            "energy function and our conditions for the convergence also provide a way to\n",
            "compare the qualities of different blur kernels, and we demonstrate its\n",
            "applicability to automatic blur kernel size selection, blur kernel estimation\n",
            "using light streaks, and defocus estimation.\n",
            "Actual Title : \n",
            "Convergence Analysis of MAP based Blur Kernel Estimation\n",
            "Generated Title : \n",
            "the of of map blind deconvolution\n",
            "Abstract : \n",
            "In this paper, we take a new look at the possibilistic c-means (PCM) and\n",
            "adaptive PCM (APCM) clustering algorithms from the perspective of uncertainty.\n",
            "This new perspective offers us insights into the clustering process, and also\n",
            "provides us greater degree of flexibility. We analyze the clustering behavior\n",
            "of PCM-based algorithms and introduce parameters $\\sigma_v$ and $\\alpha$ to\n",
            "characterize uncertainty of estimated bandwidth and noise level of the dataset\n",
            "respectively. Then uncertainty (fuzziness) of membership values caused by\n",
            "uncertainty of the estimated bandwidth parameter is modeled by a conditional\n",
            "fuzzy set, which is a new formulation of the type-2 fuzzy set. Experiments show\n",
            "that parameters $\\sigma_v$ and $\\alpha$ make the clustering process more easy\n",
            "to control, and main features of PCM and APCM are unified in this new\n",
            "clustering framework (UPCM). More specifically, UPCM reduces to PCM when we set\n",
            "a small $\\alpha$ or a large $\\sigma_v$, and UPCM reduces to APCM when clusters\n",
            "are confined in their physical clusters and possible cluster elimination are\n",
            "ensured. Finally we present further researches of this paper.\n",
            "Actual Title : \n",
            "PCM and APCM Revisited: An Uncertainty Perspective\n",
            "Generated Title : \n",
            "and <unk> revisited  an uncertainty perspective\n",
            "Abstract : \n",
            "This article demonstrates a new conceptor network based classifier in\n",
            "classifying images. Mathematical descriptions and analysis are presented.\n",
            "Various tests are experimented using three benchmark datasets: MNIST, CIFAR-10\n",
            "and CIFAR-100. The experiments displayed that conceptor network can offer\n",
            "superior results and flexible configurations than conventional classifiers such\n",
            "as Softmax Regression and Support Vector Machine (SVM).\n",
            "Actual Title : \n",
            "Classify Images with Conceptor Network\n",
            "Generated Title : \n",
            "images with conceptor network\n",
            "Abstract : \n",
            "In this work we consider the problem of anomaly detection in heterogeneous,\n",
            "multivariate, variable-length time series datasets. Our focus is on the\n",
            "aviation safety domain, where data objects are flights and time series are\n",
            "sensor readings and pilot switches. In this context the goal is to detect\n",
            "anomalous flight segments, due to mechanical, environmental, or human factors\n",
            "in order to identifying operationally significant events and provide insights\n",
            "into the flight operations and highlight otherwise unavailable potential safety\n",
            "risks and precursors to accidents. For this purpose, we propose a framework\n",
            "which represents each flight using a semi-Markov switching vector\n",
            "autoregressive (SMS-VAR) model. Detection of anomalies is then based on\n",
            "measuring dissimilarities between the model's prediction and data observation.\n",
            "The framework is scalable, due to the inherent parallel nature of most\n",
            "computations, and can be used to perform online anomaly detection. Extensive\n",
            "experimental results on simulated and real datasets illustrate that the\n",
            "framework can detect various types of anomalies along with the key parameters\n",
            "involved.\n",
            "Actual Title : \n",
            "Semi-Markov Switching Vector Autoregressive Model-based Anomaly\n",
            "  Detection in Aviation Systems\n",
            "Generated Title : \n",
            "detection for stationary autoregressive processes\n",
            "Abstract : \n",
            "This paper derives a formula for computing the conditional probability of a\n",
            "set of candidates, where a candidate is a set of disorders that explain a given\n",
            "set of positive findings. Such candidate sets are produced by a recent method\n",
            "for multidisorder diagnosis called symptom clustering. A symptom clustering\n",
            "represents a set of candidates compactly as a cartesian product of differential\n",
            "diagnoses. By evaluating the probability of a candidate set, then, a large set\n",
            "of candidates can be validated or pruned simultaneously. The probability of a\n",
            "candidate set is then specialized to obtain the probability of a single\n",
            "candidate. Unlike earlier results, the equation derived here allows the\n",
            "specification of positive, negative, and unknown symptoms and does not make\n",
            "assumptions about disorders not in the candidate.\n",
            "Actual Title : \n",
            "Probabilistic Evaluation of Candidates and Symptom Clustering for\n",
            "  Multidisorder Diagnosis\n",
            "Generated Title : \n",
            "<unk> and <unk> diagnosis of <unk> and symptom\n",
            "Abstract : \n",
            "This paper presents a novel framework for generating texture mosaics with\n",
            "convolutional neural networks. Our method is called GANosaic and performs\n",
            "optimization in the latent noise space of a generative texture model, which\n",
            "allows the transformation of a content image into a mosaic exhibiting the\n",
            "visual properties of the underlying texture manifold. To represent that\n",
            "manifold, we use a state-of-the-art generative adversarial method for texture\n",
            "synthesis, which can learn expressive texture representations from data and\n",
            "produce mosaic images with very high resolution. This fully convolutional model\n",
            "generates smooth (without any visible borders) mosaic images which morph and\n",
            "blend different textures locally. In addition, we develop a new type of\n",
            "differentiable statistical regularization appropriate for optimization over the\n",
            "prior noise space of the PSGAN model.\n",
            "Actual Title : \n",
            "GANosaic: Mosaic Creation with Generative Texture Manifolds\n",
            "Generated Title : \n",
            "mosaic creation with a texture\n",
            "Abstract : \n",
            "Detection and learning based appearance feature play the central role in data\n",
            "association based multiple object tracking (MOT), but most recent MOT works\n",
            "usually ignore them and only focus on the hand-crafted feature and association\n",
            "algorithms. In this paper, we explore the high-performance detection and deep\n",
            "learning based appearance feature, and show that they lead to significantly\n",
            "better MOT results in both online and offline setting. We make our detection\n",
            "and appearance feature publicly available. In the following part, we first\n",
            "summarize the detection and appearance feature, and then introduce our tracker\n",
            "named Person of Interest (POI), which has both online and offline version.\n",
            "Actual Title : \n",
            "POI: Multiple Object Tracking with High Performance Detection and\n",
            "  Appearance Feature\n",
            "Generated Title : \n",
            "an online and ensemble for for object detection\n",
            "Abstract : \n",
            "K-Nearest Neighbours (k-NN) is a popular classification and regression\n",
            "algorithm, yet one of its main limitations is the difficulty in choosing the\n",
            "number of neighbours. We present a Bayesian algorithm to compute the posterior\n",
            "probability distribution for k given a target point within a data-set,\n",
            "efficiently and without the use of Markov Chain Monte Carlo (MCMC) methods or\n",
            "simulation - alongside an exact solution for distributions within the\n",
            "exponential family. The central idea is that data points around our target are\n",
            "generated by the same probability distribution, extending outwards over the\n",
            "appropriate, though unknown, number of neighbours. Once the data is projected\n",
            "onto a distance metric of choice, we can transform the choice of k into a\n",
            "change-point detection problem, for which there is an efficient solution: we\n",
            "recursively compute the probability of the last change-point as we move towards\n",
            "our target, and thus de facto compute the posterior probability distribution\n",
            "over k. Applying this approach to both a classification and a regression UCI\n",
            "data-sets, we compare favourably and, most importantly, by removing the need\n",
            "for simulation, we are able to compute the posterior probability of k exactly\n",
            "and rapidly. As an example, the computational time for the Ripley data-set is a\n",
            "few milliseconds compared to a few hours when using a MCMC approach.\n",
            "Actual Title : \n",
            "An Efficient Algorithm for Bayesian Nearest Neighbours\n",
            "Generated Title : \n",
            "bayesian algorithm for bayesian computation in bayesian nearest\n",
            "Abstract : \n",
            "Transductive learning considers situations when a learner observes $m$\n",
            "labelled training points and $u$ unlabelled test points with the final goal of\n",
            "giving correct answers for the test points. This paper introduces a new\n",
            "complexity measure for transductive learning called Permutational Rademacher\n",
            "Complexity (PRC) and studies its properties. A novel symmetrization inequality\n",
            "is proved, which shows that PRC provides a tighter control over expected\n",
            "suprema of empirical processes compared to what happens in the standard i.i.d.\n",
            "setting. A number of comparison results are also provided, which show the\n",
            "relation between PRC and other popular complexity measures used in statistical\n",
            "learning theory, including Rademacher complexity and Transductive Rademacher\n",
            "Complexity (TRC). We argue that PRC is a more suitable complexity measure for\n",
            "transductive learning. Finally, these results are combined with a standard\n",
            "concentration argument to provide novel data-dependent risk bounds for\n",
            "transductive learning.\n",
            "Actual Title : \n",
            "Permutational Rademacher Complexity: a New Complexity Measure for\n",
            "  Transductive Learning\n",
            "Generated Title : \n",
            "the <unk> distance metric learning\n",
            "Abstract : \n",
            "Exemplar-based face sketch synthesis plays an important role in both digital\n",
            "entertainment and law enforcement. It generally consists of two parts: neighbor\n",
            "selection and reconstruction weight representation. The most time-consuming or\n",
            "main computation complexity for exemplar-based face sketch synthesis methods\n",
            "lies in the neighbor selection process. State-of-the-art face sketch synthesis\n",
            "methods perform neighbor selection online in a data-driven manner by $K$\n",
            "nearest neighbor ($K$-NN) searching. Actually, the online search increases the\n",
            "time consuming for synthesis. Moreover, since these methods need to traverse\n",
            "the whole training dataset for neighbor selection, the computational complexity\n",
            "increases with the scale of the training database and hence these methods have\n",
            "limited scalability. In this paper, we proposed a simple but effective offline\n",
            "random sampling in place of online $K$-NN search to improve the synthesis\n",
            "efficiency. Extensive experiments on public face sketch databases demonstrate\n",
            "the superiority of the proposed method in comparison to state-of-the-art\n",
            "methods, in terms of both synthesis quality and time consumption. The proposed\n",
            "method could be extended to other heterogeneous face image transformation\n",
            "problems such as face hallucination. We release the source codes of our\n",
            "proposed methods and the evaluation metrics for future study online:\n",
            "http://www.ihitworld.com/RSLCR.html.\n",
            "Actual Title : \n",
            "Random Sampling for Fast Face Sketch Synthesis\n",
            "Generated Title : \n",
            "sampling for for face sketch synthesis\n",
            "Abstract : \n",
            "Backdoors of answer-set programs are sets of atoms that represent clever\n",
            "reasoning shortcuts through the search space. Assignments to backdoor atoms\n",
            "reduce the given program to several programs that belong to a tractable target\n",
            "class. Previous research has considered target classes based on notions of\n",
            "acyclicity where various types of cycles (good and bad cycles) are excluded\n",
            "from graph representations of programs. We generalize the target classes by\n",
            "taking the parity of the number of negative edges on bad cycles into account\n",
            "and consider backdoors for such classes. We establish new hardness results and\n",
            "non-uniform polynomial-time tractability relative to directed or undirected\n",
            "cycles.\n",
            "Actual Title : \n",
            "The Good, the Bad, and the Odd: Cycles in Answer-Set Programs\n",
            "Generated Title : \n",
            "unsolvable problem with <unk>\n",
            "Abstract : \n",
            "A recent theoretical analysis shows the equivalence between non-negative\n",
            "matrix factorization (NMF) and spectral clustering based approach to subspace\n",
            "clustering. As NMF and many of its variants are essentially linear, we\n",
            "introduce a nonlinear NMF with explicit orthogonality and derive general\n",
            "kernel-based orthogonal multiplicative update rules to solve the subspace\n",
            "clustering problem. In nonlinear orthogonal NMF framework, we propose two\n",
            "subspace clustering algorithms, named kernel-based non-negative subspace\n",
            "clustering KNSC-Ncut and KNSC-Rcut and establish their connection with spectral\n",
            "normalized cut and ratio cut clustering. We further extend the nonlinear\n",
            "orthogonal NMF framework and introduce a graph regularization to obtain a\n",
            "factorization that respects a local geometric structure of the data after the\n",
            "nonlinear mapping. The proposed NMF-based approach to subspace clustering takes\n",
            "into account the nonlinear nature of the manifold, as well as its intrinsic\n",
            "local geometry, which considerably improves the clustering performance when\n",
            "compared to the several recently proposed state-of-the-art methods.\n",
            "Actual Title : \n",
            "A Nonlinear Orthogonal Non-Negative Matrix Factorization Approach to\n",
            "  Subspace Clustering\n",
            "Generated Title : \n",
            "matrix decomposition for non negative matrix factorization\n",
            "Abstract : \n",
            "We develop a natural language interface for human robot interaction that\n",
            "implements reasoning about deep semantics in natural language. To realize the\n",
            "required deep analysis, we employ methods from cognitive linguistics, namely\n",
            "the modular and compositional framework of Embodied Construction Grammar (ECG)\n",
            "[Feldman, 2009]. Using ECG, robots are able to solve fine-grained reference\n",
            "resolution problems and other issues related to deep semantics and\n",
            "compositionality of natural language. This also includes verbal interaction\n",
            "with humans to clarify commands and queries that are too ambiguous to be\n",
            "executed safely. We implement our NLU framework as a ROS package and present\n",
            "proof-of-concept scenarios with different robots, as well as a survey on the\n",
            "state of the art.\n",
            "Actual Title : \n",
            "Exploiting Deep Semantics and Compositionality of Natural Language for\n",
            "  Human-Robot-Interaction\n",
            "Generated Title : \n",
            "language semantics and compositionality with deep semantics\n",
            "Abstract : \n",
            "This paper describes a method for identification of the informative variables\n",
            "in the information system with discrete decision variables. It is targeted\n",
            "specifically towards discovery of the variables that are non-informative when\n",
            "considered alone, but are informative when the synergistic interactions between\n",
            "multiple variables are considered. To this end, the mutual entropy of all\n",
            "possible k-tuples of variables with decision variable is computed. Then, for\n",
            "each variable the maximal information gain due to interactions with other\n",
            "variables is obtained. For non-informative variables this quantity conforms to\n",
            "the well known statistical distributions. This allows for discerning truly\n",
            "informative variables from non-informative ones. For demonstration of the\n",
            "approach, the method is applied to several synthetic datasets that involve\n",
            "complex multidimensional interactions between variables. It is capable of\n",
            "identifying most important informative variables, even in the case when the\n",
            "dimensionality of the analysis is smaller than the true dimensionality of the\n",
            "problem. What is more, the high sensitivity of the algorithm allows for\n",
            "detection of the influence of nuisance variables on the response variable.\n",
            "Actual Title : \n",
            "All-relevant feature selection using multidimensional filters with\n",
            "  exhaustive search\n",
            "Generated Title : \n",
            "discovery of nonparametric filtering\n",
            "Abstract : \n",
            "Given two subsets A and B of nodes in a directed graph, the conduciveness of\n",
            "the graph from A to B is the ratio representing how many of the edges outgoing\n",
            "from nodes in A are incoming to nodes in B. When the graph's nodes stand for\n",
            "the possible solutions to certain problems of combinatorial optimization,\n",
            "choosing its edges appropriately has been shown to lead to conduciveness\n",
            "properties that provide useful insight into the performance of algorithms to\n",
            "solve those problems. Here we study the conduciveness of CA-rule graphs, that\n",
            "is, graphs whose node set is the set of all CA rules given a cell's number of\n",
            "possible states and neighborhood size. We consider several different edge sets\n",
            "interconnecting these nodes, both deterministic and random ones, and derive\n",
            "analytical expressions for the resulting graph's conduciveness toward rules\n",
            "having a fixed number of non-quiescent entries. We demonstrate that one of the\n",
            "random edge sets, characterized by allowing nodes to be sparsely interconnected\n",
            "across any Hamming distance between the corresponding rules, has the potential\n",
            "of providing reasonable conduciveness toward the desired rules. We conjecture\n",
            "that this may lie at the bottom of the best strategies known to date for\n",
            "discovering complex rules to solve specific problems, all of an evolutionary\n",
            "nature.\n",
            "Actual Title : \n",
            "The conduciveness of CA-rule graphs\n",
            "Generated Title : \n",
            "conduciveness of <unk> graphs\n",
            "Abstract : \n",
            "Artificial reinforcement learning (RL) is a widely used technique in\n",
            "artificial intelligence that provides a general method for training agents to\n",
            "perform a wide variety of behaviours. RL as used in computer science has\n",
            "striking parallels to reward and punishment learning in animal and human\n",
            "brains. I argue that present-day artificial RL agents have a very small but\n",
            "nonzero degree of ethical importance. This is particularly plausible for views\n",
            "according to which sentience comes in degrees based on the abilities and\n",
            "complexities of minds, but even binary views on consciousness should assign\n",
            "nonzero probability to RL programs having morally relevant experiences. While\n",
            "RL programs are not a top ethical priority today, they may become more\n",
            "significant in the coming decades as RL is increasingly applied to industry,\n",
            "robotics, video games, and other areas. I encourage scientists, philosophers,\n",
            "and citizens to begin a conversation about our ethical duties to reduce the\n",
            "harm that we inflict on powerless, voiceless RL agents.\n",
            "Actual Title : \n",
            "Do Artificial Reinforcement-Learning Agents Matter Morally?\n",
            "Generated Title : \n",
            "agents matter <unk>\n",
            "Abstract : \n",
            "Commercial head-mounted eye trackers provide useful features to customers in\n",
            "industry and research but are expensive and rely on closed source hardware and\n",
            "software. This limits the application areas and use of mobile eye tracking to\n",
            "expert users and inhibits user-driven development, customisation, and\n",
            "extension. In this paper we present Pupil -- an accessible, affordable, and\n",
            "extensible open source platform for mobile eye tracking and gaze-based\n",
            "interaction. Pupil comprises 1) a light-weight headset with high-resolution\n",
            "cameras, 2) an open source software framework for mobile eye tracking, as well\n",
            "as 3) a graphical user interface (GUI) to playback and visualize video and gaze\n",
            "data. Pupil features high-resolution scene and eye cameras for monocular and\n",
            "binocular gaze estimation. The software and GUI are platform-independent and\n",
            "include state-of-the-art algorithms for real-time pupil detection and tracking,\n",
            "calibration, and accurate gaze estimation. Results of a performance evaluation\n",
            "show that Pupil can provide an average gaze estimation accuracy of 0.6 degree\n",
            "of visual angle (0.08 degree precision) with a latency of the processing\n",
            "pipeline of only 0.045 seconds.\n",
            "Actual Title : \n",
            "Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile\n",
            "  Gaze-based Interaction\n",
            "Generated Title : \n",
            "panoramic camera system for mobile camera\n",
            "Abstract : \n",
            "We propose a novel method for semi-supervised learning (SSL) based on\n",
            "data-driven distributionally robust optimization (DRO) using optimal transport\n",
            "metrics. Our proposed method enhances generalization error by using the\n",
            "unlabeled data to restrict the support of the worst case distribution in our\n",
            "DRO formulation. We enable the implementation of our DRO formulation by\n",
            "proposing a stochastic gradient descent algorithm which allows to easily\n",
            "implement the training procedure. We demonstrate that our Semi-supervised DRO\n",
            "method is able to improve the generalization error over natural supervised\n",
            "procedures and state-of-the-art SSL estimators. Finally, we include a\n",
            "discussion on the large sample behavior of the optimal uncertainty region in\n",
            "the DRO formulation. Our discussion exposes important aspects such as the role\n",
            "of dimension reduction in SSL.\n",
            "Actual Title : \n",
            "Semi-supervised Learning based on Distributionally Robust Optimization\n",
            "Generated Title : \n",
            "learning based on distributionally robust optimization\n",
            "Abstract : \n",
            "Unsupervised models of dependency parsing typically require large amounts of\n",
            "clean, unlabeled data plus gold-standard part-of-speech tags. Adding indirect\n",
            "supervision (e.g. language universals and rules) can help, but we show that\n",
            "obtaining small amounts of direct supervision - here, partial dependency\n",
            "annotations - provides a strong balance between zero and full supervision. We\n",
            "adapt the unsupervised ConvexMST dependency parser to learn from partial\n",
            "dependencies expressed in the Graph Fragment Language. With less than 24 hours\n",
            "of total annotation, we obtain 7% and 17% absolute improvement in unlabeled\n",
            "dependency scores for English and Spanish, respectively, compared to the same\n",
            "parser using only universal grammar constraints.\n",
            "Actual Title : \n",
            "Fill it up: Exploiting partial dependency annotations in a minimum\n",
            "  spanning tree parser\n",
            "Generated Title : \n",
            "it up  exploiting dependency annotations in semi supervised learning\n",
            "Abstract : \n",
            "This paper introduces a simple but highly efficient ensemble for robust\n",
            "texture classification, which can effectively deal with translation, scale and\n",
            "changes of significant viewpoint problems. The proposed method first inherits\n",
            "the spirit of spatial pyramid matching model (SPM), which is popular for\n",
            "encoding spatial distribution of local features, but in a flexible way,\n",
            "partitioning the original image into different levels and incorporating\n",
            "different overlapping patterns of each level. This flexible setup helps capture\n",
            "the informative features and produces sufficient local feature codes by some\n",
            "well-chosen aggregation statistics or pooling operations within each\n",
            "partitioned region, even when only a few sample images are available for\n",
            "training. Then each texture image is represented by several orderless feature\n",
            "codes and thereby all the training data form a reliable feature pond. Finally,\n",
            "to take full advantage of this feature pond, we develop a collaborative\n",
            "representation-based strategy with locality constraint (LC-CRC) for the final\n",
            "classification, and experimental results on three well-known public texture\n",
            "datasets demonstrate the proposed approach is very competitive and even\n",
            "outperforms several state-of-the-art methods. Particularly, when only a few\n",
            "samples of each category are available for training, our approach still\n",
            "achieves very high classification performance.\n",
            "Actual Title : \n",
            "Multi-Level Feature Descriptor for Robust Texture Classification via\n",
            "  Locality-Constrained Collaborative Strategy\n",
            "Generated Title : \n",
            "classification classification using multi level feature descriptor\n",
            "Abstract : \n",
            "This paper describes the method of visualization of periodic constituents and\n",
            "instability areas in series of measurements, being based on the algorithm of\n",
            "smoothing out and concept of one-dimensional cellular automata. A method can be\n",
            "used at the analysis of temporal series, related to the volumes of thematic\n",
            "publications in web-space.\n",
            "Actual Title : \n",
            "Visualization of features of a series of measurements with\n",
            "  one-dimensional cellular structure\n",
            "Generated Title : \n",
            "of of of measurements of measurements of measurements\n",
            "Abstract : \n",
            "The superposition of temporal point processes has been studied for many\n",
            "years, although the usefulness of such models for practical applications has\n",
            "not be fully developed. We investigate superposed Hawkes process as an\n",
            "important class of such models, with properties studied in the framework of\n",
            "least squares estimation. The superposition of Hawkes processes is demonstrated\n",
            "to be beneficial for tightening the upper bound of excess risk under certain\n",
            "conditions, and we show the feasibility of the benefit in typical situations.\n",
            "The usefulness of superposed Hawkes processes is verified on synthetic data,\n",
            "and its potential to solve the cold-start problem of recommendation systems is\n",
            "demonstrated on real-world data.\n",
            "Actual Title : \n",
            "Benefits from Superposed Hawkes Processes\n",
            "Generated Title : \n",
            "from superposed hawkes processes\n",
            "Abstract : \n",
            "Understanding physical phenomena is a key competence that enables humans and\n",
            "animals to act and interact under uncertain perception in previously unseen\n",
            "environments containing novel object and their configurations. Developmental\n",
            "psychology has shown that such skills are acquired by infants from observations\n",
            "at a very early stage.\n",
            "  In this paper, we contrast a more traditional approach of taking a\n",
            "model-based route with explicit 3D representations and physical simulation by\n",
            "an end-to-end approach that directly predicts stability and related quantities\n",
            "from appearance. We ask the question if and to what extent and quality such a\n",
            "skill can directly be acquired in a data-driven way bypassing the need for an\n",
            "explicit simulation.\n",
            "  We present a learning-based approach based on simulated data that predicts\n",
            "stability of towers comprised of wooden blocks under different conditions and\n",
            "quantities related to the potential fall of the towers. The evaluation is\n",
            "carried out on synthetic data and compared to human judgments on the same\n",
            "stimuli.\n",
            "Actual Title : \n",
            "To Fall Or Not To Fall: A Visual Approach to Physical Stability\n",
            "  Prediction\n",
            "Generated Title : \n",
            "the principle of linear dynamical models by physical\n",
            "Abstract : \n",
            "Standing at the paradigm shift towards data-intensive science, machine\n",
            "learning techniques are becoming increasingly important. In particular, as a\n",
            "major breakthrough in the field, deep learning has proven as an extremely\n",
            "powerful tool in many fields. Shall we embrace deep learning as the key to all?\n",
            "Or, should we resist a 'black-box' solution? There are controversial opinions\n",
            "in the remote sensing community. In this article, we analyze the challenges of\n",
            "using deep learning for remote sensing data analysis, review the recent\n",
            "advances, and provide resources to make deep learning in remote sensing\n",
            "ridiculously simple to start with. More importantly, we advocate remote sensing\n",
            "scientists to bring their expertise into deep learning, and use it as an\n",
            "implicit general model to tackle unprecedented large-scale influential\n",
            "challenges, such as climate change and urbanization.\n",
            "Actual Title : \n",
            "Deep learning in remote sensing: a review\n",
            "Generated Title : \n",
            "learning works in deep learning for image analysis\n",
            "Abstract : \n",
            "Commute Time Distance (CTD) is a random walk based metric on graphs. CTD has\n",
            "found widespread applications in many domains including personalized search,\n",
            "collaborative filtering and making search engines robust against manipulation.\n",
            "Our interest is inspired by the use of CTD as a metric for anomaly detection.\n",
            "It has been shown that CTD can be used to simultaneously identify both global\n",
            "and local anomalies. Here we propose an accurate and efficient approximation\n",
            "for computing the CTD in an incremental fashion in order to facilitate\n",
            "real-time applications. An online anomaly detection algorithm is designed where\n",
            "the CTD of each new arriving data point to any point in the current graph can\n",
            "be estimated in constant time ensuring a real-time response. Moreover, the\n",
            "proposed approach can also be applied in many other applications that utilize\n",
            "commute time distance.\n",
            "Actual Title : \n",
            "Online Anomaly Detection Systems Using Incremental Commute Time\n",
            "Generated Title : \n",
            "anomaly online anomaly detection\n",
            "Abstract : \n",
            "The objective of the paper is to design an agent which provides efficient\n",
            "response to the caller when a call goes unanswered in smartphones. The agent\n",
            "provides responses through text messages, email etc stating the most likely\n",
            "reason as to why the callee is unable to answer a call. Responses are composed\n",
            "taking into consideration the importance of the present call and the situation\n",
            "the callee is in at the moment like driving, sleeping, at work etc. The agent\n",
            "makes decisons in the compostion of response messages based on the patterns it\n",
            "has come across in the learning environment. Initially the user helps the agent\n",
            "to compose response messages. The agent associates this message to the percept\n",
            "it recieves with respect to the environment the callee is in. The user may\n",
            "thereafter either choose to make to response system automatic or choose to\n",
            "recieve suggestions from the agent for responses messages and confirm what is\n",
            "to be sent to the caller.\n",
            "Actual Title : \n",
            "Design of an Agent for Answering Back in Smart Phones\n",
            "Generated Title : \n",
            "of of agent <unk> agent for <unk> of\n",
            "Abstract : \n",
            "We consider the problem of bottom-up compilation of knowledge bases, which is\n",
            "usually predicated on the existence of a polytime function for combining\n",
            "compilations using Boolean operators (usually called an Apply function). While\n",
            "such a polytime Apply function is known to exist for certain languages (e.g.,\n",
            "OBDDs) and not exist for others (e.g., DNNF), its existence for certain\n",
            "languages remains unknown. Among the latter is the recently introduced language\n",
            "of Sentential Decision Diagrams (SDDs), for which a polytime Apply function\n",
            "exists for unreduced SDDs, but remains unknown for reduced ones (i.e. canonical\n",
            "SDDs). We resolve this open question in this paper and consider some of its\n",
            "theoretical and practical implications. Some of the findings we report question\n",
            "the common wisdom on the relationship between bottom-up compilation, language\n",
            "canonicity and the complexity of the Apply function.\n",
            "Actual Title : \n",
            "On the Role of Canonicity in Bottom-up Knowledge Compilation\n",
            "Generated Title : \n",
            "the role of <unk> compilation of bottom up knowledge\n",
            "Abstract : \n",
            "Visual representation is crucial for a visual tracking method's performances.\n",
            "Conventionally, visual representations adopted in visual tracking rely on\n",
            "hand-crafted computer vision descriptors. These descriptors were developed\n",
            "generically without considering tracking-specific information. In this paper,\n",
            "we propose to learn complex-valued invariant representations from tracked\n",
            "sequential image patches, via strong temporal slowness constraint and stacked\n",
            "convolutional autoencoders. The deep slow local representations are learned\n",
            "offline on unlabeled data and transferred to the observational model of our\n",
            "proposed tracker. The proposed observational model retains old training samples\n",
            "to alleviate drift, and collect negative samples which are coherent with\n",
            "target's motion pattern for better discriminative tracking. With the learned\n",
            "representation and online training samples, a logistic regression classifier is\n",
            "adopted to distinguish target from background, and retrained online to adapt to\n",
            "appearance changes. Subsequently, the observational model is integrated into a\n",
            "particle filter framework to peform visual tracking. Experimental results on\n",
            "various challenging benchmark sequences demonstrate that the proposed tracker\n",
            "performs favourably against several state-of-the-art trackers.\n",
            "Actual Title : \n",
            "Self-taught learning of a deep invariant representation for visual\n",
            "  tracking via temporal slowness principle\n",
            "Generated Title : \n",
            "representation learning for visual representation for visual tracking\n",
            "Abstract : \n",
            "Diversity is an important aspect of highly efficient multi-agent teams. We\n",
            "introduce the main factors that drive a multi-agent system in either direction\n",
            "along the diversity scale. A metric for diversity is described, and we\n",
            "speculate on the concept of transient diversity. Finally, an experiment on\n",
            "social entropy using a RoboCup simulated soccer team is presented.\n",
            "Actual Title : \n",
            "Transient Diversity in Multi-Agent Systems\n",
            "Generated Title : \n",
            "diversity in multi agent systems\n",
            "Abstract : \n",
            "Intelligent agents offer a new and exciting way of understanding the world of\n",
            "work. In this paper we apply agent-based modeling and simulation to investigate\n",
            "a set of problems in a retail context. Specifically, we are working to\n",
            "understand the relationship between human resource management practices and\n",
            "retail productivity. Despite the fact we are working within a relatively novel\n",
            "and complex domain, it is clear that intelligent agents could offer potential\n",
            "for fostering sustainable organizational capabilities in the future. The\n",
            "project is still at an early stage. So far we have conducted a case study in a\n",
            "UK department store to collect data and capture impressions about operations\n",
            "and actors within departments. Furthermore, based on our case study we have\n",
            "built and tested our first version of a retail branch simulator which we will\n",
            "present in this paper.\n",
            "Actual Title : \n",
            "Using Intelligent Agents to Understand Management Practices and Retail\n",
            "  Productivity\n",
            "Generated Title : \n",
            "agents understand management practices and retail management\n",
            "Abstract : \n",
            "In parallel with the success of CNNs to solve vision problems, there is a\n",
            "growing interest in developing methodologies to understand and visualize the\n",
            "internal representations of these networks. How the responses of a trained CNN\n",
            "encode the visual information is a fundamental question both for computer and\n",
            "human vision research. Image representations provided by the first\n",
            "convolutional layer as well as the resolution change provided by the\n",
            "max-polling operation are easy to understand, however, as soon as a second and\n",
            "further convolutional layers are added in the representation, any intuition is\n",
            "lost. A usual way to deal with this problem has been to define deconvolutional\n",
            "networks that somehow allow to explore the internal representations of the most\n",
            "important activations towards the image space, where deconvolution is assumed\n",
            "as a convolution with the transposed filter. However, this assumption is not\n",
            "the best approximation of an inverse convolution. In this paper we propose a\n",
            "new assumption based on filter substitution to reverse the encoding of a\n",
            "convolutional layer. This provides us with a new tool to directly visualize any\n",
            "CNN single neuron as a filter in the first layer, this is in terms of the image\n",
            "space.\n",
            "Actual Title : \n",
            "Understanding learned CNN features through Filter Decoding with\n",
            "  Substitution\n",
            "Generated Title : \n",
            "encoding cnn activations with substitution in cnns\n",
            "Abstract : \n",
            "We present a new model-based integrative method for clustering objects given\n",
            "both vectorial data, which describes the feature of each object, and network\n",
            "data, which indicates the similarity of connected objects. The proposed general\n",
            "model is able to cluster the two types of data simultaneously within one\n",
            "integrative probabilistic model, while traditional methods can only handle one\n",
            "data type or depend on transforming one data type to another. Bayesian\n",
            "inference of the clustering is conducted based on a Markov chain Monte Carlo\n",
            "algorithm. A special case of the general model combining the Gaussian mixture\n",
            "model and the stochastic block model is extensively studied. We used both\n",
            "synthetic data and real data to evaluate this new method and compare it with\n",
            "alternative methods. The results show that our simultaneous clustering method\n",
            "performs much better. This improvement is due to the power of the model-based\n",
            "probabilistic approach for efficiently integrating information.\n",
            "Actual Title : \n",
            "A Bayesian Method for Joint Clustering of Vectorial Data and Network\n",
            "  Data\n",
            "Generated Title : \n",
            "clustering clustering for mixture of vectorial data\n",
            "Abstract : \n",
            "Hand pose estimation is to predict the pose parameters representing a 3D hand\n",
            "model, such as locations of hand joints. This problem is very challenging due\n",
            "to large changes in viewpoints and articulations, and intense self-occlusions,\n",
            "etc. Many researchers have investigated the problem from both aspects of input\n",
            "feature learning and output prediction modelling. Though effective, most of the\n",
            "existing discriminative methods only give a deterministic estimation of target\n",
            "poses. Also, due to their single-value mapping intrinsic, they fail to\n",
            "adequately handle self-occlusion problems, where occluded joints present\n",
            "multiple modes. In this paper, we tackle the self-occlusion issue and provide a\n",
            "complete description of observed poses given an input depth image through a\n",
            "hierarchical mixture density network (HMDN) framework. In particular, HMDN\n",
            "leverages the state-of-the-art CNN module to facilitate feature learning, while\n",
            "proposes a density in a two-level hierarchy to reconcile single-valued and\n",
            "multi-valued mapping in the output. The whole framework is naturally end-to-end\n",
            "trainable with a mixture of two differentiable density functions. HMDN produces\n",
            "interpretable and diverse candidate samples, and significantly outperforms the\n",
            "state-of-the-art algorithms on benchmarks that exhibit occlusions.\n",
            "Actual Title : \n",
            "Occlusion-aware Hand Pose Estimation Using Hierarchical Mixture Density\n",
            "  Network\n",
            "Generated Title : \n",
            "pose and pose estimation of human pose estimation\n",
            "Abstract : \n",
            "This paper addresses the optimal control problem known as the Linear\n",
            "Quadratic Regulator in the case when the dynamics are unknown. We propose a\n",
            "multi-stage procedure, called Coarse-ID control, that estimates a model from a\n",
            "few experimental trials, estimates the error in that model with respect to the\n",
            "truth, and then designs a controller using both the model and uncertainty\n",
            "estimate. Our technique uses contemporary tools from random matrix theory to\n",
            "bound the error in the estimation procedure. We also employ a recently\n",
            "developed approach to control synthesis called System Level Synthesis that\n",
            "enables robust control design by solving a convex optimization problem. We\n",
            "provide end-to-end bounds on the relative error in control cost that are nearly\n",
            "optimal in the number of parameters and that highlight salient properties of\n",
            "the system to be controlled such as closed-loop sensitivity and optimal control\n",
            "magnitude. We show experimentally that the Coarse-ID approach enables efficient\n",
            "computation of a stabilizing controller in regimes where simple control schemes\n",
            "that do not take the model uncertainty into account fail to stabilize the true\n",
            "system.\n",
            "Actual Title : \n",
            "On the Sample Complexity of the Linear Quadratic Regulator\n",
            "Generated Title : \n",
            "the sample complexity of linear quadratic regulator regulator\n",
            "Abstract : \n",
            "BDeu marginal likelihood score is a popular model selection criterion for\n",
            "selecting a Bayesian network structure based on sample data. This\n",
            "non-informative scoring criterion assigns same score for network structures\n",
            "that encode same independence statements. However, before applying the BDeu\n",
            "score, one must determine a single parameter, the equivalent sample size alpha.\n",
            "Unfortunately no generally accepted rule for determining the alpha parameter\n",
            "has been suggested. This is disturbing, since in this paper we show through a\n",
            "series of concrete experiments that the solution of the network structure\n",
            "optimization problem is highly sensitive to the chosen alpha parameter value.\n",
            "Based on these results, we are able to give explanations for how and why this\n",
            "phenomenon happens, and discuss ideas for solving this problem.\n",
            "Actual Title : \n",
            "On Sensitivity of the MAP Bayesian Network Structure to the Equivalent\n",
            "  Sample Size Parameter\n",
            "Generated Title : \n",
            "sensitivity of bayesian network structure in the equivalent\n",
            "Abstract : \n",
            "We study the task of cleaning scanned text documents that are strongly\n",
            "corrupted by dirt such as manual line strokes, spilled ink etc. We aim at\n",
            "autonomously removing dirt from a single letter-size page based only on the\n",
            "information the page contains. Our approach, therefore, has to learn character\n",
            "representations without supervision and requires a mechanism to distinguish\n",
            "learned representations from irregular patterns. To learn character\n",
            "representations, we use a probabilistic generative model parameterizing pattern\n",
            "features, feature variances, the features' planar arrangements, and pattern\n",
            "frequencies. The latent variables of the model describe pattern class, pattern\n",
            "position, and the presence or absence of individual pattern features. The model\n",
            "parameters are optimized using a novel variational EM approximation. After\n",
            "learning, the parameters represent, independently of their absolute position,\n",
            "planar feature arrangements and their variances. A quality measure defined\n",
            "based on the learned representation then allows for an autonomous\n",
            "discrimination between regular character patterns and the irregular patterns\n",
            "making up the dirt. The irregular patterns can thus be removed to clean the\n",
            "document. For a full Latin alphabet we found that a single page does not\n",
            "contain sufficiently many character examples. However, even if heavily\n",
            "corrupted by dirt, we show that a page containing a lower number of character\n",
            "types can efficiently and autonomously be cleaned solely based on the\n",
            "structural regularity of the characters it contains. In different examples\n",
            "using characters from different alphabets, we demonstrate generality of the\n",
            "approach and discuss its implications for future developments.\n",
            "Actual Title : \n",
            "Autonomous Cleaning of Corrupted Scanned Documents - A Generative\n",
            "  Modeling Approach\n",
            "Generated Title : \n",
            "the latent of of <unk> and <unk>\n",
            "Abstract : \n",
            "Factored neural machine translation (FNMT) is founded on the idea of using\n",
            "the morphological and grammatical decomposition of the words (factors) at the\n",
            "output side of the neural network. This architecture addresses two well-known\n",
            "problems occurring in MT, namely the size of target language vocabulary and the\n",
            "number of unknown tokens produced in the translation. FNMT system is designed\n",
            "to manage larger vocabulary and reduce the training time (for systems with\n",
            "equivalent target language vocabulary size). Moreover, we can produce\n",
            "grammatically correct words that are not part of the vocabulary. FNMT model is\n",
            "evaluated on IWSLT'15 English to French task and compared to the baseline\n",
            "word-based and BPE-based NMT systems. Promising qualitative and quantitative\n",
            "results (in terms of BLEU and METEOR) are reported.\n",
            "Actual Title : \n",
            "Neural Machine Translation by Generating Multiple Linguistic Factors\n",
            "Generated Title : \n",
            "machine translation for neural machine translation\n",
            "Abstract : \n",
            "Algorithmic image-based diagnosis and prognosis of neurodegenerative diseases\n",
            "on longitudinal data has drawn great interest from computer vision researchers.\n",
            "The current state-of-the-art models for many image classification tasks are\n",
            "based on the Convolutional Neural Networks (CNN). However, a key challenge in\n",
            "applying CNN to biological problems is that the available labeled training\n",
            "samples are very limited. Another issue for CNN to be applied in computer aided\n",
            "diagnosis applications is that to achieve better diagnosis and prognosis\n",
            "accuracy, one usually has to deal with the longitudinal dataset, i.e., the\n",
            "dataset of images scanned at different time points. Here we argue that an\n",
            "enhanced CNN model with transfer learning for the joint analysis of tasks from\n",
            "multiple time points or regions of interests may have a potential to improve\n",
            "the accuracy of computer aided diagnosis. To reach this goal, we innovate a CNN\n",
            "based deep learning multi-task dictionary learning framework to address the\n",
            "above challenges. Firstly, we pre-train CNN on the ImageNet dataset and\n",
            "transfer the knowledge from the pre-trained model to the medical imaging\n",
            "progression representation, generating the features for different tasks. Then,\n",
            "we propose a novel unsupervised learning method, termed Multi-task Stochastic\n",
            "Coordinate Coding (MSCC), for learning different tasks by using shared and\n",
            "individual dictionaries and generating the sparse features required to predict\n",
            "the future cognitive clinical scores. We apply our new model in a publicly\n",
            "available neuroimaging cohort to predict clinical measures with two different\n",
            "feature sets and compare them with seven other state-of-the-art methods. The\n",
            "experimental results show our proposed method achieved superior results.\n",
            "Actual Title : \n",
            "Multi-task Dictionary Learning based Convolutional Neural Network for\n",
            "  Computer aided Diagnosis with Longitudinal Images\n",
            "Generated Title : \n",
            "dictionary learning for convolutional neural network for remote\n",
            "Abstract : \n",
            "Mechanical learning is a computing system that is based on a set of simple\n",
            "and fixed rules, and can learn from incoming data. A learning machine is a\n",
            "system that realizes mechanical learning. Importantly, we emphasis that it is\n",
            "based on a set of simple and fixed rules, contrasting to often called machine\n",
            "learning that is sophisticated software based on very complicated mathematical\n",
            "theory, and often needs human intervene for software fine tune and manual\n",
            "adjustments. Here, we discuss some basic facts and principles of such system,\n",
            "and try to lay down a framework for further study. We propose 2 directions to\n",
            "approach mechanical learning, just like Church-Turing pair: one is trying to\n",
            "realize a learning machine, another is trying to well describe the mechanical\n",
            "learning.\n",
            "Actual Title : \n",
            "Discussion on Mechanical Learning and Learning Machine\n",
            "Generated Title : \n",
            "mechanical mechanical learning machine learning machine learning machine\n",
            "Abstract : \n",
            "This work introduces a novel framework for quantifying the presence and\n",
            "strength of recurrent dynamics in video data. Specifically, we provide\n",
            "continuous measures of periodicity (perfect repetition) and quasiperiodicity\n",
            "(superposition of periodic modes with non-commensurate periods), in a way which\n",
            "does not require segmentation, training, object tracking or 1-dimensional\n",
            "surrogate signals. Our methodology operates directly on video data. The\n",
            "approach combines ideas from nonlinear time series analysis (delay embeddings)\n",
            "and computational topology (persistent homology), by translating the problem of\n",
            "finding recurrent dynamics in video data, into the problem of determining the\n",
            "circularity or toroidality of an associated geometric space. Through extensive\n",
            "testing, we show the robustness of our scores with respect to several noise\n",
            "models/levels, we show that our periodicity score is superior to other methods\n",
            "when compared to human-generated periodicity rankings, and furthermore, we show\n",
            "that our quasiperiodicity score clearly indicates the presence of biphonation\n",
            "in videos of vibrating vocal folds, which has never before been accomplished\n",
            "end to end quantitatively.\n",
            "Actual Title : \n",
            "(Quasi)Periodicity Quantification in Video Data, Using Topology\n",
            "Generated Title : \n",
            "analysis in video in video with recurrent neural\n",
            "Abstract : \n",
            "We propose an image representation and matching approach that substantially\n",
            "improves visual-based location estimation for images. The main novelty of the\n",
            "approach, called distinctive visual element matching (DVEM), is its use of\n",
            "representations that are specific to the query image whose location is being\n",
            "predicted. These representations are based on visual element clouds, which\n",
            "robustly capture the connection between the query and visual evidence from\n",
            "candidate locations. We then maximize the influence of visual elements that are\n",
            "geo-distinctive because they do not occur in images taken at many other\n",
            "locations. We carry out experiments and analysis for both geo-constrained and\n",
            "geo-unconstrained location estimation cases using two large-scale,\n",
            "publicly-available datasets: the San Francisco Landmark dataset with $1.06$\n",
            "million street-view images and the MediaEval '15 Placing Task dataset with\n",
            "$5.6$ million geo-tagged images from Flickr. We present examples that\n",
            "illustrate the highly-transparent mechanics of the approach, which are based on\n",
            "common sense observations about the visual patterns in image collections. Our\n",
            "results show that the proposed method delivers a considerable performance\n",
            "improvement compared to the state of the art.\n",
            "Actual Title : \n",
            "Geo-distinctive Visual Element Matching for Location Estimation of\n",
            "  Images\n",
            "Generated Title : \n",
            "visual element for for location of #d images\n",
            "Abstract : \n",
            "In this doctoral thesis, we apply premises of cognitive linguistics to\n",
            "terminological definitions and present a proposal called the flexible\n",
            "terminological definition. This consists of a set of definitions of the same\n",
            "concept made up of a general definition (in this case, one encompassing the\n",
            "entire environmental domain) along with additional definitions describing the\n",
            "concept from the perspective of the subdomains in which it is relevant. Since\n",
            "context is a determining factor in the construction of the meaning of lexical\n",
            "units (including terms), we assume that terminological definitions can, and\n",
            "should, reflect the effects of context, even though definitions have\n",
            "traditionally been treated as the expression of meaning void of any contextual\n",
            "effect. The main objective of this thesis is to analyze the effects of\n",
            "contextual variation on specialized environmental concepts with a view to their\n",
            "representation in terminological definitions. Specifically, we focused on\n",
            "contextual variation based on thematic restrictions. To accomplish the\n",
            "objectives of this doctoral thesis, we conducted an empirical study consisting\n",
            "of the analysis of a set of contextually variable concepts and the creation of\n",
            "a flexible definition for two of them. As a result of the first part of our\n",
            "empirical study, we divided our notion of domain-dependent contextual variation\n",
            "into three different phenomena: modulation, perspectivization and\n",
            "subconceptualization. These phenomena are additive in that all concepts\n",
            "experience modulation, some concepts also undergo perspectivization, and\n",
            "finally, a small number of concepts are additionally subjected to\n",
            "subconceptualization. In the second part, we applied these notions to\n",
            "terminological definitions and we presented we presented guidelines on how to\n",
            "build flexible definitions, from the extraction of knowledge to the actual\n",
            "writing of the definition.\n",
            "Actual Title : \n",
            "La representacin de la variacin contextual mediante definiciones\n",
            "  terminolgicas flexibles\n",
            "Generated Title : \n",
            "  <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "Abstract : \n",
            "The inherent noise in the observed (e.g., scanned) binary document image\n",
            "degrades the image quality and harms the compression ratio through breaking the\n",
            "pattern repentance and adding entropy to the document images. In this paper, we\n",
            "design a cost function in Bayesian framework with dictionary learning.\n",
            "Minimizing our cost function produces a restored image which has better quality\n",
            "than that of the observed noisy image, and a dictionary for representing and\n",
            "encoding the image. After the restoration, we use this dictionary (from the\n",
            "same cost function) to encode the restored image following the\n",
            "symbol-dictionary framework by JBIG2 standard with the lossless mode.\n",
            "Experimental results with a variety of document images demonstrate that our\n",
            "method improves the image quality compared with the observed image, and\n",
            "simultaneously improves the compression ratio. For the test images with\n",
            "synthetic noise, our method reduces the number of flipped pixels by 48.2% and\n",
            "improves the compression ratio by 36.36% as compared with the best encoding\n",
            "methods. For the test images with real noise, our method visually improves the\n",
            "image quality, and outperforms the cutting-edge method by 28.27% in terms of\n",
            "the compression ratio.\n",
            "Actual Title : \n",
            "Model-based Iterative Restoration for Binary Document Image Compression\n",
            "  with Dictionary Learning\n",
            "Generated Title : \n",
            "compression and <unk> for image restoration\n",
            "Abstract : \n",
            "This paper proposes an image dehazing model built with a convolutional neural\n",
            "network (CNN), called All-in-One Dehazing Network (AOD-Net). It is designed\n",
            "based on a re-formulated atmospheric scattering model. Instead of estimating\n",
            "the transmission matrix and the atmospheric light separately as most previous\n",
            "models did, AOD-Net directly generates the clean image through a light-weight\n",
            "CNN. Such a novel end-to-end design makes it easy to embed AOD-Net into other\n",
            "deep models, e.g., Faster R-CNN, for improving high-level task performance on\n",
            "hazy images. Experimental results on both synthesized and natural hazy image\n",
            "datasets demonstrate our superior performance than the state-of-the-art in\n",
            "terms of PSNR, SSIM and the subjective visual quality. Furthermore, when\n",
            "concatenating AOD-Net with Faster R-CNN and training the joint pipeline from\n",
            "end to end, we witness a large improvement of the object detection performance\n",
            "on hazy images.\n",
            "Actual Title : \n",
            "An All-in-One Network for Dehazing and Beyond\n",
            "Generated Title : \n",
            "connected network network for dehazing and beyond\n",
            "Abstract : \n",
            "Our team won the second prize of the Safe Aging with SPHERE Challenge\n",
            "organized by SPHERE, in conjunction with ECML-PKDD and Driven Data. The goal of\n",
            "the competition was to recognize activities performed by humans, using sensor\n",
            "data. This paper presents our solution. It is based on a rich pre-processing\n",
            "and state of the art machine learning methods. From the raw train data, we\n",
            "generate a synthetic train set with the same statistical characteristics as the\n",
            "test set. We then perform feature engineering. The machine learning modeling\n",
            "part is based on stacking weak learners through a grid searched XGBoost\n",
            "algorithm. Finally, we use post-processing to smooth our predictions over time.\n",
            "Actual Title : \n",
            "Dataiku's Solution to SPHERE's Activity Recognition Challenge\n",
            "Generated Title : \n",
            "learning for real time activity recognition challenge\n",
            "Abstract : \n",
            "We discuss the evolution of aspects of nonmonotonic reasoning towards the\n",
            "computational paradigm of answer-set programming (ASP). We give a general\n",
            "overview of the roots of ASP and follow up with the personal perspective on\n",
            "research developments that helped verbalize the main principles of ASP and\n",
            "differentiated it from the classical logic programming.\n",
            "Actual Title : \n",
            "Origins of Answer-Set Programming - Some Background And Two Personal\n",
            "  Accounts\n",
            "Generated Title : \n",
            "of answer set programming     preliminary report\n",
            "Abstract : \n",
            "In this paper, we show how absolute orientation measurements provided by\n",
            "low-cost but high-fidelity IMU sensors can be integrated into the KinectFusion\n",
            "pipeline. We show that integration improves both runtime, robustness and\n",
            "quality of the 3D reconstruction. In particular, we use this orientation data\n",
            "to seed and regularize the ICP registration technique. We also present a\n",
            "technique to filter the pairs of 3D matched points based on the distribution of\n",
            "their distances. This filter is implemented efficiently on the GPU. Estimating\n",
            "the distribution of the distances helps control the number of iterations\n",
            "necessary for the convergence of the ICP algorithm. Finally, we show\n",
            "experimental results that highlight improvements in robustness, a speed-up of\n",
            "almost 12%, and a gain in tracking quality of 53% for the ATE metric on the\n",
            "Freiburg benchmark.\n",
            "Actual Title : \n",
            "Integration of Absolute Orientation Measurements in the KinectFusion\n",
            "  Reconstruction pipeline\n",
            "Generated Title : \n",
            "the of absolute orientation measurements in the kinectfusion\n",
            "Abstract : \n",
            "We propose a general class of language models that treat reference as an\n",
            "explicit stochastic latent variable. This architecture allows models to create\n",
            "mentions of entities and their attributes by accessing external databases\n",
            "(required by, e.g., dialogue generation and recipe generation) and internal\n",
            "state (required by, e.g. language models which are aware of coreference). This\n",
            "facilitates the incorporation of information that can be accessed in\n",
            "predictable locations in databases or discourse context, even when the targets\n",
            "of the reference may be rare words. Experiments on three tasks shows our model\n",
            "variants based on deterministic attention.\n",
            "Actual Title : \n",
            "Reference-Aware Language Models\n",
            "Generated Title : \n",
            "language models\n",
            "Abstract : \n",
            "The most direct way to express arbitrary dependencies in datasets is to\n",
            "estimate the joint distribution and to apply afterwards the argmax-function to\n",
            "obtain the mode of the corresponding conditional distribution. This method is\n",
            "in practice difficult, because it requires a global optimization of a\n",
            "complicated function, the joint distribution by fixed input variables. This\n",
            "article proposes a method for finding global maxima if the joint distribution\n",
            "is modeled by a kernel density estimation. Some experiments show advantages and\n",
            "shortcomings of the resulting regression method in comparison to the standard\n",
            "Nadaraya-Watson regression technique, which approximates the optimum by the\n",
            "expectation value.\n",
            "Actual Title : \n",
            "Kernel Regression by Mode Calculation of the Conditional Probability\n",
            "  Distribution\n",
            "Generated Title : \n",
            "distribution estimation of conditional density estimation\n",
            "Abstract : \n",
            "This paper presents a renewed overview of photosensor oculography (PSOG), an\n",
            "eye-tracking technique based on the principle of using simple photosensors to\n",
            "measure the amount of reflected (usually infrared) light when the eye rotates.\n",
            "Photosensor oculography can provide measurements with high precision, low\n",
            "latency and reduced power consumption, and thus it appears as an attractive\n",
            "option for performing eye-tracking in the emerging head-mounted interaction\n",
            "devices, e.g. augmented and virtual reality (AR/VR) headsets. In our current\n",
            "work we employ an adjustable simulation framework as a common basis for\n",
            "performing an exploratory study of the eye-tracking behavior of different\n",
            "photosensor oculography designs. With the performed experiments we explore the\n",
            "effects from the variation of some basic parameters of the designs on the\n",
            "resulting accuracy and cross-talk, which are crucial characteristics for the\n",
            "seamless operation of human-computer interaction applications based on\n",
            "eye-tracking. Our experimental results reveal the design trade-offs that need\n",
            "to be adopted to tackle the competing conditions that lead to optimum\n",
            "performance of different eye-tracking characteristics. We also present the\n",
            "transformations that arise in the eye-tracking output when sensor shifts occur,\n",
            "and assess the resulting degradation in accuracy for different combinations of\n",
            "eye movements and sensor shifts.\n",
            "Actual Title : \n",
            "Photosensor Oculography: Survey and Parametric Analysis of Designs using\n",
            "  Model-Based Simulation\n",
            "Generated Title : \n",
            "<unk> and evaluation of designs using particle swarm\n",
            "Abstract : \n",
            "Many models of interest in the natural and social sciences have no\n",
            "closed-form likelihood function, which means that they cannot be treated using\n",
            "the usual techniques of statistical inference. In the case where such models\n",
            "can be efficiently simulated, Bayesian inference is still possible thanks to\n",
            "the Approximate Bayesian Computation (ABC) algorithm. Although many refinements\n",
            "have been suggested, ABC inference is still far from routine. ABC is often\n",
            "excruciatingly slow due to very low acceptance rates. In addition, ABC requires\n",
            "introducing a vector of \"summary statistics\", the choice of which is relatively\n",
            "arbitrary, and often require some trial and error, making the whole process\n",
            "quite laborious for the user.\n",
            "  We introduce in this work the EP-ABC algorithm, which is an adaptation to the\n",
            "likelihood-free context of the variational approximation algorithm known as\n",
            "Expectation Propagation (Minka, 2001). The main advantage of EP-ABC is that it\n",
            "is faster by a few orders of magnitude than standard algorithms, while\n",
            "producing an overall approximation error which is typically negligible. A\n",
            "second advantage of EP-ABC is that it replaces the usual global ABC constraint\n",
            "on the vector of summary statistics computed on the whole dataset, by n local\n",
            "constraints of the form that apply separately to each data-point. As a\n",
            "consequence, it is often possible to do away with summary statistics entirely.\n",
            "In that case, EP-ABC approximates directly the evidence (marginal likelihood)\n",
            "of the model.\n",
            "  Comparisons are performed in three real-world applications which are typical\n",
            "of likelihood-free inference, including one application in neuroscience which\n",
            "is novel, and possibly too challenging for standard ABC techniques.\n",
            "Actual Title : \n",
            "Expectation-Propagation for Likelihood-Free Inference\n",
            "Generated Title : \n",
            "for likelihood free inference\n",
            "Abstract : \n",
            "In this paper we develop a dynamic form of Bayesian optimization for machine\n",
            "learning models with the goal of rapidly finding good hyperparameter settings.\n",
            "Our method uses the partial information gained during the training of a machine\n",
            "learning model in order to decide whether to pause training and start a new\n",
            "model, or resume the training of a previously-considered model. We specifically\n",
            "tailor our method to machine learning problems by developing a novel\n",
            "positive-definite covariance kernel to capture a variety of training curves.\n",
            "Furthermore, we develop a Gaussian process prior that scales gracefully with\n",
            "additional temporal observations. Finally, we provide an information-theoretic\n",
            "framework to automate the decision process. Experiments on several common\n",
            "machine learning models show that our approach is extremely effective in\n",
            "practice.\n",
            "Actual Title : \n",
            "Freeze-Thaw Bayesian Optimization\n",
            "Generated Title : \n",
            "bayesian optimization for learning <unk> learning\n",
            "Abstract : \n",
            "We are presenting work on recognising acronyms of the form Long-Form\n",
            "(Short-Form) such as \"International Monetary Fund (IMF)\" in millions of news\n",
            "articles in twenty-two languages, as part of our more general effort to\n",
            "recognise entities and their variants in news text and to use them for the\n",
            "automatic analysis of the news, including the linking of related news across\n",
            "languages. We show how the acronym recognition patterns, initially developed\n",
            "for medical terms, needed to be adapted to the more general news domain and we\n",
            "present evaluation results. We describe our effort to automatically merge the\n",
            "numerous long-form variants referring to the same short-form, while keeping\n",
            "non-related long-forms separate. Finally, we provide extensive statistics on\n",
            "the frequency and the distribution of short-form/long-form pairs across\n",
            "languages.\n",
            "Actual Title : \n",
            "Acronym recognition and processing in 22 languages\n",
            "Generated Title : \n",
            "recognition in in language  processing\n",
            "Abstract : \n",
            "The recent, remarkable growth of machine learning has led to intense interest\n",
            "in the privacy of the data on which machine learning relies, and to new\n",
            "techniques for preserving privacy. However, older ideas about privacy may well\n",
            "remain valid and useful. This note reviews two recent works on privacy in the\n",
            "light of the wisdom of some of the early literature, in particular the\n",
            "principles distilled by Saltzer and Schroeder in the 1970s.\n",
            "Actual Title : \n",
            "On the Protection of Private Information in Machine Learning Systems:\n",
            "  Two Recent Approaches\n",
            "Generated Title : \n",
            "<unk> <unk> <unk> <unk> <unk>\n",
            "Abstract : \n",
            "The field of Automatic Facial Expression Analysis has grown rapidly in recent\n",
            "years. However, despite progress in new approaches as well as benchmarking\n",
            "efforts, most evaluations still focus on either posed expressions, near-frontal\n",
            "recordings, or both. This makes it hard to tell how existing expression\n",
            "recognition approaches perform under conditions where faces appear in a wide\n",
            "range of poses (or camera views), displaying ecologically valid expressions.\n",
            "The main obstacle for assessing this is the availability of suitable data, and\n",
            "the challenge proposed here addresses this limitation. The FG 2017 Facial\n",
            "Expression Recognition and Analysis challenge (FERA 2017) extends FERA 2015 to\n",
            "the estimation of Action Units occurrence and intensity under different camera\n",
            "views. In this paper we present the third challenge in automatic recognition of\n",
            "facial expressions, to be held in conjunction with the 12th IEEE conference on\n",
            "Face and Gesture Recognition, May 2017, in Washington, United States. Two\n",
            "sub-challenges are defined: the detection of AU occurrence, and the estimation\n",
            "of AU intensity. In this work we outline the evaluation protocol, the data\n",
            "used, and the results of a baseline method for both sub-challenges.\n",
            "Actual Title : \n",
            "FERA 2017 - Addressing Head Pose in the Third Facial Expression\n",
            "  Recognition and Analysis Challenge\n",
            "Generated Title : \n",
            "of the and recognition of the <unk> of\n",
            "Abstract : \n",
            "We present a loss function for neural networks that encompasses an idea of\n",
            "trivial versus non-trivial predictions, such that the network jointly\n",
            "determines its own prediction goals and learns to satisfy them. This permits\n",
            "the network to choose sub-sets of a problem which are most amenable to its\n",
            "abilities to focus on solving, while discarding 'distracting' elements that\n",
            "interfere with its learning. To do this, the network first transforms the raw\n",
            "data into a higher-level categorical representation, and then trains a\n",
            "predictor from that new time series to its future. To prevent a trivial\n",
            "solution of mapping the signal to zero, we introduce a measure of\n",
            "non-triviality via a contrast between the prediction error of the learned model\n",
            "with a naive model of the overall signal statistics. The transform can learn to\n",
            "discard uninformative and unpredictable components of the signal in favor of\n",
            "the features which are both highly predictive and highly predictable. This\n",
            "creates a coarse-grained model of the time-series dynamics, focusing on\n",
            "predicting the slowly varying latent parameters which control the statistics of\n",
            "the time-series, rather than predicting the fast details directly. The result\n",
            "is a semi-supervised algorithm which is capable of extracting latent\n",
            "parameters, segmenting sections of time-series with differing statistics, and\n",
            "building a higher-level representation of the underlying dynamics from\n",
            "unlabeled data.\n",
            "Actual Title : \n",
            "Neural Coarse-Graining: Extracting slowly-varying latent degrees of\n",
            "  freedom with neural networks\n",
            "Generated Title : \n",
            "the training of neural networks\n",
            "Abstract : \n",
            "Information on the web is prodigious; searching relevant information is\n",
            "difficult making web users to rely on search engines for finding relevant\n",
            "information on the web. Search engines index and categorize web pages according\n",
            "to their contents using crawlers and rank them accordingly. For given user\n",
            "query they retrieve millions of webpages and display them to users according to\n",
            "web-page rank. Every search engine has their own algorithms based on certain\n",
            "parameters for ranking web-pages. Search Engine Optimization (SEO) is that\n",
            "technique by which webmasters try to improve ranking of their websites by\n",
            "optimizing it according to search engines ranking parameters. It is the aim of\n",
            "this research to identify the most popular SEO techniques used by search\n",
            "engines for ranking web-pages and to establish their importance for indexing\n",
            "and categorizing web data. The research tries to establish that using more SEO\n",
            "parameters in ranking algorithms helps in retrieving better search results thus\n",
            "increasing user satisfaction.\n",
            "  In the accomplished research, a web based Meta search engine is proposed to\n",
            "aggregates search results from different search engines and rank web-pages\n",
            "based on new page ranking algorithm which will assign heuristic page rank to\n",
            "web-pages based on SEO parameters such as title tag, Meta description, sitemap\n",
            "etc. The research also provides insight into techniques which webmasters can\n",
            "use for better ranking their websites in Google and Bing.\n",
            "  Initial results has shown that using certain SEO parameters in present\n",
            "ranking algorithm helps in retrieving more useful results for user queries.\n",
            "These results generated from Meta search engine outperformed existing search\n",
            "engines in terms of better retrieved search results and high precision.\n",
            "Actual Title : \n",
            "Intelligent Search Optimization using Artificial Fuzzy Logics\n",
            "Generated Title : \n",
            "search using using intelligent search and search\n",
            "Abstract : \n",
            "Discussion forums are an important source of information. They are often used\n",
            "to answer specific questions a user might have and to discover more about a\n",
            "topic of interest. Discussions in these forums may evolve in intricate ways,\n",
            "making it difficult for users to follow the flow of ideas. We propose a novel\n",
            "approach for automatically identifying the underlying thread structure of a\n",
            "forum discussion. Our approach is based on a neural model that computes\n",
            "coherence scores of possible reconstructions and then selects the highest\n",
            "scoring, i.e., the most coherent one. Preliminary experiments demonstrate\n",
            "promising results outperforming a number of strong baseline methods.\n",
            "Actual Title : \n",
            "Thread Reconstruction in Conversational Data using Neural Coherence\n",
            "  Models\n",
            "Generated Title : \n",
            "reconstruction using using neural networks\n",
            "Abstract : \n",
            "PARAFAC2 has demonstrated success in modeling irregular tensors, where the\n",
            "tensor dimensions vary across one of the modes. An example scenario is jointly\n",
            "modeling treatments across a set of patients with varying number of medical\n",
            "encounters, where the alignment of events in time bears no clinical meaning,\n",
            "and it may also be impossible to align them due to their varying length.\n",
            "Despite recent improvements on scaling up unconstrained PARAFAC2, its model\n",
            "factors are usually dense and sensitive to noise which limits their\n",
            "interpretability. As a result, the following open challenges remain: a) various\n",
            "modeling constraints, such as temporal smoothness, sparsity and non-negativity,\n",
            "are needed to be imposed for interpretable temporal modeling and b) a scalable\n",
            "approach is required to support those constraints efficiently for large\n",
            "datasets. To tackle these challenges, we propose a COnstrained PARAFAC2 (COPA)\n",
            "method, which carefully incorporates optimization constraints such as temporal\n",
            "smoothness, sparsity, and non-negativity in the resulting factors. To\n",
            "efficiently support all those constraints, COPA adopts a hybrid optimization\n",
            "framework using alternating optimization and alternating direction method of\n",
            "multiplier (AO-ADMM). As evaluated on large electronic health record (EHR)\n",
            "datasets with hundreds of thousands of patients, COPA achieves significant\n",
            "speedups (up to 36x faster) over prior PARAFAC2 approaches that only attempt to\n",
            "handle a subset of the constraints that COPA enables. Overall, our method\n",
            "outperforms all the baselines attempting to handle a subset of the constraints\n",
            "in terms of speed, while achieving the same level of accuracy.\n",
            "Actual Title : \n",
            "COPA: Constrained PARAFAC2 for Sparse & Large Datasets\n",
            "Generated Title : \n",
            "constrained parafac# for sparse sparse datasets\n",
            "Abstract : \n",
            "Although face recognition has been improved much as the development of Deep\n",
            "Neural Networks, SIPP(Single Image Per Person) problem in face recognition has\n",
            "not been better solved, especially in practical applications where searching\n",
            "over complicated database. In this paper, a combination of modified mean search\n",
            "and LSH method would be introduced orderly to improve the precision and recall\n",
            "of SIPP face recognition without retrain of the DNN model. First, a modified\n",
            "SVD based augmentation method would be introduced to get more intra-class\n",
            "variations even for person with only one image. Second, an unique rule based\n",
            "combination of modified mean search and LSH method was proposed the first time\n",
            "to help get the most similar personID in a complicated dataset, and some\n",
            "theoretical explaining followed. Third, we would like to emphasize, no need to\n",
            "retrain of the DNN model and would easy to be extended without much efforts. We\n",
            "do some practical testing in competition of Msceleb challenge-2 2017 which was\n",
            "hold by Microsoft Research, great improvement of coverage from 13.39% to\n",
            "19.25%, 29.94%, 42.11%, 47.52% at precision 99%(P99) would be shown latter,\n",
            "coverage reach 94.2% and 100% at precision 97%(P97) and 95%(P95) respectively.\n",
            "As far as we known, this is the only paper who do not fine-tuning on\n",
            "competition dataset and ranked top-10. A similar test on CASIA WebFace dataset\n",
            "also demonstrated the same improvements on both precision and recall.\n",
            "Actual Title : \n",
            "Improving precision and recall of face recognition in SIPP with\n",
            "  combination of modified mean search and LSH\n",
            "Generated Title : \n",
            "optimized for improving face recognition recognition and recall\n",
            "Abstract : \n",
            "We propose a novel word embedding pre-training approach that exploits writing\n",
            "errors in learners' scripts. We compare our method to previous models that tune\n",
            "the embeddings based on script scores and the discrimination between correct\n",
            "and corrupt word contexts in addition to the generic commonly-used embeddings\n",
            "pre-trained on large corpora. The comparison is achieved by using the\n",
            "aforementioned models to bootstrap a neural network that learns to predict a\n",
            "holistic score for scripts. Furthermore, we investigate augmenting our model\n",
            "with error corrections and monitor the impact on performance. Our results show\n",
            "that our error-oriented approach outperforms other comparable ones which is\n",
            "further demonstrated when training on more data. Additionally, extending the\n",
            "model with corrections provides further performance gains when data sparsity is\n",
            "an issue.\n",
            "Actual Title : \n",
            "An Error-Oriented Approach to Word Embedding Pre-Training\n",
            "Generated Title : \n",
            "embeddings of word embeddings neural neural word embeddings\n",
            "Abstract : \n",
            "A standard approach to approximate inference in state-space models isto apply\n",
            "a particle filter, e.g., the Condensation Algorithm.However, the performance of\n",
            "particle filters often varies significantlydue to their stochastic nature.We\n",
            "present a class of algorithms, called lattice particle filters, thatcircumvent\n",
            "this difficulty by placing the particles deterministicallyaccording to a\n",
            "Quasi-Monte Carlo integration rule.We describe a practical realization of this\n",
            "idea, discuss itstheoretical properties, and its efficiency.Experimental\n",
            "results with a synthetic 2D tracking problem show that thelattice particle\n",
            "filter is equivalent to a conventional particle filterthat has between 10 and\n",
            "60% more particles, depending ontheir \"sparsity\" in the state-space.We also\n",
            "present results on inferring 3D human motion frommoving light displays.\n",
            "Actual Title : \n",
            "Lattice Particle Filters\n",
            "Generated Title : \n",
            "particle filters and <unk> lattice\n",
            "Abstract : \n",
            "One of the most famous drawings by Leonardo da Vinci is a self-portrait in\n",
            "red chalk, where he looks quite old. In fact, there is a sketch in one of his\n",
            "notebooks, partially covered by written notes, that can be a self-portrait of\n",
            "the artist when he was young. The use of image processing, to remove the\n",
            "handwritten text and improve the image, allows a comparison of the two\n",
            "portraits.\n",
            "Actual Title : \n",
            "A self-portrait of young Leonardo\n",
            "Generated Title : \n",
            "self portrait of young leonardo\n",
            "Abstract : \n",
            "Recovering matrices from compressive and grossly corrupted observations is a\n",
            "fundamental problem in robust statistics, with rich applications in computer\n",
            "vision and machine learning. In theory, under certain conditions, this problem\n",
            "can be solved in polynomial time via a natural convex relaxation, known as\n",
            "Compressive Principal Component Pursuit (CPCP). However, all existing provable\n",
            "algorithms for CPCP suffer from superlinear per-iteration cost, which severely\n",
            "limits their applicability to large scale problems. In this paper, we propose\n",
            "provable, scalable and efficient methods to solve CPCP with (essentially)\n",
            "linear per-iteration cost. Our method combines classical ideas from Frank-Wolfe\n",
            "and proximal methods. In each iteration, we mainly exploit Frank-Wolfe to\n",
            "update the low-rank component with rank-one SVD and exploit the proximal step\n",
            "for the sparse term. Convergence results and implementation details are also\n",
            "discussed. We demonstrate the scalability of the proposed approach with\n",
            "promising numerical experiments on visual data.\n",
            "Actual Title : \n",
            "Scalable Robust Matrix Recovery: Frank-Wolfe Meets Proximal Methods\n",
            "Generated Title : \n",
            "robust scalable recovery  frank wolfe via proximal methods\n",
            "Abstract : \n",
            "A semi-parametric, non-linear regression model in the presence of latent\n",
            "variables is introduced. These latent variables can correspond to unmodeled\n",
            "phenomena or unmeasured agents in a complex networked system. This new\n",
            "formulation allows joint estimation of certain non-linearities in the system,\n",
            "the direct interactions between measured variables, and the effects of\n",
            "unmodeled elements on the observed system. The particular form of the model\n",
            "adopted is justified, and learning is posed as a regularized maximum likelihood\n",
            "estimation. This leads to classes of structured convex optimization problems\n",
            "with a \"sparse plus low-rank\" flavor. Relations between the proposed model and\n",
            "several common model paradigms, such as those of Robust Principal Component\n",
            "Analysis (PCA) and Vector Autoregression (VAR), are established. Particularly\n",
            "in the VAR setting, the low-rank contributions can come from broad trends\n",
            "exhibited in the time series. Details of the algorithm for learning the model\n",
            "are presented. Experiments demonstrate the performance of the model and the\n",
            "estimation algorithm on simulated and real data.\n",
            "Actual Title : \n",
            "SILVar: Single Index Latent Variable Models\n",
            "Generated Title : \n",
            "latent variable <unk> latent variable models\n",
            "Abstract : \n",
            "An inductive probabilistic classification rule must generally obey the\n",
            "principles of Bayesian predictive inference, such that all observed and\n",
            "unobserved stochastic quantities are jointly modeled and the parameter\n",
            "uncertainty is fully acknowledged through the posterior predictive\n",
            "distribution. Several such rules have been recently considered and their\n",
            "asymptotic behavior has been characterized under the assumption that the\n",
            "observed features or variables used for building a classifier are conditionally\n",
            "independent given a simultaneous labeling of both the training samples and\n",
            "those from an unknown origin. Here we extend the theoretical results to\n",
            "predictive classifiers acknowledging feature dependencies either through\n",
            "graphical models or sparser alternatives defined as stratified graphical\n",
            "models. We also show through experimentation with both synthetic and real data\n",
            "that the predictive classifiers based on stratified graphical models have\n",
            "consistently best accuracy compared with the predictive classifiers based on\n",
            "either conditionally independent features or on ordinary graphical models.\n",
            "Actual Title : \n",
            "Marginal and simultaneous predictive classification using stratified\n",
            "  graphical models\n",
            "Generated Title : \n",
            "of predictive models models\n",
            "Abstract : \n",
            "Learning by children and animals occurs effortlessly and largely without\n",
            "obvious supervision. Successes in automating supervised learning have not\n",
            "translated to the more ambiguous realm of unsupervised learning where goals and\n",
            "labels are not provided. Barlow (1961) suggested that the signal that brains\n",
            "leverage for unsupervised learning is dependence, or redundancy, in the sensory\n",
            "environment. Dependence can be characterized using the information-theoretic\n",
            "multivariate mutual information measure called total correlation. The principle\n",
            "of Total Cor-relation Ex-planation (CorEx) is to learn representations of data\n",
            "that \"explain\" as much dependence in the data as possible. We review some\n",
            "manifestations of this principle along with successes in unsupervised learning\n",
            "problems across diverse domains including human behavior, biology, and\n",
            "language.\n",
            "Actual Title : \n",
            "Unsupervised Learning via Total Correlation Explanation\n",
            "Generated Title : \n",
            "the benefits of <unk> in in learning\n",
            "Abstract : \n",
            "The heuristic identification of peaks from noisy complex spectra often leads\n",
            "to misunderstanding of the physical and chemical properties of matter. In this\n",
            "paper, we propose a framework based on Bayesian inference, which enables us to\n",
            "separate multipeak spectra into single peaks statistically and consists of two\n",
            "steps. The first step is estimating both the noise variance and the number of\n",
            "peaks as hyperparameters based on Bayes free energy, which generally is not\n",
            "analytically tractable. The second step is fitting the parameters of each peak\n",
            "function to the given spectrum by calculating the posterior density, which has\n",
            "a problem of local minima and saddles since multipeak models are nonlinear and\n",
            "hierarchical. Our framework enables the escape from local minima or saddles by\n",
            "using the exchange Monte Carlo method and calculates Bayes free energy via the\n",
            "multiple histogram method. We discuss a simulation demonstrating how efficient\n",
            "our framework is and show that estimating both the noise variance and the\n",
            "number of peaks prevents overfitting, overpenalizing, and misunderstanding the\n",
            "precision of parameter estimation.\n",
            "Actual Title : \n",
            "Simultaneous Estimation of Noise Variance and Number of Peaks in\n",
            "  Bayesian Spectral Deconvolution\n",
            "Generated Title : \n",
            "estimation of noise variance and number of peaks\n",
            "Abstract : \n",
            "This paper proposes an unsupervised learning technique by using Multi-layer\n",
            "Mirroring Neural Network and Forgy's clustering algorithm. Multi-layer\n",
            "Mirroring Neural Network is a neural network that can be trained with\n",
            "generalized data inputs (different categories of image patterns) to perform\n",
            "non-linear dimensionality reduction and the resultant low-dimensional code is\n",
            "used for unsupervised pattern classification using Forgy's algorithm. By\n",
            "adapting the non-linear activation function (modified sigmoidal function) and\n",
            "initializing the weights and bias terms to small random values, mirroring of\n",
            "the input pattern is initiated. In training, the weights and bias terms are\n",
            "changed in such a way that the input presented is reproduced at the output by\n",
            "back propagating the error. The mirroring neural network is capable of reducing\n",
            "the input vector to a great degree (approximately 1/30th the original size) and\n",
            "also able to reconstruct the input pattern at the output layer from this\n",
            "reduced code units. The feature set (output of central hidden layer) extracted\n",
            "from this network is fed to Forgy's algorithm, which classify input data\n",
            "patterns into distinguishable classes. In the implementation of Forgy's\n",
            "algorithm, initial seed points are selected in such a way that they are distant\n",
            "enough to be perfectly grouped into different categories. Thus a new method of\n",
            "unsupervised learning is formulated and demonstrated in this paper. This method\n",
            "gave impressive results when applied to classification of different image\n",
            "patterns.\n",
            "Actual Title : \n",
            "Automatic Pattern Classification by Unsupervised Learning Using\n",
            "  Dimensionality Reduction of Data with Mirroring Neural Networks\n",
            "Generated Title : \n",
            "unsupervised of unsupervised unsupervised learning of unsupervised classification\n",
            "Abstract : \n",
            "Authorship attribution mainly deals with undecided authorship of literary\n",
            "texts. Authorship attribution is useful in resolving issues like uncertain\n",
            "authorship, recognize authorship of unknown texts, spot plagiarism so on.\n",
            "Statistical methods can be used to set apart the approach of an author\n",
            "numerically. The basic methodologies that are made use in computational\n",
            "stylometry are word length, sentence length, vocabulary affluence, frequencies\n",
            "etc. Each author has an inborn style of writing, which is particular to\n",
            "himself. Statistical quantitative techniques can be used to differentiate the\n",
            "approach of an author in a numerical way. The problem can be broken down into\n",
            "three sub problems as author identification, author characterization and\n",
            "similarity detection. The steps involved are pre-processing, extracting\n",
            "features, classification and author identification. For this different\n",
            "classifiers can be used. Here fuzzy learning classifier and SVM are used. After\n",
            "author identification the SVM was found to have more accuracy than Fuzzy\n",
            "classifier. Later combined the classifiers to obtain a better accuracy when\n",
            "compared to individual SVM and fuzzy classifier.\n",
            "Actual Title : \n",
            "Text Classification For Authorship Attribution Analysis\n",
            "Generated Title : \n",
            "comparison of <unk> authorship attribution\n",
            "Abstract : \n",
            "POMDPs are standard models for probabilistic planning problems, where an\n",
            "agent interacts with an uncertain environment. We study the problem of\n",
            "almost-sure reachability, where given a set of target states, the question is\n",
            "to decide whether there is a policy to ensure that the target set is reached\n",
            "with probability 1 (almost-surely). While in general the problem is\n",
            "EXPTIME-complete, in many practical cases policies with a small amount of\n",
            "memory suffice. Moreover, the existing solution to the problem is explicit,\n",
            "which first requires to construct explicitly an exponential reduction to a\n",
            "belief-support MDP. In this work, we first study the existence of\n",
            "observation-stationary strategies, which is NP-complete, and then small-memory\n",
            "strategies. We present a symbolic algorithm by an efficient encoding to SAT and\n",
            "using a SAT solver for the problem. We report experimental results\n",
            "demonstrating the scalability of our symbolic (SAT-based) approach.\n",
            "Actual Title : \n",
            "A Symbolic SAT-based Algorithm for Almost-sure Reachability with Small\n",
            "  Strategies in POMDPs\n",
            "Generated Title : \n",
            "symbolic sat based algorithm for almost sure reachability with a\n",
            "Abstract : \n",
            "While deep reinforcement learning (RL) methods have achieved unprecedented\n",
            "successes in a range of challenging problems, their applicability has been\n",
            "mainly limited to simulation or game domains due to the high sample complexity\n",
            "of the trial-and-error learning process. However, real-world robotic\n",
            "applications often need a data-efficient learning process with safety-critical\n",
            "constraints. In this paper, we consider the challenging problem of learning\n",
            "unmanned aerial vehicle (UAV) control for tracking a moving target. To acquire\n",
            "a strategy that combines perception and control, we represent the policy by a\n",
            "convolutional neural network. We develop a hierarchical approach that combines\n",
            "a model-free policy gradient method with a conventional feedback\n",
            "proportional-integral-derivative (PID) controller to enable stable learning\n",
            "without catastrophic failure. The neural network is trained by a combination of\n",
            "supervised learning from raw images and reinforcement learning from games of\n",
            "self-play. We show that the proposed approach can learn a target following\n",
            "policy in a simulator efficiently and the learned behavior can be successfully\n",
            "transferred to the DJI quadrotor platform for real-world UAV control.\n",
            "Actual Title : \n",
            "Learning Unmanned Aerial Vehicle Control for Autonomous Target Following\n",
            "Generated Title : \n",
            "learning and q learning for robotics autonomous grounding\n",
            "Abstract : \n",
            "When designing systems that are complex, dynamic and stochastic in nature,\n",
            "simulation is generally recognised as one of the best design support\n",
            "technologies, and a valuable aid in the strategic and tactical decision making\n",
            "process. A simulation model consists of a set of rules that define how a system\n",
            "changes over time, given its current state. Unlike analytical models, a\n",
            "simulation model is not solved but is run and the changes of system states can\n",
            "be observed at any point in time. This provides an insight into system dynamics\n",
            "rather than just predicting the output of a system based on specific inputs.\n",
            "Simulation is not a decision making tool but a decision support tool, allowing\n",
            "better informed decisions to be made. Due to the complexity of the real world,\n",
            "a simulation model can only be an approximation of the target system. The\n",
            "essence of the art of simulation modelling is abstraction and simplification.\n",
            "Only those characteristics that are important for the study and analysis of the\n",
            "target system should be included in the simulation model.\n",
            "Actual Title : \n",
            "Introduction to Multi-Agent Simulation\n",
            "Generated Title : \n",
            "simulation of simulation simulation of the human computation\n",
            "Abstract : \n",
            "One of the important requirements in image retrieval, indexing,\n",
            "classification, clustering and etc. is extracting efficient features from\n",
            "images. The color feature is one of the most widely used visual features. Use\n",
            "of color histogram is the most common way for representing color feature. One\n",
            "of disadvantage of the color histogram is that it does not take the color\n",
            "spatial distribution into consideration. In this paper dynamic color\n",
            "distribution entropy of neighborhoods method based on color distribution\n",
            "entropy is presented, which effectively describes the spatial information of\n",
            "colors. The image retrieval results in compare to improved color distribution\n",
            "entropy show the acceptable efficiency of this approach.\n",
            "Actual Title : \n",
            "A New Color Feature Extraction Method Based on Dynamic Color\n",
            "  Distribution Entropy of Neighborhoods\n",
            "Generated Title : \n",
            "image enhancement using color adaptive entropy based on\n",
            "Abstract : \n",
            "Recent studies have highlighted the vulnerability of deep neural networks\n",
            "(DNNs) to adversarial examples - a visually indistinguishable adversarial image\n",
            "can easily be crafted to cause a well-trained model to misclassify. Existing\n",
            "methods for crafting adversarial examples are based on $L_2$ and $L_\\infty$\n",
            "distortion metrics. However, despite the fact that $L_1$ distortion accounts\n",
            "for the total variation and encourages sparsity in the perturbation, little has\n",
            "been developed for crafting $L_1$-based adversarial examples. In this paper, we\n",
            "formulate the process of attacking DNNs via adversarial examples as an\n",
            "elastic-net regularized optimization problem. Our elastic-net attacks to DNNs\n",
            "(EAD) feature $L_1$-oriented adversarial examples and include the\n",
            "state-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,\n",
            "CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial\n",
            "examples with small $L_1$ distortion and attains similar attack performance to\n",
            "the state-of-the-art methods in different attack scenarios. More importantly,\n",
            "EAD leads to improved attack transferability and complements adversarial\n",
            "training for DNNs, suggesting novel insights on leveraging $L_1$ distortion in\n",
            "adversarial machine learning and security implications of DNNs.\n",
            "Actual Title : \n",
            "EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial\n",
            "  Examples\n",
            "Generated Title : \n",
            "elastic net attacks to adversarial deep neural networks\n",
            "Abstract : \n",
            "Two types of probabilistic maps are popular in the mobile robotics\n",
            "literature: occupancy grids and geometric maps. Occupancy grids have the\n",
            "advantages of simplicity and speed, but they represent only a restricted class\n",
            "of maps and they make incorrect independence assumptions. On the other hand,\n",
            "current geometric approaches, which characterize the environment by features\n",
            "such as line segments, can represent complex environments compactly. However,\n",
            "they do not reason explicitly about occupancy, a necessity for motion planning;\n",
            "and, they lack a complete probability model over environmental structures. In\n",
            "this paper we present a probabilistic mapping technique based on polygonal\n",
            "random fields (PRF), which combines the advantages of both approaches. Our\n",
            "approach explicitly represents occupancy using a geometric representation, and\n",
            "it is based upon a consistent probability distribution over environments which\n",
            "avoids the incorrect independence assumptions made by occupancy grids. We show\n",
            "how sampling techniques for PRFs can be applied to localized laser and sonar\n",
            "data, and we demonstrate significant improvements in mapping performance over\n",
            "occupancy grids.\n",
            "Actual Title : \n",
            "Robotic Mapping with Polygonal Random Fields\n",
            "Generated Title : \n",
            "of polygonal random fields for robotic robotic robotic\n",
            "Abstract : \n",
            "Unlike traditional programs (such as operating systems or word processors)\n",
            "which have large amounts of code, machine learning tasks use programs with\n",
            "relatively small amounts of code (written in machine learning libraries), but\n",
            "voluminous amounts of data. Just like developers of traditional programs debug\n",
            "errors in their code, developers of machine learning tasks debug and fix errors\n",
            "in their data. However, algorithms and tools for debugging and fixing errors in\n",
            "data are less common, when compared to their counterparts for detecting and\n",
            "fixing errors in code. In this paper, we consider classification tasks where\n",
            "errors in training data lead to misclassifications in test points, and propose\n",
            "an automated method to find the root causes of such misclassifications. Our\n",
            "root cause analysis is based on Pearl's theory of causation, and uses Pearl's\n",
            "PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,\n",
            "encodes the computation of PS as a probabilistic program, and uses recent work\n",
            "on probabilistic programs and transformations on probabilistic programs (along\n",
            "with gray-box models of machine learning algorithms) to efficiently compute PS.\n",
            "Psi is able to identify root causes of data errors in interesting data sets.\n",
            "Actual Title : \n",
            "Debugging Machine Learning Tasks\n",
            "Generated Title : \n",
            "machine learning\n",
            "Abstract : \n",
            "The present study is focused on the automatic identification and description\n",
            "of frozen similes in British and French novels written between the 19 th\n",
            "century and the beginning of the 20 th century. Two main patterns of frozen\n",
            "similes were considered: adjectival ground + simile marker + nominal vehicle\n",
            "(e.g. happy as a lark) and eventuality + simile marker + nominal vehicle (e.g.\n",
            "sleep like a top). All potential similes and their components were first\n",
            "extracted using a rule-based algorithm. Then, frozen similes were identified\n",
            "based on reference lists of existing similes and semantic distance between the\n",
            "tenor and the vehicle. The results obtained tend to confirm the fact that\n",
            "frozen similes are not used haphazardly in literary texts. In addition,\n",
            "contrary to how they are often presented, frozen similes often go beyond the\n",
            "ground or the eventuality and the vehicle to also include the tenor.\n",
            "Actual Title : \n",
            "\"Pale as death\" or \"ple comme la mort\" : Frozen similes used as\n",
            "  literary clichs\n",
            "Generated Title : \n",
            "is death  or <unk> comme la <unk> similes\n",
            "Abstract : \n",
            "Symbol detection techniques in online handwritten graphics (e.g. diagrams and\n",
            "mathematical expressions) consist of methods specifically designed for a single\n",
            "graphic type. In this work, we evaluate the Faster R-CNN object detection\n",
            "algorithm as a general method for detection of symbols in handwritten graphics.\n",
            "We evaluate different configurations of the Faster R-CNN method, and point out\n",
            "issues relative to the handwritten nature of the data. Considering the online\n",
            "recognition context, we evaluate efficiency and accuracy trade-offs of using\n",
            "Deep Neural Networks of different complexities as feature extractors. We\n",
            "evaluate the method on publicly available flowchart and mathematical expression\n",
            "(CROHME-2016) datasets. Results show that Faster R-CNN can be effectively used\n",
            "on both datasets, enabling the possibility of developing general methods for\n",
            "symbol detection, and furthermore, general graphic understanding methods that\n",
            "could be built on top of the algorithm.\n",
            "Actual Title : \n",
            "Symbol detection in online handwritten graphics using Faster R-CNN\n",
            "Generated Title : \n",
            "handwritten of online handwritten graphics using online handwritten\n",
            "Abstract : \n",
            "Our study applies statistical methods to French and Italian corpora to\n",
            "examine the phenomenon of multi-word term reduction in specialty languages.\n",
            "There are two kinds of reduction: anaphoric and lexical. We show that anaphoric\n",
            "reduction depends on the discourse type (vulgarization, pedagogical,\n",
            "specialized) but is independent of both domain and language; that lexical\n",
            "reduction depends on domain and is more frequent in technical, rapidly evolving\n",
            "domains; and that anaphoric reductions tend to follow full terms rather than\n",
            "precede them. We define the notion of the anaphoric tree of the term and study\n",
            "its properties. Concerning lexical reduction, we attempt to prove statistically\n",
            "that there is a notion of term lifecycle, where the full form is progressively\n",
            "replaced by a lexical reduction. ----- Nous \\'etudions par des m\\'ethodes\n",
            "statistiques sur des corpus fran\\c{c}ais et italiens, le ph\\'enom\\`ene de\n",
            "r\\'eduction des termes complexes dans les langues de sp\\'ecialit\\'e. Il existe\n",
            "deux types de r\\'eductions : anaphorique et lexicale. Nous montrons que la\n",
            "r\\'eduction anaphorique d\\'epend du type de discours (de vulgarisation,\n",
            "p\\'edagogique, sp\\'ecialis\\'e) mais ne d\\'epend ni du domaine, ni de la langue,\n",
            "alors que la r\\'eduction lexicale d\\'epend du domaine et est plus fr\\'equente\n",
            "dans les domaines techniques \\`a \\'evolution rapide. D'autre part, nous\n",
            "montrons que la r\\'eduction anaphorique a tendance \\`a suivre la forme pleine\n",
            "du terme, nous d\\'efinissons une notion d'arbre anaphorique de terme et nous\n",
            "\\'etudions ses propri\\'et\\'es. Concernant la r\\'eduction lexicale, nous tentons\n",
            "de d\\'emontrer statistiquement qu'il existe une notion de cycle de vie de\n",
            "terme, o\\`u la forme pleine est progressivement remplac\\'ee par une r\\'eduction\n",
            "lexicale.\n",
            "Actual Title : \n",
            "La rduction de termes complexes dans les langues de spcialit\n",
            "Generated Title : \n",
            "<unk> <unk> <unk> <unk> dans les <unk> <unk>\n",
            "Abstract : \n",
            "Distance metric learning is of fundamental interest in machine learning\n",
            "because the distance metric employed can significantly affect the performance\n",
            "of many learning methods. Quadratic Mahalanobis metric learning is a popular\n",
            "approach to the problem, but typically requires solving a semidefinite\n",
            "programming (SDP) problem, which is computationally expensive. Standard\n",
            "interior-point SDP solvers typically have a complexity of $O(D^{6.5})$ (with\n",
            "$D$ the dimension of input data), and can thus only practically solve problems\n",
            "exhibiting less than a few thousand variables. Since the number of variables is\n",
            "$D (D+1) / 2 $, this implies a limit upon the size of problem that can\n",
            "practically be solved of around a few hundred dimensions. The complexity of the\n",
            "popular quadratic Mahalanobis metric learning approach thus limits the size of\n",
            "problem to which metric learning can be applied. Here we propose a\n",
            "significantly more efficient approach to the metric learning problem based on\n",
            "the Lagrange dual formulation of the problem. The proposed formulation is much\n",
            "simpler to implement, and therefore allows much larger Mahalanobis metric\n",
            "learning problems to be solved. The time complexity of the proposed method is\n",
            "$O (D ^ 3) $, which is significantly lower than that of the SDP approach.\n",
            "Experiments on a variety of datasets demonstrate that the proposed method\n",
            "achieves an accuracy comparable to the state-of-the-art, but is applicable to\n",
            "significantly larger problems. We also show that the proposed method can be\n",
            "applied to solve more general Frobenius-norm regularized SDP problems\n",
            "approximately.\n",
            "Actual Title : \n",
            "An Efficient Dual Approach to Distance Metric Learning\n",
            "Generated Title : \n",
            "dual approach to learning\n",
            "Abstract : \n",
            "Bounds on the log partition function are important in a variety of contexts,\n",
            "including approximate inference, model fitting, decision theory, and large\n",
            "deviations analysis. We introduce a new class of upper bounds on the log\n",
            "partition function, based on convex combinations of distributions in the\n",
            "exponential domain, that is applicable to an arbitrary undirected graphical\n",
            "model. In the special case of convex combinations of tree-structured\n",
            "distributions, we obtain a family of variational problems, similar to the Bethe\n",
            "free energy, but distinguished by the following desirable properties: i. they\n",
            "are cnvex, and have a unique global minimum; and ii. the global minimum gives\n",
            "an upper bound on the log partition function. The global minimum is defined by\n",
            "stationary conditions very similar to those defining fixed points of belief\n",
            "propagation or tree-based reparameterization Wainwright et al., 2001. As with\n",
            "BP fixed points, the elements of the minimizing argument can be used as\n",
            "approximations to the marginals of the original model. The analysis described\n",
            "here can be extended to structures of higher treewidth e.g., hypertrees,\n",
            "thereby making connections with more advanced approximations e.g., Kikuchi and\n",
            "variants Yedidia et al., 2001; Minka, 2001.\n",
            "Actual Title : \n",
            "A New Class of Upper Bounds on the Log Partition Function\n",
            "Generated Title : \n",
            "the upper bound on the log partition function\n",
            "Abstract : \n",
            "In the last decade, special purpose computing systems, such as Neuromorphic\n",
            "computing, have become very popular in the field of computer vision and machine\n",
            "learning for classification tasks. In 2015, IBM's released the TrueNorth\n",
            "Neuromorphic system, kick-starting a new era of Neuromorphic computing.\n",
            "Alternatively, Deep Learning approaches such as Deep Convolutional Neural\n",
            "Networks (DCNN) show almost human-level accuracies for detection and\n",
            "classification tasks. IBM's 2016 release of a deep learning framework for\n",
            "DCNNs, called Energy Efficient Deep Neuromorphic Networks (Eedn). Eedn shows\n",
            "promise for delivering high accuracies across a number of different benchmarks,\n",
            "while consuming very low power, using IBM's TrueNorth chip. However, there are\n",
            "many things that remained undiscovered using the Eedn framework for\n",
            "classification tasks on a Neuromorphic system. In this paper, we have\n",
            "empirically evaluated the performance of different DCNN architectures\n",
            "implemented within the Eedn framework. The goal of this work was discover the\n",
            "most efficient way to implement DCNN models for object classification tasks\n",
            "using the TrueNorth system. We performed our experiments using benchmark data\n",
            "sets such as MNIST, COIL 20, and COIL 100. The experimental results show very\n",
            "promising classification accuracies with very low power consumption on IBM's\n",
            "NS1e Neurosynaptic system. The results show that for datasets with large\n",
            "numbers of classes, wider networks perform better when compared to deep\n",
            "networks comprised of nearly the same core complexity on IBM's TrueNorth\n",
            "system.\n",
            "Actual Title : \n",
            "Deep Versus Wide Convolutional Neural Networks for Object Recognition on\n",
            "  Neuromorphic System\n",
            "Generated Title : \n",
            "wide convolutional neural networks for object recognition\n",
            "Abstract : \n",
            "Recent advances in AI and robotics have claimed many incredible results with\n",
            "deep learning, yet no work to date has applied deep learning to the problem of\n",
            "liquid perception and reasoning. In this paper, we apply fully-convolutional\n",
            "deep neural networks to the tasks of detecting and tracking liquids. We\n",
            "evaluate three models: a single-frame network, multi-frame network, and a LSTM\n",
            "recurrent network. Our results show that the best liquid detection results are\n",
            "achieved when aggregating data over multiple frames and that the LSTM network\n",
            "outperforms the other two in both tasks. This suggests that LSTM-based neural\n",
            "networks have the potential to be a key component for enabling robots to handle\n",
            "liquids using robust, closed-loop controllers.\n",
            "Actual Title : \n",
            "Towards Learning to Perceive and Reason About Liquids\n",
            "Generated Title : \n",
            "the of of liquids with deep learning and\n",
            "Abstract : \n",
            "Let $G=(V,E,w)$ be a finite, connected graph with weighted edges. We are\n",
            "interested in the problem of finding a subset $W \\subset V$ of vertices and\n",
            "weights $a_w$ such that $$ \\frac{1}{|V|}\\sum_{v \\in V}^{}{f(v)} \\sim \\sum_{w\n",
            "\\in W}{a_w f(w)}$$ for functions $f:V \\rightarrow \\mathbb{R}$ that are `smooth'\n",
            "with respect to the geometry of the graph. The main application are problems\n",
            "where $f$ is known to somehow depend on the underlying graph but is expensive\n",
            "to evaluate on even a single vertex. We prove an inequality showing that the\n",
            "integration problem can be rewritten as a geometric problem (`the optimal\n",
            "packing of heat balls'). We discuss how one would construct approximate\n",
            "solutions of the heat ball packing problem; numerical examples demonstrate the\n",
            "efficiency of the method.\n",
            "Actual Title : \n",
            "Numerical Integration on Graphs: where to sample and how to weigh\n",
            "Generated Title : \n",
            "solutions of graphs  where to weigh\n",
            "Abstract : \n",
            "The implicit bias of gradient descent is not fully understood even in simple\n",
            "linear classification tasks (e.g., logistic regression). Soudry et al. (2018)\n",
            "studied this bias on separable data, where there are multiple solutions that\n",
            "correctly classify the data. It was found that, when optimizing monotonically\n",
            "decreasing loss functions with exponential tails using gradient descent, the\n",
            "linear classifier specified by the gradient descent iterates converge to the\n",
            "$L_2$ max margin separator. However, the convergence rate to the maximum margin\n",
            "solution with fixed step size was found to be extremely slow: $1/\\log(t)$.\n",
            "  Here we examine how the convergence is influenced by using different loss\n",
            "functions and by using variable step sizes. First, we calculate the convergence\n",
            "rate for loss functions with poly-exponential tails near $\\exp(-u^{\\nu})$. We\n",
            "prove that $\\nu=1$ yields the optimal convergence rate in the range $\\nu>0.25$.\n",
            "Based on further analysis we conjecture that this remains the optimal rate for\n",
            "$\\nu \\leq 0.25$, and even for sub-poly-exponential tails --- until loss\n",
            "functions with polynomial tails no longer converge to the max margin. Second,\n",
            "we prove the convergence rate could be improved to $(\\log t) /\\sqrt{t}$ for the\n",
            "exponential loss, by using aggressive step sizes which compensate for the\n",
            "rapidly vanishing gradients.\n",
            "Actual Title : \n",
            "Convergence of Gradient Descent on Separable Data\n",
            "Generated Title : \n",
            "gradient gradient with gradient descent\n",
            "Abstract : \n",
            "Artificial bee colony (ABC), an optimization algorithm is a recent addition\n",
            "to the family of population based search algorithm. ABC has taken its\n",
            "inspiration from the collective intelligent foraging behavior of honey bees. In\n",
            "this study we have incorporated golden section search mechanism in the\n",
            "structure of basic ABC to improve the global convergence and prevent to stick\n",
            "on a local solution. The proposed variant is termed as ILS-ABC. Comparative\n",
            "numerical results with the state-of-art algorithms show the performance of the\n",
            "proposal when applied to the set of unconstrained engineering design problems.\n",
            "The simulated results show that the proposed variant can be successfully\n",
            "applied to solve real life problems.\n",
            "Actual Title : \n",
            "Improved Local Search in Artificial Bee Colony using Golden Section\n",
            "  Search\n",
            "Generated Title : \n",
            "local search search algorithm bee colony algorithm\n",
            "Abstract : \n",
            "In this paper, we propose a new network architecture for Chinese typography\n",
            "transformation based on deep learning. The architecture consists of two\n",
            "sub-networks: (1)a fully convolutional network(FCN) aiming at transferring\n",
            "specified typography style to another in condition of preserving structure\n",
            "information; (2)an adversarial network aiming at generating more realistic\n",
            "strokes in some details. Unlike models proposed before 2012 relying on the\n",
            "complex segmentation of Chinese components or strokes, our model treats every\n",
            "Chinese character as an inseparable image, so pre-processing or\n",
            "post-preprocessing are abandoned. Besides, our model adopts end-to-end training\n",
            "without pre-trained used in other deep models. The experiments demonstrates\n",
            "that our model can synthesize realistic-looking target typography from any\n",
            "source typography both on printed style and handwriting style.\n",
            "Actual Title : \n",
            "Chinese Typography Transfer\n",
            "Generated Title : \n",
            "typography transfer\n",
            "Abstract : \n",
            "Deep neural networks have been shown to achieve state-of-the-art performance\n",
            "in several machine learning tasks. Stochastic Gradient Descent (SGD) is the\n",
            "preferred optimization algorithm for training these networks and asynchronous\n",
            "SGD (ASGD) has been widely adopted for accelerating the training of large-scale\n",
            "deep networks in a distributed computing environment. However, in practice it\n",
            "is quite challenging to tune the training hyperparameters (such as learning\n",
            "rate) when using ASGD so as achieve convergence and linear speedup, since the\n",
            "stability of the optimization algorithm is strongly influenced by the\n",
            "asynchronous nature of parameter updates. In this paper, we propose a variant\n",
            "of the ASGD algorithm in which the learning rate is modulated according to the\n",
            "gradient staleness and provide theoretical guarantees for convergence of this\n",
            "algorithm. Experimental verification is performed on commonly-used image\n",
            "classification benchmarks: CIFAR10 and Imagenet to demonstrate the superior\n",
            "effectiveness of the proposed approach, compared to SSGD (Synchronous SGD) and\n",
            "the conventional ASGD algorithm.\n",
            "Actual Title : \n",
            "Staleness-aware Async-SGD for Distributed Deep Learning\n",
            "Generated Title : \n",
            "<unk> for distributed deep learning\n",
            "Abstract : \n",
            "In this paper, we propose an automated evaluation metric for text entry. We\n",
            "also consider possible improvements to existing text entry evaluation metrics,\n",
            "such as the minimum string distance error rate, keystrokes per character, cost\n",
            "per correction, and a unified approach proposed by MacKenzie, so they can\n",
            "accommodate the special characteristics of Chinese text. Current methods lack\n",
            "an integrated concern about both typing speed and accuracy for Chinese text\n",
            "entry evaluation. Our goal is to remove the bias that arises due to human\n",
            "factors. First, we propose a new metric, called the correction penalty (P),\n",
            "based on Fitts' law and Hick's law. Next, we transform it into the approximate\n",
            "amortized cost (AAC) of information theory. An analysis of the AAC of Chinese\n",
            "text input methods with different context lengths is also presented.\n",
            "Actual Title : \n",
            "An Automated Evaluation Metric for Chinese Text Entry\n",
            "Generated Title : \n",
            "text simplification for chinese text entry evaluation metrics\n",
            "Abstract : \n",
            "Recent literature on online learning has focused on developing adaptive\n",
            "algorithms that take advantage of a regularity of the sequence of observations,\n",
            "yet retain worst-case performance guarantees. A complementary direction is to\n",
            "develop prediction methods that perform well against complex benchmarks. In\n",
            "this paper, we address these two directions together. We present a fully\n",
            "adaptive method that competes with dynamic benchmarks in which regret guarantee\n",
            "scales with regularity of the sequence of cost functions and comparators.\n",
            "Notably, the regret bound adapts to the smaller complexity measure in the\n",
            "problem environment. Finally, we apply our results to drifting zero-sum,\n",
            "two-player games where both players achieve no regret guarantees against best\n",
            "sequences of actions in hindsight.\n",
            "Actual Title : \n",
            "Online Optimization : Competing with Dynamic Comparators\n",
            "Generated Title : \n",
            "dynamic dynamic dynamic dynamic\n",
            "Abstract : \n",
            "While the research on convolutional neural networks (CNNs) is progressing\n",
            "quickly, the real-world deployment of these models is often limited by\n",
            "computing resources and memory constraints. In this paper, we address this\n",
            "issue by proposing a novel filter pruning method to compress and accelerate\n",
            "CNNs. Our work is based on the linear relationship identified in different\n",
            "feature map subspaces via visualization of feature maps. Such linear\n",
            "relationship implies that the information in CNNs is redundant. Our method\n",
            "eliminates the redundancy in convolutional filters by applying subspace\n",
            "clustering to feature maps. In this way, most of the representative information\n",
            "in the network can be retained in each cluster. Therefore, our method provides\n",
            "an effective solution to filter pruning for which most existing methods\n",
            "directly remove filters based on simple heuristics. The proposed method is\n",
            "independent of the network structure, thus it can be adopted by any\n",
            "off-the-shelf deep learning libraries. Experiments on different networks and\n",
            "tasks show that our method outperforms existing techniques before fine-tuning,\n",
            "and achieves the state-of-the-art results after fine-tuning.\n",
            "Actual Title : \n",
            "Exploring Linear Relationship in Feature Map Subspace for ConvNets\n",
            "  Compression\n",
            "Generated Title : \n",
            "compression and compression based on linear feature maps\n",
            "Abstract : \n",
            "We review the problem of defining and inferring a \"state\" for a control\n",
            "system based on complex, high-dimensional, highly uncertain measurement streams\n",
            "such as videos. Such a state, or representation, should contain all and only\n",
            "the information needed for control, and discount nuisance variability in the\n",
            "data. It should also have finite complexity, ideally modulated depending on\n",
            "available resources. This representation is what we want to store in memory in\n",
            "lieu of the data, as it \"separates\" the control task from the measurement\n",
            "process. For the trivial case with no dynamics, a representation can be\n",
            "inferred by minimizing the Information Bottleneck Lagrangian in a function\n",
            "class realized by deep neural networks. The resulting representation has much\n",
            "higher dimension than the data, already in the millions, but it is smaller in\n",
            "the sense of information content, retaining only what is needed for the task.\n",
            "This process also yields representations that are invariant to nuisance factors\n",
            "and having maximally independent components. We extend these ideas to the\n",
            "dynamic case, where the representation is the posterior density of the task\n",
            "variable given the measurements up to the current time, which is in general\n",
            "much simpler than the prediction density maintained by the classical Bayesian\n",
            "filter. Again this can be finitely-parametrized using a deep neural network,\n",
            "and already some applications are beginning to emerge. No explicit assumption\n",
            "of Markovianity is needed; instead, complexity trades off approximation of an\n",
            "optimal representation, including the degree of Markovianity.\n",
            "Actual Title : \n",
            "A Separation Principle for Control in the Age of Deep Learning\n",
            "Generated Title : \n",
            "density networks\n",
            "Abstract : \n",
            "Metaheuristic algorithms are becoming an important part of modern\n",
            "optimization. A wide range of metaheuristic algorithms have emerged over the\n",
            "last two decades, and many metaheuristics such as particle swarm optimization\n",
            "are becoming increasingly popular. Despite their popularity, mathematical\n",
            "analysis of these algorithms lacks behind. Convergence analysis still remains\n",
            "unsolved for the majority of metaheuristic algorithms, while efficiency\n",
            "analysis is equally challenging. In this paper, we intend to provide an\n",
            "overview of convergence and efficiency studies of metaheuristics, and try to\n",
            "provide a framework for analyzing metaheuristics in terms of convergence and\n",
            "efficiency. This can form a basis for analyzing other algorithms. We also\n",
            "outline some open questions as further research topics.\n",
            "Actual Title : \n",
            "Metaheuristic Optimization: Algorithm Analysis and Open Problems\n",
            "Generated Title : \n",
            "of of the algorithm  algorithm  the the algorithm \n",
            "Abstract : \n",
            "We consider the exploration-exploitation tradeoff in linear quadratic (LQ)\n",
            "control problems, where the state dynamics is linear and the cost function is\n",
            "quadratic in states and controls. We analyze the regret of Thompson sampling\n",
            "(TS) (a.k.a. posterior-sampling for reinforcement learning) in the frequentist\n",
            "setting, i.e., when the parameters characterizing the LQ dynamics are fixed.\n",
            "Despite the empirical and theoretical success in a wide range of problems from\n",
            "multi-armed bandit to linear bandit, we show that when studying the frequentist\n",
            "regret TS in control problems, we need to trade-off the frequency of sampling\n",
            "optimistic parameters and the frequency of switches in the control policy. This\n",
            "results in an overall regret of $O(T^{2/3})$, which is significantly worse than\n",
            "the regret $O(\\sqrt{T})$ achieved by the optimism-in-face-of-uncertainty\n",
            "algorithm in LQ control problems.\n",
            "Actual Title : \n",
            "Thompson Sampling for Linear-Quadratic Control Problems\n",
            "Generated Title : \n",
            "sampling for for for for policies\n",
            "Abstract : \n",
            "For analysis of a high-dimensional dataset, a common approach is to test a\n",
            "null hypothesis of statistical independence on all variable pairs using a\n",
            "non-parametric measure of dependence. However, because this approach attempts\n",
            "to identify any non-trivial relationship no matter how weak, it often\n",
            "identifies too many relationships to be useful. What is needed is a way of\n",
            "identifying a smaller set of relationships that merit detailed further\n",
            "analysis.\n",
            "  Here we formally present and characterize equitability, a property of\n",
            "measures of dependence that aims to overcome this challenge. Notionally, an\n",
            "equitable statistic is a statistic that, given some measure of noise, assigns\n",
            "similar scores to equally noisy relationships of different types [Reshef et al.\n",
            "2011]. We begin by formalizing this idea via a new object called the\n",
            "interpretable interval, which functions as an interval estimate of the amount\n",
            "of noise in a relationship of unknown type. We define an equitable statistic as\n",
            "one with small interpretable intervals.\n",
            "  We then draw on the equivalence of interval estimation and hypothesis testing\n",
            "to show that under moderate assumptions an equitable statistic is one that\n",
            "yields well powered tests for distinguishing not only between trivial and\n",
            "non-trivial relationships of all kinds but also between non-trivial\n",
            "relationships of different strengths. This means that equitability allows us to\n",
            "specify a threshold relationship strength $x_0$ and to search for relationships\n",
            "of all kinds with strength greater than $x_0$. Thus, equitability can be\n",
            "thought of as a strengthening of power against independence that enables\n",
            "fruitful analysis of data sets with a small number of strong, interesting\n",
            "relationships and a large number of weaker ones. We conclude with a\n",
            "demonstration of how our two equivalent characterizations of equitability can\n",
            "be used to evaluate the equitability of a statistic in practice.\n",
            "Actual Title : \n",
            "Equitability, interval estimation, and statistical power\n",
            "Generated Title : \n",
            "interval estimation  a statistical estimators for penalized\n",
            "Abstract : \n",
            "We consider the problem of distributed statistical machine learning in\n",
            "adversarial settings, where some unknown and time-varying subset of working\n",
            "machines may be compromised and behave arbitrarily to prevent an accurate model\n",
            "from being learned. This setting captures the potential adversarial attacks\n",
            "faced by Federated Learning -- a modern machine learning paradigm that is\n",
            "proposed by Google researchers and has been intensively studied for ensuring\n",
            "user privacy. Formally, we focus on a distributed system consisting of a\n",
            "parameter server and $m$ working machines. Each working machine keeps $N/m$\n",
            "data samples, where $N$ is the total number of samples. The goal is to\n",
            "collectively learn the underlying true model parameter of dimension $d$.\n",
            "  In classical batch gradient descent methods, the gradients reported to the\n",
            "server by the working machines are aggregated via simple averaging, which is\n",
            "vulnerable to a single Byzantine failure. In this paper, we propose a Byzantine\n",
            "gradient descent method based on the geometric median of means of the\n",
            "gradients. We show that our method can tolerate $q \\le (m-1)/2$ Byzantine\n",
            "failures, and the parameter estimate converges in $O(\\log N)$ rounds with an\n",
            "estimation error of $\\sqrt{d(2q+1)/N}$, hence approaching the optimal error\n",
            "rate $\\sqrt{d/N}$ in the centralized and failure-free setting. The total\n",
            "computational complexity of our algorithm is of $O((Nd/m) \\log N)$ at each\n",
            "working machine and $O(md + kd \\log^3 N)$ at the central server, and the total\n",
            "communication cost is of $O(m d \\log N)$. We further provide an application of\n",
            "our general results to the linear regression problem.\n",
            "  A key challenge arises in the above problem is that Byzantine failures create\n",
            "arbitrary and unspecified dependency among the iterations and the aggregated\n",
            "gradients. We prove that the aggregated gradient converges uniformly to the\n",
            "true gradient function.\n",
            "Actual Title : \n",
            "Distributed Statistical Machine Learning in Adversarial Settings:\n",
            "  Byzantine Gradient Descent\n",
            "Generated Title : \n",
            "the the the for the of of machine\n",
            "Abstract : \n",
            "Seasonality is a distinctive characteristic which is often observed in many\n",
            "practical time series. Artificial Neural Networks (ANNs) are a class of\n",
            "promising models for efficiently recognizing and forecasting seasonal patterns.\n",
            "In this paper, the Particle Swarm Optimization (PSO) approach is used to\n",
            "enhance the forecasting strengths of feedforward ANN (FANN) as well as Elman\n",
            "ANN (EANN) models for seasonal data. Three widely popular versions of the basic\n",
            "PSO algorithm, viz. Trelea-I, Trelea-II and Clerc-Type1 are considered here.\n",
            "The empirical analysis is conducted on three real-world seasonal time series.\n",
            "Results clearly show that each version of the PSO algorithm achieves notably\n",
            "better forecasting accuracies than the standard Backpropagation (BP) training\n",
            "method for both FANN and EANN models. The neural network forecasting results\n",
            "are also compared with those from the three traditional statistical models,\n",
            "viz. Seasonal Autoregressive Integrated Moving Average (SARIMA), Holt-Winters\n",
            "(HW) and Support Vector Machine (SVM). The comparison demonstrates that both\n",
            "PSO and BP based neural networks outperform SARIMA, HW and SVM models for all\n",
            "three time series datasets. The forecasting performances of ANNs are further\n",
            "improved through combining the outputs from the three PSO based models.\n",
            "Actual Title : \n",
            "PSO based Neural Networks vs. Traditional Statistical Models for\n",
            "  Seasonal Time Series Forecasting\n",
            "Generated Title : \n",
            "swarm optimization for neural networks for forecasting\n",
            "Abstract : \n",
            "Word embeddings are now a standard technique for inducing meaning\n",
            "representations for words. For getting good representations, it is important to\n",
            "take into account different senses of a word. In this paper, we propose a\n",
            "mixture model for learning multi-sense word embeddings. Our model generalizes\n",
            "the previous works in that it allows to induce different weights of different\n",
            "senses of a word. The experimental results show that our model outperforms\n",
            "previous models on standard evaluation tasks.\n",
            "Actual Title : \n",
            "A Mixture Model for Learning Multi-Sense Word Embeddings\n",
            "Generated Title : \n",
            "the limits of multi sense word embeddings\n",
            "Abstract : \n",
            "Symmetric matrices are widely used in machine learning problems such as\n",
            "kernel machines and manifold learning. Using large datasets often requires\n",
            "computing low-rank approximations of these symmetric matrices so that they fit\n",
            "in memory. In this paper, we present a novel method based on biharmonic\n",
            "interpolation for low-rank matrix approximation. The method exploits knowledge\n",
            "of the data manifold to learn an interpolation operator that approximates\n",
            "values using a subset of randomly selected landmark points. This operator is\n",
            "readily sparsified, reducing memory requirements by at least two orders of\n",
            "magnitude without significant loss in accuracy. We show that our method can\n",
            "approximate very large datasets using twenty times more landmarks than other\n",
            "methods. Further, numerical results suggest that our method is stable even when\n",
            "numerical difficulties arise for other methods.\n",
            "Actual Title : \n",
            "Sparse and low-rank approximations of large symmetric matrices using\n",
            "  biharmonic interpolation\n",
            "Generated Title : \n",
            "low rank sparse sparse sparse approximations\n",
            "Abstract : \n",
            "Globally, in 2016, one out of eleven adults suffered from Diabetes Mellitus.\n",
            "Diabetic Foot Ulcers (DFU) are a major complication of this disease, which if\n",
            "not managed properly can lead to amputation. Current clinical approaches to DFU\n",
            "treatment rely on patient and clinician vigilance, which has significant\n",
            "limitations such as the high cost involved in the diagnosis, treatment and\n",
            "lengthy care of the DFU. We collected an extensive dataset of foot images,\n",
            "which contain DFU from different patients. In this paper, we have proposed the\n",
            "use of traditional computer vision features for detecting foot ulcers among\n",
            "diabetic patients, which represent a cost-effective, remote and convenient\n",
            "healthcare solution. Furthermore, we used Convolutional Neural Networks (CNNs)\n",
            "for the first time in DFU classification. We have proposed a novel\n",
            "convolutional neural network architecture, DFUNet, with better feature\n",
            "extraction to identify the feature differences between healthy skin and the\n",
            "DFU. Using 10-fold cross-validation, DFUNet achieved an AUC score of 0.962.\n",
            "This outperformed both the machine learning and deep learning classifiers we\n",
            "have tested. Here we present the development of a novel and highly sensitive\n",
            "DFUNet for objectively detecting the presence of DFUs. This novel approach has\n",
            "the potential to deliver a paradigm shift in diabetic foot care.\n",
            "Actual Title : \n",
            "DFUNet: Convolutional Neural Networks for Diabetic Foot Ulcer\n",
            "  Classification\n",
            "Generated Title : \n",
            "convolutional neural networks for diabetic foot ulcer classification\n",
            "Abstract : \n",
            "We propose a simple and fast algorithm called PatchLift for computing\n",
            "distances between patches (contiguous block of samples) extracted from a given\n",
            "one-dimensional signal. PatchLift is based on the observation that the patch\n",
            "distances can be efficiently computed from a matrix that is derived from the\n",
            "one-dimensional signal using lifting; importantly, the number of operations\n",
            "required to compute the patch distances using this approach does not scale with\n",
            "the patch length. We next demonstrate how PatchLift can be used for patch-based\n",
            "denoising of images corrupted with Gaussian noise. In particular, we propose a\n",
            "separable formulation of the classical Non-Local Means (NLM) algorithm that can\n",
            "be implemented using PatchLift. We demonstrate that the PatchLift-based\n",
            "implementation of separable NLM is few orders faster than standard NLM, and is\n",
            "competitive with existing fast implementations of NLM. Moreover, its denoising\n",
            "performance is shown to be consistently superior to that of NLM and some of its\n",
            "variants, both in terms of PSNR/SSIM and visual quality.\n",
            "Actual Title : \n",
            "Fast Separable Non-Local Means\n",
            "Generated Title : \n",
            "separable non local means\n",
            "Abstract : \n",
            "Generative Adversarial Networks (GANs) have been shown to be able to sample\n",
            "impressively realistic images. GAN training consists of a saddle point\n",
            "optimization problem that can be thought of as an adversarial game between a\n",
            "generator which produces the images, and a discriminator, which judges if the\n",
            "images are real. Both the generator and the discriminator are commonly\n",
            "parametrized as deep convolutional neural networks. The goal of this paper is\n",
            "to disentangle the contribution of the optimization procedure and the network\n",
            "parametrization to the success of GANs. To this end we introduce and study\n",
            "Generative Latent Optimization (GLO), a framework to train a generator without\n",
            "the need to learn a discriminator, thus avoiding challenging adversarial\n",
            "optimization problems. We show experimentally that GLO enjoys many of the\n",
            "desirable properties of GANs: learning from large data, synthesizing\n",
            "visually-appealing samples, interpolating meaningfully between samples, and\n",
            "performing linear arithmetic with noise vectors.\n",
            "Actual Title : \n",
            "Optimizing the Latent Space of Generative Networks\n",
            "Generated Title : \n",
            "the training of generative adversarial networks\n",
            "Abstract : \n",
            "In this work a new way to calculate the multivariate joint entropy is\n",
            "presented. This measure is the basis for a fast information-theoretic based\n",
            "evaluation of gene relevance in a Microarray Gene Expression data context. Its\n",
            "low complexity is based on the reuse of previous computations to calculate\n",
            "current feature relevance. The mu-TAFS algorithm --named as such to\n",
            "differentiate it from previous TAFS algorithms-- implements a simulated\n",
            "annealing technique specially designed for feature subset selection. The\n",
            "algorithm is applied to the maximization of gene subset relevance in several\n",
            "public-domain microarray data sets. The experimental results show a notoriously\n",
            "high classification performance and low size subsets formed by biologically\n",
            "meaningful genes.\n",
            "Actual Title : \n",
            "Feature Selection for Microarray Gene Expression Data using Simulated\n",
            "  Annealing guided by the Multivariate Joint Entropy\n",
            "Generated Title : \n",
            "wavelet wavelet transform based wavelet wavelet transform\n",
            "Abstract : \n",
            "We propose the first fully-adaptive algorithm for pure exploration in linear\n",
            "bandits---the task to find the arm with the largest expected reward, which\n",
            "depends on an unknown parameter linearly. While existing methods partially or\n",
            "entirely fix sequences of arm selections before observing rewards, our method\n",
            "adaptively changes the arm selection strategy based on past observations at\n",
            "each round. We show our sample complexity matches the achievable lower bound up\n",
            "to a constant factor in an extreme case. Furthermore, we evaluate the\n",
            "performance of the methods by simulations based on both synthetic setting and\n",
            "real-world data, in which our method shows vast improvement over existing\n",
            "methods.\n",
            "Actual Title : \n",
            "Fully adaptive algorithm for pure exploration in linear bandits\n",
            "Generated Title : \n",
            "pure exploration via simple pure exploration\n",
            "Abstract : \n",
            "Existing learning-based atmospheric particle-removal approaches such as those\n",
            "used for rainy and hazy images are designed with strong assumptions regarding\n",
            "spatial frequency, trajectory, and translucency. However, the removal of snow\n",
            "particles is more complicated because it possess the additional attributes of\n",
            "particle size and shape, and these attributes may vary within a single image.\n",
            "Currently, hand-crafted features are still the mainstream for snow removal,\n",
            "making significant generalization difficult to achieve. In response, we have\n",
            "designed a multistage network codenamed DesnowNet to in turn deal with the\n",
            "removal of translucent and opaque snow particles. We also differentiate snow\n",
            "into attributes of translucency and chromatic aberration for accurate\n",
            "estimation. Moreover, our approach individually estimates residual complements\n",
            "of the snow-free images to recover details obscured by opaque snow.\n",
            "Additionally, a multi-scale design is utilized throughout the entire network to\n",
            "model the diversity of snow. As demonstrated in experimental results, our\n",
            "approach outperforms state-of-the-art learning-based atmospheric phenomena\n",
            "removal methods and one semantic segmentation baseline on the proposed Snow100K\n",
            "dataset in both qualitative and quantitative comparisons. The results indicate\n",
            "our network would benefit applications involving computer vision and graphics.\n",
            "Actual Title : \n",
            "DesnowNet: Context-Aware Deep Network for Snow Removal\n",
            "Generated Title : \n",
            "deep deep network for for snow removal\n",
            "Abstract : \n",
            "A good clustering algorithm should not only be able to discover clusters of\n",
            "arbitrary shapes (global view) but also provide additional information, which\n",
            "can be used to gain more meaningful insights into the internal structure of the\n",
            "clusters (local view). In this work we use the mathematical framework of factor\n",
            "graphs and message passing algorithms to optimize a pairwise similarity based\n",
            "cost function, in the same spirit as was done in Affinity Propagation. Using\n",
            "this framework we develop two variants of a new clustering algorithm, EAP and\n",
            "SHAPE. EAP/SHAPE can not only discover clusters of arbitrary shapes but also\n",
            "provide a rich local view in the form of meaningful local representatives\n",
            "(exemplars) and connections between these local exemplars. We discuss how this\n",
            "local information can be used to gain various insights about the clusters\n",
            "including varying relative cluster densities and indication of local strength\n",
            "in different regions of a cluster . We also discuss how this can help an\n",
            "analyst in discovering and resolving potential inconsistencies in the results.\n",
            "The efficacy of EAP/SHAPE is shown by applying it to various synthetic and real\n",
            "world benchmark datasets.\n",
            "Actual Title : \n",
            "Clustering with Simultaneous Local and Global View of Data: A message\n",
            "  passing based approach\n",
            "Generated Title : \n",
            "novel clustering algorithm for the analysis of <unk>\n",
            "Abstract : \n",
            "We present a neural encoder-decoder model to convert images into\n",
            "presentational markup based on a scalable coarse-to-fine attention mechanism.\n",
            "Our method is evaluated in the context of image-to-LaTeX generation, and we\n",
            "introduce a new dataset of real-world rendered mathematical expressions paired\n",
            "with LaTeX markup. We show that unlike neural OCR techniques using CTC-based\n",
            "models, attention-based approaches can tackle this non-standard OCR task. Our\n",
            "approach outperforms classical mathematical OCR systems by a large margin on\n",
            "in-domain rendered data, and, with pretraining, also performs well on\n",
            "out-of-domain handwritten data. To reduce the inference complexity associated\n",
            "with the attention-based approaches, we introduce a new coarse-to-fine\n",
            "attention layer that selects a support region before applying attention.\n",
            "Actual Title : \n",
            "Image-to-Markup Generation with Coarse-to-Fine Attention\n",
            "Generated Title : \n",
            "generation using a <unk>\n",
            "Abstract : \n",
            "Deep Web databases contain more than 90% of pertinent information of the Web.\n",
            "Despite their importance, users don't profit of this treasury. Many deep web\n",
            "services are offering competitive services in term of prices, quality of\n",
            "service, and facilities. As the number of services is growing rapidly, users\n",
            "have difficulty to ask many web services in the same time. In this paper, we\n",
            "imagine a system where users have the possibility to formulate one query using\n",
            "one query interface and then the system translates query to the rest of query\n",
            "interfaces. However, interfaces are created by designers in order to be\n",
            "interpreted visually by users, machines can not interpret query from a given\n",
            "interface. We propose a new approach which emulates capacity of interpretation\n",
            "of users and extracts query from deep web query interfaces. Our approach has\n",
            "proved good performances on two standard datasets.\n",
            "Actual Title : \n",
            "VIQI: A New Approach for Visual Interpretation of Deep Web Query\n",
            "  Interfaces\n",
            "Generated Title : \n",
            "a visual approach for visual interpretation of web\n",
            "Abstract : \n",
            "We propose a deep neural network for the prediction of future frames in\n",
            "natural video sequences. To effectively handle complex evolution of pixels in\n",
            "videos, we propose to decompose the motion and content, two key components\n",
            "generating dynamics in videos. Our model is built upon the Encoder-Decoder\n",
            "Convolutional Neural Network and Convolutional LSTM for pixel-level prediction,\n",
            "which independently capture the spatial layout of an image and the\n",
            "corresponding temporal dynamics. By independently modeling motion and content,\n",
            "predicting the next frame reduces to converting the extracted content features\n",
            "into the next frame content by the identified motion features, which simplifies\n",
            "the task of prediction. Our model is end-to-end trainable over multiple time\n",
            "steps, and naturally learns to decompose motion and content without separate\n",
            "training. We evaluate the proposed network architecture on human activity\n",
            "videos using KTH, Weizmann action, and UCF-101 datasets. We show\n",
            "state-of-the-art performance in comparison to recent approaches. To the best of\n",
            "our knowledge, this is the first end-to-end trainable network architecture with\n",
            "motion and content separation to model the spatiotemporal dynamics for\n",
            "pixel-level future prediction in natural videos.\n",
            "Actual Title : \n",
            "Decomposing Motion and Content for Natural Video Sequence Prediction\n",
            "Generated Title : \n",
            "motion and content for video natural estimation\n",
            "Abstract : \n",
            "We present an approach towards convex optimization that relies on a novel\n",
            "scheme which converts online adaptive algorithms into offline methods. In the\n",
            "offline optimization setting, our derived methods are shown to obtain\n",
            "favourable adaptive guarantees which depend on the harmonic sum of the queried\n",
            "gradients. We further show that our methods implicitly adapt to the objective's\n",
            "structure: in the smooth case fast convergence rates are ensured without any\n",
            "prior knowledge of the smoothness parameter, while still maintaining guarantees\n",
            "in the non-smooth setting. Our approach has a natural extension to the\n",
            "stochastic setting, resulting in a lazy version of SGD (stochastic GD), where\n",
            "minibathces are chosen \\emph{adaptively} depending on the magnitude of the\n",
            "gradients. Thus providing a principled approach towards choosing minibatch\n",
            "sizes.\n",
            "Actual Title : \n",
            "Online to Offline Conversions, Universality and Adaptive Minibatch Sizes\n",
            "Generated Title : \n",
            "an adaptive online adaptive adaptive adaptive adaptive adaptive\n",
            "Abstract : \n",
            "We introduce LAMBADA, a dataset to evaluate the capabilities of computational\n",
            "models for text understanding by means of a word prediction task. LAMBADA is a\n",
            "collection of narrative passages sharing the characteristic that human subjects\n",
            "are able to guess their last word if they are exposed to the whole passage, but\n",
            "not if they only see the last sentence preceding the target word. To succeed on\n",
            "LAMBADA, computational models cannot simply rely on local context, but must be\n",
            "able to keep track of information in the broader discourse. We show that\n",
            "LAMBADA exemplifies a wide range of linguistic phenomena, and that none of\n",
            "several state-of-the-art language models reaches accuracy above 1% on this\n",
            "novel benchmark. We thus propose LAMBADA as a challenging test set, meant to\n",
            "encourage the development of new models capable of genuine understanding of\n",
            "broad context in natural language text.\n",
            "Actual Title : \n",
            "The LAMBADA dataset: Word prediction requiring a broad discourse context\n",
            "Generated Title : \n",
            "lambada dataset  word requiring requiring a broad discourse\n",
            "Abstract : \n",
            "Dominance-based Rough Set Approach (DRSA), as the extension of Pawlak's Rough\n",
            "Set theory, is effective and fundamentally important in Multiple Criteria\n",
            "Decision Analysis (MCDA). In previous DRSA models, the definitions of the upper\n",
            "and lower approximations are preserving the class unions rather than the\n",
            "singleton class. In this paper, we propose a new Class-based Rough\n",
            "Approximation with respect to a series of previous DRSA models, including\n",
            "Classical DRSA model, VC-DRSA model and VP-DRSA model. In addition, the new\n",
            "class-based reducts are investigated.\n",
            "Actual Title : \n",
            "Class-based Rough Approximation with Dominance Principle\n",
            "Generated Title : \n",
            "rough approximation with dominance principle\n",
            "Abstract : \n",
            "We address the issue of adapting optical images-based edge detection\n",
            "techniques for use in Polarimetric Synthetic Aperture Radar (PolSAR) imagery.\n",
            "We modify the gravitational edge detection technique (inspired by the Law of\n",
            "Universal Gravity) proposed by Lopez-Molina et al, using the non-standard\n",
            "neighbourhood configuration proposed by Fu et al, to reduce the speckle noise\n",
            "in polarimetric SAR imagery. We compare the modified and unmodified versions of\n",
            "the gravitational edge detection technique with the well-established one\n",
            "proposed by Canny, as well as with a recent multiscale fuzzy-based technique\n",
            "proposed by Lopez-Molina et Alejandro We also address the issues of aggregation\n",
            "of gray level images before and after edge detection and of filtering. All\n",
            "techniques addressed here are applied to a mosaic built using class\n",
            "distributions obtained from a real scene, as well as to the true PolSAR image;\n",
            "the mosaic results are assessed using Baddeley's Delta Metric. Our experiments\n",
            "show that modifying the gravitational edge detection technique with a\n",
            "non-standard neighbourhood configuration produces better results than the\n",
            "original technique, as well as the other techniques used for comparison. The\n",
            "experiments show that adapting edge detection methods from Computational\n",
            "Intelligence for use in PolSAR imagery is a new field worthy of exploration.\n",
            "Actual Title : \n",
            "Optical images-based edge detection in Synthetic Aperture Radar images\n",
            "Generated Title : \n",
            "edge detection with synthetic aperture radar\n",
            "Abstract : \n",
            "A problem faced by many instructors is that of designing exams that\n",
            "accurately assess the abilities of the students. Typically these exams are\n",
            "prepared several days in advance, and generic question scores are used based on\n",
            "rough approximation of the question difficulty and length. For example, for a\n",
            "recent class taught by the author, there were 30 multiple choice questions\n",
            "worth 3 points, 15 true/false with explanation questions worth 4 points, and 5\n",
            "analytical exercises worth 10 points. We describe a novel framework where\n",
            "algorithms from machine learning are used to modify the exam question weights\n",
            "in order to optimize the exam scores, using the overall class grade as a proxy\n",
            "for a student's true ability. We show that significant error reduction can be\n",
            "obtained by our approach over standard weighting schemes, and we make several\n",
            "new observations regarding the properties of the \"good\" and \"bad\" exam\n",
            "questions that can have impact on the design of improved future evaluation\n",
            "methods.\n",
            "Actual Title : \n",
            "Optimal Weighting for Exam Composition\n",
            "Generated Title : \n",
            "of exam composition using using voting\n",
            "Abstract : \n",
            "Visual representation is crucial for a visual tracking method's performances.\n",
            "Conventionally, visual representations adopted in visual tracking rely on\n",
            "hand-crafted computer vision descriptors. These descriptors were developed\n",
            "generically without considering tracking-specific information. In this paper,\n",
            "we propose to learn complex-valued invariant representations from tracked\n",
            "sequential image patches, via strong temporal slowness constraint and stacked\n",
            "convolutional autoencoders. The deep slow local representations are learned\n",
            "offline on unlabeled data and transferred to the observational model of our\n",
            "proposed tracker. The proposed observational model retains old training samples\n",
            "to alleviate drift, and collect negative samples which are coherent with\n",
            "target's motion pattern for better discriminative tracking. With the learned\n",
            "representation and online training samples, a logistic regression classifier is\n",
            "adopted to distinguish target from background, and retrained online to adapt to\n",
            "appearance changes. Subsequently, the observational model is integrated into a\n",
            "particle filter framework to peform visual tracking. Experimental results on\n",
            "various challenging benchmark sequences demonstrate that the proposed tracker\n",
            "performs favourably against several state-of-the-art trackers.\n",
            "Actual Title : \n",
            "Self-taught learning of a deep invariant representation for visual\n",
            "  tracking via temporal slowness principle\n",
            "Generated Title : \n",
            "representation learning for visual representation for visual tracking\n",
            "Abstract : \n",
            "To recover the three dimensional (3D) volumetric distribution of matter in an\n",
            "object, images of the object are captured from multiple directions and\n",
            "locations. Using these images tomographic computations extract the\n",
            "distribution. In highly scattering media and constrained, natural irradiance,\n",
            "tomography must explicitly account for off-axis scattering. Furthermore, the\n",
            "tomographic model and recovery must function when imaging is done in-situ, as\n",
            "occurs in medical imaging and ground-based atmospheric sensing. We formulate\n",
            "tomography that handles arbitrary orders of scattering, using a monte-carlo\n",
            "model. Moreover, the model is highly parallelizable in our formulation. This\n",
            "enables large scale rendering and recovery of volumetric scenes having a large\n",
            "number of variables. We solve stability and conditioning problems that stem\n",
            "from radiative transfer (RT) modeling in-situ.\n",
            "Actual Title : \n",
            "In-situ multi-scattering tomography\n",
            "Generated Title : \n",
            "<unk> tomography tomography using x ray tomography\n",
            "Abstract : \n",
            "We describe our system for SemEval-2018 Shared Task on Semantic Relation\n",
            "Extraction and Classification in Scientific Papers where we focus on the\n",
            "Classification task. Our simple piecewise convolution neural encoder performs\n",
            "decently in an end to end manner. A simple inter-task data augmentation\n",
            "signifi- cantly boosts the performance of the model. Our best-performing\n",
            "systems stood 8th out of 20 teams on the classification task on noisy data and\n",
            "12th out of 28 teams on the classification task on clean data.\n",
            "Actual Title : \n",
            "OhioState at SemEval-2018 Task 7: Exploiting Data Augmentation for\n",
            "  Relation Classification in Scientific Papers using Piecewise Convolutional\n",
            "  Neural Networks\n",
            "Generated Title : \n",
            "new for relation classification for relation classification using\n",
            "Abstract : \n",
            "We present a method of training a differentiable function approximator for a\n",
            "regression task using negative examples. We effect this training using negative\n",
            "learning rates. We also show how this method can be used to perform direct\n",
            "policy learning in a reinforcement learning setting.\n",
            "Actual Title : \n",
            "Negative Learning Rates and P-Learning\n",
            "Generated Title : \n",
            "learning and function of\n",
            "Abstract : \n",
            "Many high dimensional sparse learning problems are formulated as nonconvex\n",
            "optimization. A popular approach to solve these nonconvex optimization problems\n",
            "is through convex relaxations such as linear and semidefinite programming. In\n",
            "this paper, we study the statistical limits of convex relaxations.\n",
            "Particularly, we consider two problems: Mean estimation for sparse principal\n",
            "submatrix and edge probability estimation for stochastic block model. We\n",
            "exploit the sum-of-squares relaxation hierarchy to sharply characterize the\n",
            "limits of a broad class of convex relaxations. Our result shows statistical\n",
            "optimality needs to be compromised for achieving computational tractability\n",
            "using convex relaxations. Compared with existing results on computational lower\n",
            "bounds for statistical problems, which consider general polynomial-time\n",
            "algorithms and rely on computational hardness hypotheses on problems like\n",
            "planted clique detection, our theory focuses on a broad class of convex\n",
            "relaxations and does not rely on unproven hypotheses.\n",
            "Actual Title : \n",
            "Statistical Limits of Convex Relaxations\n",
            "Generated Title : \n",
            "relaxations of relaxations\n",
            "Abstract : \n",
            "We propose RoBiRank, a ranking algorithm that is motivated by observing a\n",
            "close connection between evaluation metrics for learning to rank and loss\n",
            "functions for robust classification. The algorithm shows a very competitive\n",
            "performance on standard benchmark datasets against other representative\n",
            "algorithms in the literature. On the other hand, in large scale problems where\n",
            "explicit feature vectors and scores are not given, our algorithm can be\n",
            "efficiently parallelized across a large number of machines; for a task that\n",
            "requires 386,133 x 49,824,519 pairwise interactions between items to be ranked,\n",
            "our algorithm finds solutions that are of dramatically higher quality than that\n",
            "can be found by a state-of-the-art competitor algorithm, given the same amount\n",
            "of wall-clock time for computation.\n",
            "Actual Title : \n",
            "Ranking via Robust Binary Classification and Parallel Parameter\n",
            "  Estimation in Large-Scale Data\n",
            "Generated Title : \n",
            "learning with ranking via the missing data\n",
            "Abstract : \n",
            "We investigate a novel cluster-of-bandit algorithm CAB for collaborative\n",
            "recommendation tasks that implements the underlying feedback sharing mechanism\n",
            "by estimating the neighborhood of users in a context-dependent manner. CAB\n",
            "makes sharp departures from the state of the art by incorporating collaborative\n",
            "effects into inference as well as learning processes in a manner that\n",
            "seamlessly interleaving explore-exploit tradeoffs and collaborative steps. We\n",
            "prove regret bounds under various assumptions on the data, which exhibit a\n",
            "crisp dependence on the expected number of clusters over the users, a natural\n",
            "measure of the statistical difficulty of the learning task. Experiments on\n",
            "production and real-world datasets show that CAB offers significantly increased\n",
            "prediction performance against a representative pool of state-of-the-art\n",
            "methods.\n",
            "Actual Title : \n",
            "On Context-Dependent Clustering of Bandits\n",
            "Generated Title : \n",
            "bandits on the bandits\n",
            "Abstract : \n",
            "How universal is human conceptual structure? The way concepts are organized\n",
            "in the human brain may reflect distinct features of cultural, historical, and\n",
            "environmental background in addition to properties universal to human\n",
            "cognition. Semantics, or meaning expressed through language, provides direct\n",
            "access to the underlying conceptual structure, but meaning is notoriously\n",
            "difficult to measure, let alone parameterize. Here we provide an empirical\n",
            "measure of semantic proximity between concepts using cross-linguistic\n",
            "dictionaries. Across languages carefully selected from a phylogenetically and\n",
            "geographically stratified sample of genera, translations of words reveal cases\n",
            "where a particular language uses a single polysemous word to express concepts\n",
            "represented by distinct words in another. We use the frequency of polysemies\n",
            "linking two concepts as a measure of their semantic proximity, and represent\n",
            "the pattern of such linkages by a weighted network. This network is highly\n",
            "uneven and fragmented: certain concepts are far more prone to polysemy than\n",
            "others, and there emerge naturally interpretable clusters loosely connected to\n",
            "each other. Statistical analysis shows such structural properties are\n",
            "consistent across different language groups, largely independent of geography,\n",
            "environment, and literacy. It is therefore possible to conclude the conceptual\n",
            "structure connecting basic vocabulary studied is primarily due to universal\n",
            "features of human cognition and language use.\n",
            "Actual Title : \n",
            "On the universal structure of human lexical semantics\n",
            "Generated Title : \n",
            "model of the human emotions through a novel\n",
            "Abstract : \n",
            "We propose an end-to-end learning framework for segmenting generic objects in\n",
            "videos. Our method learns to combine appearance and motion information to\n",
            "produce pixel level segmentation masks for all prominent objects in videos. We\n",
            "formulate this task as a structured prediction problem and design a two-stream\n",
            "fully convolutional neural network which fuses together motion and appearance\n",
            "in a unified framework. Since large-scale video datasets with pixel level\n",
            "segmentations are problematic, we show how to bootstrap weakly annotated videos\n",
            "together with existing image recognition datasets for training. Through\n",
            "experiments on three challenging video segmentation benchmarks, our method\n",
            "substantially improves the state-of-the-art for segmenting generic (unseen)\n",
            "objects. Code and pre-trained models are available on the project website.\n",
            "Actual Title : \n",
            "FusionSeg: Learning to combine motion and appearance for fully automatic\n",
            "  segmention of generic objects in videos\n",
            "Generated Title : \n",
            "video object combine motion and appearance consistency convolutional\n",
            "Abstract : \n",
            "Principal component analysis (PCA) is largely adopted for chemical process\n",
            "monitoring and numerous PCA-based systems have been developed to solve various\n",
            "fault detection and diagnosis problems. Since PCA-based methods assume that the\n",
            "monitored process is linear, nonlinear PCA models, such as autoencoder models\n",
            "and kernel principal component analysis (KPCA), has been proposed and applied\n",
            "to nonlinear process monitoring. However, KPCA-based methods need to perform\n",
            "eigen-decomposition (ED) on the kernel Gram matrix whose dimensions depend on\n",
            "the number of training data. Moreover, prefixed kernel parameters cannot be\n",
            "most effective for different faults which may need different parameters to\n",
            "maximize their respective detection performances. Autoencoder models lack the\n",
            "consideration of orthogonal constraints which is crucial for PCA-based\n",
            "algorithms. To address these problems, this paper proposes a novel nonlinear\n",
            "method, called neural component analysis (NCA), which intends to train a\n",
            "feedforward neural work with orthogonal constraints such as those used in PCA.\n",
            "NCA can adaptively learn its parameters through backpropagation and the\n",
            "dimensionality of the nonlinear features has no relationship with the number of\n",
            "training samples. Extensive experimental results on the Tennessee Eastman (TE)\n",
            "benchmark process show the superiority of NCA in terms of missed detection rate\n",
            "(MDR) and false alarm rate (FAR). The source code of NCA can be found in\n",
            "https://github.com/haitaozhao/Neural-Component-Analysis.git.\n",
            "Actual Title : \n",
            "Neural Component Analysis for Fault Detection\n",
            "Generated Title : \n",
            "component analysis for brain detection in high dimensional data\n",
            "Abstract : \n",
            "This paper contributes to the human-machine interface community in two ways:\n",
            "as a critique of the closed-loop AC (augmented cognition) approach, and as a\n",
            "way to introduce concepts from complex systems and systems physiology into the\n",
            "field. Of particular relevance is a comparison of the inverted-U (or Gaussian)\n",
            "model of optimal performance and multidimensional fitness landscape model.\n",
            "Hypothetical examples will be given from human physiology and learning and\n",
            "memory. In particular, a four-step model will be introduced that is proposed as\n",
            "a better means to characterize multivariate systems during behavioral processes\n",
            "with complex dynamics such as learning. Finally, the alternate approach\n",
            "presented herein is considered as a preferable design alternate in\n",
            "human-machine systems. It is within this context that future directions are\n",
            "discussed.\n",
            "Actual Title : \n",
            "The adaptability of physiological systems optimizes performance: new\n",
            "  directions in augmentation\n",
            "Generated Title : \n",
            "adaptability of physiological systems optimizes performance  error directions\n",
            "Abstract : \n",
            "After data selection, pre-processing, transformation, and feature extraction,\n",
            "knowledge extraction is not the final step in a data mining process. It is then\n",
            "necessary to understand this knowledge in order to apply it efficiently and\n",
            "effectively. Up to now, there is a lack of appropriate techniques that support\n",
            "this significant step. This is partly due to the fact that the assessment of\n",
            "knowledge is often highly subjective, e.g., regarding aspects such as novelty\n",
            "or usefulness. These aspects depend on the specific knowledge and requirements\n",
            "of the data miner. There are, however, a number of aspects that are objective\n",
            "and for which it is possible to provide appropriate measures. In this article\n",
            "we focus on classification problems and use probabilistic generative\n",
            "classifiers based on mixture density models that are quite common in data\n",
            "mining applications. We define objective measures to assess the\n",
            "informativeness, uniqueness, importance, discrimination, representativity,\n",
            "uncertainty, and distinguishability of rules contained in these classifiers\n",
            "numerically. These measures not only support a data miner in evaluating results\n",
            "of a data mining process based on such classifiers. As we will see in\n",
            "illustrative case studies, they may also be used to improve the data mining\n",
            "process itself or to support the later application of the extracted knowledge.\n",
            "Actual Title : \n",
            "Towards Automation of Knowledge Understanding: An Approach for\n",
            "  Probabilistic Generative Classifiers\n",
            "Generated Title : \n",
            "and classification of data mining\n",
            "Abstract : \n",
            "We consider a team of reinforcement learning agents that concurrently learn\n",
            "to operate in a common environment. We identify three properties - adaptivity,\n",
            "commitment, and diversity - which are necessary for efficient coordinated\n",
            "exploration and demonstrate that straightforward extensions to single-agent\n",
            "optimistic and posterior sampling approaches fail to satisfy them. As an\n",
            "alternative, we propose seed sampling, which extends posterior sampling in a\n",
            "manner that meets these requirements. Simulation results investigate how\n",
            "per-agent regret decreases as the number of agents grows, establishing\n",
            "substantial advantages of seed sampling over alternative exploration schemes.\n",
            "Actual Title : \n",
            "Coordinated Exploration in Concurrent Reinforcement Learning\n",
            "Generated Title : \n",
            "exploration for reinforcement learning\n",
            "Abstract : \n",
            "We consider the problem of learning Bayesian network classifiers that\n",
            "maximize the marginover a set of classification variables. We find that this\n",
            "problem is harder for Bayesian networks than for undirected graphical models\n",
            "like maximum margin Markov networks. The main difficulty is that the parameters\n",
            "in a Bayesian network must satisfy additional normalization constraints that an\n",
            "undirected graphical model need not respect. These additional constraints\n",
            "complicate the optimization task. Nevertheless, we derive an effective training\n",
            "algorithm that solves the maximum margin training problem for a range of\n",
            "Bayesian network topologies, and converges to an approximate solution for\n",
            "arbitrary network topologies. Experimental results show that the method can\n",
            "demonstrate improved generalization performance over Markov networks when the\n",
            "directed graphical structure encodes relevant knowledge. In practice, the\n",
            "training technique allows one to combine prior knowledge expressed as a\n",
            "directed (causal) model with state of the art discriminative learning methods.\n",
            "Actual Title : \n",
            "Maximum Margin Bayesian Networks\n",
            "Generated Title : \n",
            "bayesian of bayesian network bayesian network for bayesian\n",
            "Abstract : \n",
            "This paper addresses the problem of correlation estimation in sets of\n",
            "compressed images. We consider a framework where images are represented under\n",
            "the form of linear measurements due to low complexity sensing or security\n",
            "requirements. We assume that the images are correlated through the displacement\n",
            "of visual objects due to motion or viewpoint change and the correlation is\n",
            "effectively represented by optical flow or motion field models. The correlation\n",
            "is estimated in the compressed domain by jointly processing the linear\n",
            "measurements. We first show that the correlated images can be efficiently\n",
            "related using a linear operator. Using this linear relationship we then\n",
            "describe the dependencies between images in the compressed domain. We further\n",
            "cast a regularized optimization problem where the correlation is estimated in\n",
            "order to satisfy both data consistency and motion smoothness objectives with a\n",
            "Graph Cut algorithm. We analyze in detail the correlation estimation\n",
            "performance and quantify the penalty due to image compression. Extensive\n",
            "experiments in stereo and video imaging applications show that our novel\n",
            "solution stays competitive with methods that implement complex image\n",
            "reconstruction steps prior to correlation estimation. We finally use the\n",
            "estimated correlation in a novel joint image reconstruction scheme that is\n",
            "based on an optimization problem with sparsity priors on the reconstructed\n",
            "images. Additional experiments show that our correlation estimation algorithm\n",
            "leads to an effective reconstruction of pairs of images in distributed image\n",
            "coding schemes that outperform independent reconstruction algorithms by 2 to 4\n",
            "dB.\n",
            "Actual Title : \n",
            "Correlation Estimation from Compressed Images\n",
            "Generated Title : \n",
            "sensing of compressed images\n",
            "Abstract : \n",
            "We consider the problem of maximizing submodular functions; while this\n",
            "problem is known to be NP-hard, several numerically efficient local search\n",
            "techniques with approximation guarantees are available. In this paper, we\n",
            "propose a novel convex relaxation which is based on the relationship between\n",
            "submodular functions, entropies and probabilistic graphical models. In a\n",
            "graphical model, the entropy of the joint distribution decomposes as a sum of\n",
            "marginal entropies of subsets of variables; moreover, for any distribution, the\n",
            "entropy of the closest distribution factorizing in the graphical model provides\n",
            "an bound on the entropy. For directed graphical models, this last property\n",
            "turns out to be a direct consequence of the submodularity of the entropy\n",
            "function, and allows the generalization of graphical-model-based upper bounds\n",
            "to any submodular functions. These upper bounds may then be jointly maximized\n",
            "with respect to a set, while minimized with respect to the graph, leading to a\n",
            "convex variational inference scheme for maximizing submodular functions, based\n",
            "on outer approximations of the marginal polytope and maximum likelihood bounded\n",
            "treewidth structures. By considering graphs of increasing treewidths, we may\n",
            "then explore the trade-off between computational complexity and tightness of\n",
            "the relaxation. We also present extensions to constrained problems and\n",
            "maximizing the difference of submodular functions, which include all possible\n",
            "set functions.\n",
            "Actual Title : \n",
            "Maximizing submodular functions using probabilistic graphical models\n",
            "Generated Title : \n",
            "functions for marginal value maximization\n",
            "Abstract : \n",
            "In a recent article we described a new type of deep neural network - a\n",
            "Perpetual Learning Machine (PLM) - which is capable of learning 'on the fly'\n",
            "like a brain by existing in a state of Perpetual Stochastic Gradient Descent\n",
            "(PSGD). Here, by simulating the process of practice, we demonstrate both\n",
            "selective memory and selective forgetting when we introduce statistical recall\n",
            "biases during PSGD. Frequently recalled memories are remembered, whilst\n",
            "memories recalled rarely are forgotten. This results in a 'use it or lose it'\n",
            "stimulus driven memory process that is similar to human memory.\n",
            "Actual Title : \n",
            "Use it or Lose it: Selective Memory and Forgetting in a Perpetual\n",
            "  Learning Machine\n",
            "Generated Title : \n",
            "learning  an empirical study on the perpetual learning\n",
            "Abstract : \n",
            "Skin cancer, the most common human malignancy, is primarily diagnosed\n",
            "visually by physicians [1]. Classification with an automated method like CNN\n",
            "[2, 3] shows potential for challenging tasks [1]. By now, the deep\n",
            "convolutional neural networks are on par with human dermatologist [1]. This\n",
            "abstract is dedicated on developing a Deep Learning method for ISIC [5] 2017\n",
            "Skin Lesion Detection Competition hosted at [6] to classify the dermatology\n",
            "pictures, which is aimed at improving the diagnostic accuracy rate and general\n",
            "level of the human health. The challenge falls into three sub-challenges,\n",
            "including Lesion Segmentation, Lesion Dermoscopic Feature Extraction and Lesion\n",
            "Classification. This project only participates in the Lesion Classification\n",
            "part. This algorithm is comprised of three steps: (1) original images\n",
            "preprocessing, (2) modelling the processed images using CNN [2, 3] in Caffe [4]\n",
            "framework, (3) predicting the test images and calculating the scores that\n",
            "represent the likelihood of corresponding classification. The models are built\n",
            "on the source images are using the Caffe [4] framework. The scores in\n",
            "prediction step are obtained by two different models from the source images.\n",
            "Actual Title : \n",
            "Using Deep Learning Method for Classification: A Proposed Algorithm for\n",
            "  the ISIC 2017 Skin Lesion Classification Challenge\n",
            "Generated Title : \n",
            "lesion analysis using deep learning challenge\n",
            "Abstract : \n",
            "Neural networks are analogous in many ways to spin glasses, systems which are\n",
            "known for their rich set of dynamics and equally complex phase diagrams. We\n",
            "apply well-known techniques in the study of spin glasses to a convolutional\n",
            "sparsely encoding neural network and observe power law finite-size scaling\n",
            "behavior in the sparsity and reconstruction error as the network denoises\n",
            "32$\\times$32 RGB CIFAR-10 images. This finite-size scaling indicates the\n",
            "presence of a continuous phase transition at a critical value of this sparsity.\n",
            "By using the power law scaling relations inherent to finite-size scaling, we\n",
            "can determine the optimal value of sparsity for any network size by tuning the\n",
            "system to the critical point and operate the system at the minimum denoising\n",
            "error.\n",
            "Actual Title : \n",
            "Phase Transitions in Image Denoising via Sparsely Coding Convolutional\n",
            "  Neural Networks\n",
            "Generated Title : \n",
            "transitions in denoising via sparsely coding networks\n",
            "Abstract : \n",
            "Automatic segmentation of medical images is an important task for many\n",
            "clinical applications. In practice, a wide range of anatomical structures are\n",
            "visualised using different imaging modalities. In this paper, we investigate\n",
            "whether a single convolutional neural network (CNN) can be trained to perform\n",
            "different segmentation tasks.\n",
            "  A single CNN is trained to segment six tissues in MR brain images, the\n",
            "pectoral muscle in MR breast images, and the coronary arteries in cardiac CTA.\n",
            "The CNN therefore learns to identify the imaging modality, the visualised\n",
            "anatomical structures, and the tissue classes.\n",
            "  For each of the three tasks (brain MRI, breast MRI and cardiac CTA), this\n",
            "combined training procedure resulted in a segmentation performance equivalent\n",
            "to that of a CNN trained specifically for that task, demonstrating the high\n",
            "capacity of CNN architectures. Hence, a single system could be used in clinical\n",
            "practice to automatically perform diverse segmentation tasks without\n",
            "task-specific training.\n",
            "Actual Title : \n",
            "Deep Learning for Multi-Task Medical Image Segmentation in Multiple\n",
            "  Modalities\n",
            "Generated Title : \n",
            "convolutional neural networks for for prostate segmentation\n",
            "Abstract : \n",
            "In this paper we present a unified framework for modeling multi-relational\n",
            "representations, scoring, and learning, and conduct an empirical study of\n",
            "several recent multi-relational embedding models under the framework. We\n",
            "investigate the different choices of relation operators based on linear and\n",
            "bilinear transformations, and also the effects of entity representations by\n",
            "incorporating unsupervised vectors pre-trained on extra textual resources. Our\n",
            "results show several interesting findings, enabling the design of a simple\n",
            "embedding model that achieves the new state-of-the-art performance on a popular\n",
            "knowledge base completion task evaluated on Freebase.\n",
            "Actual Title : \n",
            "Learning Multi-Relational Semantics Using Neural-Embedding Models\n",
            "Generated Title : \n",
            "compact and for representation learning for action recognition\n",
            "Abstract : \n",
            "The classical mixture of Gaussians model is related to K-means via\n",
            "small-variance asymptotics: as the covariances of the Gaussians tend to zero,\n",
            "the negative log-likelihood of the mixture of Gaussians model approaches the\n",
            "K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis\n",
            "& Jordan (2012) used this observation to obtain a novel K-means-like algorithm\n",
            "from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead\n",
            "consider applying small-variance asymptotics directly to the posterior in\n",
            "Bayesian nonparametric models. This framework is independent of any specific\n",
            "Bayesian inference algorithm, and it has the major advantage that it\n",
            "generalizes immediately to a range of models beyond the DP mixture. To\n",
            "illustrate, we apply our framework to the feature learning setting, where the\n",
            "beta process and Indian buffet process provide an appropriate Bayesian\n",
            "nonparametric prior. We obtain a novel objective function that goes beyond\n",
            "clustering to learn (and penalize new) groupings for which we relax the mutual\n",
            "exclusivity and exhaustivity assumptions of clustering. We demonstrate several\n",
            "other algorithms, all of which are scalable and simple to implement. Empirical\n",
            "results demonstrate the benefits of the new framework.\n",
            "Actual Title : \n",
            "MAD-Bayes: MAP-based Asymptotic Derivations from Bayes\n",
            "Generated Title : \n",
            "mixture model for the parameter estimation\n",
            "Abstract : \n",
            "Conventional dual-frequency fringe projection algorithm often suffers from\n",
            "phase unwrapping failure when the frequency ratio between the high frequency\n",
            "and the low one is too large. Zhang et.al. proposed an enhanced two-frequency\n",
            "phase-shifting method to use geometric constraints of digital fringe\n",
            "projection(DFP) to reduce the noise impact due to the large frequency ratio.\n",
            "However, this method needs to calibrate the DFP system and calculate the\n",
            "minimum phase map at the nearest position from the camera perspective, these\n",
            "procedures are are relatively complex and more time-cosuming. In this paper, we\n",
            "proposed an improved method, which eliminates the system calibration and\n",
            "determination in Zhang's method,meanwhile does not need to use the low\n",
            "frequency fringe pattern. In the proposed method,we only need a set of high\n",
            "frequency fringe patterns to measure the object after the high frequency is\n",
            "directly estimated by the experiment. Thus the proposed method can simplify the\n",
            "procedure and improve the speed. Finally, the experimental evaluation is\n",
            "conducted to prove the validity of the proposed method.The results demonstrate\n",
            "that the proposed method can overcome the main disadvantages encountered by\n",
            "Zhang's method.\n",
            "Actual Title : \n",
            "Improved phase-unwrapping method using geometric constraints\n",
            "Generated Title : \n",
            "constrained optimization method using\n",
            "Abstract : \n",
            "In this work, we present a new dataset for computational humor, specifically\n",
            "comparative humor ranking, which attempts to eschew the ubiquitous binary\n",
            "approach to humor detection. The dataset consists of tweets that are humorous\n",
            "responses to a given hashtag. We describe the motivation for this new dataset,\n",
            "as well as the collection process, which includes a description of our\n",
            "semi-automated system for data collection. We also present initial experiments\n",
            "for this dataset using both unsupervised and supervised approaches. Our best\n",
            "supervised system achieved 63.7% accuracy, suggesting that this task is much\n",
            "more difficult than comparable humor detection tasks. Initial experiments\n",
            "indicate that a character-level model is more suitable for this task than a\n",
            "token-level model, likely due to a large amount of puns that can be captured by\n",
            "a character-level model.\n",
            "Actual Title : \n",
            "#HashtagWars: Learning a Sense of Humor\n",
            "Generated Title : \n",
            "a deep learning of humor\n",
            "Abstract : \n",
            "Over the past century, personality theory and research has successfully\n",
            "identified core sets of characteristics that consistently describe and explain\n",
            "fundamental differences in the way people think, feel and behave. Such\n",
            "characteristics were derived through theory, dictionary analyses, and survey\n",
            "research using explicit self-reports. The availability of social media data\n",
            "spanning millions of users now makes it possible to automatically derive\n",
            "characteristics from language use -- at large scale. Taking advantage of\n",
            "linguistic information available through Facebook, we study the process of\n",
            "inferring a new set of potential human traits based on unprompted language use.\n",
            "We subject these new traits to a comprehensive set of evaluations and compare\n",
            "them with a popular five factor model of personality. We find that our\n",
            "language-based trait construct is often more generalizable in that it often\n",
            "predicts non-questionnaire-based outcomes better than questionnaire-based\n",
            "traits (e.g. entities someone likes, income and intelligence quotient), while\n",
            "the factors remain nearly as stable as traditional factors. Our approach\n",
            "suggests a value in new constructs of personality derived from everyday human\n",
            "language use.\n",
            "Actual Title : \n",
            "Latent Human Traits in the Language of Social Media: An Open-Vocabulary\n",
            "  Approach\n",
            "Generated Title : \n",
            "modeling the language of social media  a social\n",
            "Abstract : \n",
            "We first present our work in machine translation, during which we used\n",
            "aligned sentences to train a neural network to embed n-grams of different\n",
            "languages into an $d$-dimensional space, such that n-grams that are the\n",
            "translation of each other are close with respect to some metric. Good n-grams\n",
            "to n-grams translation results were achieved, but full sentences translation is\n",
            "still problematic. We realized that learning semantics of sentences and\n",
            "documents was the key for solving a lot of natural language processing\n",
            "problems, and thus moved to the second part of our work: sentence compression.\n",
            "We introduce a flexible neural network architecture for learning embeddings of\n",
            "words and sentences that extract their semantics, propose an efficient\n",
            "implementation in the Torch framework and present embedding results comparable\n",
            "to the ones obtained with classical neural language models, while being more\n",
            "powerful.\n",
            "Actual Title : \n",
            "Semantic Vector Machines\n",
            "Generated Title : \n",
            "level with paragraph vectors\n",
            "Abstract : \n",
            "This paper describes a new kind of knowledge representation and mining system\n",
            "which we are calling the Semantic Knowledge Graph. At its heart, the Semantic\n",
            "Knowledge Graph leverages an inverted index, along with a complementary\n",
            "uninverted index, to represent nodes (terms) and edges (the documents within\n",
            "intersecting postings lists for multiple terms/nodes). This provides a layer of\n",
            "indirection between each pair of nodes and their corresponding edge, enabling\n",
            "edges to materialize dynamically from underlying corpus statistics. As a\n",
            "result, any combination of nodes can have edges to any other nodes materialize\n",
            "and be scored to reveal latent relationships between the nodes. This provides\n",
            "numerous benefits: the knowledge graph can be built automatically from a\n",
            "real-world corpus of data, new nodes - along with their combined edges - can be\n",
            "instantly materialized from any arbitrary combination of preexisting nodes\n",
            "(using set operations), and a full model of the semantic relationships between\n",
            "all entities within a domain can be represented and dynamically traversed using\n",
            "a highly compact representation of the graph. Such a system has widespread\n",
            "applications in areas as diverse as knowledge modeling and reasoning, natural\n",
            "language processing, anomaly detection, data cleansing, semantic search,\n",
            "analytics, data classification, root cause analysis, and recommendations\n",
            "systems. The main contribution of this paper is the introduction of a novel\n",
            "system - the Semantic Knowledge Graph - which is able to dynamically discover\n",
            "and score interesting relationships between any arbitrary combination of\n",
            "entities (words, phrases, or extracted concepts) through dynamically\n",
            "materializing nodes and edges from a compact graphical representation built\n",
            "automatically from a corpus of data representative of a knowledge domain.\n",
            "Actual Title : \n",
            "The Semantic Knowledge Graph: A compact, auto-generated model for\n",
            "  real-time traversal and ranking of any relationship within a domain\n",
            "Generated Title : \n",
            "to the <unk> graph  a compact  auto generated model\n",
            "Abstract : \n",
            "In previous work we developed a method of learning Bayesian Network models\n",
            "from raw data. This method relies on the well known minimal description length\n",
            "(MDL) principle. The MDL principle is particularly well suited to this task as\n",
            "it allows us to tradeoff, in a principled way, the accuracy of the learned\n",
            "network against its practical usefulness. In this paper we present some new\n",
            "results that have arisen from our work. In particular, we present a new local\n",
            "way of computing the description length. This allows us to make significant\n",
            "improvements in our search algorithm. In addition, we modify our algorithm so\n",
            "that it can take into account partial domain information that might be provided\n",
            "by a domain expert. The local computation of description length also opens the\n",
            "door for local refinement of an existent network. The feasibility of our\n",
            "approach is demonstrated by experiments involving networks of a practical size.\n",
            "Actual Title : \n",
            "Using Causal Information and Local Measures to Learn Bayesian Networks\n",
            "Generated Title : \n",
            "inference independence independence and independence\n",
            "Abstract : \n",
            "Non-negative Matrix Factorization (NMF) has already been applied to learn\n",
            "speaker characterizations from single or non-simultaneous speech for speaker\n",
            "recognition applications. It is also known for its good performance in (blind)\n",
            "source separation for simultaneous speech. This paper explains how NMF can be\n",
            "used to jointly solve the two problems in a multichannel speaker recognizer for\n",
            "simultaneous speech. It is shown how state-of-the-art multichannel NMF for\n",
            "blind source separation can be easily extended to incorporate speaker\n",
            "recognition. Experiments on the CHiME corpus show that this method outperforms\n",
            "the sequential approach of first applying source separation, followed by\n",
            "speaker recognition that uses state-of-the-art i-vector techniques.\n",
            "Actual Title : \n",
            "Joint Sound Source Separation and Speaker Recognition\n",
            "Generated Title : \n",
            "separation and speaker recognition\n",
            "Abstract : \n",
            "The majority of online reviews consist of plain-text feedback together with a\n",
            "single numeric score. However, there are multiple dimensions to products and\n",
            "opinions, and understanding the `aspects' that contribute to users' ratings may\n",
            "help us to better understand their individual preferences. For example, a\n",
            "user's impression of an audiobook presumably depends on aspects such as the\n",
            "story and the narrator, and knowing their opinions on these aspects may help us\n",
            "to recommend better products. In this paper, we build models for rating systems\n",
            "in which such dimensions are explicit, in the sense that users leave separate\n",
            "ratings for each aspect of a product. By introducing new corpora consisting of\n",
            "five million reviews, rated with between three and six aspects, we evaluate our\n",
            "models on three prediction tasks: First, we use our model to uncover which\n",
            "parts of a review discuss which of the rated aspects. Second, we use our model\n",
            "to summarize reviews, which for us means finding the sentences that best\n",
            "explain a user's rating. Finally, since aspect ratings are optional in many of\n",
            "the datasets we consider, we use our model to recover those ratings that are\n",
            "missing from a user's evaluation. Our model matches state-of-the-art approaches\n",
            "on existing small-scale datasets, while scaling to the real-world datasets we\n",
            "introduce. Moreover, our model is able to `disentangle' content and sentiment\n",
            "words: we automatically learn content words that are indicative of a particular\n",
            "aspect as well as the aspect-specific sentiment words that are indicative of a\n",
            "particular rating.\n",
            "Actual Title : \n",
            "Learning Attitudes and Attributes from Multi-Aspect Reviews\n",
            "Generated Title : \n",
            "attitudes and attributes of online reviews\n",
            "Abstract : \n",
            "Deep generative models (DGMs) are effective on learning multilayered\n",
            "representations of complex data and performing inference of input data by\n",
            "exploring the generative ability. However, it is relatively insufficient to\n",
            "empower the discriminative ability of DGMs on making accurate predictions. This\n",
            "paper presents max-margin deep generative models (mmDGMs) and a\n",
            "class-conditional variant (mmDCGMs), which explore the strongly discriminative\n",
            "principle of max-margin learning to improve the predictive performance of DGMs\n",
            "in both supervised and semi-supervised learning, while retaining the generative\n",
            "capability. In semi-supervised learning, we use the predictions of a max-margin\n",
            "classifier as the missing labels instead of performing full posterior inference\n",
            "for efficiency; we also introduce additional max-margin and label-balance\n",
            "regularization terms of unlabeled data for effectiveness. We develop an\n",
            "efficient doubly stochastic subgradient algorithm for the piecewise linear\n",
            "objectives in different settings. Empirical results on various datasets\n",
            "demonstrate that: (1) max-margin learning can significantly improve the\n",
            "prediction performance of DGMs and meanwhile retain the generative ability; (2)\n",
            "in supervised learning, mmDGMs are competitive to the best fully discriminative\n",
            "networks when employing convolutional neural networks as the generative and\n",
            "recognition models; and (3) in semi-supervised learning, mmDCGMs can perform\n",
            "efficient inference and achieve state-of-the-art classification results on\n",
            "several benchmarks.\n",
            "Actual Title : \n",
            "Max-Margin Deep Generative Models for (Semi-)Supervised Learning\n",
            "Generated Title : \n",
            "deep generative models for  semi supervised learning\n",
            "Abstract : \n",
            "Positron Emission Tomography (PET) scan images are one of the bio medical\n",
            "imaging techniques similar to that of MRI scan images but PET scan images are\n",
            "helpful in finding the development of tumors.The PET scan images requires\n",
            "expertise in the segmentation where clustering plays an important role in the\n",
            "automation process.The segmentation of such images is manual to automate the\n",
            "process clustering is used.Clustering is commonly known as unsupervised\n",
            "learning process of n dimensional data sets are clustered into k groups so as\n",
            "to maximize the inter cluster similarity and to minimize the intra cluster\n",
            "similarity.This paper is proposed to implement the commonly used K Means and\n",
            "Fuzzy CMeans (FCM) clustering algorithm.This work is implemented using MATrix\n",
            "LABoratory (MATLAB) and tested with sample PET scan image. The sample data is\n",
            "collected from Alzheimers Disease Neuro imaging Initiative ADNI. Medical Image\n",
            "Processing and Visualization Tool (MIPAV) are used to compare the resultant\n",
            "images.\n",
            "Actual Title : \n",
            "Segmentation of Alzheimers Disease in PET scan datasets using MATLAB\n",
            "Generated Title : \n",
            "of alzheimers disease in pet pet scan images\n",
            "Abstract : \n",
            "This paper describes the results of some experiments exploring statistical\n",
            "methods to infer syntactic behavior of words and morphemes from a raw corpus in\n",
            "an unsupervised fashion. It shares certain points in common with Brown et al\n",
            "(1992) and work that has grown out of that: it employs statistical techniques\n",
            "to analyze syntactic behavior based on what words occur adjacent to a given\n",
            "word. However, we use an eigenvector decomposition of a nearest-neighbor graph\n",
            "to produce a two-dimensional rendering of the words of a corpus in which words\n",
            "of the same syntactic category tend to form neighborhoods. We exploit this\n",
            "technique for extending the value of automatic learning of morphology. In\n",
            "particular, we look at the suffixes derived from a corpus by unsupervised\n",
            "learning of morphology, and we ask which of these suffixes have a consistent\n",
            "syntactic function (e.g., in English, -tion is primarily a mark of nouns, but\n",
            "-s marks both noun plurals and 3rd person present on verbs), and we determine\n",
            "that this method works well for this task.\n",
            "Actual Title : \n",
            "Using eigenvectors of the bigram graph to infer morpheme identity\n",
            "Generated Title : \n",
            "of the <unk> of the and their application\n",
            "Abstract : \n",
            "Filters in convolutional networks are typically parameterized in a pixel\n",
            "basis, that does not take prior knowledge about the visual world into account.\n",
            "We investigate the generalized notion of frames designed with image properties\n",
            "in mind, as alternatives to this parametrization. We show that frame-based\n",
            "ResNets and Densenets can improve performance on Cifar-10+ consistently, while\n",
            "having additional pleasant properties like steerability. By exploiting these\n",
            "transformation properties explicitly, we arrive at dynamic steerable blocks.\n",
            "They are an extension of residual blocks, that are able to seamlessly transform\n",
            "filters under pre-defined transformations, conditioned on the input at training\n",
            "and inference time. Dynamic steerable blocks learn the degree of invariance\n",
            "from data and locally adapt filters, allowing them to apply a different\n",
            "geometrical variant of the same filter to each location of the feature map.\n",
            "When evaluated on the Berkeley Segmentation contour detection dataset, our\n",
            "approach outperforms all competing approaches that do not utilize pre-training.\n",
            "Our results highlight the benefits of image-based regularization to deep\n",
            "networks.\n",
            "Actual Title : \n",
            "Dynamic Steerable Blocks in Deep Residual Networks\n",
            "Generated Title : \n",
            "deep networks with residual structure\n",
            "Abstract : \n",
            "Machine learning and computer vision have driven many of the greatest\n",
            "advances in the modeling of Deep Convolutional Neural Networks (DCNNs).\n",
            "Nowadays, most of the research has been focused on improving recognition\n",
            "accuracy with better DCNN models and learning approaches. The recurrent\n",
            "convolutional approach is not applied very much, other than in a few DCNN\n",
            "architectures. On the other hand, Inception-v4 and Residual networks have\n",
            "promptly become popular among computer the vision community. In this paper, we\n",
            "introduce a new DCNN model called the Inception Recurrent Residual\n",
            "Convolutional Neural Network (IRRCNN), which utilizes the power of the\n",
            "Recurrent Convolutional Neural Network (RCNN), the Inception network, and the\n",
            "Residual network. This approach improves the recognition accuracy of the\n",
            "Inception-residual network with same number of network parameters. In addition,\n",
            "this proposed architecture generalizes the Inception network, the RCNN, and the\n",
            "Residual network with significantly improved training accuracy. We have\n",
            "empirically evaluated the performance of the IRRCNN model on different\n",
            "benchmarks including CIFAR-10, CIFAR-100, TinyImageNet-200, and CU3D-100. The\n",
            "experimental results show higher recognition accuracy against most of the\n",
            "popular DCNN models including the RCNN. We have also investigated the\n",
            "performance of the IRRCNN approach against the Equivalent Inception Network\n",
            "(EIN) and the Equivalent Inception Residual Network (EIRN) counterpart on the\n",
            "CIFAR-100 dataset. We report around 4.53%, 4.49% and 3.56% improvement in\n",
            "classification accuracy compared with the RCNN, EIN, and EIRN on the CIFAR-100\n",
            "dataset respectively. Furthermore, the experiment has been conducted on the\n",
            "TinyImageNet-200 and CU3D-100 datasets where the IRRCNN provides better testing\n",
            "accuracy compared to the Inception Recurrent CNN (IRCNN), the EIN, and the\n",
            "EIRN.\n",
            "Actual Title : \n",
            "Improved Inception-Residual Convolutional Neural Network for Object\n",
            "  Recognition\n",
            "Generated Title : \n",
            "inception residual convolutional neural network for object recognition\n",
            "Abstract : \n",
            "We consider the problem of computing a lightest derivation of a global\n",
            "structure using a set of weighted rules. A large variety of inference problems\n",
            "in AI can be formulated in this framework. We generalize A* search and\n",
            "heuristics derived from abstractions to a broad class of lightest derivation\n",
            "problems. We also describe a new algorithm that searches for lightest\n",
            "derivations using a hierarchy of abstractions. Our generalization of A* gives a\n",
            "new algorithm for searching AND/OR graphs in a bottom-up fashion. We discuss\n",
            "how the algorithms described here provide a general architecture for addressing\n",
            "the pipeline problem --- the problem of passing information back and forth\n",
            "between various stages of processing in a perceptual system. We consider\n",
            "examples in computer vision and natural language processing. We apply the\n",
            "hierarchical search algorithm to the problem of estimating the boundaries of\n",
            "convex objects in grayscale images and compare it to other search methods. A\n",
            "second set of experiments demonstrate the use of a new compositional model for\n",
            "finding salient curves in images.\n",
            "Actual Title : \n",
            "The Generalized A* Architecture\n",
            "Generated Title : \n",
            "hierarchical of hierarchical structure\n",
            "Abstract : \n",
            "Often, when dealing with real-world recognition problems, we do not need, and\n",
            "often cannot have, knowledge of the entire set of possible classes that might\n",
            "appear during operational testing. Moreover, sometimes some of these classes\n",
            "may be ill-sampled, not sampled at all or undefined. In such cases, we need to\n",
            "think of robust classification methods able to deal with the \"unknown\" and\n",
            "properly reject samples belonging to classes never seen during training.\n",
            "Notwithstanding, almost all existing classifiers to date were mostly developed\n",
            "for the closed-set scenario, i.e., the classification setup in which it is\n",
            "assumed that all test samples belong to one of the classes with which the\n",
            "classifier was trained. In the open-set scenario, however, a test sample can\n",
            "belong to none of the known classes and the classifier must properly reject it\n",
            "by classifying it as unknown. In this work, we extend upon the well-known\n",
            "Support Vector Machines (SVM) classifier and introduce the Specialized Support\n",
            "Vector Machines (SSVM), which is suitable for recognition in open-set setups.\n",
            "SSVM balances the empirical risk and the risk of the unknown and ensures that\n",
            "the region of the feature space in which a test sample would be classified as\n",
            "known (one of the known classes) is always bounded, ensuring a finite risk of\n",
            "the unknown. The same cannot be guaranteed by the traditional SVM formulation,\n",
            "even when using the Radial Basis Function (RBF) kernel. In this work, we also\n",
            "highlight the properties of the SVM classifier related to the open-set\n",
            "scenario, and provide necessary and sufficient conditions for an RBF SVM to\n",
            "have bounded open-space risk. An extensive set of experiments compares the\n",
            "proposed method with existing solutions in the literature for open-set\n",
            "recognition and the reported results show its effectiveness.\n",
            "Actual Title : \n",
            "Specialized Support Vector Machines for open-set recognition\n",
            "Generated Title : \n",
            "evaluation of of support vector machines for open set\n",
            "Abstract : \n",
            "In this paper, we propose a recurrent framework for Joint Unsupervised\n",
            "LEarning (JULE) of deep representations and image clusters. In our framework,\n",
            "successive operations in a clustering algorithm are expressed as steps in a\n",
            "recurrent process, stacked on top of representations output by a Convolutional\n",
            "Neural Network (CNN). During training, image clusters and representations are\n",
            "updated jointly: image clustering is conducted in the forward pass, while\n",
            "representation learning in the backward pass. Our key idea behind this\n",
            "framework is that good representations are beneficial to image clustering and\n",
            "clustering results provide supervisory signals to representation learning. By\n",
            "integrating two processes into a single model with a unified weighted triplet\n",
            "loss and optimizing it end-to-end, we can obtain not only more powerful\n",
            "representations, but also more precise image clusters. Extensive experiments\n",
            "show that our method outperforms the state-of-the-art on image clustering\n",
            "across a variety of image datasets. Moreover, the learned representations\n",
            "generalize well when transferred to other tasks.\n",
            "Actual Title : \n",
            "Joint Unsupervised Learning of Deep Representations and Image Clusters\n",
            "Generated Title : \n",
            "learning with deep networks\n",
            "Abstract : \n",
            "A database of objects discovered in houses in the Roman city of Pompeii\n",
            "provides a unique view of ordinary life in an ancient city. Experts have used\n",
            "this collection to study the structure of Roman households, exploring the\n",
            "distribution and variability of tasks in architectural spaces, but such\n",
            "approaches are necessarily affected by modern cultural assumptions. In this\n",
            "study we present a data-driven approach to household archeology, treating it as\n",
            "an unsupervised labeling problem. This approach scales to large data sets and\n",
            "provides a more objective complement to human interpretation.\n",
            "Actual Title : \n",
            "Reconstructing Pompeian Households\n",
            "Generated Title : \n",
            "impact of <unk>\n",
            "Abstract : \n",
            "Image orientation detection requires high-level scene understanding. Humans\n",
            "use object recognition and contextual scene information to correctly orient\n",
            "images. In literature, the problem of image orientation detection is mostly\n",
            "confronted by using low-level vision features, while some approaches\n",
            "incorporate few easily detectable semantic cues to gain minor improvements. The\n",
            "vast amount of semantic content in images makes orientation detection\n",
            "challenging, and therefore there is a large semantic gap between existing\n",
            "methods and human behavior. Also, existing methods in literature report highly\n",
            "discrepant detection rates, which is mainly due to large differences in\n",
            "datasets and limited variety of test images used for evaluation. In this work,\n",
            "for the first time, we leverage the power of deep learning and adapt\n",
            "pre-trained convolutional neural networks using largest training dataset\n",
            "to-date for the image orientation detection task. An extensive evaluation of\n",
            "our model on different public datasets shows that it remarkably generalizes to\n",
            "correctly orient a large set of unconstrained images; it also significantly\n",
            "outperforms the state-of-the-art and achieves accuracy very close to that of\n",
            "humans.\n",
            "Actual Title : \n",
            "Why my photos look sideways or upside down? Detecting Canonical\n",
            "  Orientation of Images using Convolutional Neural Networks\n",
            "Generated Title : \n",
            "to <unk> down  detecting orientation in images images\n",
            "Abstract : \n",
            "Conditional random fields (CRFs) are commonly employed as a post-processing\n",
            "tool for image segmentation tasks. The unary potentials of the CRF are often\n",
            "learnt independently by a classifier, thereby decoupling the inference in CRF\n",
            "from the training of classifier. Such a scheme works effectively, when\n",
            "pixel-level labelling is available for all the images. However, in absence of\n",
            "pixel-level labels, the classifier is faced with the uphill task of selectively\n",
            "assigning the image-level labels to the pixels of the image. Prior work often\n",
            "relied on localization cues, such as saliency maps, objectness priors, bounding\n",
            "boxes etc., to address this challenging problem. In contrast, we model the\n",
            "labels of the pixels as latent variables of a CRF. The pixels and the\n",
            "image-level labels are the observed variables of the latent CRF. We amortize\n",
            "the cost of inference in the latent CRF over the entire dataset, by training an\n",
            "inference network to approximate the posterior distribution of the latent\n",
            "variables given the observed variables. The inference network can be trained in\n",
            "an end-to-end fashion, and requires no localization cues for training.\n",
            "Moreover, unlike other approaches for weakly-supervised segmentation, the\n",
            "proposed model doesn't require further post-processing. The proposed model\n",
            "achieves performance comparable with other approaches that employ saliency\n",
            "masks for the task of weakly-supervised semantic image segmentation on the\n",
            "challenging VOC 2012 dataset.\n",
            "Actual Title : \n",
            "Amortized Inference and Learning in Latent Conditional Random Fields for\n",
            "  Weakly-Supervised Semantic Image Segmentation\n",
            "Generated Title : \n",
            "inference and learning with semantic inference for weakly supervised\n",
            "Abstract : \n",
            "Millions of hearing impaired people around the world routinely use some\n",
            "variants of sign languages to communicate, thus the automatic translation of a\n",
            "sign language is meaningful and important. Currently, there are two\n",
            "sub-problems in Sign Language Recognition (SLR), i.e., isolated SLR that\n",
            "recognizes word by word and continuous SLR that translates entire sentences.\n",
            "Existing continuous SLR methods typically utilize isolated SLRs as building\n",
            "blocks, with an extra layer of preprocessing (temporal segmentation) and\n",
            "another layer of post-processing (sentence synthesis). Unfortunately, temporal\n",
            "segmentation itself is non-trivial and inevitably propagates errors into\n",
            "subsequent steps. Worse still, isolated SLR methods typically require strenuous\n",
            "labeling of each word separately in a sentence, severely limiting the amount of\n",
            "attainable training data. To address these challenges, we propose a novel\n",
            "continuous sign recognition framework, the Hierarchical Attention Network with\n",
            "Latent Space (LS-HAN), which eliminates the preprocessing of temporal\n",
            "segmentation. The proposed LS-HAN consists of three components: a two-stream\n",
            "Convolutional Neural Network (CNN) for video feature representation generation,\n",
            "a Latent Space (LS) for semantic gap bridging, and a Hierarchical Attention\n",
            "Network (HAN) for latent space based recognition. Experiments are carried out\n",
            "on two large scale datasets. Experimental results demonstrate the effectiveness\n",
            "of the proposed framework.\n",
            "Actual Title : \n",
            "Video-based Sign Language Recognition without Temporal Segmentation\n",
            "Generated Title : \n",
            "sign language recognition with gated recurrent networks\n",
            "Abstract : \n",
            "Robust foreground object segmentation via background modelling is a difficult\n",
            "problem in cluttered environments, where obtaining a clear view of the\n",
            "background to model is almost impossible. In this paper, we propose a method\n",
            "capable of robustly estimating the background and detecting regions of interest\n",
            "in such environments. In particular, we propose to extend the background\n",
            "initialisation component of a recent patch-based foreground detection algorithm\n",
            "with an elaborate technique based on Markov Random Fields, where the optimal\n",
            "labelling solution is computed using iterated conditional modes. Rather than\n",
            "relying purely on local temporal statistics, the proposed technique takes into\n",
            "account the spatial continuity of the entire background. Experiments with\n",
            "several tracking algorithms on the CAVIAR dataset indicate that the proposed\n",
            "method leads to considerable improvements in object tracking accuracy, when\n",
            "compared to methods based on Gaussian mixture models and feature histograms.\n",
            "Actual Title : \n",
            "MRF-based Background Initialisation for Improved Foreground Detection in\n",
            "  Cluttered Surveillance Videos\n",
            "Generated Title : \n",
            "image segmentation using background and background\n",
            "Abstract : \n",
            "Canonical correlation analysis (CCA) is a multivariate statistical technique\n",
            "for finding the linear relationship between two sets of variables. The kernel\n",
            "generalization of CCA named kernel CCA has been proposed to find nonlinear\n",
            "relations between datasets. Despite their wide usage, they have one common\n",
            "limitation that is the lack of sparsity in their solution. In this paper, we\n",
            "consider sparse kernel CCA and propose a novel sparse kernel CCA algorithm\n",
            "(SKCCA). Our algorithm is based on a relationship between kernel CCA and least\n",
            "squares. Sparsity of the dual transformations is introduced by penalizing the\n",
            "$\\ell_{1}$-norm of dual vectors. Experiments demonstrate that our algorithm not\n",
            "only performs well in computing sparse dual transformations but also can\n",
            "alleviate the over-fitting problem of kernel CCA.\n",
            "Actual Title : \n",
            "Sparse Kernel Canonical Correlation Analysis via $\\ell_1$-regularization\n",
            "Generated Title : \n",
            "kernel sparse kernel with  ell # regularization\n",
            "Abstract : \n",
            "Learning acoustic models directly from the raw waveform data with minimal\n",
            "processing is challenging. Current waveform-based models have generally used\n",
            "very few (~2) convolutional layers, which might be insufficient for building\n",
            "high-level discriminative features. In this work, we propose very deep\n",
            "convolutional neural networks (CNNs) that directly use time-domain waveforms as\n",
            "inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over\n",
            "very long sequences (e.g., vector of size 32000), necessary for processing\n",
            "acoustic waveforms. This is achieved through batch normalization, residual\n",
            "learning, and a careful design of down-sampling in the initial layers. Our\n",
            "networks are fully convolutional, without the use of fully connected layers and\n",
            "dropout, to maximize representation learning. We use a large receptive field in\n",
            "the first convolutional layer to mimic bandpass filters, but very small\n",
            "receptive fields subsequently to control the model capacity. We demonstrate the\n",
            "performance gains with the deeper models. Our evaluation shows that the CNN\n",
            "with 18 weight layers outperform the CNN with 3 weight layers by over 15% in\n",
            "absolute accuracy for an environmental sound recognition task and matches the\n",
            "performance of models using log-mel features.\n",
            "Actual Title : \n",
            "Very Deep Convolutional Neural Networks for Raw Waveforms\n",
            "Generated Title : \n",
            "deep convolutional neural networks on the last of\n",
            "Abstract : \n",
            "We study here the well-known propagation rules for Boolean constraints. First\n",
            "we propose a simple notion of completeness for sets of such rules and establish\n",
            "a completeness result. Then we show an equivalence in an appropriate sense\n",
            "between Boolean constraint propagation and unit propagation, a form of\n",
            "resolution for propositional logic.\n",
            "  Subsequently we characterize one set of such rules by means of the notion of\n",
            "hyper-arc consistency introduced in (Mohr and Masini 1988). Also, we clarify\n",
            "the status of a similar, though different, set of rules introduced in (Simonis\n",
            "1989a) and more fully in (Codognet and Diaz 1996).\n",
            "Actual Title : \n",
            "Some Remarks on Boolean Constraint Propagation\n",
            "Generated Title : \n",
            "remarks on propositional constraint propagation\n",
            "Abstract : \n",
            "We propose a clustering-based iterative algorithm to solve certain\n",
            "optimization problems in machine learning, where we start the algorithm by\n",
            "aggregating the original data, solving the problem on aggregated data, and then\n",
            "in subsequent steps gradually disaggregate the aggregated data. We apply the\n",
            "algorithm to common machine learning problems such as the least absolute\n",
            "deviation regression problem, support vector machines, and semi-supervised\n",
            "support vector machines. We derive model-specific data aggregation and\n",
            "disaggregation procedures. We also show optimality, convergence, and the\n",
            "optimality gap of the approximated solution in each iteration. A computational\n",
            "study is provided.\n",
            "Actual Title : \n",
            "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality\n",
            "  in Machine Learning\n",
            "Generated Title : \n",
            "aggregate and algorithm disaggregate algorithm\n",
            "Abstract : \n",
            "As with articles and journals, the customary methods for measuring books'\n",
            "academic impact mainly involve citations, which is easy but limited to\n",
            "interrogating traditional citation databases and scholarly book reviews,\n",
            "Researchers have attempted to use other metrics, such as Google Books,\n",
            "libcitation, and publisher prestige. However, these approaches lack\n",
            "content-level information and cannot determine the citation intentions of\n",
            "users. Meanwhile, the abundant online review resources concerning academic\n",
            "books can be used to mine deeper information and content utilizing altmetric\n",
            "perspectives. In this study, we measure the impacts of academic books by\n",
            "multi-granularity mining online reviews, and we identify factors that affect a\n",
            "book's impact. First, online reviews of a sample of academic books on Amazon.cn\n",
            "are crawled and processed. Then, multi-granularity review mining is conducted\n",
            "to identify review sentiment polarities and aspects' sentiment values. Lastly,\n",
            "the numbers of positive reviews and negative reviews, aspect sentiment values,\n",
            "star values, and information regarding helpfulness are integrated via the\n",
            "entropy method, and lead to the calculation of the final book impact scores.\n",
            "The results of a correlation analysis of book impact scores obtained via our\n",
            "method versus traditional book citations show that, although there are\n",
            "substantial differences between subject areas, online book reviews tend to\n",
            "reflect the academic impact. Thus, we infer that online reviews represent a\n",
            "promising source for mining book impact within the altmetric perspective and at\n",
            "the multi-granularity content level. Moreover, our proposed method might also\n",
            "be a means by which to measure other books besides academic publications.\n",
            "Actual Title : \n",
            "Measuring Book Impact Based on the Multi-granularity Online Review\n",
            "  Mining\n",
            "Generated Title : \n",
            "book impact on the multi granularity online review review\n",
            "Abstract : \n",
            "In many applications of finance, biology and sociology, complex systems\n",
            "involve entities interacting with each other. These processes have the\n",
            "peculiarity of evolving over time and of comprising latent factors, which\n",
            "influence the system without being explicitly measured. In this work we present\n",
            "latent variable time-varying graphical lasso (LTGL), a method for multivariate\n",
            "time-series graphical modelling that considers the influence of hidden or\n",
            "unmeasurable factors. The estimation of the contribution of the latent factors\n",
            "is embedded in the model which produces both sparse and low-rank components for\n",
            "each time point. In particular, the first component represents the connectivity\n",
            "structure of observable variables of the system, while the second represents\n",
            "the influence of hidden factors, assumed to be few with respect to the observed\n",
            "variables. Our model includes temporal consistency on both components,\n",
            "providing an accurate evolutionary pattern of the system. We derive a tractable\n",
            "optimisation algorithm based on alternating direction method of multipliers,\n",
            "and develop a scalable and efficient implementation which exploits proximity\n",
            "operators in closed form. LTGL is extensively validated on synthetic data,\n",
            "achieving optimal performance in terms of accuracy, structure learning and\n",
            "scalability with respect to ground truth and state-of-the-art methods for\n",
            "graphical inference. We conclude with the application of LTGL to real case\n",
            "studies, from biology and finance, to illustrate how our method can be\n",
            "successfully employed to gain insights on multivariate time-series data.\n",
            "Actual Title : \n",
            "Latent variable time-varying network inference\n",
            "Generated Title : \n",
            "latent latent variable models for latent variable\n",
            "Abstract : \n",
            "Community detection is a fundamental problem in network analysis which is\n",
            "made more challenging by overlaps between communities which often occur in\n",
            "practice. Here we propose a general, flexible, and interpretable generative\n",
            "model for overlapping communities, which can be thought of as a generalization\n",
            "of the degree-corrected stochastic block model. We develop an efficient\n",
            "spectral algorithm for estimating the community memberships, which deals with\n",
            "the overlaps by employing the K-medians algorithm rather than the usual K-means\n",
            "for clustering in the spectral domain. We show that the algorithm is\n",
            "asymptotically consistent when networks are not too sparse and the overlaps\n",
            "between communities not too large. Numerical experiments on both simulated\n",
            "networks and many real social networks demonstrate that our method performs\n",
            "very well compared to a number of benchmark methods for overlapping community\n",
            "detection.\n",
            "Actual Title : \n",
            "Detecting Overlapping Communities in Networks Using Spectral Methods\n",
            "Generated Title : \n",
            "overlapping communities in networks\n",
            "Abstract : \n",
            "Often multiple instances of an object occur in the same scene, for example in\n",
            "a warehouse. Unsupervised multi-instance object discovery algorithms are able\n",
            "to detect and identify such objects. We use such an algorithm to provide object\n",
            "proposals to a convolutional neural network (CNN) based classifier. This\n",
            "results in fewer regions to evaluate, compared to traditional region proposal\n",
            "algorithms. Additionally, it enables using the joint probability of multiple\n",
            "instances of an object, resulting in improved classification accuracy. The\n",
            "proposed technique can also split a single class into multiple sub-classes\n",
            "corresponding to the different object types, enabling hierarchical\n",
            "classification.\n",
            "Actual Title : \n",
            "Detecting and Grouping Identical Objects for Region Proposal and\n",
            "  Classification\n",
            "Generated Title : \n",
            "object discovery from object clusters to detect identical\n",
            "Abstract : \n",
            "In this work, we present a new Vector Space Model (VSM) of speech utterances\n",
            "for the task of spoken dialect identification. Generally, DID systems are built\n",
            "using two sets of features that are extracted from speech utterances; acoustic\n",
            "and phonetic. The acoustic and phonetic features are used to form vector\n",
            "representations of speech utterances in an attempt to encode information about\n",
            "the spoken dialects. The Phonotactic and Acoustic VSMs, thus formed, are used\n",
            "for the task of DID. The aim of this paper is to construct a single VSM that\n",
            "encodes information about spoken dialects from both the Phonotactic and\n",
            "Acoustic VSMs. Given the two views of the data, we make use of a well known\n",
            "multi-view dimensionality reduction technique known as Canonical Correlation\n",
            "Analysis (CCA), to form a single vector representation for each speech\n",
            "utterance that encodes dialect specific discriminative information from both\n",
            "the phonetic and acoustic representations. We refer to this approach as feature\n",
            "space combination approach and show that our CCA based feature vector\n",
            "representation performs better on the Arabic DID task than the phonetic and\n",
            "acoustic feature representations used alone. We also present the feature space\n",
            "combination approach as a viable alternative to the model based combination\n",
            "approach, where two DID systems are built using the two VSMs (Phonotactic and\n",
            "Acoustic) and the final prediction score is the output score combination from\n",
            "the two systems.\n",
            "Actual Title : \n",
            "Multi-view Dimensionality Reduction for Dialect Identification of Arabic\n",
            "  Broadcast Speech\n",
            "Generated Title : \n",
            "modeling based speech dialect dialect identification\n",
            "Abstract : \n",
            "Detecting outliers which are grossly different from or inconsistent with the\n",
            "remaining dataset is a major challenge in real-world KDD applications. Existing\n",
            "outlier detection methods are ineffective on scattered real-world datasets due\n",
            "to implicit data patterns and parameter setting issues. We define a novel\n",
            "\"Local Distance-based Outlier Factor\" (LDOF) to measure the {outlier-ness} of\n",
            "objects in scattered datasets which addresses these issues. LDOF uses the\n",
            "relative location of an object to its neighbours to determine the degree to\n",
            "which the object deviates from its neighbourhood. Properties of LDOF are\n",
            "theoretically analysed including LDOF's lower bound and its false-detection\n",
            "probability, as well as parameter settings. In order to facilitate parameter\n",
            "settings in real-world applications, we employ a top-n technique in our outlier\n",
            "detection approach, where only the objects with the highest LDOF values are\n",
            "regarded as outliers. Compared to conventional approaches (such as top-n KNN\n",
            "and top-n LOF), our method top-n LDOF is more effective at detecting outliers\n",
            "in scattered data. It is also easier to set parameters, since its performance\n",
            "is relatively stable over a large range of parameter values, as illustrated by\n",
            "experimental results on both real-world and synthetic datasets.\n",
            "Actual Title : \n",
            "A New Local Distance-Based Outlier Detection Approach for Scattered\n",
            "  Real-World Data\n",
            "Generated Title : \n",
            "new local distance based outlier detection for scattered real world\n",
            "Abstract : \n",
            "Recurrent Neural Networks (RNNs) have been widely used in natural language\n",
            "processing and computer vision. Among them, the Hierarchical Multi-scale RNN\n",
            "(HM-RNN), a kind of multi-scale hierarchical RNN proposed recently, can learn\n",
            "the hierarchical temporal structure from data automatically. In this paper, we\n",
            "extend the work to solve the computer vision task of action recognition.\n",
            "However, in sequence-to-sequence models like RNN, it is normally very hard to\n",
            "discover the relationships between inputs and outputs given static inputs. As a\n",
            "solution, attention mechanism could be applied to extract the relevant\n",
            "information from input thus facilitating the modeling of input-output\n",
            "relationships. Based on these considerations, we propose a novel attention\n",
            "network, namely Hierarchical Multi-scale Attention Network (HM-AN), by\n",
            "combining the HM-RNN and the attention mechanism and apply it to action\n",
            "recognition. A newly proposed gradient estimation method for stochastic\n",
            "neurons, namely Gumbel-softmax, is exploited to implement the temporal boundary\n",
            "detectors and the stochastic hard attention mechanism. To amealiate the\n",
            "negative effect of sensitive temperature of the Gumbel-softmax, an adaptive\n",
            "temperature training method is applied to better the system performance. The\n",
            "experimental results demonstrate the improved effect of HM-AN over LSTM with\n",
            "attention on the vision task. Through visualization of what have been learnt by\n",
            "the networks, it can be observed that both the attention regions of images and\n",
            "the hierarchical temporal structure can be captured by HM-AN.\n",
            "Actual Title : \n",
            "Hierarchical Multi-scale Attention Networks for Action Recognition\n",
            "Generated Title : \n",
            "hierarchical lstm for for action recognition\n",
            "Abstract : \n",
            "Person Re-Identification (re-id) is a challenging task in computer vision,\n",
            "especially when there are limited training data from multiple camera views. In\n",
            "this paper, we pro- pose a deep learning based person re-identification method\n",
            "by transferring knowledge of mid-level attribute features and high-level\n",
            "classification features. Building on the idea that identity classification,\n",
            "attribute recognition and re- identification share the same mid-level semantic\n",
            "representations, they can be trained sequentially by fine-tuning one based on\n",
            "another. In our framework, we train identity classification and attribute\n",
            "recognition tasks from deep Convolutional Neural Network (dCNN) to learn person\n",
            "information. The information can be transferred to the person re-id task and\n",
            "improves its accuracy by a large margin. Further- more, a Long Short Term\n",
            "Memory(LSTM) based Recurrent Neural Network (RNN) component is extended by a\n",
            "spacial gate. This component is used in the re-id model to pay attention to\n",
            "certain spacial parts in each recurrent unit. Experimental results show that\n",
            "our method achieves 78.3% of rank-1 recognition accuracy on the CUHK03\n",
            "benchmark.\n",
            "Actual Title : \n",
            "Cross Domain Knowledge Transfer for Person Re-identification\n",
            "Generated Title : \n",
            "re identification by deep transfer transfer learning person re identification\n",
            "Abstract : \n",
            "Policy optimization methods have shown great promise in solving complex\n",
            "reinforcement and imitation learning tasks. While model-free methods are\n",
            "broadly applicable, they often require many samples to optimize complex\n",
            "policies. Model-based methods greatly improve sample-efficiency but at the cost\n",
            "of poor generalization, requiring a carefully handcrafted model of the system\n",
            "dynamics for each task. Recently, hybrid methods have been successful in\n",
            "trading off applicability for improved sample-complexity. However, these have\n",
            "been limited to continuous action spaces. In this work, we present a new hybrid\n",
            "method based on an approximation of the dynamics as an expectation over the\n",
            "next state under the current policy. This relaxation allows us to derive a\n",
            "novel hybrid policy gradient estimator, combining score function and pathwise\n",
            "derivative estimators, that is applicable to discrete action spaces. We show\n",
            "significant gains in sample complexity, ranging between $1.7$ and $25\\times$,\n",
            "when learning parameterized policies on Cart Pole, Acrobot, Mountain Car and\n",
            "Hand Mass. Our method is applicable to both discrete and continuous action\n",
            "spaces, when competing pathwise methods are limited to the latter.\n",
            "Actual Title : \n",
            "Deterministic Policy Optimization by Combining Pathwise and Score\n",
            "  Function Estimators for Discrete Action Spaces\n",
            "Generated Title : \n",
            "non linear function estimation in continuous action sequences\n",
            "Abstract : \n",
            "Possibilistic answer set programming (PASP) extends answer set programming\n",
            "(ASP) by attaching to each rule a degree of certainty. While such an extension\n",
            "is important from an application point of view, existing semantics are not\n",
            "well-motivated, and do not always yield intuitive results. To develop a more\n",
            "suitable semantics, we first introduce a characterization of answer sets of\n",
            "classical ASP programs in terms of possibilistic logic where an ASP program\n",
            "specifies a set of constraints on possibility distributions. This\n",
            "characterization is then naturally generalized to define answer sets of PASP\n",
            "programs. We furthermore provide a syntactic counterpart, leading to a\n",
            "possibilistic generalization of the well-known Gelfond-Lifschitz reduct, and we\n",
            "show how our framework can readily be implemented using standard ASP solvers.\n",
            "Actual Title : \n",
            "Possibilistic Answer Set Programming Revisited\n",
            "Generated Title : \n",
            "answer set programming with answer set programming\n",
            "Abstract : \n",
            "We extend the work of Narasimhan and Bilmes [30] for minimizing set functions\n",
            "representable as a difference between submodular functions. Similar to [30],\n",
            "our new algorithms are guaranteed to monotonically reduce the objective\n",
            "function at every step. We empirically and theoretically show that the\n",
            "per-iteration cost of our algorithms is much less than [30], and our algorithms\n",
            "can be used to efficiently minimize a difference between submodular functions\n",
            "under various combinatorial constraints, a problem not previously addressed. We\n",
            "provide computational bounds and a hardness result on the mul- tiplicative\n",
            "inapproximability of minimizing the difference between submodular functions. We\n",
            "show, however, that it is possible to give worst-case additive bounds by\n",
            "providing a polynomial time computable lower-bound on the minima. Finally we\n",
            "show how a number of machine learning problems can be modeled as minimizing the\n",
            "difference between submodular functions. We experimentally show the validity of\n",
            "our algorithms by testing them on the problem of feature selection with\n",
            "submodular cost features.\n",
            "Actual Title : \n",
            "Algorithms for Approximate Minimization of the Difference Between\n",
            "  Submodular Functions, with Applications\n",
            "Generated Title : \n",
            "the difference of difference difference difference of the\n",
            "Abstract : \n",
            "We interpret HyperNetworks within the framework of variational inference\n",
            "within implicit distributions. Our method, Bayes by Hypernet, is able to model\n",
            "a richer variational distribution than previous methods. Experiments show that\n",
            "it achieves comparable predictive performance on the MNIST classification task\n",
            "while providing higher predictive uncertainties compared to MC-Dropout and\n",
            "regular maximum likelihood training.\n",
            "Actual Title : \n",
            "Implicit Weight Uncertainty in Neural Networks\n",
            "Generated Title : \n",
            "variational implicit implicit variational networks\n",
            "Abstract : \n",
            "We propose an automatic method to infer high dynamic range illumination from\n",
            "a single, limited field-of-view, low dynamic range photograph of an indoor\n",
            "scene. In contrast to previous work that relies on specialized image capture,\n",
            "user input, and/or simple scene models, we train an end-to-end deep neural\n",
            "network that directly regresses a limited field-of-view photo to HDR\n",
            "illumination, without strong assumptions on scene geometry, material\n",
            "properties, or lighting. We show that this can be accomplished in a three step\n",
            "process: 1) we train a robust lighting classifier to automatically annotate the\n",
            "location of light sources in a large dataset of LDR environment maps, 2) we use\n",
            "these annotations to train a deep neural network that predicts the location of\n",
            "lights in a scene from a single limited field-of-view photo, and 3) we\n",
            "fine-tune this network using a small dataset of HDR environment maps to predict\n",
            "light intensities. This allows us to automatically recover high-quality HDR\n",
            "illumination estimates that significantly outperform previous state-of-the-art\n",
            "methods. Consequently, using our illumination estimates for applications like\n",
            "3D object insertion, we can achieve results that are photo-realistic, which is\n",
            "validated via a perceptual user study.\n",
            "Actual Title : \n",
            "Learning to Predict Indoor Illumination from a Single Image\n",
            "Generated Title : \n",
            "to navigate by predicting light field\n",
            "Abstract : \n",
            "This paper describes a novel storyboarding scheme that uses a model trained\n",
            "on pairwise image comparisons to identify images likely to be of interest to a\n",
            "mobile robot user. Traditional storyboarding schemes typically attempt to\n",
            "summarise robot observations using predefined novelty or image quality\n",
            "objectives, but we propose a user training stage that allows the incorporation\n",
            "of user interest when storyboarding. Our approach dramatically reduces the\n",
            "number of image comparisons required to infer image interest by applying a\n",
            "Gaussian process smoothing algorithm on image features extracted using a\n",
            "pre-trained convolutional neural network. As a particularly valuable\n",
            "by-product, the proposed approach allows the generation of user-specific\n",
            "saliency or attention maps.\n",
            "Actual Title : \n",
            "User-driven mobile robot storyboarding: Learning image interest and\n",
            "  saliency from pairwise image comparisons\n",
            "Generated Title : \n",
            "mobile robot for image for image saliency detection\n",
            "Abstract : \n",
            "Change detection is one of the central problems in earth observation and was\n",
            "extensively investigated over recent decades. In this paper, we propose a novel\n",
            "recurrent convolutional neural network (ReCNN) architecture, which is trained\n",
            "to learn a joint spectral-spatial-temporal feature representation in a unified\n",
            "framework for change detection in multispectral images. To this end, we bring\n",
            "together a convolutional neural network (CNN) and a recurrent neural network\n",
            "(RNN) into one end-to-end network. The former is able to generate rich\n",
            "spectral-spatial feature representations, while the latter effectively analyzes\n",
            "temporal dependency in bi-temporal images. In comparison with previous\n",
            "approaches to change detection, the proposed network architecture possesses\n",
            "three distinctive properties: 1) It is end-to-end trainable, in contrast to\n",
            "most existing methods whose components are separately trained or computed; 2)\n",
            "it naturally harnesses spatial information that has been proven to be\n",
            "beneficial to change detection task; 3) it is capable of adaptively learning\n",
            "the temporal dependency between multitemporal images, unlike most of algorithms\n",
            "that use fairly simple operation like image differencing or stacking. As far as\n",
            "we know, this is the first time that a recurrent convolutional network\n",
            "architecture has been proposed for multitemporal remote sensing image analysis.\n",
            "The proposed network is validated on real multispectral data sets. Both visual\n",
            "and quantitative analysis of experimental results demonstrates competitive\n",
            "performance in the proposed mode.\n",
            "Actual Title : \n",
            "Learning Spectral-Spatial-Temporal Features via a Recurrent\n",
            "  Convolutional Neural Network for Change Detection in Multispectral Imagery\n",
            "Generated Title : \n",
            "and representation learning for convolutional neural networks for\n",
            "Abstract : \n",
            "Feature representations, both hand-designed and learned ones, are often hard\n",
            "to analyze and interpret, even when they are extracted from visual data. We\n",
            "propose a new approach to study image representations by inverting them with an\n",
            "up-convolutional neural network. We apply the method to shallow representations\n",
            "(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our\n",
            "approach provides significantly better reconstructions than existing methods,\n",
            "revealing that there is surprisingly rich information contained in these\n",
            "features. Inverting a deep network trained on ImageNet provides several\n",
            "insights into the properties of the feature representation learned by the\n",
            "network. Most strikingly, the colors and the rough contours of an image can be\n",
            "reconstructed from activations in higher network layers and even from the\n",
            "predicted class probabilities.\n",
            "Actual Title : \n",
            "Inverting Visual Representations with Convolutional Networks\n",
            "Generated Title : \n",
            "in deep convolutional networks\n",
            "Abstract : \n",
            "We investigate neural techniques for end-to-end computational argumentation\n",
            "mining (AM). We frame AM both as a token-based dependency parsing and as a\n",
            "token-based sequence tagging problem, including a multi-task learning setup.\n",
            "Contrary to models that operate on the argument component level, we find that\n",
            "framing AM as dependency parsing leads to subpar performance results. In\n",
            "contrast, less complex (local) tagging models based on BiLSTMs perform robustly\n",
            "across classification scenarios, being able to catch long-range dependencies\n",
            "inherent to the AM problem. Moreover, we find that jointly learning 'natural'\n",
            "subtasks, in a multi-task learning setup, improves performance.\n",
            "Actual Title : \n",
            "Neural End-to-End Learning for Computational Argumentation Mining\n",
            "Generated Title : \n",
            "parsing parsing with a transition based parsing\n",
            "Abstract : \n",
            "Face alignment is a classic problem in the computer vision field. Previous\n",
            "works mostly focus on sparse alignment with a limited number of facial landmark\n",
            "points, i.e., facial landmark detection. In this paper, for the first time, we\n",
            "aim at providing a very dense 3D alignment for large-pose face images. To\n",
            "achieve this, we train a CNN to estimate the 3D face shape, which not only\n",
            "aligns limited facial landmarks but also fits face contours and SIFT feature\n",
            "points. Moreover, we also address the bottleneck of training CNN with multiple\n",
            "datasets, due to different landmark markups on different datasets, such as 5,\n",
            "34, 68. Experimental results show our method not only provides high-quality,\n",
            "dense 3D face fitting but also outperforms the state-of-the-art facial landmark\n",
            "detection methods on the challenging datasets. Our model can run at real time\n",
            "during testing.\n",
            "Actual Title : \n",
            "Dense Face Alignment\n",
            "Generated Title : \n",
            "alignment alignment for dense #d facial expression\n",
            "Abstract : \n",
            "In the domain of image processing, often real-time constraints are required.\n",
            "In particular, in safety-critical applications, such as X-ray computed\n",
            "tomography in medical imaging or advanced driver assistance systems in the\n",
            "automotive domain, timing is of utmost importance. A common approach to\n",
            "maintain real-time capabilities of compute-intensive applications is to offload\n",
            "those computations to dedicated accelerator hardware, such as Field\n",
            "Programmable Gate Arrays (FPGAs). Programming such architectures is a\n",
            "challenging task, with respect to the typical FPGA-specific design criteria:\n",
            "Achievable overall algorithm latency and resource usage of FPGA primitives\n",
            "(BRAM, FF, LUT, and DSP). High-Level Synthesis (HLS) dramatically simplifies\n",
            "this task by enabling the description of algorithms in well-known higher\n",
            "languages (C/C++) and its automatic synthesis that can be accomplished by HLS\n",
            "tools. However, algorithm developers still need expert knowledge about the\n",
            "target architecture, in order to achieve satisfying results. Therefore, in\n",
            "previous work, we have shown that elevating the description of image algorithms\n",
            "to an even higher abstraction level, by using a Domain-Specific Language (DSL),\n",
            "can significantly cut down the complexity for designing such algorithms for\n",
            "FPGAs. To give the developer even more control over the common trade-off,\n",
            "latency vs. resource usage, we will present an automatic optimization process\n",
            "where these criteria are analyzed and fed back to the DSL compiler, in order to\n",
            "generate code that is closer to the desired design specifications. Finally, we\n",
            "generate code for stereo block matching algorithms and compare it with\n",
            "handwritten implementations to quantify the quality of our results.\n",
            "Actual Title : \n",
            "Automatic Optimization of Hardware Accelerators for Image Processing\n",
            "Generated Title : \n",
            "architecture for computer vision\n",
            "Abstract : \n",
            "We study the problem of inducing interpretability in KG embeddings.\n",
            "Specifically, we explore the Universal Schema (Riedel et al., 2013) and propose\n",
            "a method to induce interpretability. There have been many vector space models\n",
            "proposed for the problem, however, most of these methods don't address the\n",
            "interpretability (semantics) of individual dimensions. In this work, we study\n",
            "this problem and propose a method for inducing interpretability in KG\n",
            "embeddings using entity co-occurrence statistics. The proposed method\n",
            "significantly improves the interpretability, while maintaining comparable\n",
            "performance in other KG tasks.\n",
            "Actual Title : \n",
            "Inducing Interpretability in Knowledge Graph Embeddings\n",
            "Generated Title : \n",
            "interpretability in knowledge graph embeddings\n",
            "Abstract : \n",
            "Deep learning has significantly advanced the state of the art in artificial\n",
            "intelligence, gaining wide popularity from both industry and academia. Special\n",
            "interest is around Convolutional Neural Networks (CNN), which take inspiration\n",
            "from the hierarchical structure of the visual cortex, to form deep layers of\n",
            "convolutional operations, along with fully connected classifiers. Hardware\n",
            "implementations of these deep CNN architectures are challenged with memory\n",
            "bottlenecks that require many convolution and fully-connected layers demanding\n",
            "large amount of communication for parallel computation. Multi-core CPU based\n",
            "solutions have demonstrated their inadequacy for this problem due to the memory\n",
            "wall and low parallelism. Many-core GPU architectures show superior performance\n",
            "but they consume high power and also have memory constraints due to\n",
            "inconsistencies between cache and main memory. FPGA design solutions are also\n",
            "actively being explored, which allow implementing the memory hierarchy using\n",
            "embedded BlockRAM. This boosts the parallel use of shared memory elements\n",
            "between multiple processing units, avoiding data replicability and\n",
            "inconsistencies. This makes FPGAs potentially powerful solutions for real-time\n",
            "classification of CNNs. Both Altera and Xilinx have adopted OpenCL co-design\n",
            "framework from GPU for FPGA designs as a pseudo-automatic development solution.\n",
            "In this paper, a comprehensive evaluation and comparison of Altera and Xilinx\n",
            "OpenCL frameworks for a 5-layer deep CNN is presented. Hardware resources,\n",
            "temporal performance and the OpenCL architecture for CNNs are discussed. Xilinx\n",
            "demonstrates faster synthesis, better FPGA resource utilization and more\n",
            "compact boards. Altera provides multi-platforms tools, mature design community\n",
            "and better execution times.\n",
            "Actual Title : \n",
            "Comprehensive Evaluation of OpenCL-based Convolutional Neural Network\n",
            "  Accelerators in Xilinx and Altera FPGAs\n",
            "Generated Title : \n",
            "a large scale # deep embedded deep embedded\n",
            "Abstract : \n",
            "We introduce GAMSEL (Generalized Additive Model Selection), a penalized\n",
            "likelihood approach for fitting sparse generalized additive models in high\n",
            "dimension. Our method interpolates between null, linear and additive models by\n",
            "allowing the effect of each variable to be estimated as being either zero,\n",
            "linear, or a low-complexity curve, as determined by the data. We present a\n",
            "blockwise coordinate descent procedure for efficiently optimizing the penalized\n",
            "likelihood objective over a dense grid of the tuning parameter, producing a\n",
            "regularization path of additive models. We demonstrate the performance of our\n",
            "method on both real and simulated data examples, and compare it with existing\n",
            "techniques for additive model selection.\n",
            "Actual Title : \n",
            "Generalized Additive Model Selection\n",
            "Generated Title : \n",
            "additive model selection\n",
            "Abstract : \n",
            "There have been intensive research interests in ship detection and\n",
            "segmentation due to high demands on a wide range of civil applications in the\n",
            "last two decades. However, existing approaches, which are mainly based on\n",
            "statistical properties of images, fail to detect smaller ships and boats.\n",
            "Specifically, known techniques are not robust enough in view of inevitable\n",
            "small geometric and photometric changes in images consisting of ships. In this\n",
            "paper a novel approach for ship detection is proposed based on correlation of\n",
            "maritime images. The idea comes from the observation that a fine pattern of the\n",
            "sea surface changes considerably from time to time whereas the ship appearance\n",
            "basically keeps unchanged. We want to examine whether the images have a common\n",
            "unaltered part, a ship in this case. To this end, we developed a method -\n",
            "Focused Correlation (FC) to achieve robustness to geometric distortions of the\n",
            "image content. Various experiments have been conducted to evaluate the\n",
            "effectiveness of the proposed approach.\n",
            "Actual Title : \n",
            "Ship Detection and Segmentation using Image Correlation\n",
            "Generated Title : \n",
            "detection using correlation explanation\n",
            "Abstract : \n",
            "Close-range Photogrammetry is widely used in many industries because of the\n",
            "cost effectiveness and efficiency of the technique. In this research, we\n",
            "introduce an automated coded target detection method which can be used to\n",
            "enhance the efficiency of the Photogrammetry.\n",
            "Actual Title : \n",
            "Automatic Detection and Decoding of Photogrammetric Coded Targets\n",
            "Generated Title : \n",
            "detection and decoding of <unk> coded targets\n",
            "Abstract : \n",
            "Developers often wonder how to implement a certain functionality (e.g., how\n",
            "to parse XML files) using APIs. Obtaining an API usage sequence based on an\n",
            "API-related natural language query is very helpful in this regard. Given a\n",
            "query, existing approaches utilize information retrieval models to search for\n",
            "matching API sequences. These approaches treat queries and APIs as bag-of-words\n",
            "(i.e., keyword matching or word-to-word alignment) and lack a deep\n",
            "understanding of the semantics of the query.\n",
            "  We propose DeepAPI, a deep learning based approach to generate API usage\n",
            "sequences for a given natural language query. Instead of a bags-of-words\n",
            "assumption, it learns the sequence of words in a query and the sequence of\n",
            "associated APIs. DeepAPI adapts a neural language model named RNN\n",
            "Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length\n",
            "context vector, and generates an API sequence based on the context vector. We\n",
            "also augment the RNN Encoder-Decoder by considering the importance of\n",
            "individual APIs. We empirically evaluate our approach with more than 7 million\n",
            "annotated code snippets collected from GitHub. The results show that our\n",
            "approach generates largely accurate API sequences and outperforms the related\n",
            "approaches.\n",
            "Actual Title : \n",
            "Deep API Learning\n",
            "Generated Title : \n",
            "api learning with deep learning\n",
            "Abstract : \n",
            "Maximally stable component detection is a very popular method for feature\n",
            "analysis in images, mainly due to its low computation cost and high\n",
            "repeatability. With the recent advance of feature-based methods in geometric\n",
            "shape analysis, there is significant interest in finding analogous approaches\n",
            "in the 3D world. In this paper, we formulate a diffusion-geometric framework\n",
            "for stable component detection in non-rigid 3D shapes, which can be used for\n",
            "geometric feature detection and description. A quantitative evaluation of our\n",
            "method on the SHREC'10 feature detection benchmark shows its potential as a\n",
            "source of high-quality features.\n",
            "Actual Title : \n",
            "Diffusion-geometric maximally stable component detection in deformable\n",
            "  shapes\n",
            "Generated Title : \n",
            "maximally stable component detection and detection\n",
            "Abstract : \n",
            "In recent years genetic algorithms have emerged as a useful tool for the\n",
            "heuristic solution of complex discrete optimisation problems. In particular\n",
            "there has been considerable interest in their use in tackling problems arising\n",
            "in the areas of scheduling and timetabling. However, the classical genetic\n",
            "algorithm paradigm is not well equipped to handle constraints and successful\n",
            "implementations usually require some sort of modification to enable the search\n",
            "to exploit problem specific knowledge in order to overcome this shortcoming.\n",
            "This paper is concerned with the development of a family of genetic algorithms\n",
            "for the solution of a nurse rostering problem at a major UK hospital. The\n",
            "hospital is made up of wards of up to 30 nurses. Each ward has its own group of\n",
            "nurses whose shifts have to be scheduled on a weekly basis. In addition to\n",
            "fulfilling the minimum demand for staff over three daily shifts, nurses' wishes\n",
            "and qualifications have to be taken into account. The schedules must also be\n",
            "seen to be fair, in that unpopular shifts have to be spread evenly amongst all\n",
            "nurses, and other restrictions, such as team nursing and special conditions for\n",
            "senior staff, have to be satisfied. The basis of the family of genetic\n",
            "algorithms is a classical genetic algorithm consisting of n-point crossover,\n",
            "single-bit mutation and a rank-based selection. The solution space consists of\n",
            "all schedules in which each nurse works the required number of shifts, but the\n",
            "remaining constraints, both hard and soft, are relaxed and penalised in the\n",
            "fitness function. The talk will start with a detailed description of the\n",
            "problem and the initial implementation and will go on to highlight the\n",
            "shortcomings of such an approach, in terms of the key element of balancing\n",
            "feasibility, i.e. covering the demand and work regulations, and quality, as\n",
            "measured by the nurses' preferences. A series of experiments involving\n",
            "parameter adaptation, niching, intelligent weights, delta coding, local hill\n",
            "climbing, migration and special selection rules will then be outlined and it\n",
            "will be shown how a series of these enhancements were able to eradicate these\n",
            "difficulties. Results based on several months' real data will be used to\n",
            "measure the impact of each modification, and to show that the final algorithm\n",
            "is able to compete with a tabu search approach currently employed at the\n",
            "hospital. The talk will conclude with some observations as to the overall\n",
            "quality of this approach to this and similar problems.\n",
            "Actual Title : \n",
            "Nurse Rostering with Genetic Algorithms\n",
            "Generated Title : \n",
            "rostering with genetic algorithms\n",
            "Abstract : \n",
            "Automatic continuous speech recognition (CSR) is sufficiently mature that a\n",
            "variety of real world applications are now possible including large vocabulary\n",
            "transcription and interactive spoken dialogues. This paper reviews the\n",
            "evolution of the statistical modelling techniques which underlie current-day\n",
            "systems, specifically hidden Markov models (HMMs) and N-grams. Starting from a\n",
            "description of the speech signal and its parameterisation, the various\n",
            "modelling assumptions and their consequences are discussed. It then describes\n",
            "various techniques by which the effects of these assumptions can be mitigated.\n",
            "Despite the progress that has been made, the limitations of current modelling\n",
            "techniques are still evident. The paper therefore concludes with a brief review\n",
            "of some of the more fundamental modelling work now in progress.\n",
            "Actual Title : \n",
            "Statistical Modeling in Continuous Speech Recognition (CSR)(Invited\n",
            "  Talk)\n",
            "Generated Title : \n",
            "#### of the recognition and continuous speech\n",
            "Abstract : \n",
            "This paper deals with the revision of partially ordered beliefs. It proposes\n",
            "a semantic representation of epistemic states by partial pre-orders on\n",
            "interpretations and a syntactic representation by partially ordered belief\n",
            "bases. Two revision operations, the revision stemming from the history of\n",
            "observations and the possibilistic revision, defined when the epistemic state\n",
            "is represented by a total pre-order, are generalized, at a semantic level, to\n",
            "the case of a partial pre-order on interpretations, and at a syntactic level,\n",
            "to the case of a partially ordered belief base. The equivalence between the two\n",
            "representations is shown for the two revision operations.\n",
            "Actual Title : \n",
            "Revising Partially Ordered Beliefs\n",
            "Generated Title : \n",
            "partially ordered beliefs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf5HB9EzEsuT"
      },
      "source": [
        "df2 = pd.DataFrame(df1)\n",
        "df2.to_csv('./drive/MyDrive/generated_titles_5.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmQoIvoIF3Ot"
      },
      "source": [
        "#Calculate BLEU SCORE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEhtRprZF3Ow"
      },
      "source": [
        "ref = []\n",
        "pred= []\n",
        "ref=[[t.split()] for t in df1['Title']]\n",
        "pred=[t.split() for t in df1['Generated Title']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Vw_owRKYW6"
      },
      "source": [
        "ref=  [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\n",
        "\n",
        "pred = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_qvqlsYF3O0"
      },
      "source": [
        "ref"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nALKSthYF3O5",
        "outputId": "b6b5d21b-6d8b-48aa-bf12-9fd9ab52fa4d"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "bleu_score(pred,ref,weights=[1,0,0,0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12435863708392587"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFQowqVg31yd"
      },
      "source": [
        "# Final model selected = final_model_5 ( from experiment 5)\n",
        "# Cherry Picking some examples from generated titles and comparing with Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcyWxYTu-WIe"
      },
      "source": [
        "final_titles = pd.read_csv('./drive/MyDrive/generated_titles_5.csv')\n",
        "base_titles = pd.read_csv('./drive/MyDrive/generated_titles_baseline.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3PRC8fR_wJN"
      },
      "source": [
        "idx = [41,42,58,61,121,143,160,162,188,192,231,232,240,291,403,452,947,992,941,883] ##Cheery picking\n",
        "compare_titles = base_titles.loc[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuftZm9EAnWc"
      },
      "source": [
        "compare_titles['Generated Title(Final)']=final_titles.loc[idx]['Generated Title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X14GTbj-B1aa"
      },
      "source": [
        "compare_titles.columns = ['No.','Abstract', 'Actual title','Generated Title(Baseline)','Generated Title(Final)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NykQzACvBF25",
        "outputId": "b6a24897-f182-4629-a281-fb54c71521cb"
      },
      "source": [
        "pd.set_option(\"display.max_colwidth\",1000)\n",
        "compare_titles.iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Actual title</th>\n",
              "      <th>Generated Title(Baseline)</th>\n",
              "      <th>Generated Title(Final)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>This paper reviews Kunchenko's polynomials using as template matching method\\nto recognize template in one-dimensional input signal. Kunchenko's polynomials\\nmethod is compared with classical methods - cross-correlation and sum of\\nsquared differences according to numerical statistical example.</td>\n",
              "      <td>Kunchenko's Polynomials for Template Matching</td>\n",
              "      <td>a new framework for learning work quality of phylogenetic qualitative</td>\n",
              "      <td>polynomials for template matching</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Although many successful ensemble clustering approaches have been developed\\nin recent years, there are still two limitations to most of the existing\\napproaches. First, they mostly overlook the issue of uncertain links, which may\\nmislead the overall consensus process. Second, they generally lack the ability\\nto incorporate global information to refine the local links. To address these\\ntwo limitations, in this paper, we propose a novel ensemble clustering approach\\nbased on sparse graph representation and probability trajectory analysis. In\\nparticular, we present the elite neighbor selection strategy to identify the\\nuncertain links by locally adaptive thresholds and build a sparse graph with a\\nsmall number of probably reliable links. We argue that a small number of\\nprobably reliable links can lead to significantly better consensus results than\\nusing all graph links regardless of their reliability. The random walk process\\ndriven by a new transition probability matrix is util...</td>\n",
              "      <td>Robust Ensemble Clustering Using Probability Trajectories</td>\n",
              "      <td>a new framework for work single linkage optimization &lt;eos&gt;</td>\n",
              "      <td>ensemble clustering with sparse ensemble</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>We study black-box attacks on machine learning classifiers where each query\\nto the model incurs some cost or risk of detection to the adversary. We focus\\nexplicitly on minimizing the number of queries as a major objective.\\nSpecifically, we consider the problem of attacking machine learning classifiers\\nsubject to a budget of feature modification cost while minimizing the number of\\nqueries, where each query returns only a class and confidence score. We\\ndescribe an approach that uses Bayesian optimization to minimize the number of\\nqueries, and find that the number of queries can be reduced to approximately\\none tenth of the number needed through a random strategy for scenarios where\\nthe feature modification cost budget is low.</td>\n",
              "      <td>Query-limited Black-box Attacks to Classifiers</td>\n",
              "      <td>a new approach to work &lt;unk&gt; of &lt;unk&gt; ai based &lt;eos&gt;</td>\n",
              "      <td>black box attacks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>We consider probabilistic topic models and more recent word embedding\\ntechniques from a perspective of learning hidden semantic representations.\\nInspired by a striking similarity of the two approaches, we merge them and\\nlearn probabilistic embeddings with online EM-algorithm on word co-occurrence\\ndata. The resulting embeddings perform on par with Skip-Gram Negative Sampling\\n(SGNS) on word similarity tasks and benefit in the interpretability of the\\ncomponents. Next, we learn probabilistic document embeddings that outperform\\nparagraph2vec on a document similarity task and require less memory and time\\nfor training. Finally, we employ multimodal Additive Regularization of Topic\\nModels (ARTM) to obtain a high sparsity and learn embeddings for other\\nmodalities, such as timestamps and categories. We observe further improvement\\nof word similarity performance and meaningful inter-modality similarities.</td>\n",
              "      <td>Interpretable probabilistic embeddings: bridging the gap between topic\\n  models and neural networks</td>\n",
              "      <td>a filters of work &lt;unk&gt;  roughly authorship  pseudo likelihood and and</td>\n",
              "      <td>probabilistic embeddings  bridging the topic models</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>The production of color language is essential for grounded language\\ngeneration. Color descriptions have many challenging properties: they can be\\nvague, compositionally complex, and denotationally rich. We present an\\neffective approach to generating color descriptions using recurrent neural\\nnetworks and a Fourier-transformed color representation. Our model outperforms\\nprevious work on a conditional language modeling task over a large corpus of\\nnaturalistic color descriptions. In addition, probing the model's output\\nreveals that it can accurately produce not only basic color terms but also\\ndescriptors with non-convex denotations (\"greenish\"), bare modifiers (\"bright\",\\n\"dull\"), and compositional phrases (\"faded teal\") not seen in training.</td>\n",
              "      <td>Learning to Generate Compositional Color Descriptions</td>\n",
              "      <td>&lt;unk&gt; a &lt;unk&gt; for hmm flaws structure  &lt;eos&gt;</td>\n",
              "      <td>to generate compositional color descriptions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>Short-term tracking is an open and challenging problem for which\\ndiscriminative correlation filters (DCF) have shown excellent performance. We\\nintroduce the channel and spatial reliability concepts to DCF tracking and\\nprovide a novel learning algorithm for its efficient and seamless integration\\nin the filter update and the tracking process. The spatial reliability map\\nadjusts the filter support to the part of the object suitable for tracking.\\nThis both allows to enlarge the search region and improves tracking of\\nnon-rectangular objects. Reliability scores reflect channel-wise quality of the\\nlearned filters and are used as feature weighting coefficients in localization.\\nExperimentally, with only two simple standard features, HoGs and Colornames,\\nthe novel CSR-DCF method -- DCF with Channel and Spatial Reliability --\\nachieves state-of-the-art results on VOT 2016, VOT 2015 and OTB100. The CSR-DCF\\nruns in real-time on a CPU.</td>\n",
              "      <td>Discriminative Correlation Filter with Channel and Spatial Reliability</td>\n",
              "      <td>a new approach to work &lt;unk&gt; of audience  ai based using</td>\n",
              "      <td>correlation filter for for multi object tracking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>We consider the problem of using sentence compression techniques to\\nfacilitate query-focused multi-document summarization. We present a\\nsentence-compression-based framework for the task, and design a series of\\nlearning-based compression models built on parse trees. An innovative beam\\nsearch decoder is proposed to efficiently find highly probable compressions.\\nUnder this framework, we show how to integrate various indicative metrics such\\nas linguistic motivation and query relevance into the compression process by\\nderiving a novel formulation of a compression scoring function. Our best model\\nachieves statistically significant improvement over the state-of-the-art\\nsystems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2\\nrespectively) for the DUC 2006 and 2007 summarization task.</td>\n",
              "      <td>A Sentence Compression Based Framework to Query-Focused Multi-Document\\n  Summarization</td>\n",
              "      <td>a bayesian model for leverages developments  digital in a squared loss</td>\n",
              "      <td>sentence compression for automatic text summarization</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>Neural sequence-to-sequence models have provided a viable new approach for\\nabstractive text summarization (meaning they are not restricted to simply\\nselecting and rearranging passages from the original text). However, these\\nmodels have two shortcomings: they are liable to reproduce factual details\\ninaccurately, and they tend to repeat themselves. In this work we propose a\\nnovel architecture that augments the standard sequence-to-sequence attentional\\nmodel in two orthogonal ways. First, we use a hybrid pointer-generator network\\nthat can copy words from the source text via pointing, which aids accurate\\nreproduction of information, while retaining the ability to produce novel words\\nthrough the generator. Second, we use coverage to keep track of what has been\\nsummarized, which discourages repetition. We apply our model to the CNN / Daily\\nMail summarization task, outperforming the current abstractive state-of-the-art\\nby at least 2 ROUGE points.</td>\n",
              "      <td>Get To The Point: Summarization with Pointer-Generator Networks</td>\n",
              "      <td>a new framework for work single linkage learning of deep neural</td>\n",
              "      <td>abstractive summarization with pointer generator networks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>Ezhil is a Tamil language based interpreted procedural programming language.\\nTamil keywords and grammar are chosen to make the native Tamil speaker write\\nprograms in the Ezhil system. Ezhil allows easy representation of computer\\nprogram closer to the Tamil language logical constructs equivalent to the\\nconditional, branch and loop statements in modern English based programming\\nlanguages. Ezhil is a compact programming language aimed towards Tamil speaking\\nnovice computer users. Grammar for Ezhil and a few example programs are\\nreported here, from the initial proof-of-concept implementation using the\\nPython programming language1. To the best of our knowledge, Ezhil language is\\nthe first freely available Tamil programming language.</td>\n",
              "      <td>Ezhil: A Tamil Programming Language</td>\n",
              "      <td>&lt;unk&gt; a deep neural network for standardized learning &lt;eos&gt;</td>\n",
              "      <td>a tamil programming language</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>Modeling emotional-cognition is in a nascent stage and therefore wide-open\\nfor new ideas and discussions. In this paper the author looks at the modeling\\nproblem by bringing in ideas from axiomatic mathematics, information theory,\\ncomputer science, molecular biology, non-linear dynamical systems and quantum\\ncomputing and explains how ideas from these disciplines may have applications\\nin modeling emotional-cognition.</td>\n",
              "      <td>A novice looks at emotional cognition</td>\n",
              "      <td>a new approach to work autoencoding problem problem &lt;eos&gt;</td>\n",
              "      <td>novice looks at emotional cognition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>We consider additive models built with trend filtering, i.e., additive models\\nwhose components are each regularized by the (discrete) total variation of\\ntheir $(k+1)$st (discrete) derivative, for a chosen integer $k \\geq 0$. This\\nresults in $k$th degree piecewise polynomial components, (e.g., $k=0$ gives\\npiecewise constant components, $k=1$ gives piecewise linear, $k=2$ gives\\npiecewise quadratic, etc.). In univariate nonparametric regression, the\\nlocalized nature of the total variation regularizer used by trend filtering has\\nbeen shown to produce estimates with superior local adaptivity to those from\\nsmoothing splines (and linear smoothers, more generally) (Tibshirani [2014]).\\nFurther, the structured nature of this regularizer has been shown to lead to\\nhighly efficient computational routines for trend filtering (Kim et al. [2009],\\nRamdas and Tibshirani [2016]). In this paper, we argue that both of these\\nproperties carry over to the additive models setting. We derive fas...</td>\n",
              "      <td>Additive Models with Trend Filtering</td>\n",
              "      <td>a bayesian optimization approach to find work discussion of biology</td>\n",
              "      <td>additive models with trend filtering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>Dictionary learning for sparse representations is traditionally approached\\nwith sequential atom updates, in which an optimized atom is used immediately\\nfor the optimization of the next atoms. We propose instead a Jacobi version, in\\nwhich groups of atoms are updated independently, in parallel. Extensive\\nnumerical evidence for sparse image representation shows that the parallel\\nalgorithms, especially when all atoms are updated simultaneously, give better\\ndictionaries than their sequential counterparts.</td>\n",
              "      <td>Overcomplete Dictionary Learning with Jacobi Atom Updates</td>\n",
              "      <td>a new approach to work l # norm &lt;unk&gt; problem &lt;eos&gt;</td>\n",
              "      <td>dictionary learning with jacobi atom updates</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>Evolutionary algorithms (EAs) are population-based general-purpose\\noptimization algorithms, and have been successfully applied in various\\nreal-world optimization tasks. However, previous theoretical studies often\\nemploy EAs with only a parent or offspring population and focus on specific\\nproblems. Furthermore, they often only show upper bounds on the running time,\\nwhile lower bounds are also necessary to get a complete understanding of an\\nalgorithm. In this paper, we analyze the running time of the\\n($\\mu$+$\\lambda$)-EA (a general population-based EA with mutation only) on the\\nclass of pseudo-Boolean functions with a unique global optimum. By applying the\\nrecently proposed switch analysis approach, we prove the lower bound $\\Omega(n\\n\\ln n+ \\mu + \\lambda n\\ln\\ln n/ \\ln n)$ for the first time. Particularly on the\\ntwo widely-studied problems, OneMax and LeadingOnes, the derived lower bound\\ndiscloses that the ($\\mu$+$\\lambda$)-EA will be strictly slower than the\\n(1+1)-EA wh...</td>\n",
              "      <td>A Lower Bound Analysis of Population-based Evolutionary Algorithms for\\n  Pseudo-Boolean Functions</td>\n",
              "      <td>a bayesian optimization approach to raising and separate generator &lt;eos&gt;</td>\n",
              "      <td>lower bound for population based evolutionary algorithms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>This paper proposes to use probabilistic model checking to synthesize optimal\\nrobot policies in multi-tasking autonomous systems that are subject to\\nhuman-robot interaction. Given the convincing empirical evidence that human\\nbehavior can be related to reinforcement models, we take as input a\\nwell-studied Q-table model of the human behavior for flexible scenarios. We\\nfirst describe an automated procedure to distill a Markov decision process\\n(MDP) for the human in an arbitrary but fixed scenario. The distinctive issue\\nis that -- in contrast to existing models -- under-specification of the human\\nbehavior is included. Probabilistic model checking is used to predict the\\nhuman's behavior. Finally, the MDP model is extended with a robot model.\\nOptimal robot policies are synthesized by analyzing the resulting two-player\\nstochastic game. Experimental results with a prototypical implementation using\\nPRISM show promising results.</td>\n",
              "      <td>Probabilistic Model Checking for Complex Cognitive Tasks -- A case study\\n  in human-robot interaction</td>\n",
              "      <td>a new approach to work &lt;unk&gt; of audience  skip connections and</td>\n",
              "      <td>model checking for cognitive behavior in a probabilistic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>We analyze differences between two information-theoretically motivated\\napproaches to statistical inference and model selection: the Minimum\\nDescription Length (MDL) principle, and the Minimum Message Length (MML)\\nprinciple. Based on this analysis, we present two revised versions of MML: a\\npointwise estimator which gives the MML-optimal single parameter model, and a\\nvolumewise estimator which gives the MML-optimal region in the parameter space.\\nOur empirical results suggest that with small data sets, the MDL approach\\nyields more accurate predictions than the MML estimators. The empirical results\\nalso demonstrate that the revised MML estimators introduced here perform better\\nthan the original MML estimator suggested by Wallace and Freeman.</td>\n",
              "      <td>Minimum Encoding Approaches for Predictive Modeling</td>\n",
              "      <td>a filters of work &lt;unk&gt; algorithm for work &lt;unk&gt; of</td>\n",
              "      <td>encoding approaches for predictive analysis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>Flower pollination algorithm is a new nature-inspired algorithm, based on the\\ncharacteristics of flowering plants. In this paper, we extend this flower\\nalgorithm to solve multi-objective optimization problems in engineering. By\\nusing the weighted sum method with random weights, we show that the proposed\\nmulti-objective flower algorithm can accurately find the Pareto fronts for a\\nset of test functions. We then solve a bi-objective disc brake design problem,\\nwhich indeed converges quickly.</td>\n",
              "      <td>Multi-objective Flower Algorithm for Optimization</td>\n",
              "      <td>a bayesian model for leverages developments  come &lt;eos&gt;</td>\n",
              "      <td>flower algorithm for optimization</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>947</th>\n",
              "      <td>In this work, we present a new dataset for computational humor, specifically\\ncomparative humor ranking, which attempts to eschew the ubiquitous binary\\napproach to humor detection. The dataset consists of tweets that are humorous\\nresponses to a given hashtag. We describe the motivation for this new dataset,\\nas well as the collection process, which includes a description of our\\nsemi-automated system for data collection. We also present initial experiments\\nfor this dataset using both unsupervised and supervised approaches. Our best\\nsupervised system achieved 63.7% accuracy, suggesting that this task is much\\nmore difficult than comparable humor detection tasks. Initial experiments\\nindicate that a character-level model is more suitable for this task than a\\ntoken-level model, likely due to a large amount of puns that can be captured by\\na character-level model.</td>\n",
              "      <td>#HashtagWars: Learning a Sense of Humor</td>\n",
              "      <td>a new approach to work autoencoding of in structure  soccer</td>\n",
              "      <td>a deep learning of humor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992</th>\n",
              "      <td>We introduce GAMSEL (Generalized Additive Model Selection), a penalized\\nlikelihood approach for fitting sparse generalized additive models in high\\ndimension. Our method interpolates between null, linear and additive models by\\nallowing the effect of each variable to be estimated as being either zero,\\nlinear, or a low-complexity curve, as determined by the data. We present a\\nblockwise coordinate descent procedure for efficiently optimizing the penalized\\nlikelihood objective over a dense grid of the tuning parameter, producing a\\nregularization path of additive models. We demonstrate the performance of our\\nmethod on both real and simulated data examples, and compare it with existing\\ntechniques for additive model selection.</td>\n",
              "      <td>Generalized Additive Model Selection</td>\n",
              "      <td>a &lt;unk&gt; approach to work &lt;unk&gt; certain of cifar soccer</td>\n",
              "      <td>additive model selection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>941</th>\n",
              "      <td>Skin cancer, the most common human malignancy, is primarily diagnosed\\nvisually by physicians [1]. Classification with an automated method like CNN\\n[2, 3] shows potential for challenging tasks [1]. By now, the deep\\nconvolutional neural networks are on par with human dermatologist [1]. This\\nabstract is dedicated on developing a Deep Learning method for ISIC [5] 2017\\nSkin Lesion Detection Competition hosted at [6] to classify the dermatology\\npictures, which is aimed at improving the diagnostic accuracy rate and general\\nlevel of the human health. The challenge falls into three sub-challenges,\\nincluding Lesion Segmentation, Lesion Dermoscopic Feature Extraction and Lesion\\nClassification. This project only participates in the Lesion Classification\\npart. This algorithm is comprised of three steps: (1) original images\\npreprocessing, (2) modelling the processed images using CNN [2, 3] in Caffe [4]\\nframework, (3) predicting the test images and calculating the scores that\\nreprese...</td>\n",
              "      <td>Using Deep Learning Method for Classification: A Proposed Algorithm for\\n  the ISIC 2017 Skin Lesion Classification Challenge</td>\n",
              "      <td>a new approach to sw image cohorts &lt;eos&gt;</td>\n",
              "      <td>lesion analysis using deep learning challenge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>Recent studies have highlighted the vulnerability of deep neural networks\\n(DNNs) to adversarial examples - a visually indistinguishable adversarial image\\ncan easily be crafted to cause a well-trained model to misclassify. Existing\\nmethods for crafting adversarial examples are based on $L_2$ and $L_\\infty$\\ndistortion metrics. However, despite the fact that $L_1$ distortion accounts\\nfor the total variation and encourages sparsity in the perturbation, little has\\nbeen developed for crafting $L_1$-based adversarial examples. In this paper, we\\nformulate the process of attacking DNNs via adversarial examples as an\\nelastic-net regularized optimization problem. Our elastic-net attacks to DNNs\\n(EAD) feature $L_1$-oriented adversarial examples and include the\\nstate-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,\\nCIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial\\nexamples with small $L_1$ distortion and attains similar attack perfor...</td>\n",
              "      <td>EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial\\n  Examples</td>\n",
              "      <td>a new approach to work &lt;unk&gt; of speed up  dnns in</td>\n",
              "      <td>elastic net attacks to adversarial deep neural networks</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Abstract  ...                                     Generated Title(Final)\n",
              "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   This paper reviews Kunchenko's polynomials using as template matching method\\nto recognize template in one-dimensional input signal. Kunchenko's polynomials\\nmethod is compared with classical methods - cross-correlation and sum of\\nsquared differences according to numerical statistical example.  ...                          polynomials for template matching\n",
              "42   Although many successful ensemble clustering approaches have been developed\\nin recent years, there are still two limitations to most of the existing\\napproaches. First, they mostly overlook the issue of uncertain links, which may\\nmislead the overall consensus process. Second, they generally lack the ability\\nto incorporate global information to refine the local links. To address these\\ntwo limitations, in this paper, we propose a novel ensemble clustering approach\\nbased on sparse graph representation and probability trajectory analysis. In\\nparticular, we present the elite neighbor selection strategy to identify the\\nuncertain links by locally adaptive thresholds and build a sparse graph with a\\nsmall number of probably reliable links. We argue that a small number of\\nprobably reliable links can lead to significantly better consensus results than\\nusing all graph links regardless of their reliability. The random walk process\\ndriven by a new transition probability matrix is util...  ...                   ensemble clustering with sparse ensemble\n",
              "58                                                                                                                                                                                                                                                                     We study black-box attacks on machine learning classifiers where each query\\nto the model incurs some cost or risk of detection to the adversary. We focus\\nexplicitly on minimizing the number of queries as a major objective.\\nSpecifically, we consider the problem of attacking machine learning classifiers\\nsubject to a budget of feature modification cost while minimizing the number of\\nqueries, where each query returns only a class and confidence score. We\\ndescribe an approach that uses Bayesian optimization to minimize the number of\\nqueries, and find that the number of queries can be reduced to approximately\\none tenth of the number needed through a random strategy for scenarios where\\nthe feature modification cost budget is low.  ...                                          black box attacks\n",
              "61                                                                                     We consider probabilistic topic models and more recent word embedding\\ntechniques from a perspective of learning hidden semantic representations.\\nInspired by a striking similarity of the two approaches, we merge them and\\nlearn probabilistic embeddings with online EM-algorithm on word co-occurrence\\ndata. The resulting embeddings perform on par with Skip-Gram Negative Sampling\\n(SGNS) on word similarity tasks and benefit in the interpretability of the\\ncomponents. Next, we learn probabilistic document embeddings that outperform\\nparagraph2vec on a document similarity task and require less memory and time\\nfor training. Finally, we employ multimodal Additive Regularization of Topic\\nModels (ARTM) to obtain a high sparsity and learn embeddings for other\\nmodalities, such as timestamps and categories. We observe further improvement\\nof word similarity performance and meaningful inter-modality similarities.  ...        probabilistic embeddings  bridging the topic models\n",
              "121                                                                                                                                                                                                                                                      The production of color language is essential for grounded language\\ngeneration. Color descriptions have many challenging properties: they can be\\nvague, compositionally complex, and denotationally rich. We present an\\neffective approach to generating color descriptions using recurrent neural\\nnetworks and a Fourier-transformed color representation. Our model outperforms\\nprevious work on a conditional language modeling task over a large corpus of\\nnaturalistic color descriptions. In addition, probing the model's output\\nreveals that it can accurately produce not only basic color terms but also\\ndescriptors with non-convex denotations (\"greenish\"), bare modifiers (\"bright\",\\n\"dull\"), and compositional phrases (\"faded teal\") not seen in training.  ...               to generate compositional color descriptions\n",
              "143                                                       Short-term tracking is an open and challenging problem for which\\ndiscriminative correlation filters (DCF) have shown excellent performance. We\\nintroduce the channel and spatial reliability concepts to DCF tracking and\\nprovide a novel learning algorithm for its efficient and seamless integration\\nin the filter update and the tracking process. The spatial reliability map\\nadjusts the filter support to the part of the object suitable for tracking.\\nThis both allows to enlarge the search region and improves tracking of\\nnon-rectangular objects. Reliability scores reflect channel-wise quality of the\\nlearned filters and are used as feature weighting coefficients in localization.\\nExperimentally, with only two simple standard features, HoGs and Colornames,\\nthe novel CSR-DCF method -- DCF with Channel and Spatial Reliability --\\nachieves state-of-the-art results on VOT 2016, VOT 2015 and OTB100. The CSR-DCF\\nruns in real-time on a CPU.  ...           correlation filter for for multi object tracking\n",
              "160                                                                                                                                                                                               We consider the problem of using sentence compression techniques to\\nfacilitate query-focused multi-document summarization. We present a\\nsentence-compression-based framework for the task, and design a series of\\nlearning-based compression models built on parse trees. An innovative beam\\nsearch decoder is proposed to efficiently find highly probable compressions.\\nUnder this framework, we show how to integrate various indicative metrics such\\nas linguistic motivation and query relevance into the compression process by\\nderiving a novel formulation of a compression scoring function. Our best model\\nachieves statistically significant improvement over the state-of-the-art\\nsystems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2\\nrespectively) for the DUC 2006 and 2007 summarization task.  ...      sentence compression for automatic text summarization\n",
              "162                                    Neural sequence-to-sequence models have provided a viable new approach for\\nabstractive text summarization (meaning they are not restricted to simply\\nselecting and rearranging passages from the original text). However, these\\nmodels have two shortcomings: they are liable to reproduce factual details\\ninaccurately, and they tend to repeat themselves. In this work we propose a\\nnovel architecture that augments the standard sequence-to-sequence attentional\\nmodel in two orthogonal ways. First, we use a hybrid pointer-generator network\\nthat can copy words from the source text via pointing, which aids accurate\\nreproduction of information, while retaining the ability to produce novel words\\nthrough the generator. Second, we use coverage to keep track of what has been\\nsummarized, which discourages repetition. We apply our model to the CNN / Daily\\nMail summarization task, outperforming the current abstractive state-of-the-art\\nby at least 2 ROUGE points.  ...  abstractive summarization with pointer generator networks\n",
              "188                                                                                                                                                                                                                                                               Ezhil is a Tamil language based interpreted procedural programming language.\\nTamil keywords and grammar are chosen to make the native Tamil speaker write\\nprograms in the Ezhil system. Ezhil allows easy representation of computer\\nprogram closer to the Tamil language logical constructs equivalent to the\\nconditional, branch and loop statements in modern English based programming\\nlanguages. Ezhil is a compact programming language aimed towards Tamil speaking\\nnovice computer users. Grammar for Ezhil and a few example programs are\\nreported here, from the initial proof-of-concept implementation using the\\nPython programming language1. To the best of our knowledge, Ezhil language is\\nthe first freely available Tamil programming language.  ...                               a tamil programming language\n",
              "192                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Modeling emotional-cognition is in a nascent stage and therefore wide-open\\nfor new ideas and discussions. In this paper the author looks at the modeling\\nproblem by bringing in ideas from axiomatic mathematics, information theory,\\ncomputer science, molecular biology, non-linear dynamical systems and quantum\\ncomputing and explains how ideas from these disciplines may have applications\\nin modeling emotional-cognition.  ...                        novice looks at emotional cognition\n",
              "231  We consider additive models built with trend filtering, i.e., additive models\\nwhose components are each regularized by the (discrete) total variation of\\ntheir $(k+1)$st (discrete) derivative, for a chosen integer $k \\geq 0$. This\\nresults in $k$th degree piecewise polynomial components, (e.g., $k=0$ gives\\npiecewise constant components, $k=1$ gives piecewise linear, $k=2$ gives\\npiecewise quadratic, etc.). In univariate nonparametric regression, the\\nlocalized nature of the total variation regularizer used by trend filtering has\\nbeen shown to produce estimates with superior local adaptivity to those from\\nsmoothing splines (and linear smoothers, more generally) (Tibshirani [2014]).\\nFurther, the structured nature of this regularizer has been shown to lead to\\nhighly efficient computational routines for trend filtering (Kim et al. [2009],\\nRamdas and Tibshirani [2016]). In this paper, we argue that both of these\\nproperties carry over to the additive models setting. We derive fas...  ...                       additive models with trend filtering\n",
              "232                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Dictionary learning for sparse representations is traditionally approached\\nwith sequential atom updates, in which an optimized atom is used immediately\\nfor the optimization of the next atoms. We propose instead a Jacobi version, in\\nwhich groups of atoms are updated independently, in parallel. Extensive\\nnumerical evidence for sparse image representation shows that the parallel\\nalgorithms, especially when all atoms are updated simultaneously, give better\\ndictionaries than their sequential counterparts.  ...               dictionary learning with jacobi atom updates\n",
              "240  Evolutionary algorithms (EAs) are population-based general-purpose\\noptimization algorithms, and have been successfully applied in various\\nreal-world optimization tasks. However, previous theoretical studies often\\nemploy EAs with only a parent or offspring population and focus on specific\\nproblems. Furthermore, they often only show upper bounds on the running time,\\nwhile lower bounds are also necessary to get a complete understanding of an\\nalgorithm. In this paper, we analyze the running time of the\\n($\\mu$+$\\lambda$)-EA (a general population-based EA with mutation only) on the\\nclass of pseudo-Boolean functions with a unique global optimum. By applying the\\nrecently proposed switch analysis approach, we prove the lower bound $\\Omega(n\\n\\ln n+ \\mu + \\lambda n\\ln\\ln n/ \\ln n)$ for the first time. Particularly on the\\ntwo widely-studied problems, OneMax and LeadingOnes, the derived lower bound\\ndiscloses that the ($\\mu$+$\\lambda$)-EA will be strictly slower than the\\n(1+1)-EA wh...  ...   lower bound for population based evolutionary algorithms\n",
              "291                                                         This paper proposes to use probabilistic model checking to synthesize optimal\\nrobot policies in multi-tasking autonomous systems that are subject to\\nhuman-robot interaction. Given the convincing empirical evidence that human\\nbehavior can be related to reinforcement models, we take as input a\\nwell-studied Q-table model of the human behavior for flexible scenarios. We\\nfirst describe an automated procedure to distill a Markov decision process\\n(MDP) for the human in an arbitrary but fixed scenario. The distinctive issue\\nis that -- in contrast to existing models -- under-specification of the human\\nbehavior is included. Probabilistic model checking is used to predict the\\nhuman's behavior. Finally, the MDP model is extended with a robot model.\\nOptimal robot policies are synthesized by analyzing the resulting two-player\\nstochastic game. Experimental results with a prototypical implementation using\\nPRISM show promising results.  ...   model checking for cognitive behavior in a probabilistic\n",
              "403                                                                                                                                                                                                                                                     We analyze differences between two information-theoretically motivated\\napproaches to statistical inference and model selection: the Minimum\\nDescription Length (MDL) principle, and the Minimum Message Length (MML)\\nprinciple. Based on this analysis, we present two revised versions of MML: a\\npointwise estimator which gives the MML-optimal single parameter model, and a\\nvolumewise estimator which gives the MML-optimal region in the parameter space.\\nOur empirical results suggest that with small data sets, the MDL approach\\nyields more accurate predictions than the MML estimators. The empirical results\\nalso demonstrate that the revised MML estimators introduced here perform better\\nthan the original MML estimator suggested by Wallace and Freeman.  ...                encoding approaches for predictive analysis\n",
              "452                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Flower pollination algorithm is a new nature-inspired algorithm, based on the\\ncharacteristics of flowering plants. In this paper, we extend this flower\\nalgorithm to solve multi-objective optimization problems in engineering. By\\nusing the weighted sum method with random weights, we show that the proposed\\nmulti-objective flower algorithm can accurately find the Pareto fronts for a\\nset of test functions. We then solve a bi-objective disc brake design problem,\\nwhich indeed converges quickly.  ...                          flower algorithm for optimization\n",
              "947                                                                                                                            In this work, we present a new dataset for computational humor, specifically\\ncomparative humor ranking, which attempts to eschew the ubiquitous binary\\napproach to humor detection. The dataset consists of tweets that are humorous\\nresponses to a given hashtag. We describe the motivation for this new dataset,\\nas well as the collection process, which includes a description of our\\nsemi-automated system for data collection. We also present initial experiments\\nfor this dataset using both unsupervised and supervised approaches. Our best\\nsupervised system achieved 63.7% accuracy, suggesting that this task is much\\nmore difficult than comparable humor detection tasks. Initial experiments\\nindicate that a character-level model is more suitable for this task than a\\ntoken-level model, likely due to a large amount of puns that can be captured by\\na character-level model.  ...                                   a deep learning of humor\n",
              "992                                                                                                                                                                                                                                                                        We introduce GAMSEL (Generalized Additive Model Selection), a penalized\\nlikelihood approach for fitting sparse generalized additive models in high\\ndimension. Our method interpolates between null, linear and additive models by\\nallowing the effect of each variable to be estimated as being either zero,\\nlinear, or a low-complexity curve, as determined by the data. We present a\\nblockwise coordinate descent procedure for efficiently optimizing the penalized\\nlikelihood objective over a dense grid of the tuning parameter, producing a\\nregularization path of additive models. We demonstrate the performance of our\\nmethod on both real and simulated data examples, and compare it with existing\\ntechniques for additive model selection.  ...                                   additive model selection\n",
              "941  Skin cancer, the most common human malignancy, is primarily diagnosed\\nvisually by physicians [1]. Classification with an automated method like CNN\\n[2, 3] shows potential for challenging tasks [1]. By now, the deep\\nconvolutional neural networks are on par with human dermatologist [1]. This\\nabstract is dedicated on developing a Deep Learning method for ISIC [5] 2017\\nSkin Lesion Detection Competition hosted at [6] to classify the dermatology\\npictures, which is aimed at improving the diagnostic accuracy rate and general\\nlevel of the human health. The challenge falls into three sub-challenges,\\nincluding Lesion Segmentation, Lesion Dermoscopic Feature Extraction and Lesion\\nClassification. This project only participates in the Lesion Classification\\npart. This algorithm is comprised of three steps: (1) original images\\npreprocessing, (2) modelling the processed images using CNN [2, 3] in Caffe [4]\\nframework, (3) predicting the test images and calculating the scores that\\nreprese...  ...              lesion analysis using deep learning challenge\n",
              "883  Recent studies have highlighted the vulnerability of deep neural networks\\n(DNNs) to adversarial examples - a visually indistinguishable adversarial image\\ncan easily be crafted to cause a well-trained model to misclassify. Existing\\nmethods for crafting adversarial examples are based on $L_2$ and $L_\\infty$\\ndistortion metrics. However, despite the fact that $L_1$ distortion accounts\\nfor the total variation and encourages sparsity in the perturbation, little has\\nbeen developed for crafting $L_1$-based adversarial examples. In this paper, we\\nformulate the process of attacking DNNs via adversarial examples as an\\nelastic-net regularized optimization problem. Our elastic-net attacks to DNNs\\n(EAD) feature $L_1$-oriented adversarial examples and include the\\nstate-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,\\nCIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial\\nexamples with small $L_1$ distortion and attains similar attack perfor...  ...    elastic net attacks to adversarial deep neural networks\n",
              "\n",
              "[20 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}