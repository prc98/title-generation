,Abstract,Title,Generated Title
0,"In this paper, a sparse Markov decision process (MDP) with novel causal
sparse Tsallis entropy regularization is proposed.The proposed policy
regularization induces a sparse and multi-modal optimal policy distribution of
a sparse MDP. The full mathematical analysis of the proposed sparse MDP is
provided.We first analyze the optimality condition of a sparse MDP. Then, we
propose a sparse value iteration method which solves a sparse MDP and then
prove the convergence and optimality of sparse value iteration using the Banach
fixed point theorem. The proposed sparse MDP is compared to soft MDPs which
utilize causal entropy regularization. We show that the performance error of a
sparse MDP has a constant bound, while the error of a soft MDP increases
logarithmically with respect to the number of actions, where this performance
error is caused by the introduced regularization term. In experiments, we apply
sparse MDPs to reinforcement learning problems. The proposed method outperforms
existing methods in terms of the convergence speed and performance.","Sparse Markov Decision Processes with Causal Sparse Tsallis Entropy
  Regularization for Reinforcement Learning",<unk> rank
1,"Roguelike games generally feature exploration problems as a critical, yet
often repetitive element of gameplay. Automated approaches, however, face
challenges in terms of optimality, as well as due to incomplete information,
such as from the presence of secret doors. This paper presents an algorithmic
approach to exploration of roguelike dungeon environments. Our design aims to
minimize exploration time, balancing coverage and discovery of secret areas
with resource cost. Our algorithm is based on the concept of occupancy maps
popular in robotics, adapted to encourage efficient discovery of secret access
points. Through extensive experimentation on NetHack maps we show that this
technique is significantly more efficient than simpler greedy approaches. We
further investigate optimized parameterization for the algorithm through a
comprehensive data analysis. These results point towards better automation for
players as well as heuristics applicable to fully automated gameplay.",Exploration in NetHack with Secret Discovery,programming using noisy show step
2,"Deep learning has achieved substantial success in a series of tasks in
computer vision. Intelligent video analysis, which can be broadly applied to
video surveillance in various smart city applications, can also be driven by
such powerful deep learning engines. To practically facilitate deep neural
network models in the large-scale video analysis, there are still unprecedented
challenges for the large-scale video data management. Deep feature coding,
instead of video coding, provides a practical solution for handling the
large-scale video surveillance data. To enable interoperability in the context
of deep feature coding, standardization is urgent and important. However, due
to the explosion of deep learning algorithms and the particularity of feature
coding, there are numerous remaining problems in the standardization process.
This paper envisions the future deep feature coding standard for the AI
oriented large-scale video management, and discusses existing techniques,
standards and possible solutions for these open problems.","AI Oriented Large-Scale Video Management for Smart City: Technologies,
  Standards and Beyond",mechanism video direction discrete
3,"Quorum sensing is a decentralized biological process, through which a
community of cells with no global awareness coordinate their functional
behaviors based solely on cell-medium interactions and local decisions. This
paper draws inspirations from quorum sensing and colony competition to derive a
new algorithm for data clustering. The algorithm treats each data as a single
cell, and uses knowledge of local connectivity to cluster cells into multiple
colonies simultaneously. It simulates auto-inducers secretion in quorum sensing
to tune the influence radius for each cell. At the same time, sparsely
distributed core cells spread their influences to form colonies, and
interactions between colonies eventually determine each cell's identity. The
algorithm has the flexibility to analyze not only static but also time-varying
data, which surpasses the capacity of many existing algorithms. Its stability
and convergence properties are established. The algorithm is tested on several
applications, including both synthetic and real benchmarks data sets, alleles
clustering, community detection, image segmentation. In particular, the
algorithm's distinctive capability to deal with time-varying data allows us to
experiment it on novel applications such as robotic swarms grouping and
switching model identification. We believe that the algorithm's promising
performance would stimulate many more exciting applications.",A Quorum Sensing Inspired Algorithm for Dynamic Clustering,involves context  modelling proposed <unk>
4,"A compact information-rich representation of the environment, also called a
feature abstraction, can simplify a robot's task of mapping its raw sensory
inputs to useful action sequences. However, in environments that are
non-stationary and only partially observable, a single abstraction is probably
not sufficient to encode most variations. Therefore, learning multiple sets of
spatially or temporally local, modular abstractions of the inputs would be
beneficial. How can a robot learn these local abstractions without a teacher?
More specifically, how can it decide from where and when to start learning a
new abstraction? A recently proposed algorithm called Curious Dr. MISFA
addresses this problem. The algorithm is based on two underlying learning
principles called artificial curiosity and slowness. The former is used to make
the robot self-motivated to explore by rewarding itself whenever it makes
progress learning an abstraction; the later is used to update the abstraction
by extracting slowly varying components from raw sensory inputs. Curious Dr.
MISFA's application is, however, limited to discrete domains constrained by a
pre-defined state space and has design limitations that make it unstable in
certain situations. This paper presents a significant improvement that is
applicable to continuous environments, is computationally less expensive,
simpler to use with fewer hyper parameters, and stable in certain
non-stationary environments. We demonstrate the efficacy and stability of our
method in a vision-based robot simulator.","Intrinsically Motivated Acquisition of Modular Slow Features for
  Humanoids in Continuous and Non-Stationary Environments",data researchers <unk> show underdetermined set limited
5,"Since many real-world concepts are associated with colour, for example danger
with red, linguistic information is often complimented with the use of
appropriate colours in information visualization and product marketing. Yet,
there is no comprehensive resource that captures concept-colour associations.
We present a method to create a large word-colour association lexicon by
crowdsourcing. We focus especially on abstract concepts and emotions to show
that even though they cannot be physically visualized, they too tend to have
strong colour associations. Finally, we show how word-colour associations
manifest themselves in language, and quantify usefulness of co-occurrence and
polarity cues in automatically detecting colour associations.",Colourful Language: Measuring Word-Colour Associations,significant both we cross correlation data around
6,"Reordering is a challenge to machine translation (MT) systems. In MT, the
widely used approach is to apply word based language model (LM) which considers
the constituent units of a sentence as words. In speech recognition (SR), some
phrase based LM have been proposed. However, those LMs are not necessarily
suitable or optimal for reordering. We propose two phrase based LMs which
considers the constituent units of a sentence as phrases. Experiments show that
our phrase based LMs outperform the word based LM with the respect of
perplexity and n-best list re-ranking.","Phrase Based Language Model for Statistical Machine Translation:
  Empirical Study",method pour example proposed <unk> operator for
7,"Obstacle Detection is a central problem for any robotic system, and critical
for autonomous systems that travel at high speeds in unpredictable environment.
This is often achieved through scene depth estimation, by various means. When
fast motion is considered, the detection range must be longer enough to allow
for safe avoidance and path planning. Current solutions often make assumption
on the motion of the vehicle that limit their applicability, or work at very
limited ranges due to intrinsic constraints. We propose a novel
appearance-based Object Detection system that is able to detect obstacles at
very long range and at a very high speed (~300Hz), without making assumptions
on the type of motion. We achieve these results using a Deep Neural Network
approach trained on real and synthetic images and trading some depth accuracy
for fast, robust and consistent operation. We show how photo-realistic
synthetic images are able to solve the problem of training set dimension and
variety typical of machine learning approaches, and how our system is robust to
massive blurring of test images.","Fast Robust Monocular Depth Estimation for Obstacle Detection with Fully
  Convolutional Networks",using <unk>
8,"Proof nets are a graph theoretical representation of proofs in various
fragments of type-logical grammar. In spite of this basis in graph theory,
there has been relatively little attention to the use of graph theoretic
algorithms for type-logical proof search. In this paper we will look at several
ways in which standard graph theoretic algorithms can be used to restrict the
search space. In particular, we will provide an O(n4) algorithm for selecting
an optimal axiom link at any stage in the proof search as well as a O(kn3)
algorithm for selecting the k best proof candidates.",Graph Algorithms for Improving Type-Logical Proof Search,offers noisy show preferences
9,"We create and release the first publicly available commercial customer
service corpus with annotated relational segments. Human-computer data from
three live customer service Intelligent Virtual Agents (IVAs) in the domains of
travel and telecommunications were collected, and reviewers marked all text
that was deemed unnecessary to the determination of user intention. After
merging the selections of multiple reviewers to create highlighted texts, a
second round of annotation was done to determine the classes of language
present in the highlighted sections such as the presence of Greetings,
Backstory, Justification, Gratitude, Rants, or Emotions. This resulting corpus
is a valuable resource for improving the quality and relational abilities of
IVAs. As well as discussing the corpus itself, we compare the usage of such
language in human-human interactions on TripAdvisor forums. We show that
removal of this language from task-based inputs has a positive effect on IVA
understanding by both an increase in confidence and improvement in responses,
demonstrating the need for automated methods of its discovery.",An Annotated Corpus of Relational Strategies in Customer Service,produces <unk> two show for  for
10,"Representation learning and unsupervised learning are two central topics of
machine learning and signal processing. Deep learning is one of the most
effective unsupervised representation learning approach. The main contributions
of this paper to the topics are as follows. (i) We propose to view the
representative deep learning approaches as special cases of the knowledge reuse
framework of clustering ensemble. (ii) We propose to view sparse coding when
used as a feature encoder as the consensus function of clustering ensemble, and
view dictionary learning as the training process of the base clusterings of
clustering ensemble. (ii) Based on the above two views, we propose a very
simple deep learning algorithm, named deep random model ensemble (DRME). It is
a stack of random model ensembles. Each random model ensemble is a special
k-means ensemble that discards the expectation-maximization optimization of
each base k-means but only preserves the default initialization method of the
base k-means. (iv) We propose to select the most powerful representation among
the layers by applying DRME to clustering where the single-linkage is used as
the clustering algorithm. Moreover, the DRME based clustering can also detect
the number of the natural clusters accurately. Extensive experimental
comparisons with 5 representation learning methods on 19 benchmark data sets
demonstrate the effectiveness of DRME.",Simple Deep Random Model Ensemble,despite <unk> two show similarities without
11,"The main prospective aim of modern research related to Artificial
Intelligence is the creation of technical systems that implement the idea of
Strong Intelligence. According our point of view the path to the development of
such systems comes through the research in the field related to perceptions.
Here we formulate the model of the perception of external world which may be
used for the description of perceptual activity of intelligent beings. We
consider a number of issues related to the development of the set of patterns
which will be used by the intelligent system when interacting with environment.
The key idea of the presented perception model is the idea of subjective
reality. The principle of the relativity of perceived world is formulated. It
is shown that this principle is the immediate consequence of the idea of
subjective reality. In this paper we show how the methodology of subjective
reality may be used for the creation of different types of Strong AI systems.",Subjective Reality and Strong Artificial Intelligence,using <unk>
12,"Automatic detection of pulmonary nodules in thoracic computed tomography (CT)
scans has been an active area of research for the last two decades. However,
there have only been few studies that provide a comparative performance
evaluation of different systems on a common database. We have therefore set up
the LUNA16 challenge, an objective evaluation framework for automatic nodule
detection algorithms using the largest publicly available reference database of
chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their
algorithm and upload their predictions on 888 CT scans in one of the two
tracks: 1) the complete nodule detection track where a complete CAD system
should be developed, or 2) the false positive reduction track where a provided
set of nodule candidates should be classified. This paper describes the setup
of LUNA16 and presents the results of the challenge so far. Moreover, the
impact of combining individual systems on the detection performance was also
investigated. It was observed that the leading solutions employed convolutional
networks and used the provided set of nodule candidates. The combination of
these solutions achieved an excellent sensitivity of over 95% at fewer than 1.0
false positives per scan. This highlights the potential of combining algorithms
to improve the detection performance. Our observer study with four expert
readers has shown that the best system detects nodules that were missed by
expert readers who originally annotated the LIDC-IDRI data. We released this
set of additional nodules for further development of CAD systems.","Validation, comparison, and combination of algorithms for automatic
  detection of pulmonary nodules in computed tomography images: the LUNA16
  challenge",concepts method predictive <unk>
13,"The rigorous theoretical analyses of algorithms for exact 3-satisfiability
(X3SAT) have been proposed in the literature. As we know, previous algorithms
for solving X3SAT have been analyzed only regarding the number of variables as
the parameter. However, the time complexity for solving X3SAT instances depends
not only on the number of variables, but also on the number of clauses.
Therefore, it is significant to exploit the time complexity from the other
point of view, i.e. the number of clauses. In this paper, we present algorithms
for solving X3SAT with rigorous complexity analyses using the number of clauses
as the parameter. By analyzing the algorithms, we obtain the new worst-case
upper bounds O(1.15855m), where m is the number of clauses.",New Worst-Case Upper Bound for X3SAT,technique physical
14,"This paper challenges a cross-genre document retrieval task, where the
queries are in formal writing and the target documents are in conversational
writing. In this task, a query, is a sentence extracted from either a summary
or a plot of an episode in a TV show, and the target document consists of
transcripts from the corresponding episode. To establish a strong baseline, we
employ the current state-of-the-art search engine to perform document retrieval
on the dataset collected for this work. We then introduce a structure reranking
approach to improve the initial ranking by utilizing syntactic and semantic
structures generated by NLP tools. Our evaluation shows an improvement of more
than 4% when the structure reranking is applied, which is very promising.","Cross-genre Document Retrieval: Matching between Conversational and
  Formal Writings",method learning fixed model review networks
15,"Video classification is productive in many practical applications, and the
recent deep learning has greatly improved its accuracy. However, existing works
often model video frames indiscriminately, but from the view of motion, video
frames can be decomposed into salient and non-salient areas naturally. Salient
and non-salient areas should be modeled with different networks, for the former
present both appearance and motion information, and the latter present static
background information. To address this problem, in this paper, video saliency
is predicted by optical flow without supervision firstly. Then two streams of
3D CNN are trained individually for raw frames and optical flow on salient
areas, and another 2D CNN is trained for raw frames on non-salient areas. For
the reason that these three streams play different roles for each class, the
weights of each stream are adaptively learned for each class. Experimental
results show that saliency-guided modeling and adaptively weighted learning can
reinforce each other, and we achieve the state-of-the-art results.",Saliency-guided video classification via adaptively weighted learning,learning increase model model <unk> knowledge different unlabeled
16,"We present a new image inpainting algorithm, the Averaging and Hypoelliptic
Evolution (AHE) algorithm, inspired by the one presented in [SIAM J. Imaging
Sci., vol. 7, no. 2, pp. 669--695, 2014] and based upon a semi-discrete
variation of the Citti-Petitot-Sarti model of the primary visual cortex V1. The
AHE algorithm is based on a suitable combination of sub-Riemannian hypoelliptic
diffusion and ad-hoc local averaging techniques. In particular, we focus on
reconstructing highly corrupted images (i.e. where more than the 80% of the
image is missing), for which we obtain reconstructions comparable with the
state-of-the-art.",Highly corrupted image inpainting through hypoelliptic diffusion,scale scale scale scale
17,"We derive high-probability finite-sample uniform rates of consistency for
$k$-NN regression that are optimal up to logarithmic factors under mild
assumptions. We moreover show that $k$-NN regression adapts to an unknown lower
intrinsic dimension automatically. We then apply the $k$-NN regression rates to
establish new results about estimating the level sets and global maxima of a
function from noisy observations.",Rates of Uniform Consistency for k-NN Regression,models model categorization data  method applicable texture
18,"A new image denoising algorithm to deal with the Poisson noise model is
given, which is based on the idea of Non-Local Mean. By using the ""Oracle""
concept, we establish a theorem to show that the Non-Local Means Filter can
effectively deal with Poisson noise with some modification. Under the
theoretical result, we construct our new algorithm called Non-Local Means
Poisson Filter and demonstrate in theory that the filter converges at the usual
optimal rate. The filter is as simple as the classic Non-Local Means and the
simulation results show that our filter is very competitive.",A Non-Local Means Filter for Removing the Poisson Noise,video step size noise critical state of the art
19,"At the core of interpretable machine learning is the question of whether
humans are able to make accurate predictions about a model's behavior. Assumed
in this question are three properties of the interpretable output: coverage,
precision, and effort. Coverage refers to how often humans think they can
predict the model's behavior, precision to how accurate humans are in those
predictions, and effort is either the up-front effort required in interpreting
the model, or the effort required to make predictions about a model's behavior.
  In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that
produces high-precision rule-based explanations for which the coverage
boundaries are very clear. We compare aLIME to linear LIME with simulated
experiments, and demonstrate the flexibility of aLIME with qualitative examples
from a variety of domains and tasks.","Nothing Else Matters: Model-Agnostic Explanations By Identifying
  Prediction Invariance",techniques <unk> gan method  using method  diffusion method
20,"Decomposition methods have been proposed in the past to approximate solutions
to large sequential decision making problems. In contexts where an agent
interacts with multiple entities, utility decomposition can be used where each
individual entity is considered independently. The individual utility functions
are then combined in real time to solve the global problem. Although these
techniques can perform well empirically, they sacrifice optimality. This paper
proposes an approach inspired from multi-fidelity optimization to learn a
correction term with a neural network representation. Learning this correction
can significantly improve performance. We demonstrate this approach on a
pedestrian avoidance problem for autonomous driving. By leveraging strategies
to avoid a single pedestrian, the decomposition method can scale to avoid
multiple pedestrians. We verify empirically that the proposed correction method
leads to a significant improvement over the decomposition method alone and
outperforms a policy trained on the full scale problem without utility
decomposition.","Utility Decomposition with Deep Corrections for Scalable Planning under
  Uncertainty",improve model show concepts #### 
21,"Building machines that learn and think like humans is essential not only for
cognitive science, but also for computational neuroscience, whose ultimate goal
is to understand how cognition is implemented in biological brains. A new
cognitive computational neuroscience should build cognitive-level and neural-
level models, understand their relationships, and test both types of models
with both brain and behavioral data.",Building machines that adapt and compute like brains,simultaneously streaming problem sensitivity rate 
22,"We present an efficient learning algorithm for the problem of training neural
networks with discrete synapses, a well-known hard (NP-complete) discrete
optimization problem. The algorithm is a variant of the so-called Max-Sum (MS)
algorithm. In particular, we show how, for bounded integer weights with $q$
distinct states and independent concave a priori distribution (e.g. $l_{1}$
regularization), the algorithm's time complexity can be made to scale as
$O\left(N\log N\right)$ per node update, thus putting it on par with
alternative schemes, such as Belief Propagation (BP), without resorting to
approximations. Two special cases are of particular interest: binary synapses
$W\in\{-1,1\}$ and ternary synapses $W\in\{-1,0,1\}$ with $l_{0}$
regularization. The algorithm we present performs as well as BP on binary
perceptron learning problems, and may be better suited to address the problem
on fully-connected two-layer networks, since inherent symmetries in two layer
networks are naturally broken using the MS approach.",A Max-Sum algorithm for training discrete neural networks,word for show significant kernel
23,"Ontologies usually suffer from the semantic heterogeneity when simultaneously
used in information sharing, merging, integrating and querying processes.
Therefore, the similarity identification between ontologies being used becomes
a mandatory task for all these processes to handle the problem of semantic
heterogeneity. In this paper, we propose an efficient technique for similarity
measurement between two ontologies. The proposed technique identifies all
candidate pairs of similar concepts without omitting any similar pair. The
proposed technique can be used in different types of operations on ontologies
such as merging, mapping and aligning. By analyzing its results a reasonable
improvement in terms of completeness, correctness and overall quality of the
results has been found.",An Efficient Technique for Similarity Identification between Ontologies,level present scene use show unlabeled improve models
24,"Transfer learning has revolutionized computer vision, but existing approaches
in NLP still require task-specific modifications and training from scratch. We
propose Fine-tuned Language Models (FitLaM), an effective transfer learning
method that can be applied to any task in NLP, and introduce techniques that
are key for fine-tuning a state-of-the-art language model. Our method
significantly outperforms the state-of-the-art on five text classification
tasks, reducing the error by 18-24% on the majority of datasets. We open-source
our pretrained models and code to enable adoption by the community.",Fine-tuned Language Models for Text Classification,new rank pedestrian
25,"GANs excel at learning high dimensional distributions, but they can update
generator parameters in directions that do not correspond to the steepest
descent direction of the objective. Prominent examples of problematic update
directions include those used in both Goodfellow's original GAN and the
WGAN-GP. To formally describe an optimal update direction, we introduce a
theoretical framework which allows the derivation of requirements on both the
divergence and corresponding method for determining an update direction. These
requirements guarantee unbiased mini-batch updates in the direction of steepest
descent. We propose a novel divergence which approximates the Wasserstein
distance while regularizing the critic's first order information. Together with
an accompanying update direction, this divergence fulfills the requirements for
unbiased steepest descent updates. We verify our method, the First Order GAN,
with CelebA image generation and set a new state of the art on the One Billion
Word language generation task.",First Order Generative Adversarial Networks,similarity show data need models
26,"In this paper, we consider active information acquisition when the prediction
model is meant to be applied on a targeted subset of the population. The goal
is to label a pre-specified fraction of customers in the target or test set by
iteratively querying for information from the non-target or training set. The
number of queries is limited by an overall budget. Arising in the context of
two rather disparate applications- banking and medical diagnosis, we pose the
active information acquisition problem as a constrained optimization problem.
We propose two greedy iterative algorithms for solving the above problem. We
conduct experiments with synthetic data and compare results of our proposed
algorithms with few other baseline approaches. The experimental results show
that our proposed approaches perform better than the baseline schemes.","Test Set Selection using Active Information Acquisition for Predictive
  Models",based graph feature models number
27,"Providing security for webservers against unwanted and automated
registrations has become a big concern. To prevent these kinds of false
registrations many websites use CAPTCHAs. Among all kinds of CAPTCHAs OCR-Based
or visual CAPTCHAs are very common. Actually visual CAPTCHA is an image
containing a sequence of characters. So far most of visual CAPTCHAs, in order
to resist against OCR programs, use some common implementations such as
wrapping the characters, random placement and rotations of characters, etc. In
this paper we applied Gaussian Blur filter, which is an image transformation,
to visual CAPTCHAs to reduce their readability by OCR programs. We concluded
that this technique made CAPTCHAs almost unreadable for OCR programs but, their
readability by human users still remained high.",Improve CAPTCHA's Security Using Gaussian Blur Filter,evaluation similarly show online
28,"This paper investigates energy efficiency for two-tier femtocell networks
through combining game theory and stochastic learning. With the Stackelberg
game formulation, a hierarchical reinforcement learning framework is applied to
study the joint average utility maximization of macrocells and femtocells
subject to the minimum signal-to-interference-plus-noise-ratio requirements.
The macrocells behave as the leaders and the femtocells are followers during
the learning procedure. At each time step, the leaders commit to dynamic
strategies based on the best responses of the followers, while the followers
compete against each other with no further information but the leaders'
strategy information. In this paper, we propose two learning algorithms to
schedule each cell's stochastic power levels, leading by the macrocells.
Numerical experiments are presented to validate the proposed studies and show
that the two learning algorithms substantially improve the energy efficiency of
the femtocell networks.","Improving Energy Efficiency in Femtocell Networks: A Hierarchical
  Reinforcement Learning Framework",models domain 
29,"Neural spikes in the brain form stochastic sequences, i.e., belong to the
class of pulse noises. This stochasticity is a counterintuitive feature because
extracting information - such as the commonly supposed neural information of
mean spike frequency - requires long times for reasonably low error
probability. The mystery could be solved by noise-based logic, wherein
randomness has an important function and allows large speed enhancements for
special-purpose tasks, and the same mechanism is at work for the brain logic
version of this concept.",Brain: Biological noise-based logic,model mostly obtaining common problem contrast imagery 
30,"We present weight normalization: a reparameterization of the weight vectors
in a neural network that decouples the length of those weight vectors from
their direction. By reparameterizing the weights in this way we improve the
conditioning of the optimization problem and we speed up convergence of
stochastic gradient descent. Our reparameterization is inspired by batch
normalization but does not introduce any dependencies between the examples in a
minibatch. This means that our method can also be applied successfully to
recurrent models such as LSTMs and to noise-sensitive applications such as deep
reinforcement learning or generative models, for which batch normalization is
less well suited. Although our method is much simpler, it still provides much
of the speed-up of full batch normalization. In addition, the computational
overhead of our method is lower, permitting more optimization steps to be taken
in the same amount of time. We demonstrate the usefulness of our method on
applications in supervised image recognition, generative modelling, and deep
reinforcement learning.","Weight Normalization: A Simple Reparameterization to Accelerate Training
  of Deep Neural Networks",based function  improvements  method <unk> different
31,"Multi-label classification has attracted an increasing amount of attention in
recent years. To this end, many algorithms have been developed to classify
multi-label data in an effective manner. However, they usually do not consider
the pairwise relations indicated by sample labels, which actually play
important roles in multi-label classification. Inspired by this, we naturally
extend the traditional pairwise constraints to the multi-label scenario via a
flexible thresholding scheme. Moreover, to improve the generalization ability
of the classifier, we adopt a boosting-like strategy to construct a multi-label
ensemble from a group of base classifiers. To achieve these goals, this paper
presents a novel multi-label classification framework named Variable Pairwise
Constraint projection for Multi-label Ensemble (VPCME). Specifically, we take
advantage of the variable pairwise constraint projection to learn a
lower-dimensional data representation, which preserves the correlations between
samples and labels. Thereafter, the base classifiers are trained in the new
data space. For the boosting-like strategy, we employ both the variable
pairwise constraints and the bootstrap steps to diversify the base classifiers.
Empirical studies have shown the superiority of the proposed method in
comparison with other approaches.",Multi-label ensemble based on variable pairwise constraint projection,model review inference on size data predictive settings 
32,"In this work, we describe a system that detects paraphrases in Indian
Languages as part of our participation in the shared Task on detecting
paraphrases in Indian Languages (DPIL) organized by Forum for Information
Retrieval Evaluation (FIRE) in 2016. Our paraphrase detection method uses a
multinomial logistic regression model trained with a variety of features which
are basically lexical and semantic level similarities between two sentences in
a pair. The performance of the system has been evaluated against the test set
released for the FIRE 2016 shared task on DPIL. Our system achieves the highest
f-measure of 0.95 on task1 in Punjabi language.The performance of our system on
task1 in Hindi language is f-measure of 0.90. Out of 11 teams participated in
the shared task, only four teams participated in all four languages, Hindi,
Punjabi, Malayalam and Tamil, but the remaining 7 teams participated in one of
the four languages. We also participated in task1 and task2 both for all four
Indian Languages. The overall average performance of our system including task1
and task2 overall four languages is F1-score of 0.81 which is the second
highest score among the four systems that participated in all four languages.","KS_JU@DPIL-FIRE2016:Detecting Paraphrases in Indian Languages Using
  Multinomial Logistic Regression Model",accurate paper  for
33,"In this paper a deterministic preprocessing algorithm is presented, whose
output can be given as input to most state-of-the-art epipolar geometry
estimation algorithms, improving their results considerably. They are now able
to succeed on hard cases for which they failed before. The algorithm consists
of three steps, whose scope changes from local to global. In the local step it
extracts from a pair of images local features (e.g. SIFT). Similar features
from each image are clustered and the clusters are matched yielding a large
number of putative matches. In the second step pairs of spatially close
features (called 2keypoints) are matched and ranked by a classifier. The
2keypoint matches with the highest ranks are selected. In the global step, from
each two 2keypoint matches a fundamental matrix is computed. As quite a few of
the matrices are generated from correct matches they are used to rank the
putative matches found in the first step. For each match the number of
fundamental matrices, for which it approximately satisfies the epipolar
constraint, is calculated. This set of matches is combined with the putative
matches generated by standard methods and their probabilities to be correct are
estimated by a classifier. These are then given as input to state-of-the-art
epipolar geometry estimation algorithms such as BEEM, BLOGS and USAC yielding
much better results than the original algorithms. This was shown in extensive
testing performed on almost 900 image pairs from six publicly available
data-sets.","A General Preprocessing Method for Improved Performance of Epipolar
  Geometry Estimation Algorithms",model learning <unk> model digitization require often
34,"In this work, we present the results of a systematic study to investigate the
(commercial) benefits of automatic text summarization systems in a real world
scenario. More specifically, we define a use case in the context of media
monitoring and media response analysis and claim that even using a simple
query-based extractive approach can dramatically save the processing time of
the employees without significantly reducing the quality of their work.","On (Commercial) Benefits of Automatic Text Summarization Systems in the
  News Domain: A Case of Media Monitoring and Media Response Analysis",two show <unk> <unk>
35,"In this paper the elicitation of probabilities from human experts is
considered as a measurement process, which may be disturbed by random
'measurement noise'. Using Bayesian concepts a second order probability
distribution is derived reflecting the uncertainty of the input probabilities.
The algorithm is based on an approximate sample representation of the basic
probabilities. This sample is continuously modified by a stochastic simulation
procedure, the Metropolis algorithm, such that the sequence of successive
samples corresponds to the desired posterior distribution. The procedure is
able to combine inconsistent probabilities according to their reliability and
is applicable to general inference networks with arbitrary structure.
Dempster-Shafer probability mass functions may be included using specific
measurement distributions. The properties of the approach are demonstrated by
numerical experiments.",Second Order Probabilities for Uncertain and Conflicting Evidence,data <unk> show hardness multi agent various derive
36,"In a knowledge discovery process, interpretation and evaluation of the mined
results are indispensable in practice. In the case of data clustering, however,
it is often difficult to see in what aspect each cluster has been formed. This
paper proposes a method for automatic and objective characterization or
""verbalization"" of the clusters obtained by mixture models, in which we collect
conjunctions of propositions (attribute-value pairs) that help us interpret or
evaluate the clusters. The proposed method provides us with a new, in-depth and
consistent tool for cluster interpretation/evaluation, and works for various
types of datasets including continuous attributes and missing values.
Experimental results with a couple of standard datasets exhibit the utility of
the proposed method, and the importance of the feedbacks from the
interpretation/evaluation step.","Verbal Characterization of Probabilistic Clusters using Minimal
  Discriminative Propositions",effectively method greedy connection stochastic
37,"Retinal vessel information is helpful in retinal disease screening and
diagnosis. Retinal vessel segmentation provides useful information about
vessels and can be used by physicians during intraocular surgery and retinal
diagnostic operations. Convolutional neural networks (CNNs) are powerful tools
for classification and segmentation of medical images. Complexity of CNNs makes
it difficult to implement them in portable devices such as binocular indirect
ophthalmoscopes. In this paper a simplification approach is proposed for CNNs
based on combination of quantization and pruning. Fully connected layers are
quantized and convolutional layers are pruned to have a simple and efficient
network structure. Experiments on images of the STARE dataset show that our
simplified network is able to segment retinal vessels with acceptable accuracy
and low complexity.","Low complexity convolutional neural network for vessel segmentation in
  portable retinal diagnostic devices",learning model model <unk> users estimate
38,"Understanding how users navigate in a network is of high interest in many
applications. We consider a setting where only aggregate node-level traffic is
observed and tackle the task of learning edge transition probabilities. We cast
it as a preference learning problem, and we study a model where choices follow
Luce's axiom. In this case, the $O(n)$ marginal counts of node visits are a
sufficient statistic for the $O(n^2)$ transition probabilities. We show how to
make the inference problem well-posed regardless of the network's structure,
and we present ChoiceRank, an iterative algorithm that scales to networks that
contains billions of nodes and edges. We apply the model to two clickstream
datasets and show that it successfully recovers the transition probabilities
using only the network structure and marginal (node-level) traffic data.
Finally, we also consider an application to mobility networks and apply the
model to one year of rides on New York City's bicycle-sharing system.",ChoiceRank: Identifying Preferences from Node Traffic in Networks,learning regression  may flow using specifically automated
39,"Text processing is one of the sub-branches of natural language processing.
Recently, the use of machine learning and neural networks methods has been
given greater consideration. For this reason, the representation of words has
become very important. This article is about word representation or converting
words into vectors in Persian text. In this research GloVe, CBOW and skip-gram
methods are updated to produce embedded vectors for Persian words. In order to
train a neural networks, Bijankhan corpus, Hamshahri corpus and UPEC corpus
have been compound and used. Finally, we have 342,362 words that obtained
vectors in all three models for this words. These vectors have many usage for
Persian natural language processing.",word representation or word embedding in Persian text,offers <unk> model <unk>
40,"Sparse decomposition has been widely used for different applications, such as
source separation, image classification and image denoising. This paper
presents a new algorithm for segmentation of an image into background and
foreground text and graphics using sparse decomposition. First, the background
is represented using a suitable smooth model, which is a linear combination of
a few smoothly varying basis functions, and the foreground text and graphics
are modeled as a sparse component overlaid on the smooth background. Then the
background and foreground are separated using a sparse decomposition framework
and imposing some prior information, which promote the smoothness of
background, and the sparsity and connectivity of foreground pixels. This
algorithm has been tested on a dataset of images extracted from HEVC standard
test sequences for screen content coding, and is shown to outperform prior
methods, including least absolute deviation fitting, k-means clustering based
segmentation in DjVu, and shape primitive extraction and coding algorithm.",Image Segmentation Using Overlapping Group Sparsity,based engineering equipped method
41,"This paper reviews Kunchenko's polynomials using as template matching method
to recognize template in one-dimensional input signal. Kunchenko's polynomials
method is compared with classical methods - cross-correlation and sum of
squared differences according to numerical statistical example.",Kunchenko's Polynomials for Template Matching, alm  high field feedforward method learning
42,"Although many successful ensemble clustering approaches have been developed
in recent years, there are still two limitations to most of the existing
approaches. First, they mostly overlook the issue of uncertain links, which may
mislead the overall consensus process. Second, they generally lack the ability
to incorporate global information to refine the local links. To address these
two limitations, in this paper, we propose a novel ensemble clustering approach
based on sparse graph representation and probability trajectory analysis. In
particular, we present the elite neighbor selection strategy to identify the
uncertain links by locally adaptive thresholds and build a sparse graph with a
small number of probably reliable links. We argue that a small number of
probably reliable links can lead to significantly better consensus results than
using all graph links regardless of their reliability. The random walk process
driven by a new transition probability matrix is utilized to explore the global
information in the graph. We derive a novel and dense similarity measure from
the sparse graph by analyzing the probability trajectories of the random
walkers, based on which two consensus functions are further proposed.
Experimental results on multiple real-world datasets demonstrate the
effectiveness and efficiency of our approach.",Robust Ensemble Clustering Using Probability Trajectories,video model based <unk>
43,"BP3TKI Palembang is the government agencies that coordinate, execute and
selection of prospective migrants registration and placement. To simplify the
existing procedures and improve decision-making is necessary to build a
decision support system (DSS) to determine eligibility for employment abroad by
applying Fuzzy Multiple Attribute Decision Making (FMADM), using the linear
sequential systems development methods. The system is built using Microsoft
Visual Basic. Net 2010 and SQL Server 2008 database. The design of the system
using use case diagrams and class diagrams to identify the needs of users and
systems as well as systems implementation guidelines. This Decision Support
System able to rank and produce the prospective migrants, making it easier for
parties to take decision BP3TKI the workers who will be working out of the
country.","Sistem pendukung keputusan kelayakan TKI ke luar negeri menggunakan
  FMADM",feature models method word for
44,"Conventional decision trees have a number of favorable properties, including
interpretability, a small computational footprint and the ability to learn from
little training data. However, they lack a key quality that has helped fuel the
deep learning revolution: that of being end-to-end trainable, and to learn from
scratch those features that best allow to solve a given supervised learning
problem. Recent work (Kontschieder 2015) has addressed this deficit, but at the
cost of losing a main attractive trait of decision trees: the fact that each
sample is routed along a small subset of tree nodes only. We here propose a
model and Expectation-Maximization training scheme for decision trees that are
fully probabilistic at train time, but after a deterministic annealing process
become deterministic at test time. We also analyze the learned oblique split
parameters on image datasets and show that Neural Networks can be trained at
each split node. In summary, we present the first end-to-end learning scheme
for deterministic decision trees and present results on par with or superior to
published standard oblique decision tree algorithms.",End-to-end Learning of Deterministic Decision Trees,cluster model show demonstrated example model modified
45,"In this paper, we present a Convolutional Neural Network (CNN) regression
approach for real-time 2-D/3-D registration. Different from optimization-based
methods, which iteratively optimize the transformation parameters over a
scalar-valued metric function representing the quality of the registration, the
proposed method exploits the information embedded in the appearances of the
Digitally Reconstructed Radiograph and X-ray images, and employs CNN regressors
to directly estimate the transformation parameters. The CNN regressors are
trained for local zones and applied in a hierarchical manner to break down the
complex regression task into simpler sub-tasks that can be learned separately.
Our experiment results demonstrate the advantage of the proposed method in
computational efficiency with negligible degradation of registration accuracy
compared to intensity-based methods.",Real-time 2D/3D Registration via CNN Regression,linear model next  encodings using linear knowledge
46,"In this paper, we present a co-designed petascale high-density GPU cluster to
expedite distributed deep learning training with synchronous Stochastic
Gradient Descent~(SSGD). This architecture of our heterogeneous cluster is
inspired by Harvard architecture. Regarding to different roles in the system,
nodes are configured as different specifications. Based on the topology of the
whole system's network and properties of different types of nodes, we develop
and implement a novel job server parallel software framework, named by
""\textit{MiMatrix}"", for distributed deep learning training. Compared to the
parameter server framework, in which parameter server is a bottleneck of data
transfer in AllReduce algorithm of SSGD, the job server undertakes all of
controlling, scheduling and monitoring tasks without model data transfer. In
MiMatrix, we propose a novel GPUDirect Remote direct memory access~(RDMA)-aware
parallel algorithm of AllReucde executed by computing servers, which both
computation and handshake message are $O(1)$ at each epoch","MiMatrix: A Massively Distributed Deep Learning Framework on a Petascale
  High-density Heterogeneous Cluster",new networks tracking  different <unk>
47,"While the philosophical literature has extensively studied how decisions
relate to arguments, reasons and justifications, decision theory almost
entirely ignores the latter notions and rather focuses on preference and
belief. In this article, we argue that decision theory can largely benefit from
explicitly taking into account the stance that decision-makers take towards
arguments and counter-arguments. To that end, we elaborate a formal framework
aiming to integrate the role of arguments and argumentation in decision theory
and decision aid. We start from a decision situation, where an individual
requests decision support. In this context, we formally define, as a
commendable basis for decision-aid, this individual's deliberated judgment,
popularized by Rawls. We explain how models of deliberated judgment can be
validated empirically. We then identify conditions upon which the existence of
a valid model can be taken for granted, and analyze how these conditions can be
relaxed. We then explore the significance of our proposed framework for
decision aiding practice. We argue that our concept of deliberated judgment
owes its normative credentials both to its normative foundations (the idea of
rationality based on arguments) and to its reference to empirical reality (the
stance that real, empirical individuals hold towards arguments and
counter-arguments, on due reflection). We then highlight that our framework
opens promising avenues for future research involving both philosophical and
decision theoretic approaches, as well as empirical implementations.",A formal framework for deliberated judgment,material proposed types learning  demonstrate
48,"Many robotic planning applications involve continuous actions with highly
non-linear constraints, which cannot be modeled using modern planners that
construct a propositional representation. We introduce STRIPStream: an
extension of the STRIPS language which can model these domains by supporting
the specification of blackbox generators to handle complex constraints. The
outputs of these generators interact with actions through possibly infinite
streams of objects and static predicates. We provide two algorithms which both
reduce STRIPStream problems to a sequence of finite-domain planning problems.
The representation and algorithms are entirely domain independent. We
demonstrate our framework on simple illustrative domains, and then on a
high-dimensional, continuous robotic task and motion planning domain.",STRIPS Planning in Infinite Domains,models problem so  solving
49,"Hyperspectral cameras can provide unique spectral signatures for consistently
distinguishing materials that can be used to solve surveillance tasks. In this
paper, we propose a novel real-time hyperspectral likelihood maps-aided
tracking method (HLT) inspired by an adaptive hyperspectral sensor. A moving
object tracking system generally consists of registration, object detection,
and tracking modules. We focus on the target detection part and remove the
necessity to build any offline classifiers and tune a large amount of
hyperparameters, instead learning a generative target model in an online manner
for hyperspectral channels ranging from visible to infrared wavelengths. The
key idea is that, our adaptive fusion method can combine likelihood maps from
multiple bands of hyperspectral imagery into one single more distinctive
representation increasing the margin between mean value of foreground and
background pixels in the fused map. Experimental results show that the HLT not
only outperforms all established fusion methods but is on par with the current
state-of-the-art hyperspectral target tracking frameworks.","Aerial Vehicle Tracking by Adaptive Fusion of Hyperspectral Likelihood
  Maps",learning model learning would model still <unk>
50,"A perturbative approach is used to quantify the effect of noise in data
points on fitted parameters in a general homogeneous linear model, and the
results applied to the case of conic sections. There is an optimal choice of
normalisation that minimises bias, and iteration with the correct reweighting
significantly improves statistical reliability. By conditioning on an
appropriate prior, an unbiased type-specific fit can be obtained. Error
estimates for the conic coefficients may also be used to obtain both bias
corrections and confidence intervals for other curve parameters.",A Bayesian approach to type-specific conic fitting,models cluster problem demonstrated control weight lipschitz
51,"This paper describes an interdisciplinary approach which brings together the
fields of corpus linguistics and translation studies. It presents ongoing work
on the creation of a corpus resource in which translation shifts are explicitly
annotated. Translation shifts denote departures from formal correspondence
between source and target text, i.e. deviations that have occurred during the
translation process. A resource in which such shifts are annotated in a
systematic way will make it possible to study those phenomena that need to be
addressed if machine translation output is to resemble human translation. The
resource described in this paper contains English source texts (parliamentary
proceedings) and their German translations. The shift annotation is based on
predicate-argument structures and proceeds in two steps: first, predicates and
their arguments are annotated monolingually in a straightforward manner. Then,
the corresponding English and German predicates and arguments are aligned with
each other. Whenever a shift - mainly grammatical or semantic -has occurred,
the alignment is tagged accordingly.",Building a resource for studying translation shifts,open inference super resolution analyze
52,"The machine learning community has become increasingly concerned with the
potential for bias and discrimination in predictive models. This has motivated
a growing line of work on what it means for a classification procedure to be
""fair."" In this paper, we investigate the tension between minimizing error
disparity across different population groups while maintaining calibrated
probability estimates. We show that calibration is compatible only with a
single error constraint (i.e. equal false-negatives rates across groups), and
show that any algorithm that satisfies this relaxation is no better than
randomizing a percentage of predictions for an existing classifier. These
unsettling findings, which extend and generalize existing results, are
empirically confirmed on several datasets.",On Fairness and Calibration,stochastic show models
53,"This paper details the implementation of an algorithm for automatically
generating a high-level knowledge network to perform commonsense reasoning,
specifically with the application of robotic task repair. The network is
represented using a Bayesian Logic Network (BLN) (Jain, Waldherr, and Beetz
2009), which combines a set of directed relations between abstract concepts,
including IsA, AtLocation, HasProperty, and UsedFor, with a corresponding
probability distribution that models the uncertainty inherent in these
relations. Inference over this network enables reasoning over the abstract
concepts in order to perform appropriate object substitution or to locate
missing objects in the robot's environment. The structure of the network is
generated by combining information from two existing knowledge sources:
ConceptNet (Speer and Havasi 2012), and WordNet (Miller 1995). This is done in
a ""situated"" manner by only including information relevant a given context.
Results show that the generated network is able to accurately predict object
categories, locations, properties, and affordances in three different household
scenarios.","Situated Structure Learning of a Bayesian Logic Network for Commonsense
  Reasoning",learning query models model <unk> new objects based
54,"The generation of meaningless ""words"" matching certain statistical and/or
linguistic criteria is frequently needed for experimental purposes in
Psycholinguistics. Such stimuli receive the name of pseudowords or nonwords in
the Cognitive Neuroscience literatue. The process for building nonwords
sometimes has to be based on linguistic units such as syllables or morphemes,
resulting in a numerical explosion of combinations when the size of the
nonwords is increased. In this paper, a reactive tabu search scheme is proposed
to generate nonwords of variables size. The approach builds pseudowords by
using a modified Metaheuristic algorithm based on a local search procedure
enhanced by a feedback-based scheme. Experimental results show that the new
algorithm is a practical and effective tool for nonword generation.","A Reactive Tabu Search Algorithm for Stimuli Generation in
  Psycholinguistics",graph model discovery camera errors method compression robustness
55,"Control Strategies for hierarchical tree-like probabilistic inference
networks are formulated and investigated. Strategies that utilize staged
look-ahead and temporary focus on subgoals are formalized and refined using the
Depth Vector concept that serves as a tool for defining the 'virtual tree'
regarded by the control strategy. The concept is illustrated by four types of
control strategies for three-level trees that are characterized according to
their Depth Vector, and according to the way they consider intermediate nodes
and the role that they let these nodes play. INFERENTI is a computerized
inference system written in Prolog, which provides tools for exercising a
variety of control strategies. The system also provides tools for simulating
test data and for comparing the relative average performance under different
strategies.",A Framework for Control Strategies in Uncertain Inference Networks,models show show control search
56,"360$^{\circ}$ video requires human viewers to actively control ""where"" to
look while watching the video. Although it provides a more immersive experience
of the visual content, it also introduces additional burden for viewers;
awkward interfaces to navigate the video lead to suboptimal viewing
experiences. Virtual cinematography is an appealing direction to remedy these
problems, but conventional methods are limited to virtual environments or rely
on hand-crafted heuristics. We propose a new algorithm for virtual
cinematography that automatically controls a virtual camera within a
360$^{\circ}$ video. Compared to the state of the art, our algorithm allows
more general camera control, avoids redundant outputs, and extracts its output
videos substantially more efficiently. Experimental results on over 7 hours of
real ""in the wild"" video show that our generalized camera control is crucial
for viewing 360$^{\circ}$ video, while the proposed efficient algorithm is
essential for making the generalized control computationally tractable.","Making 360$^{\circ}$ Video Watchable in 2D: Learning Videography for
  Click Free Viewing",offers limited show video examples
57,"Automatically recognized terminology is widely used for various
domain-specific texts processing tasks, such as machine translation,
information retrieval or sentiment analysis. However, there is still no
agreement on which methods are best suited for particular settings and,
moreover, there is no reliable comparison of already developed methods. We
believe that one of the main reasons is the lack of state-of-the-art methods
implementations, which are usually non-trivial to recreate. In order to address
these issues, we present ATR4S, an open-source software written in Scala that
comprises more than 15 methods for automatic terminology recognition (ATR) and
implements the whole pipeline from text document preprocessing, to term
candidates collection, term candidates scoring, and finally, term candidates
ranking. It is highly scalable, modular and configurable tool with support of
automatic caching. We also compare 10 state-of-the-art methods on 7 open
datasets by average precision and processing time. Experimental comparison
reveals that no single method demonstrates best average precision for all
datasets and that other available tools for ATR do not contain the best
methods.","ATR4S: Toolkit with State-of-the-art Automatic Terms Recognition Methods
  in Scala",based problem  <unk> tensor based tensor based tensor based reflect estimate
58,"We study black-box attacks on machine learning classifiers where each query
to the model incurs some cost or risk of detection to the adversary. We focus
explicitly on minimizing the number of queries as a major objective.
Specifically, we consider the problem of attacking machine learning classifiers
subject to a budget of feature modification cost while minimizing the number of
queries, where each query returns only a class and confidence score. We
describe an approach that uses Bayesian optimization to minimize the number of
queries, and find that the number of queries can be reduced to approximately
one tenth of the number needed through a random strategy for scenarios where
the feature modification cost budget is low.",Query-limited Black-box Attacks to Classifiers,similar data around model query models
59,"Procedural terrain generation for video games has been traditionally been
done with smartly designed but handcrafted algorithms that generate heightmaps.
We propose a first step toward the learning and synthesis of these using recent
advances in deep generative modelling with openly available satellite imagery
from NASA.",A step towards procedural terrain generation with GANs,research natural agent show significant agent
60,"Recent progress in synthetic aperture sonar (SAS) technology and processing
has led to significant advances in underwater imaging, outperforming previously
common approaches in both accuracy and efficiency. There are, however, inherent
limitations to current SAS reconstruction methodology. In particular, popular
and efficient Fourier domain SAS methods require a 2D interpolation which is
often ill conditioned and inaccurate, inevitably reducing robustness with
regard to speckle and inaccurate sound-speed estimation. To overcome these
issues, we propose using the frame theoretic convolution gridding (FTCG)
algorithm to handle the non-uniform Fourier data. FTCG extends upon non-uniform
fast Fourier transform (NUFFT) algorithms by casting the NUFFT as an
approximation problem given Fourier frame data. The FTCG has been show to yield
improved accuracy at little more computational cost. Using simulated data, we
outline how the FTCG can be used to enhance current SAS processing.","Using Frame Theoretic Convolutional Gridding for Robust Synthetic
  Aperture Sonar Imaging",mechanism significant stochastic method received
61,"We consider probabilistic topic models and more recent word embedding
techniques from a perspective of learning hidden semantic representations.
Inspired by a striking similarity of the two approaches, we merge them and
learn probabilistic embeddings with online EM-algorithm on word co-occurrence
data. The resulting embeddings perform on par with Skip-Gram Negative Sampling
(SGNS) on word similarity tasks and benefit in the interpretability of the
components. Next, we learn probabilistic document embeddings that outperform
paragraph2vec on a document similarity task and require less memory and time
for training. Finally, we employ multimodal Additive Regularization of Topic
Models (ARTM) to obtain a high sparsity and learn embeddings for other
modalities, such as timestamps and categories. We observe further improvement
of word similarity performance and meaningful inter-modality similarities.","Interpretable probabilistic embeddings: bridging the gap between topic
  models and neural networks",video step different feature word paper  for
62,"We present results from the first geological field tests of the `Cyborg
Astrobiologist', which is a wearable computer and video camcorder system that
we are using to test and train a computer-vision system towards having some of
the autonomous decision-making capabilities of a field-geologist and
field-astrobiologist. The Cyborg Astrobiologist platform has thus far been used
for testing and development of these algorithms and systems: robotic
acquisition of quasi-mosaics of images, real-time image segmentation, and
real-time determination of interesting points in the image mosaics. The
hardware and software systems function reliably, and the computer-vision
algorithms are adequate for the first field tests. In addition to the
proof-of-concept aspect of these field tests, the main result of these field
tests is the enumeration of those issues that we can improve in the future,
including: first, detection and accounting for shadows caused by 3D jagged
edges in the outcrop; second, reincorporation of more sophisticated
texture-analysis algorithms into the system; third, creation of hardware and
software capabilities to control the camera's zoom lens in an intelligent
manner; and fourth, development of algorithms for interpretation of complex
geological scenery. Nonetheless, despite these technical inadequacies, this
Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer
and its computer-vision algorithms, has demonstrated its ability of finding
genuinely interesting points in real-time in the geological scenery, and then
gathering more information about these interest points in an automated manner.",The Cyborg Astrobiologist: First Field Experience,<unk> online model paper  for show empirical online
63,"This work aims to make it easier for a specialist in one field to find and
explore ideas from another field which may be useful in solving a new problem
arising in his practice. It presents a methodology which serves to represent
the relationships that exist between concepts, problems, and solution patterns
from different fields of human activity in the form of a graph. Our approach is
based upon generalization and specialization relationships and problem solving.
It is simple enough to be understood quite easily, and general enough to enable
coherent integration of concepts and problems from virtually any field. We have
built an implementation which uses the World Wide Web as a support to allow
navigation between graph nodes and collaborative development of the graph.",Towards Solving the Interdisciplinary Language Barrier Problem,model model <unk> <unk> different <unk>
64,"Similarity-based clustering and semi-supervised learning methods separate the
data into clusters or classes according to the pairwise similarity between the
data, and the pairwise similarity is crucial for their performance. In this
paper, we propose a novel discriminative similarity learning framework which
learns discriminative similarity for either data clustering or semi-supervised
learning. The proposed framework learns classifier from each hypothetical
labeling, and searches for the optimal labeling by minimizing the
generalization error of the learned classifiers associated with the
hypothetical labeling. Kernel classifier is employed in our framework. By
generalization analysis via Rademacher complexity, the generalization error
bound for the kernel classifier learned from hypothetical labeling is expressed
as the sum of pairwise similarity between the data from different classes,
parameterized by the weights of the kernel classifier. Such pairwise similarity
serves as the discriminative similarity for the purpose of clustering and
semi-supervised learning, and discriminative similarity with similar form can
also be induced by the integrated squared error bound for kernel density
classification. Based on the discriminative similarity induced by the kernel
classifier, we propose new clustering and semi-supervised learning methods.",Discriminative Similarity for Clustering and Semi-Supervised Learning,kernel model complexity vision
65,"We consider the problem of identifying the causal direction between two
discrete random variables using observational data. Unlike previous work, we
keep the most general functional model but make an assumption on the unobserved
exogenous variable: Inspired by Occam's razor, we assume that the exogenous
variable is simple in the true causal direction. We quantify simplicity using
R\'enyi entropy. Our main result is that, under natural assumptions, if the
exogenous variable has low $H_0$ entropy (cardinality) in the true direction,
it must have high $H_0$ entropy in the wrong direction. We establish several
algorithmic hardness results about estimating the minimum entropy exogenous
variable. We show that the problem of finding the exogenous variable with
minimum entropy is equivalent to the problem of finding minimum joint entropy
given $n$ marginal distributions, also known as minimum entropy coupling
problem. We propose an efficient greedy algorithm for the minimum entropy
coupling problem, that for $n=2$ provably finds a local optimum. This gives a
greedy algorithm for finding the exogenous variable with minimum $H_1$ (Shannon
Entropy). Our greedy entropy-based causal inference algorithm has similar
performance to the state of the art additive noise models in real datasets. One
advantage of our approach is that we make no use of the values of random
variables but only their distributions. Our method can therefore be used for
causal inference for both ordinal and also categorical data, unlike additive
noise models.",Entropic Causal Inference,however   f  show show feature models
66,"An approach to robotics called layered evolution and merging features from
the subsumption architecture into evolutionary robotics is presented, and its
advantages are discussed. This approach is used to construct a layered
controller for a simulated robot that learns which light source to approach in
an environment with obstacles. The evolvability and performance of layered
evolution on this task is compared to (standard) monolithic evolution,
incremental and modularised evolution. To corroborate the hypothesis that a
layered controller performs at least as well as an integrated one, the evolved
layers are merged back into a single network. On the grounds of the test
results, it is argued that layered evolution provides a superior approach for
many tasks, and it is suggested that this approach may be the key to scaling up
evolutionary robotics.",Evolution of a Subsumption Architecture Neurocontroller,software <unk> <unk> <unk> <unk>
67,"We propose an original particle-based implementation of the Loopy Belief
Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a
continuous state space. The algorithm constructs adaptively efficient proposal
distributions approximating the local beliefs at each note of the MRF. This is
achieved by considering proposal distributions in the exponential family whose
parameters are updated iterately in an Expectation Propagation (EP) framework.
The proposed particle scheme provides consistent estimation of the LBP
marginals as the number of particles increases. We demonstrate that it provides
more accurate results than the Particle Belief Propagation (PBP) algorithm of
Ihler and McAllester (2009) at a fraction of the computational cost and is
additionally more robust empirically. The computational complexity of our
algorithm at each iteration is quadratic in the number of particles. We also
propose an accelerated implementation with sub-quadratic computational
complexity which still provides consistent estimates of the loopy BP marginal
distributions and performs almost as well as the original procedure.",Expectation Particle Belief Propagation,model <unk> using preferences
68,"This paper proves that in iris recognition, the concepts of sheep, goats,
lambs and wolves - as proposed by Doddington and Yager in the so-called
Biometric Menagerie, are at most fuzzy and at least not quite well defined.
They depend not only on the users or on their biometric templates, but also on
the parameters that calibrate the iris recognition system. This paper shows
that, in the case of iris recognition, the extensions of these concepts have
very unsharp and unstable (non-stationary) boundaries. The membership of a user
to these categories is more often expressed as a degree (as a fuzzy value)
rather than as a crisp value. Moreover, they are defined by fuzzy Sugeno rules
instead of classical (crisp) definitions. For these reasons, we said that the
Biometric Menagerie proposed by Doddington and Yager could be at most a fuzzy
concept of biometry, but even this status is conditioned by improving its
definition. All of these facts are confirmed experimentally in a series of 12
exhaustive iris recognition tests undertaken for University of Bath Iris Image
Database while using three different iris code dimensions (256x16, 128x8 and
64x4), two different iris texture encoders (Log-Gabor and Haar-Hilbert) and two
different types of safety models.",The Biometric Menagerie - A Fuzzy and Inconsistent Concept,theory ranking problem video step
69,"This paper presents a scalable method for integrating compositional
morphological representations into a vector-based probabilistic language model.
Our approach is evaluated in the context of log-bilinear language models,
rendered suitably efficient for implementation inside a machine translation
decoder by factoring the vocabulary. We perform both intrinsic and extrinsic
evaluations, presenting results on a range of languages which demonstrate that
our model learns morphological representations that both perform well on word
similarity tasks and lead to substantial reductions in perplexity. When used
for translation into morphologically rich languages with large vocabularies,
our models obtain improvements of up to 1.2 BLEU points relative to a baseline
system using back-off n-gram models.",Compositional Morphology for Word Representations and Language Modelling,model <unk> show technique observation
70,"Realistic mobility models can demonstrate more precise evaluation results
because their parameters are closer to the reality. In this paper a realistic
Fuzzy Mobility Model has been proposed. This model has rules which is
changeable depending on nodes and environment conditions. This model is more
complete and precise than the other mobility models and this is the advantage
of this model. After simulation, it was found out that not only considering
nodes movement as being imprecise (fuzzy) has a positive effects on most of ad
hoc network parameters, but also, more importantly as they are closer to the
real world condition, they can have a more positive effect on the
implementation of ad hoc network protocols.",A Fuzzy Realistic Mobility Model For Ad hoc Networks,however  strategy models show domain  model known using
71,"A standard objective in partially-observable Markov decision processes
(POMDPs) is to find a policy that maximizes the expected discounted-sum payoff.
However, such policies may still permit unlikely but highly undesirable
outcomes, which is problematic especially in safety-critical applications.
Recently, there has been a surge of interest in POMDPs where the goal is to
maximize the probability to ensure that the payoff is at least a given
threshold, but these approaches do not consider any optimization beyond
satisfying this threshold constraint. In this work we go beyond both the
""expectation"" and ""threshold"" approaches and consider a ""guaranteed payoff
optimization (GPO)"" problem for POMDPs, where we are given a threshold $t$ and
the objective is to find a policy $\sigma$ such that a) each possible outcome
of $\sigma$ yields a discounted-sum payoff of at least $t$, and b) the expected
discounted-sum payoff of $\sigma$ is optimal (or near-optimal) among all
policies satisfying a). We present a practical approach to tackle the GPO
problem and evaluate it on standard POMDP benchmarks.",Optimizing Expectation with Guarantees in POMDPs (Technical Report),based function  detector show researchers high models
72,"This paper explores the use of the standard approach for proving runtime
bounds in discrete domains---often referred to as drift analysis---in the
context of optimization on a continuous domain. Using this framework we analyze
the (1+1) Evolution Strategy with one-fifth success rule on the sphere
function. To deal with potential functions that are not lower-bounded, we
formulate novel drift theorems. We then use the theorems to prove bounds on the
expected hitting time to reach a certain target fitness in finite dimension
$d$. The bounds are akin to linear convergence. We then study the dependency of
the different terms on $d$ proving a convergence rate dependency of
$\Theta(1/d)$. Our results constitute the first non-asymptotic analysis for the
algorithm considered as well as the first explicit application of drift
analysis to a randomized search heuristic with continuous domain.","Drift Theory in Continuous Search Spaces: Expected Hitting Time of the
  (1+1)-ES with 1/5 Success Rule",graph applied  #  show show deal approaches
73,"We propose a convolutional network with hierarchical classifiers for
per-pixel semantic segmentation, which is able to be trained on multiple,
heterogeneous datasets and exploit their semantic hierarchy. Our network is the
first to be simultaneously trained on three different datasets from the
intelligent vehicles domain, i.e. Cityscapes, GTSDB and Mapillary Vistas, and
is able to handle different semantic level-of-detail, class imbalances, and
different annotation types, i.e. dense per-pixel and sparse bounding-box
labels. We assess our hierarchical approach, by comparing against flat,
non-hierarchical classifiers and we show improvements in mean pixel accuracy of
13.0% for Cityscapes classes and 2.4% for Vistas classes and 32.3% for GTSDB
classes. Our implementation achieves inference rates of 17 fps at a resolution
of 520 x 706 for 108 classes running on a GPU.","Training of Convolutional Networks on Multiple Heterogeneous Datasets
  for Street Scene Semantic Segmentation",services first examples test intuitionistic model referenced
74,"Transformation-based learning has been successfully employed to solve many
natural language processing problems. It achieves state-of-the-art performance
on many natural language processing tasks and does not overtrain easily.
However, it does have a serious drawback: the training time is often
intorelably long, especially on the large corpora which are often used in NLP.
In this paper, we present a novel and realistic method for speeding up the
training time of a transformation-based learner without sacrificing
performance. The paper compares and contrasts the training time needed and
performance achieved by our modified learner with two other systems: a standard
transformation-based learner, and the ICA system \cite{hepple00:tbl}. The
results of these experiments show that our system is able to achieve a
significant improvement in training time while still achieving the same
performance as a standard transformation-based learner. This is a valuable
contribution to systems and algorithms which utilize transformation-based
learning at any part of the execution.",Transformation-Based Learning in the Fast Lane,uncertainty  model achieve texture
75,"Sparse coding is an unsupervised learning algorithm that learns a succinct
high-level representation of the inputs given only unlabeled data; it
represents each input as a sparse linear combination of a set of basis
functions. Originally applied to modeling the human visual cortex, sparse
coding has also been shown to be useful for self-taught learning, in which the
goal is to solve a supervised classification task given access to additional
unlabeled data drawn from different classes than that in the supervised
learning problem. Shift-invariant sparse coding (SISC) is an extension of
sparse coding which reconstructs a (usually time-series) input using all of the
basis functions in all possible shifts. In this paper, we present an efficient
algorithm for learning SISC bases. Our method is based on iteratively solving
two large convex optimization problems: The first, which computes the linear
coefficients, is an L1-regularized linear least squares problem with
potentially hundreds of thousands of variables. Existing methods typically use
a heuristic to select a small subset of the variables to optimize, but we
present a way to efficiently compute the exact solution. The second, which
solves for bases, is a constrained linear least squares problem. By optimizing
over complex-valued variables in the Fourier domain, we reduce the coupling
between the different variables, allowing the problem to be solved efficiently.
We show that SISC's learned high-level representations of speech and music
provide useful features for classification tasks within those domains. When
applied to classification, under certain conditions the learned features
outperform state of the art spectral and cepstral features.",Shift-Invariance Sparse Coding for Audio Classification, map  <unk> called previously learning <unk> model model
76,"In the paper, region based stereo matching algorithms are developed for
extraction depth information from two color stereo image pair. A filter
eliminating unreliable disparity estimation was used for increasing reliability
of the disparity map. Obtained results by algorithms were represented and
compared.","Obtaining Depth Maps From Color Images By Region Based Stereo Matching
  Algorithms",learning model model despite reward item different different
77,"Radar sensors can be used for analyzing the induced frequency shifts due to
micro-motions in both range and velocity dimensions identified as micro-Doppler
($\boldsymbol{\mu}$-D) and micro-Range ($\boldsymbol{\mu}$-R), respectively.
Different moving targets will have unique $\boldsymbol{\mu}$-D and
$\boldsymbol{\mu}$-R signatures that can be used for target classification.
Such classification can be used in numerous fields, such as gait recognition,
safety and surveillance. In this paper, a 25 GHz FMCW Single-Input
Single-Output (SISO) radar is used in industrial safety for real-time
human-robot identification. Due to the real-time constraint, joint
Range-Doppler (R-D) maps are directly analyzed for our classification problem.
Furthermore, a comparison between the conventional classical learning
approaches with handcrafted extracted features, ensemble classifiers and deep
learning approaches is presented. For ensemble classifiers, restructured range
and velocity profiles are passed directly to ensemble trees, such as gradient
boosting and random forest without feature extraction. Finally, a Deep
Convolutional Neural Network (DCNN) is used and raw R-D images are directly fed
into the constructed network. DCNN shows a superior performance of 99\%
accuracy in identifying humans from robots on a single R-D map.","Micro-Doppler Based Human-Robot Classification Using Ensemble and Deep
  Learning Approaches",quality step size demonstrate show the
78,"In this paper, we consider the problem of automatically segmenting neuronal
cells in dual-color confocal microscopy images. This problem is a key task in
various quantitative analysis applications in neuroscience, such as tracing
cell genesis in Danio rerio (zebrafish) brains. Deep learning, especially using
fully convolutional networks (FCN), has profoundly changed segmentation
research in biomedical imaging. We face two major challenges in this problem.
First, neuronal cells may form dense clusters, making it difficult to correctly
identify all individual cells (even to human experts). Consequently,
segmentation results of the known FCN-type models are not accurate enough.
Second, pixel-wise ground truth is difficult to obtain. Only a limited amount
of approximate instance-wise annotation can be collected, which makes the
training of FCN models quite cumbersome. We propose a new FCN-type deep
learning model, called deep complete bipartite networks (CB-Net), and a new
scheme for leveraging approximate instance-wise annotation to train our
pixel-wise prediction model. Evaluated using seven real datasets, our proposed
new CB-Net model outperforms the state-of-the-art FCN models and produces
neuron segmentation results of remarkable quality",Neuron Segmentation Using Deep Complete Bipartite Networks,learning networks linear method complexity for
79,"The increasing demand of world wide web raises the need of predicting the
user's web page request.The most widely used approach to predict the web pages
is the pattern discovery process of Web usage mining. This process involves
inevitability of many techniques like Markov model, association rules and
clustering. Fuzzy theory with different techniques has been introduced for the
better results. Our focus is on Markov models. This paper is introducing the
vague Rules with Markov models for more accuracy using the vague set theory.",Integrating Vague Association Mining with Markov Model,theory ranking size srdcf <unk>
80,"In this paper, we present a novel low rank representation (LRR) algorithm for
data lying on the manifold of square root densities. Unlike traditional LRR
methods which rely on the assumption that the data points are vectors in the
Euclidean space, our new algorithm is designed to incorporate the intrinsic
geometric structure and geodesic distance of the manifold. Experiments on
several computer vision datasets showcase its noise robustness and superior
performance on classification and subspace clustering compared to other
state-of-the-art approaches.",Low Rank Representation on Riemannian Manifold of Square Root Densities,noise for
81,"This paper presents a challenge to the community: given a large corpus of
written text aligned to its normalized spoken form, train an RNN to learn the
correct normalization function. We present a data set of general text where the
normalizations were generated using an existing text normalization component of
a text-to-speech system. This data set will be released open-source in the near
future.
  We also present our own experiments with this data set with a variety of
different RNN architectures. While some of the architectures do in fact produce
very good results when measured in terms of overall accuracy, the errors that
are produced are problematic, since they would convey completely the wrong
message if such a system were deployed in a speech application. On the other
hand, we show that a simple FST-based filter can mitigate those errors, and
achieve a level of accuracy not achievable by the RNN alone.
  Though our conclusions are largely negative on this point, we are actually
not arguing that the text normalization problem is intractable using an pure
RNN approach, merely that it is not going to be something that can be solved
merely by having huge amounts of annotated text data and feeding that to a
general RNN model. And when we open-source our data, we will be providing a
novel data set for sequence-to-sequence modeling in the hopes that the the
community can find better solutions.
  The data used in this work have been released and are available at:
https://github.com/rwsproat/text-normalization-data",RNN Approaches to Text Normalization: A Challenge,models show show model computationally
82,"Today, we can find many search engines which provide us with information
which is more operational in nature. None of the search engines provide domain
specific information. This becomes very troublesome to a novice user who wishes
to have information in a particular domain. In this paper, we have developed an
ontology which can be used by a domain specific search engine. We have
developed an ontology on human anatomy, which captures information regarding
cardiovascular system, digestive system, skeleton and nervous system. This
information can be used by people working in medical and health care domain.",OntoAna: Domain Ontology for Human Anatomy,<unk> show tensor time varying domain
83,"Acoustic models based on long short-term memory recurrent neural networks
(LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and
showed significant improvements in naturalness and latency over those based on
hidden Markov models (HMMs). This paper describes further optimizations of
LSTM-RNN-based SPSS for deployment on mobile devices; weight quantization,
multi-frame inference, and robust inference using an {\epsilon}-contaminated
Gaussian loss function. Experimental results in subjective listening tests show
that these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based
SPSS in runtime speed while maintaining naturalness. Evaluations between
LSTM-RNN- based SPSS and HMM-driven unit selection speech synthesis are also
presented.","Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric
  Speech Synthesizers for Mobile Devices",model model  binary direction
84,"Many data mining and data analysis techniques operate on dense matrices or
complete tables of data. Real-world data sets, however, often contain unknown
values. Even many classification algorithms that are designed to operate with
missing values still exhibit deteriorated accuracy. One approach to handling
missing values is to fill in (impute) the missing values. In this paper, we
present a technique for unsupervised learning called Unsupervised
Backpropagation (UBP), which trains a multi-layer perceptron to fit to the
manifold sampled by a set of observed point-vectors. We evaluate UBP with the
task of imputing missing values in datasets, and show that UBP is able to
predict missing values with significantly lower sum-squared error than other
collaborative filtering and imputation techniques. We also demonstrate with 24
datasets and 9 supervised learning algorithms that classification accuracy is
usually higher when randomly-withheld values are imputed using UBP, rather than
with other methods.",Missing Value Imputation With Unsupervised Backpropagation,presents upper online proposed <unk> data denoising  system
85,"Based on a weighted knowledge graph to represent first-order knowledge and
combining it with a probabilistic model, we propose a methodology for the
creation of a medical knowledge network (MKN) in medical diagnosis. When a set
of symptoms is activated for a specific patient, we can generate a ground
medical knowledge network composed of symptom nodes and potential disease
nodes. By Incorporating a Boltzmann machine into the potential function of a
Markov network, we investigated the joint probability distribution of the MKN.
In order to deal with numerical symptoms, a multivariate inference model is
presented that uses conditional probability. In addition, the weights for the
knowledge graph were efficiently learned from manually annotated Chinese
Electronic Medical Records (CEMRs). In our experiments, we found numerically
that the optimum choice of the quality of disease node and the expression of
symptom variable can improve the effectiveness of medical diagnosis. Our
experimental results comparing a Markov logic network and the logistic
regression algorithm on an actual CEMR database indicate that our method holds
promise and that MKN can facilitate studies of intelligent diagnosis.","Learning and inference in knowledge-based probabilistic model for
  medical diagnosis",step proposed communities significant simple
86,"We identify obfuscated gradients, a kind of gradient masking, as a phenomenon
that leads to a false sense of security in defenses against adversarial
examples. While defenses that cause obfuscated gradients appear to defeat
iterative optimization-based attacks, we find defenses relying on this effect
can be circumvented. For each of the three types of obfuscated gradients we
discover, we describe characteristic behaviors of defenses exhibiting this
effect and develop attack techniques to overcome it. In a case study, examining
non-certified white-box-secure defenses at ICLR 2018, we find obfuscated
gradients are a common occurrence, with 7 of 8 defenses relying on obfuscated
gradients. Our new attacks successfully circumvent 6 completely and 1
partially.","Obfuscated Gradients Give a False Sense of Security: Circumventing
  Defenses to Adversarial Examples",studied model episodes consider learning
87,"We propose a new method for robust PCA -- the task of recovering a low-rank
matrix from sparse corruptions that are of unknown value and support. Our
method involves alternating between projecting appropriate residuals onto the
set of low-rank matrices, and the set of sparse matrices; each projection is
{\em non-convex} but easy to compute. In spite of this non-convexity, we
establish exact recovery of the low-rank matrix, under the same conditions that
are required by existing methods (which are based on convex optimization). For
an $m \times n$ input matrix ($m \leq n)$, our method has a running time of
$O(r^2mn)$ per iteration, and needs $O(\log(1/\epsilon))$ iterations to reach
an accuracy of $\epsilon$. This is close to the running time of simple PCA via
the power method, which requires $O(rmn)$ per iteration, and
$O(\log(1/\epsilon))$ iterations. In contrast, existing methods for robust PCA,
which are based on convex optimization, have $O(m^2n)$ complexity per
iteration, and take $O(1/\epsilon)$ iterations, i.e., exponentially more
iterations for the same accuracy.
  Experiments on both synthetic and real data establishes the improved speed
and accuracy of our method over existing convex implementations.",Non-convex Robust PCA,models model show services
88,"Fine-grained classification is challenging because categories can only be
discriminated by subtle and local differences. Variances in the pose, scale or
rotation usually make the problem more difficult. Most fine-grained
classification systems follow the pipeline of finding foreground object or
object parts (where) to extract discriminative features (what).
  In this paper, we propose to apply visual attention to fine-grained
classification task using deep neural network. Our pipeline integrates three
types of attention: the bottom-up attention that propose candidate patches, the
object-level top-down attention that selects relevant patches to a certain
object, and the part-level top-down attention that localizes discriminative
parts. We combine these attentions to train domain-specific deep nets, then use
it to improve both the what and where aspects. Importantly, we avoid using
expensive annotations like bounding box or part information from end-to-end.
The weak supervision constraint makes our work easier to generalize.
  We have verified the effectiveness of the method on the subsets of ILSVRC2012
dataset and CUB200_2011 dataset. Our pipeline delivered significant
improvements and achieved the best accuracy under the weakest supervision
condition. The performance is competitive against other methods that rely on
additional annotations.","The Application of Two-level Attention Models in Deep Convolutional
  Neural Network for Fine-grained Image Classification",provides high provides estimated show the proposed known
89,"In this paper, we consider a fitness-level model of a non-elitist
mutation-only evolutionary algorithm (EA) with tournament selection. The model
provides upper and lower bounds for the expected proportion of the individuals
with fitness above given thresholds. In the case of so-called monotone
mutation, the obtained bounds imply that increasing the tournament size
improves the EA performance. As corollaries, we obtain an exponentially
vanishing tail bound for the Randomized Local Search on unimodal functions and
polynomial upper bounds on the runtime of EAs on 2-SAT problem and on a family
of Set Cover problems proposed by E. Balas.","On Proportions of Fit Individuals in Population of Evolutionary
  Algorithm with Tournament Selection",potential instances linear show <unk>
90,"We consider a framework for structured prediction based on search in the
space of complete structured outputs. Given a structured input, an output is
produced by running a time-bounded search procedure guided by a learned cost
function, and then returning the least cost output uncovered during the search.
This framework can be instantiated for a wide range of search spaces and search
procedures, and easily incorporates arbitrary structured-prediction loss
functions. In this paper, we make two main technical contributions. First, we
define the limited-discrepancy search space over structured outputs, which is
able to leverage powerful classification learning algorithms to improve the
search space quality. Second, we give a generic cost function learning
approach, where the key idea is to learn a cost function that attempts to mimic
the behavior of conducting searches guided by the true loss function. Our
experiments on six benchmark domains demonstrate that using our framework with
only a small amount of search is sufficient for significantly improving on
state-of-the-art structured-prediction performance.",Output Space Search for Structured Prediction,scoring model <unk> theory ranking
91,"Deep networks trained on demonstrations of human driving have learned to
follow roads and avoid obstacles. However, driving policies trained via
imitation learning cannot be controlled at test time. A vehicle trained
end-to-end to imitate an expert cannot be guided to take a specific turn at an
upcoming intersection. This limits the utility of such systems. We propose to
condition imitation learning on high-level command input. At test time, the
learned driving policy functions as a chauffeur that handles sensorimotor
coordination but continues to respond to navigational commands. We evaluate
different architectures for conditional imitation learning in vision-based
driving. We conduct experiments in realistic three-dimensional simulations of
urban driving and on a 1/5 scale robotic truck that is trained to drive in a
residential area. Both systems drive based on visual input yet remain
responsive to high-level navigational commands. The supplementary video can be
viewed at https://youtu.be/cFtnflNe5fM",End-to-end Driving via Conditional Imitation Learning,new pour theory <unk> <unk> types paper  for
92,"We introduce a novel evolutionary formulation of the problem of finding a
maximum independent set of a graph. The new formulation is based on the
relationship that exists between a graph's independence number and its acyclic
orientations. It views such orientations as individuals and evolves them with
the aid of evolutionary operators that are very heavily based on the structure
of the graph and its acyclic orientations. The resulting heuristic has been
tested on some of the Second DIMACS Implementation Challenge benchmark graphs,
and has been found to be competitive when compared to several of the other
heuristics that have also been tested on those graphs.",A novel evolutionary formulation of the maximum independent set problem,social set domain model <unk> for
93,"The paper introduces a new modular action language, ALM, and illustrates the
methodology of its use. It is based on the approach of Gelfond and Lifschitz
(1993; 1998) in which a high-level action language is used as a front end for a
logic programming system description. The resulting logic programming
representation is used to perform various computational tasks. The methodology
based on existing action languages works well for small and even medium size
systems, but is not meant to deal with larger systems that require structuring
of knowledge. ALM is meant to remedy this problem. Structuring of knowledge in
ALM is supported by the concepts of module (a formal description of a specific
piece of knowledge packaged as a unit), module hierarchy, and library, and by
the division of a system description of ALM into two parts: theory and
structure. A theory consists of one or more modules with a common theme,
possibly organized into a module hierarchy based on a dependency relation. It
contains declarations of sorts, attributes, and properties of the domain
together with axioms describing them. Structures are used to describe the
domain's objects. These features, together with the means for defining classes
of a domain as special cases of previously defined ones, facilitate the
stepwise development, testing, and readability of a knowledge base, as well as
the creation of knowledge representation libraries. To appear in Theory and
Practice of Logic Programming (TPLP).",Modular Action Language ALM,using game method word accurate for show video
94,"Recommendation systems rely on historical user data to provide suggestions.
We propose an explicit and simple model for the interaction between users and
recommendations provided by a platform, and relate this model to the
multi-armed bandit literature. First, we show that this interaction leads to a
bias in naive estimators due to selection effects. This bias leads to
suboptimal outcomes, which we quantify in terms of linear regret. We end the
first part by discussing ways to obtain unbiased estimates. The second part of
this work considers exploration of alternatives. We show that although agents
are myopic, agents' heterogeneous preferences ensure that recommendation
systems 'learn' about all alternatives without explicitly incentivizing this
exploration. This work provides new and practical insights relevant to a wide
range of systems designed to help users make better decisions.",Human Interaction with Recommendation Systems: On Bias and Exploration,function <unk> function process level paper  for
95,"Deep generative models have been wildly successful at learning coherent
latent representations for continuous data such as video and audio. However,
generative modeling of discrete data such as arithmetic expressions and
molecular structures still poses significant challenges. Crucially,
state-of-the-art methods often produce outputs that are not valid. We make the
key observation that frequently, discrete data can be represented as a parse
tree from a context-free grammar. We propose a variational autoencoder which
encodes and decodes directly to and from these parse trees, ensuring the
generated outputs are always valid. Surprisingly, we show that not only does
our model more often generate valid outputs, it also learns a more coherent
latent space in which nearby points decode to similar discrete outputs. We
demonstrate the effectiveness of our learned models by showing their improved
performance in Bayesian optimization for symbolic regression and molecular
synthesis.",Grammar Variational Autoencoder,video step problem representations ##
96,"Automated generation of high-quality topical hierarchies for a text
collection is a dream problem in knowledge engineering with many valuable
applications. In this paper a scalable and robust algorithm is proposed for
constructing a hierarchy of topics from a text collection. We divide and
conquer the problem using a top-down recursive framework, based on a tensor
orthogonal decomposition technique. We solve a critical challenge to perform
scalable inference for our newly designed hierarchical topic model. Experiments
with various real-world datasets illustrate its ability to generate robust,
high-quality hierarchies efficiently. Our method reduces the time of
construction by several orders of magnitude, and its robust feature renders it
possible for users to interactively revise the hierarchy.",Scalable and Robust Construction of Topical Hierarchies,method variety paper  for
97,"Radiomics is a term which refers to the analysis of the large amount of
quantitative tumor features extracted from medical images to find useful
predictive, diagnostic or prognostic information. Many recent studies have
proved that radiomics can offer a lot of useful information that physicians
cannot extract from the medical images and can be associated with other
information like gene or protein data. However, most of the classification
studies in radiomics report the use of feature selection methods without
identifying the machine learning challenges behind radiomics. In this paper, we
first show that the radiomics problem should be viewed as an high dimensional,
low sample size, multi view learning problem, then we compare different
solutions proposed in multi view learning for classifying radiomics data. Our
experiments, conducted on several real world multi view datasets, show that the
intermediate integration methods work significantly better than filter and
embedded feature selection methods commonly used in radiomics.",Dissimilarity-based representation for radiomics applications,points learning sensor model courses method feature for
98,"A novel unified Bayesian framework for network detection is developed, under
which a detection algorithm is derived based on random walks on graphs. The
algorithm detects threat networks using partial observations of their activity,
and is proved to be optimum in the Neyman-Pearson sense. The algorithm is
defined by a graph, at least one observation, and a diffusion model for threat.
A link to well-known spectral detection methods is provided, and the
equivalence of the random walk and harmonic solutions to the Bayesian
formulation is proven. A general diffusion model is introduced that utilizes
spatio-temporal relationships between vertices, and is used for a specific
space-time formulation that leads to significant performance improvements on
coordinated covert networks. This performance is demonstrated using a new
hybrid mixed-membership blockmodel introduced to simulate random covert
networks with realistic properties.",Bayesian Discovery of Threat Networks,produces often model association for
99,"Accumulating evidence has shown that iron is involved in the mechanism
underlying many neurodegenerative diseases, such as Alzheimer's disease,
Parkinson's disease and Huntington's disease. Abnormal (higher) iron
accumulation has been detected in the brains of most neurodegenerative
patients, especially in the basal ganglia region. Presence of iron leads to
changes in MR signal in both magnitude and phase. Accordingly, tissues with
high iron concentration appear hypo-intense (darker than usual) in MR
contrasts. In this report, we proposed an improved binary hypointensity
description and a novel nonbinary hypointensity description based on principle
components analysis. Moreover, Kendall's rank correlation coefficient was used
to compare the complementary and redundant information provided by the two
methods in order to better understand the individual descriptions of iron
accumulation in the brain.","Binary and nonbinary description of hypointensity in human brain MR
  images",natural agent model technique
100,"To understand a node's centrality in a multiplex network, its centrality
values in all the layers of the network can be aggregated. This requires a
normalization of the values, to allow their meaningful comparison and
aggregation over networks with different sizes and orders. The concrete choices
of such preprocessing steps like normalization and aggregation are almost never
discussed in network analytic papers. In this paper, we show that even sticking
to the most simple centrality index (the degree) but using different, classic
choices of normalization and aggregation strategies, can turn a node from being
among the most central to being among the least central. We present our results
by using an aggregation operator which scales between different, classic
aggregation strategies based on three multiplex networks. We also introduce a
new visualization and characterization of a node's sensitivity to the choice of
a normalization and aggregation strategy in multiplex networks. The observed
high sensitivity of single nodes to the specific choice of aggregation and
normalization strategies is of strong importance, especially for all kinds of
intelligence-analytic software as it questions the interpretations of the
findings.","Most central or least central? How much modeling decisions influence a
  node's centrality ranking in multiplex networks",paper  for
101,"Changing someone's opinion is arguably one of the most important challenges
of social interaction. The underlying process proves difficult to study: it is
hard to know how someone's opinions are formed and whether and how someone's
views shift. Fortunately, ChangeMyView, an active community on Reddit, provides
a platform where users present their own opinions and reasoning, invite others
to contest them, and acknowledge when the ensuing discussions change their
original views. In this work, we study these interactions to understand the
mechanisms behind persuasion.
  We find that persuasive arguments are characterized by interesting patterns
of interaction dynamics, such as participant entry-order and degree of
back-and-forth exchange. Furthermore, by comparing similar counterarguments to
the same opinion, we show that language factors play an essential role. In
particular, the interplay between the language of the opinion holder and that
of the counterargument provides highly predictive cues of persuasiveness.
Finally, since even in this favorable setting people may not be persuaded, we
investigate the problem of determining whether someone's opinion is susceptible
to being changed at all. For this more difficult task, we show that stylistic
choices in how the opinion is expressed carry predictive power.","Winning Arguments: Interaction Dynamics and Persuasion Strategies in
  Good-faith Online Discussions",models model control negative different bleu bandit
102,"Structural pattern recognition describes and classifies data based on the
relationships of features and parts. Topological invariants, like the Euler
number, characterize the structure of objects of any dimension. Cohomology can
provide more refined algebraic invariants to a topological space than does
homology. It assigns `quantities' to the chains used in homology to
characterize holes of any dimension. Graph pyramids can be used to describe
subdivisions of the same object at multiple levels of detail. This paper
presents cohomology in the context of structural pattern recognition and
introduces an algorithm to efficiently compute representative cocycles (the
basic elements of cohomology) in 2D using a graph pyramid. An extension to
obtain scanning and rotation invariant cocycles is given.","Invariant Representative Cocycles of Cohomology Generators using
  Irregular Graph Pyramids",model reward based function  integrative data using em
103,"We address the problem of learning in an online, bandit setting where the
learner must repeatedly select among $K$ actions, but only receives partial
feedback based on its choices. We establish two new facts: First, using a new
algorithm called Exp4.P, we show that it is possible to compete with the best
in a set of $N$ experts with probability $1-\delta$ while incurring regret at
most $O(\sqrt{KT\ln(N/\delta)})$ over $T$ time steps. The new algorithm is
tested empirically in a large-scale, real-world dataset. Second, we give a new
algorithm called VE that competes with a possibly infinite set of policies of
VC-dimension $d$ while incurring regret at most $O(\sqrt{T(d\ln(T) + \ln
(1/\delta))})$ with probability $1-\delta$. These guarantees improve on those
of all previous algorithms, whether in a stochastic or adversarial environment,
and bring us closer to providing supervised learning type guarantees for the
contextual bandit setting.",Contextual Bandit Algorithms with Supervised Learning Guarantees,present <unk>
104,"Purpose: Terminology is the set of technical words or expressions used in
specific contexts, which denotes the core concept in a formal discipline and is
usually applied in the fields of machine translation, information retrieval,
information extraction and text categorization, etc. Bilingual terminology
extraction plays an important role in the application of bilingual dictionary
compilation, bilingual Ontology construction, machine translation and
cross-language information retrieval etc. This paper addresses the issues of
monolingual terminology extraction and bilingual term alignment based on
multi-level termhood.
  Design/methodology/approach: A method based on multi-level termhood is
proposed. The new method computes the termhood of the terminology candidate as
well as the sentence that includes the terminology by the comparison of the
corpus. Since terminologies and general words usually have differently
distribution in the corpus, termhood can also be used to constrain and enhance
the performance of term alignment when aligning bilingual terms on the parallel
corpus. In this paper, bilingual term alignment based on termhood constraints
is presented.
  Findings: Experiment results show multi-level termhood can get better
performance than existing method for terminology extraction. If termhood is
used as constrain factor, the performance of bilingual term alignment can be
improved.",Bilingual Terminology Extraction Using Multi-level Termhood,model model <unk> different significant units mathematical train
105,"Preoperative templating in Total Hip Replacement (THR) is a method to
estimate the optimal size and position of the implant. Today, observational
(manual) size recognition techniques are still used to find a suitable implant
for the patient. Therefore, a digital and automated technique should be
developed so that the implant size recognition process can be effectively
implemented. For this purpose, we have introduced the new technique for
acetabular implant size recognition in THR preoperative planning based on the
diameter of acetabulum size. This technique enables the surgeon to recognise a
digital acetabular implant size automatically. Ten randomly selected X-rays of
unidentified patients were used to test the accuracy and utility of an
automated implant size recognition technique. Based on the testing result, the
new technique yielded very close results to those obtained by the observational
method in nine studies (90%).","An Automated Size Recognition Technique for Acetabular Implant in Total
  Hip Replacement",results  significant technique established different modelling
106,"We first discuss certain problems with the classical probabilistic approach
for assessing forensic evidence, in particular its inability to distinguish
between lack of belief and disbelief, and its inability to model complete
ignorance within a given population. We then discuss Shafer belief functions, a
generalization of probability distributions, which can deal with both these
objections. We use a calculus of belief functions which does not use the much
criticized Dempster rule of combination, but only the very natural
Dempster-Shafer conditioning. We then apply this calculus to some classical
forensic problems like the various island problems and the problem of parental
identification. If we impose no prior knowledge apart from assuming that the
culprit or parent belongs to a given population (something which is possible in
our setting), then our answers differ from the classical ones when uniform or
other priors are imposed. We can actually retrieve the classical answers by
imposing the relevant priors, so our setup can and should be interpreted as a
generalization of the classical methodology, allowing more flexibility. We show
how our calculus can be used to develop an analogue of Bayes' rule, with belief
functions instead of classical probabilities. We also discuss consequences of
our theory for legal practice.",Assessing forensic evidence by computing belief functions,appropriate images  mixture basis two
107,"This paper studies the problem of estimating the grahpon model - the
underlying generating mechanism of a network. Graphon estimation arises in many
applications such as predicting missing links in networks and learning user
preferences in recommender systems. The graphon model deals with a random graph
of $n$ vertices such that each pair of two vertices $i$ and $j$ are connected
independently with probability $\rho \times f(x_i,x_j)$, where $x_i$ is the
unknown $d$-dimensional label of vertex $i$, $f$ is an unknown symmetric
function, and $\rho$ is a scaling parameter characterizing the graph sparsity.
Recent studies have identified the minimax error rate of estimating the graphon
from a single realization of the random graph. However, there exists a wide gap
between the known error rates of computationally efficient estimation
procedures and the minimax optimal error rate.
  Here we analyze a spectral method, namely universal singular value
thresholding (USVT) algorithm, in the relatively sparse regime with the average
vertex degree $n\rho=\Omega(\log n)$. When $f$ belongs to H\""{o}lder or Sobolev
space with smoothness index $\alpha$, we show the error rate of USVT is at most
$(n\rho)^{ -2 \alpha / (2\alpha+d)}$, approaching the minimax optimal error
rate $\log (n\rho)/(n\rho)$ for $d=1$ as $\alpha$ increases. Furthermore, when
$f$ is analytic, we show the error rate of USVT is at most $\log^d
(n\rho)/(n\rho)$. In the special case of stochastic block model with $k$
blocks, the error rate of USVT is at most $k/(n\rho)$, which is larger than the
minimax optimal error rate by at most a multiplicative factor $k/\log k$. This
coincides with the computational gap observed for community detection. A key
step of our analysis is to derive the eigenvalue decaying rate of the edge
probability matrix using piecewise polynomial approximations of the graphon
function $f$.",Rates of Convergence of Spectral Methods for Graphon Estimation,<unk> features show <unk>
108,"We present a new Cascaded Shape Regression (CSR) architecture, namely Dynamic
Attention-Controlled CSR (DAC-CSR), for robust facial landmark detection on
unconstrained faces. Our DAC-CSR divides facial landmark detection into three
cascaded sub-tasks: face bounding box refinement, general CSR and
attention-controlled CSR. The first two stages refine initial face bounding
boxes and output intermediate facial landmarks. Then, an online dynamic model
selection method is used to choose appropriate domain-specific CSRs for further
landmark refinement. The key innovation of our DAC-CSR is the fault-tolerant
mechanism, using fuzzy set sample weighting for attention-controlled
domain-specific model training. Moreover, we advocate data augmentation with a
simple but effective 2D profile face generator, and context-aware feature
extraction for better facial feature representation. Experimental results
obtained on challenging datasets demonstrate the merits of our DAC-CSR over the
state-of-the-art.","Dynamic Attention-controlled Cascaded Shape Regression Exploiting
  Training Data Augmentation and Fuzzy-set Sample Weighting",strategy conventional show <unk>
109,"Basing on the analysis by revealing the equivalence of modern networks, we
find that both ResNet and DenseNet are essentially derived from the same ""dense
topology"", yet they only differ in the form of connection -- addition (dubbed
""inner link"") vs. concatenation (dubbed ""outer link""). However, both two forms
of connections have the superiority and insufficiency. To combine their
advantages and avoid certain limitations on representation learning, we present
a highly efficient and modularized Mixed Link Network (MixNet) which is
equipped with flexible inner link and outer link modules. Consequently, ResNet,
DenseNet and Dual Path Network (DPN) can be regarded as a special case of
MixNet, respectively. Furthermore, we demonstrate that MixNets can achieve
superior efficiency in parameter over the state-of-the-art architectures on
many competitive datasets like CIFAR-10/100, SVHN and ImageNet.",Mixed Link Networks,using demonstrated greedy language the implications proposed <unk> inference
110,"The quadratic assignment problem (QAP) is one of the most difficult
combinatorial optimization problems. One of the most powerful and commonly used
heuristics to obtain approximations to the optimal solution of the QAP is
simulated annealing (SA). We present an efficient implementation of the SA
heuristic which performs more than 100 times faster then existing
implementations for large problem sizes and a large number of SA iterations.","An efficient implementation of the simulated annealing heuristic for the
  quadratic assignment problem",tests  domains  based scene find describes
111,"Dividing sentences in chunks of words is a useful preprocessing step for
parsing, information extraction and information retrieval. (Ramshaw and Marcus,
1995) have introduced a ""convenient"" data representation for chunking by
converting it to a tagging task. In this paper we will examine seven different
data representations for the problem of recognizing noun phrase chunks. We will
show that the the data representation choice has a minor influence on chunking
performance. However, equipped with the most suitable data representation, our
memory-based learning chunker was able to improve the best published chunking
results for a standard data set.",Representing Text Chunks,natural agent
112,"Boosting is a general method of generating many simple classification rules
and combining them into a single, highly accurate rule. In this talk, I will
review the AdaBoost boosting algorithm and some of its underlying theory, and
then look at how this theory has helped us to face some of the challenges of
applying AdaBoost in two domains: In the first of these, we used boosting for
predicting and modeling the uncertainty of prices in complicated, interacting
auctions. The second application was to the classification of caller utterances
in a telephone spoken-dialogue system where we faced two challenges: the need
to incorporate prior knowledge to compensate for initially insufficient data;
and a later need to filter the large stream of unlabeled examples being
collected to select the ones whose labels are likely to be most informative.",Advances in Boosting (Invited Talk),best networks proposed based certain timed two
113,"Current approaches to computational lexicology in language technology are
knowledge-based (competence-oriented) and try to abstract away from specific
formalisms, domains, and applications. This results in severe complexity,
acquisition and reusability bottlenecks. As an alternative, we propose a
particular performance-oriented approach to Natural Language Processing based
on automatic memory-based learning of linguistic (lexical) tasks. The
consequences of the approach for computational lexicology are discussed, and
the application of the approach on a number of lexical acquisition and
disambiguation tasks in phonology, morphology and syntax is described.",Memory-Based Lexical Acquisition and Processing,based learning network structured mean
114,"Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.",Mobile Augmented Reality Applications,structure this achieved show combining inference required
115,"We analyze emotionally annotated massive data from IRC (Internet Relay Chat)
and model the dialogues between its participants by assuming that the driving
force for the discussion is the entropy growth of emotional probability
distribution. This process is claimed to be correlated to the emergence of the
power-law distribution of the discussion lengths observed in the dialogues. We
perform numerical simulations based on the noticed phenomenon obtaining a good
agreement with the real data. Finally, we propose a method to artificially
prolong the duration of the discussion that relies on the entropy of emotional
probability distribution.",Entropy-growth-based model of emotionally charged online dialogues,rate  method meaningful proposed diffusion
116,"This paper defines an argumentation semantics for extended logic programming
and shows its equivalence to the well-founded semantics with explicit negation.
We set up a general framework in which we extensively compare this semantics to
other argumentation semantics, including those of Dung, and Prakken and Sartor.
We present a general dialectical proof theory for these argumentation
semantics.",Well-Founded Argumentation Semantics for Extended Logic Programming,using research word curve show conditional process discuss
117,"Tracking multiple objects is a challenging task when objects move in groups
and occlude each other. Existing methods have investigated the problems of
group division and group energy-minimization; however, lacking overall
object-group topology modeling limits their ability in handling complex object
and group dynamics. Inspired with the social affinity property of moving
objects, we propose a Graphical Social Topology (GST) model, which estimates
the group dynamics by jointly modeling the group structure and the states of
objects using a topological representation. With such topology representation,
moving objects are not only assigned to groups, but also dynamically connected
with each other, which enables in-group individuals to be correctly associated
and the cohesion of each group to be precisely modeled. Using well-designed
topology learning modules and topology training, we infer the birth/death and
merging/splitting of dynamic groups. With the GST model, the proposed
multi-object tracker can naturally facilitate the occlusion problem by treating
the occluded object and other in-group members as a whole while leveraging
overall state transition. Experiments on both RGB and RGB-D datasets confirm
that the proposed multi-object tracker improves the state-of-the-arts
especially in crowded scenes.",A Graphical Social Topology Model for Multi-Object Tracking,preferences method significant
118,"We consider the problem of neural association for a network of non-binary
neurons. Here, the task is to first memorize a set of patterns using a network
of neurons whose states assume values from a finite number of integer levels.
Later, the same network should be able to recall previously memorized patterns
from their noisy versions. Prior work in this area consider storing a finite
number of purely random patterns, and have shown that the pattern retrieval
capacities (maximum number of patterns that can be memorized) scale only
linearly with the number of neurons in the network.
  In our formulation of the problem, we concentrate on exploiting redundancy
and internal structure of the patterns in order to improve the pattern
retrieval capacity. Our first result shows that if the given patterns have a
suitable linear-algebraic structure, i.e. comprise a sub-space of the set of
all possible patterns, then the pattern retrieval capacity is in fact
exponential in terms of the number of neurons. The second result extends the
previous finding to cases where the patterns have weak minor components, i.e.
the smallest eigenvalues of the correlation matrix tend toward zero. We will
use these minor components (or the basis vectors of the pattern null space) to
both increase the pattern retrieval capacity and error correction capabilities.
  An iterative algorithm is proposed for the learning phase, and two simple
neural update algorithms are presented for the recall phase. Using analytical
results and simulations, we show that the proposed methods can tolerate a fair
amount of errors in the input while being able to memorize an exponentially
large number of patterns.","A Non-Binary Associative Memory with Exponential Pattern Retrieval
  Capacity and Iterative Learning: Extended Results",based examples new suitable of
119,"Augmented reality (AR) displays become more and more popular recently,
because of its high intuitiveness for humans and high-quality head-mounted
display have rapidly developed. To achieve such displays with augmented
information, highly accurate image registration or ego-positioning are
required, but little attention have been paid for out-door environments. This
paper presents a method for ego-positioning in outdoor environments with low
cost monocular cameras. To reduce the computational and memory requirements as
well as the communication overheads, we formulate the model compression
algorithm as a weighted k-cover problem for better preserving model structures.
Specifically for real-world vision-based positioning applications, we consider
the issues with large scene change and propose a model update algorithm to
tackle these problems. A long- term positioning dataset with more than one
month, 106 sessions, and 14,275 images is constructed. Based on both local and
up-to-date models constructed in our approach, extensive experimental results
show that high positioning accuracy (mean ~ 30.9cm, stdev. ~ 15.4cm) can be
achieved, which outperforms existing vision-based algorithms.",To Know Where We Are: Vision-Based Positioning in Outdoor Environments,accurate for show account deal
120,"Among the many possible approaches for the parallelization of self-organizing
networks, and in particular of growing self-organizing networks, perhaps the
most common one is producing an optimized, parallel implementation of the
standard sequential algorithms reported in the literature. In this paper we
explore an alternative approach, based on a new algorithm variant specifically
designed to match the features of the large-scale, fine-grained parallelism of
GPUs, in which multiple input signals are processed at once. Comparative tests
have been performed, using both parallel and sequential implementations of the
new algorithm variant, in particular for a growing self-organizing network that
reconstructs surfaces from point clouds. The experimental results show that
this approach allows harnessing in a more effective way the intrinsic
parallelism that the self-organizing networks algorithms seem intuitively to
suggest, obtaining better performances even with networks of smaller size.","A Multi-signal Variant for the GPU-based Parallelization of Growing
  Self-Organizing Networks",however  however  egocentric test <unk> imaging 
121,"The production of color language is essential for grounded language
generation. Color descriptions have many challenging properties: they can be
vague, compositionally complex, and denotationally rich. We present an
effective approach to generating color descriptions using recurrent neural
networks and a Fourier-transformed color representation. Our model outperforms
previous work on a conditional language modeling task over a large corpus of
naturalistic color descriptions. In addition, probing the model's output
reveals that it can accurately produce not only basic color terms but also
descriptors with non-convex denotations (""greenish""), bare modifiers (""bright"",
""dull""), and compositional phrases (""faded teal"") not seen in training.",Learning to Generate Compositional Color Descriptions,theory improvements  present image different <unk>
122,"Deep learning with a convolutional neural network (CNN) has been proved to be
very effective in feature extraction and representation of images. For image
classification problems, this work aim at finding which classifier is more
competitive based on high-level deep features of images. In this report, we
have discussed the nearest neighbor, support vector machines and extreme
learning machines for image classification under deep convolutional activation
feature representation. Specifically, we adopt the benchmark object recognition
dataset from multiple sources with domain bias for evaluating different
classifiers. The deep features of the object dataset are obtained by a
well-trained CNN with five convolutional layers and three fully-connected
layers on the challenging ImageNet. Experiments demonstrate that the ELMs
outperform SVMs in cross-domain recognition tasks. In particular,
state-of-the-art results are obtained by kernel ELM which outperforms SVMs with
about 4% of the average accuracy. The features and codes are available in
http://www.escience.cn/people/lei/index.html","SVM and ELM: Who Wins? Object Recognition with Deep Convolutional
  Features from ImageNet",means method paper  for
123,"We describe a system that builds a high dynamic-range and wide-angle image of
the night sky by combining a large set of input images. The method makes use of
pixel-rank information in the individual input images to improve a ""consensus""
pixel rank in the combined image. Because it only makes use of ranks and the
complexity of the algorithm is linear in the number of images, the method is
useful for large sets of uncalibrated images that might have undergone unknown
non-linear tone mapping transformations for visualization or aesthetic reasons.
We apply the method to images of the night sky (of unknown provenance)
discovered on the Web. The method permits discovery of astronomical objects or
features that are not visible in any of the input images taken individually.
More importantly, however, it permits scientific exploitation of a huge source
of astronomical images that would not be available to astronomical research
without our automatic system.",Towards building a Crowd-Sourced Sky Map,tasks demonstrate show significant demonstrate
124,"Reputation mechanisms offer an effective alternative to verification
authorities for building trust in electronic markets with moral hazard. Future
clients guide their business decisions by considering the feedback from past
transactions; if truthfully exposed, cheating behavior is sanctioned and thus
becomes irrational.
  It therefore becomes important to ensure that rational clients have the right
incentives to report honestly. As an alternative to side-payment schemes that
explicitly reward truthful reports, we show that honesty can emerge as a
rational behavior when clients have a repeated presence in the market. To this
end we describe a mechanism that supports an equilibrium where truthful
feedback is obtained. Then we characterize the set of pareto-optimal equilibria
of the mechanism, and derive an upper bound on the percentage of false reports
that can be recorded by the mechanism. An important role in the existence of
this bound is played by the fact that rational clients can establish a
reputation for reporting honestly.",Obtaining Reliable Feedback for Sanctioning Reputation Mechanisms,potential  ii  show fundamental of
125,"In this paper, we present our investigations on the use of single objective
and multiobjective genetic algorithms based optimisation algorithms to improve
the design of OFDM pulses for radar. We discuss these optimization procedures
in the scope of a waveform design intended for two different radar processing
solutions. Lastly, we show how the encoding solution is suited to permit the
optimizations of waveform for OFDM radar related challenges such as enhanced
detection.",Optimization of OFDM radar waveforms using genetic algorithms,show scientists 
126,"Data analysis and data mining are concerned with unsupervised pattern finding
and structure determination in data sets. ""Structure"" can be understood as
symmetry and a range of symmetries are expressed by hierarchy. Such symmetries
directly point to invariants, that pinpoint intrinsic properties of the data
and of the background empirical domain of interest. We review many aspects of
hierarchy here, including ultrametric topology, generalized ultrametric,
linkages with lattices and other discrete algebraic structures and with p-adic
number representations. By focusing on symmetries in data we have a powerful
means of structuring and analyzing massive, high dimensional data stores. We
illustrate the powerfulness of hierarchical clustering in case studies in
chemistry and finance, and we provide pointers to other published case studies.","Hierarchical Clustering for Finding Symmetries and Other Patterns in
  Massive, High Dimensional Datasets",discuss advantage linear
127,"This paper describes how the elements of the SP theory (Wolff, 2003a) may be
realised with neural structures and processes. To the extent that this is
successful, the insights that have been achieved in the SP theory - the
integration and simplification of a range of phenomena in perception and
cognition - may be incorporated in a neural view of brain function.
  These proposals may be seen as a development of Hebb's (1949) concept of a
'cell assembly'. By contrast with that concept and variants of it, the version
described in this paper proposes that any one neuron can belong in one assembly
and only one assembly. A distinctive feature of the present proposals is that
any neuron or cluster of neurons within a cell assembly may serve as a proxy or
reference for another cell assembly or class of cell assemblies. This device
provides solutions to many of the problems associated with cell assemblies, it
allows information to be stored in a compressed form, and it provides a robust
mechanism by which assemblies may be connected to form hierarchies, grammars
and other kinds of knowledge structure.
  Drawing on insights derived from the SP theory, the paper also describes how
unsupervised learning may be achieved with neural structures and processes.
This theory of learning overcomes weaknesses in the Hebbian concept of learning
and it is, at the same time, compatible with the observations that Hebb's
theory was designed to explain.",Neural realisation of the SP theory: cell assemblies revisited,setting test class critical models
128,"Stochastic gradient descent based algorithms are typically used as the
general optimization tools for most deep learning models. A Restricted
Boltzmann Machine (RBM) is a probabilistic generative model that can be stacked
to construct deep architectures. For RBM with Bernoulli inputs, non-Euclidean
algorithm such as stochastic spectral descent (SSD) has been specifically
designed to speed up the convergence with improved use of the gradient
estimation by sampling methods. However, the existing algorithm and
corresponding theoretical justification depend on the assumption that the
possible configurations of inputs are finite, like binary variables. The
purpose of this paper is to generalize SSD for Gaussian RBM being capable of
mod- eling continuous data, regardless of the previous assumption. We propose
the gradient descent methods in non-Euclidean space of parameters, via de-
riving the upper bounds of logarithmic partition function for RBMs based on
Schatten-infinity norm. We empirically show that the advantage and improvement
of SSD over stochastic gradient descent (SGD).","Unifying the Stochastic Spectral Descent for Restricted Boltzmann
  Machines with Bernoulli or Gaussian Inputs",models model poetry  networks
129,"Estimation of density derivatives is a versatile tool in statistical data
analysis. A naive approach is to first estimate the density and then compute
its derivative. However, such a two-step approach does not work well because a
good density estimator does not necessarily mean a good density-derivative
estimator. In this paper, we give a direct method to approximate the density
derivative without estimating the density itself. Our proposed estimator allows
analytic and computationally efficient approximation of multi-dimensional
high-order density derivatives, with the ability that all hyper-parameters can
be chosen objectively by cross-validation. We further show that the proposed
density-derivative estimator is useful in improving the accuracy of
non-parametric KL-divergence estimation via metric learning. The practical
superiority of the proposed method is experimentally demonstrated in change
detection and feature selection.","Direct Density-Derivative Estimation and Its Application in
  KL-Divergence Approximation",model programming word paper  for
130,"Image sequences filtering have recently become a very important technical
problem especially with the advent of new technology in multimedia and video
systems applications. Often image sequences are corrupted by some amount of
noise introduced by the image sensor and therefore inherently present in the
imaging process. The main problem in the image sequences is how to deal with
spatio-temporal and non stationary signals. In this paper, we propose a robust
method for noise removal of image sequence based on coupled spatial and
temporal anisotropic diffusion. The idea is to achieve an adaptive smoothing in
both spatial and temporal directions, by solving a nonlinear diffusion
equation. This allows removing noise while preserving all spatial and temporal
discontinuities",Robust Noise Filtering in Image Sequences,<unk> noise show feature recently required
131,"One of the keys for deep learning to have made a breakthrough in various
fields was to utilize high computing powers centering around GPUs. Enabling the
use of further computing abilities by distributed processing is essential not
only to make the deep learning bigger and faster but also to tackle unsolved
challenges. We present the design, implementation, and evaluation of ChainerMN,
the distributed deep learning framework we have developed. We demonstrate that
ChainerMN can scale the learning process of the ResNet-50 model to the ImageNet
dataset up to 128 GPUs with the parallel efficiency of 90%.",ChainerMN: Scalable Distributed Deep Learning Framework, #  show <unk> <unk>
132,"We present OpenML and mldata, open science platforms that provides easy
access to machine learning data, software and results to encourage further
study and application. They go beyond the more traditional repositories for
data sets and software packages in that they allow researchers to also easily
share the results they obtained in experiments and to compare their solutions
with those of others.",Open science in machine learning,graph applying analyze proposed applying analyze efficiency games 
133,"Berkeley FrameNet is a lexico-semantic resource for English based on the
theory of frame semantics. It has been exploited in a range of natural language
processing applications and has inspired the development of framenets for many
languages. We present a methodological approach to the extraction and
generation of a computational multilingual FrameNet-based grammar and lexicon.
The approach leverages FrameNet-annotated corpora to automatically extract a
set of cross-lingual semantico-syntactic valence patterns. Based on data from
Berkeley FrameNet and Swedish FrameNet, the proposed approach has been
implemented in Grammatical Framework (GF), a categorial grammar formalism
specialized for multilingual grammars. The implementation of the grammar and
lexicon is supported by the design of FrameNet, providing a frame semantic
abstraction layer, an interlingual semantic API (application programming
interface), over the interlingual syntactic API already provided by GF Resource
Grammar Library. The evaluation of the acquired grammar and lexicon shows the
feasibility of the approach. Additionally, we illustrate how the FrameNet-based
grammar and lexicon are exploited in two distinct multilingual controlled
natural language applications. The produced resources are available under an
open source license.","A Multilingual FrameNet-based Grammar and Lexicon for Controlled Natural
  Language",implementation complexity improve proposed discovery camera relationship
134,"The on-line shortest path problem is considered under various models of
partial monitoring. Given a weighted directed acyclic graph whose edge weights
can change in an arbitrary (adversarial) way, a decision maker has to choose in
each round of a game a path between two distinguished vertices such that the
loss of the chosen path (defined as the sum of the weights of its composing
edges) be as small as possible. In a setting generalizing the multi-armed
bandit problem, after choosing a path, the decision maker learns only the
weights of those edges that belong to the chosen path. For this problem, an
algorithm is given whose average cumulative loss in n rounds exceeds that of
the best path, matched off-line to the entire sequence of the edge weights, by
a quantity that is proportional to 1/\sqrt{n} and depends only polynomially on
the number of edges of the graph. The algorithm can be implemented with linear
complexity in the number of rounds n and in the number of edges. An extension
to the so-called label efficient setting is also given, in which the decision
maker is informed about the weights of the edges corresponding to the chosen
path at a total of m << n time instances. Another extension is shown where the
decision maker competes against a time-varying path, a generalization of the
problem of tracking the best expert. A version of the multi-armed bandit
setting for shortest path is also discussed where the decision maker learns
only the total weight of the chosen path but not the weights of the individual
edges on the path. Applications to routing in packet switched networks along
with simulation results are also presented.",The on-line shortest path problem under partial monitoring,<unk> model sequential brain
135,"We present a method of training a differentiable function approximator for a
regression task using negative examples. We effect this training using negative
learning rates. We also show how this method can be used to perform direct
policy learning in a reinforcement learning setting.",Negative Learning Rates and P-Learning,deal models  deal outperform
136,"We propose a novel method for temporally pooling frames in a video for the
task of human action recognition. The method is motivated by the observation
that there are only a small number of frames which, together, contain
sufficient information to discriminate an action class present in a video, from
the rest. The proposed method learns to pool such discriminative and
informative frames, while discarding a majority of the non-informative frames
in a single temporal scan of the video. Our algorithm does so by continuously
predicting the discriminative importance of each video frame and subsequently
pooling them in a deep learning framework. We show the effectiveness of our
proposed pooling method on standard benchmarks where it consistently improves
on baseline pooling methods, with both RGB and optical flow based Convolutional
networks. Further, in combination with complementary video representations, we
show results that are competitive with respect to the state-of-the-art results
on two challenging and publicly available benchmark datasets.","AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for
  Human Action Recognition in Videos",learning effort model learning
137,"3D Convolutional Neural Networks (3D-CNN) have been used for object
recognition based on the voxelized shape of an object. However, interpreting
the decision making process of these 3D-CNNs is still an infeasible task. In
this paper, we present a unique 3D-CNN based Gradient-weighted Class Activation
Mapping method (3D-GradCAM) for visual explanations of the distinct local
geometric features of interest within an object. To enable efficient learning
of 3D geometries, we augment the voxel data with surface normals of the object
boundary. We then train a 3D-CNN with this augmented data and identify the
local features critical for decision-making using 3D GradCAM. An application of
this feature identification framework is to recognize difficult-to-manufacture
drilled hole features in a complex CAD geometry. The framework can be extended
to identify difficult-to-manufacture features at multiple spatial scales
leading to a real-time design for manufacturability decision support system.","Learning and Visualizing Localized Geometric Features Using 3D-CNN: An
  Application to Manufacturability Analysis of Drilled Holes",feature <unk> alternative
138,"IQ tests are an accepted method for assessing human intelligence. The tests
consist of several parts that must be solved under a time constraint. Of all
the tested abilities, pattern recognition has been found to have the highest
correlation with general intelligence. This is primarily because pattern
recognition is the ability to find order in a noisy environment, a necessary
skill for intelligent agents. In this paper, we propose a convolutional neural
network (CNN) model for solving geometric pattern recognition problems. The CNN
receives as input multiple ordered input images and outputs the next image
according to the pattern. Our CNN is able to solve problems involving rotation,
reflection, color, size and shape patterns and score within the top 5% of human
performance.",IQ of Neural Networks,unsupervised model show deciding  planning stochastic
139,"In this letter, a very simple no-reference image quality assessment (NR-IQA)
model for JPEG compressed images is proposed. The proposed metric called median
of unique gradients (MUG) is based on the very simple facts of unique gradient
magnitudes of JPEG compressed images. MUG is a parameterless metric and does
not need training. Unlike other NR-IQAs, MUG is independent to block size and
cropping. A more stable index called MUG+ is also introduced. The experimental
results on six benchmark datasets of natural images and a benchmark dataset of
synthetic images show that MUG is comparable to the state-of-the-art indices in
literature. In addition, its performance remains unchanged for the case of the
cropped images in which block boundaries are not known. The MATLAB source code
of the proposed metrics is available at
https://dl.dropboxusercontent.com/u/74505502/MUG.m and
https://dl.dropboxusercontent.com/u/74505502/MUGplus.m.","MUG: A Parameterless No-Reference JPEG Quality Evaluator Robust to Block
  Size and Misalignment",average art
140,"The increasing use of deep neural networks for safety-critical applications,
such as autonomous driving and flight control, raises concerns about their
safety and reliability. Formal verification can address these concerns by
guaranteeing that a deep learning system operates as intended, but the state of
the art is limited to small systems. In this work-in-progress report we give an
overview of our work on mitigating this difficulty, by pursuing two
complementary directions: devising scalable verification techniques, and
identifying design choices that result in deep learning systems that are more
amenable to verification.",Toward Scalable Verification for Safety-Critical Deep Networks,based <unk> <unk> segmentation <unk> ##  model distribution
141,"Regularization is a well studied problem in the context of neural networks.
It is usually used to improve the generalization performance when the number of
input samples is relatively small or heavily contaminated with noise. The
regularization of a parametric model can be achieved in different manners some
of which are early stopping (Morgan and Bourlard, 1990), weight decay, output
smoothing that are used to avoid overfitting during the training of the
considered model. From a Bayesian point of view, many regularization techniques
correspond to imposing certain prior distributions on model parameters (Krogh
and Hertz, 1991). Using Bishop's approximation (Bishop, 1995) of the objective
function when a restricted type of noise is added to the input of a parametric
function, we derive the higher order terms of the Taylor expansion and analyze
the coefficients of the regularization terms induced by the noisy input. In
particular we study the effect of penalizing the Hessian of the mapping
function with respect to the input in terms of generalization performance. We
also show how we can control independently this coefficient by explicitly
penalizing the Jacobian of the mapping function on corrupted inputs.","Adding noise to the input of a model trained with a regularized
  objective",significant point the method centered combining theory
142,"Learning with recurrent neural networks (RNNs) on long sequences is a
notoriously difficult task. There are three major challenges: 1) complex
dependencies, 2) vanishing and exploding gradients, and 3) efficient
parallelization. In this paper, we introduce a simple yet effective RNN
connection structure, the DilatedRNN, which simultaneously tackles all of these
challenges. The proposed architecture is characterized by multi-resolution
dilated recurrent skip connections and can be combined flexibly with diverse
RNN cells. Moreover, the DilatedRNN reduces the number of parameters needed and
enhances training efficiency significantly, while matching state-of-the-art
performance (even with standard RNN cells) in tasks involving very long-term
dependencies. To provide a theory-based quantification of the architecture's
advantages, we introduce a memory capacity measure, the mean recurrent length,
which is more suitable for RNNs with long skip connections than existing
measures. We rigorously prove the advantages of the DilatedRNN over other
recurrent neural architectures. The code for our method is publicly available
at https://github.com/code-terminator/DilatedRNN",Dilated Recurrent Neural Networks,different <unk> cross correlation <unk> observation
143,"Short-term tracking is an open and challenging problem for which
discriminative correlation filters (DCF) have shown excellent performance. We
introduce the channel and spatial reliability concepts to DCF tracking and
provide a novel learning algorithm for its efficient and seamless integration
in the filter update and the tracking process. The spatial reliability map
adjusts the filter support to the part of the object suitable for tracking.
This both allows to enlarge the search region and improves tracking of
non-rectangular objects. Reliability scores reflect channel-wise quality of the
learned filters and are used as feature weighting coefficients in localization.
Experimentally, with only two simple standard features, HoGs and Colornames,
the novel CSR-DCF method -- DCF with Channel and Spatial Reliability --
achieves state-of-the-art results on VOT 2016, VOT 2015 and OTB100. The CSR-DCF
runs in real-time on a CPU.",Discriminative Correlation Filter with Channel and Spatial Reliability,programming art model
144,"The standard HPSG analysis of Germanic verb clusters can not explain the
observed narrow-scope readings of adjuncts in such verb clusters. We present an
extension of the HPSG analysis that accounts for the systematic ambiguity of
the scope of adjuncts in verb cluster constructions, by treating adjuncts as
members of the subcat list. The extension uses powerful recursive lexical
rules, implemented as complex constraints. We show how `delayed evaluation'
techniques from constraint-logic programming can be used to process such
lexical rules.",Adjuncts and the Processing of Lexical Rules,<unk> using information show requires complexity using estimator
145,"This paper revisits the problem of complex word identification (CWI)
following up the SemEval CWI shared task. We use ensemble classifiers to
investigate how well computational methods can discriminate between complex and
non-complex words. Furthermore, we analyze the classification performance to
understand what makes lexical complexity challenging. Our findings show that
most systems performed poorly on the SemEval CWI dataset, and one of the
reasons for that is the way in which human annotation was performed.","Complex Word Identification: Challenges in Data Annotation and System
  Performance",quality domain  show weight feature models method euclidean
146,"People typically learn through exposure to visual concepts associated with
linguistic descriptions. For instance, teaching visual object categories to
children is often accompanied by descriptions in text or speech. In a machine
learning context, these observations motivates us to ask whether this learning
process could be computationally modeled to learn visual classifiers. More
specifically, the main question of this work is how to utilize purely textual
description of visual classes with no training images, to learn explicit visual
classifiers for them. We propose and investigate two baseline formulations,
based on regression and domain transfer, that predict a linear classifier.
Then, we propose a new constrained optimization formulation that combines a
regression function and a knowledge transfer function with additional
constraints to predict the parameters of a linear classifier. We also propose a
generic kernelized models where a kernel classifier is predicted in the form
defined by the representer theorem. The kernelized models allow defining and
utilizing any two RKHS (Reproducing Kernel Hilbert Space) kernel functions in
the visual space and text space, respectively. We finally propose a kernel
function between unstructured text descriptions that builds on distributional
semantics, which shows an advantage in our setting and could be useful for
other applications. We applied all the studied models to predict visual
classifiers on two fine-grained and challenging categorization datasets (CU
Birds and Flower Datasets), and the results indicate successful predictions of
our final model over several baselines that we designed.",Write a Classifier: Predicting Visual Classifiers from Unstructured Text,data making often based require earlier
147,"For simple digital circuits, conventional method of designing circuits can
easily be applied. But for complex digital circuits, the conventional method of
designing circuits is not fruitfully applicable because it is time-consuming.
On the contrary, Genetic Programming is used mostly for automatic program
generation. The modern approach for designing Arithmetic circuits, commonly
digital circuits, is based on Graphs. This graph-based evolutionary design of
arithmetic circuits is a method of optimized designing of arithmetic circuits.
In this paper, a new technique for evolutionary design of digital circuits is
proposed using Genetic Programming (GP) with Subtree Mutation in place of
Graph-based design. The results obtained using this technique demonstrates the
potential capability of genetic programming in digital circuit design with
limited computer algorithms. The proposed technique, helps to simplify and
speed up the process of designing digital circuits, discovers a variation in
the field of digital circuit design where optimized digital circuits can be
successfully and effectively designed.",Evolutionary Design of Digital Circuits Using Genetic Programming,robotics strategy show feature word for
148,"Most of the problems in genetic algorithms are very complex and demand a
large amount of resources that current technology can not offer. Our purpose
was to develop a Java-JINI distributed library that implements Genetic
Algorithms with sub-populations (coarse grain) and a graphical interface in
order to configure and follow the evolution of the search. The sub-populations
are simulated/evaluated in personal computers connected trough a network,
keeping in mind different models of sub-populations, migration policies and
network topologies. We show that this model delays the convergence of the
population keeping a higher level of genetic diversity and allows a much
greater number of evaluations since they are distributed among several
computers compared with the traditional Genetic Algorithms.",Evolutionary Demographic Algorithms,weighted networks show the
149,"The treatment of both aleatory and epistemic uncertainty by recent methods
often requires an high computational effort. In this abstract, we propose a
numerical sampling method allowing to lighten the computational burden of
treating the information by means of so-called fuzzy random variables.","Numerical Sensitivity and Efficiency in the Treatment of Epistemic and
  Aleatory Uncertainty",addition model power networks
150,"In this paper, we argue that the future of Artificial Intelligence research
resides in two keywords: integration and embodiment. We support this claim by
analyzing the recent advances of the field. Regarding integration, we note that
the most impactful recent contributions have been made possible through the
integration of recent Machine Learning methods (based in particular on Deep
Learning and Recurrent Neural Networks) with more traditional ones (e.g.
Monte-Carlo tree search, goal babbling exploration or addressable memory
systems). Regarding embodiment, we note that the traditional benchmark tasks
(e.g. visual classification or board games) are becoming obsolete as
state-of-the-art learning algorithms approach or even surpass human performance
in most of them, having recently encouraged the development of first-person 3D
game platforms embedding realistic physics. Building upon this analysis, we
first propose an embodied cognitive architecture integrating heterogenous
sub-fields of Artificial Intelligence into a unified framework. We demonstrate
the utility of our approach by showing how major contributions of the field can
be expressed within the proposed framework. We then claim that benchmarking
environments need to reproduce ecologically-valid conditions for bootstrapping
the acquisition of increasingly complex cognitive skills through the concept of
a cognitive arms race between embodied agents.","Embodied Artificial Intelligence through Distributed Adaptive Control:
  An Integrated Framework",theory ranking show minimal regularized
151,"Most languages use the relative order between words to encode meaning
relations. Languages differ, however, in what orders they use and how these
orders are mapped onto different meanings. We test the hypothesis that, despite
these differences, human languages might constitute different `solutions' to
common pressures of language use. Using Monte Carlo simulations over data from
five languages, we find that their word orders are efficient for processing in
terms of both dependency length and local lexical probability. This suggests
that biases originating in how the brain understands language strongly
constrain how human languages change over generations.",Human languages order information efficiently,objects models show video events  using conventional
152,"Natural Language Processing (NLP) systems often make use of machine learning
techniques that are unfamiliar to end-users who are interested in analyzing
clinical records. Although NLP has been widely used in extracting information
from clinical text, current systems generally do not support model revision
based on feedback from domain experts.
  We present a prototype tool that allows end users to visualize and review the
outputs of an NLP system that extracts binary variables from clinical text. Our
tool combines multiple visualizations to help the users understand these
results and make any necessary corrections, thus forming a feedback loop and
helping improve the accuracy of the NLP models. We have tested our prototype in
a formative think-aloud user study with clinicians and researchers involved in
colonoscopy research. Results from semi-structured interviews and a System
Usability Scale (SUS) analysis show that the users are able to quickly start
refining NLP models, despite having very little or no experience with machine
learning. Observations from these sessions suggest revisions to the interface
to better support review workflow and interpretation of results.",An Interactive Tool for Natural Language Processing on Clinical Text,based observation model feature theory for
153,"Counting letters in written texts is a very ancient practice. It has
accompanied the development of Cryptology, Quantitative Linguistics, and
Statistics. In Cryptology, counting frequencies of the different characters in
an encrypted message is the basis of the so called frequency analysis method.
In Quantitative Linguistics, the proportion of vowels to consonants in
different languages was studied long before authorship attribution. In
Statistics, the alternation vowel-consonants was the only example that Markov
ever gave of his theory of chained events. A short history of letter counting
is presented. The three domains, Cryptology, Quantitative Linguistics, and
Statistics, are then examined, focusing on the interactions with the other two
fields through letter counting. As a conclusion, the eclectism of past
centuries scholars, their background in humanities, and their familiarity with
cryptograms, are identified as contributing factors to the mutual enrichment
process which is described here.","Letter counting: a stem cell for Cryptology, Quantitative Linguistics,
  and Statistics",estimation aspects region show input stochastic
154,"Researchers have recently started investigating deep neural networks for
dialogue applications. In particular, generative sequence-to-sequence (Seq2Seq)
models have shown promising results for unstructured tasks, such as word-level
dialogue response generation. The hope is that such models will be able to
leverage massive amounts of data to learn meaningful natural language
representations and response generation strategies, while requiring a minimum
amount of domain knowledge and hand-crafting. An important challenge is to
develop models that can effectively incorporate dialogue context and generate
meaningful and diverse responses. In support of this goal, we review recently
proposed models based on generative encoder-decoder neural network
architectures, and show that these models have better ability to incorporate
long-term dialogue history, to model uncertainty and ambiguity in dialogue, and
to generate responses with high-level compositional structure.",Generative Deep Neural Networks for Dialogue: A Short Review,recently required show clinically model <unk> for
155,"In this paper we show the similarities and differences of two deep neural
networks by comparing the manifolds composed of activation vectors in each
fully connected layer of them. The main contribution of this paper includes 1)
a new data generating algorithm which is crucial for determining the dimension
of manifolds; 2) a systematic strategy to compare manifolds. Especially, we
take Riemann curvature and sectional curvature as part of criterion, which can
reflect the intrinsic geometric properties of manifolds. Some interesting
results and phenomenon are given, which help in specifying the similarities and
differences between the features extracted by two networks and demystifying the
intrinsic mechanism of deep neural networks.",Curvature-based Comparison of Two Neural Networks,method  fully
156,"Vocalizations and less often gestures have been the object of linguistic
research over decades. However, the development of a general theory of
communication with human language as a particular case requires a clear
understanding of the organization of communication through other means.
Infochemicals are chemical compounds that carry information and are employed by
small organisms that cannot emit acoustic signals of optimal frequency to
achieve successful communication. Here the distribution of infochemicals across
species is investigated when they are ranked by their degree or the number of
species with which it is associated (because they produce or they are sensitive
to it). The quality of the fit of different functions to the dependency between
degree and rank is evaluated with a penalty for the number of parameters of the
function. Surprisingly, a double Zipf (a Zipf distribution with two regimes
with a different exponent each) is the model yielding the best fit although it
is the function with the largest number of parameters. This suggests that the
world wide repertoire of infochemicals contains a chemical nucleus shared by
many species and reminiscent of the core vocabularies found for human language
in dictionaries or large corpora.",The infochemical core,model  n  present image similarity estimated
157,"This paper reports on results on the entropy of the Spanish language. They
are based on an analysis of natural language for n-word symbols (n = 1 to 18),
trigrams, digrams, and characters. The results obtained in this work are based
on the analysis of twelve different literary works in Spanish, as well as a
279917 word news file provided by the Spanish press agency EFE. Entropy values
are calculated by a direct method using computer processing and the probability
law of large numbers. Three samples of artificial Spanish language produced by
a first-order model software source are also analyzed and compared with natural
Spanish language.",On the Entropy of Written Spanish,video <unk>
158,"Learning distributed representations for relation instances is a central
technique in downstream NLP applications. In order to address semantic modeling
of relational patterns, this paper constructs a new dataset that provides
multiple similarity ratings for every pair of relational patterns on the
existing dataset. In addition, we conduct a comparative study of different
encoders including additive composition, RNN, LSTM, and GRU for composing
distributed representations of relational patterns. We also present Gated
Additive Composition, which is an enhancement of additive composition with the
gating mechanism. Experiments show that the new dataset does not only enable
detailed analyses of the different encoders, but also provides a gauge to
predict successes of distributed representations of relational patterns in the
relation classification task.",Composing Distributed Representations of Relational Patterns,kernels technique show technique explain <unk>
159,"This paper gives a detailed study on the performance of oil paint image
filter algorithm with various parameters applied on an image of RGB model. Oil
Paint image processing, being very performance hungry, current research tries
to find improvement using parallel pattern library. With increasing
kernel-size, the processing time of oil paint image filter algorithm increases
exponentially.","Study on performance improvement of oil paint image filter algorithm
  using parallel pattern library",decoder conditions proposed paper  conditions
160,"We consider the problem of using sentence compression techniques to
facilitate query-focused multi-document summarization. We present a
sentence-compression-based framework for the task, and design a series of
learning-based compression models built on parse trees. An innovative beam
search decoder is proposed to efficiently find highly probable compressions.
Under this framework, we show how to integrate various indicative metrics such
as linguistic motivation and query relevance into the compression process by
deriving a novel formulation of a compression scoring function. Our best model
achieves statistically significant improvement over the state-of-the-art
systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2
respectively) for the DUC 2006 and 2007 summarization task.","A Sentence Compression Based Framework to Query-Focused Multi-Document
  Summarization",theory ranking show video search
161,"A reliable, real-time simultaneous localization and mapping (SLAM) method is
crucial for the navigation of actively controlled capsule endoscopy robots.
These robots are an emerging, minimally invasive diagnostic and therapeutic
technology for use in the gastrointestinal (GI) tract. In this study, we
propose a dense, non-rigidly deformable, and real-time map fusion approach for
actively controlled endoscopic capsule robot applications. The method combines
magnetic and vision based localization, and makes use of frame-to-model fusion
and model-to-model loop closure. The performance of the method is demonstrated
using an ex-vivo porcine stomach model. Across four trajectories of varying
speed and complexity, and across three cameras, the root mean square
localization errors range from 0.42 to 1.92 cm, and the root mean square
surface reconstruction errors range from 1.23 to 2.39 cm.","Magnetic-Visual Sensor Fusion based Medical SLAM for Endoscopic Capsule
  Robot",quality show categorization stochastic
162,"Neural sequence-to-sequence models have provided a viable new approach for
abstractive text summarization (meaning they are not restricted to simply
selecting and rearranging passages from the original text). However, these
models have two shortcomings: they are liable to reproduce factual details
inaccurately, and they tend to repeat themselves. In this work we propose a
novel architecture that augments the standard sequence-to-sequence attentional
model in two orthogonal ways. First, we use a hybrid pointer-generator network
that can copy words from the source text via pointing, which aids accurate
reproduction of information, while retaining the ability to produce novel words
through the generator. Second, we use coverage to keep track of what has been
summarized, which discourages repetition. We apply our model to the CNN / Daily
Mail summarization task, outperforming the current abstractive state-of-the-art
by at least 2 ROUGE points.",Get To The Point: Summarization with Pointer-Generator Networks,model topic previously via proposed paper  for
163,"It is now widely recognized that ontologies, are one of the fundamental
cornerstones of knowledge-based systems. What is lacking, however, is a
currently accepted strategy of how to build ontology; what kinds of the
resources and techniques are indispensables to optimize the expenses and the
time on the one hand and the amplitude, the completeness, the robustness of en
ontology on the other hand. The paper offers a semi-automatic ontology
construction method from text corpora in the domain of radiological protection.
This method is composed from next steps: 1) text annotation with part-of-speech
tags; 2) revelation of the significant linguistic structures and forming the
templates; 3) search of text fragments corresponding to these templates; 4)
basic ontology instantiation process","Automatic Method Of Domain Ontology Construction based on
  Characteristics of Corpora POS-Analysis",making complexity stochastic show <unk> proposed distribution potential
164,"We developed on example-based method of metonymy interpretation. One
advantages of this method is that a hand-built database of metonymy is not
necessary because it instead uses examples in the form ``Noun X no Noun Y (Noun
Y of Noun X).'' Another advantage is that we will be able to interpret
newly-coined metonymic sentences by using a new corpus. We experimented with
metonymy interpretation and obtained a precision rate of 66% when using this
method.",Metonymy Interpretation Using X NO Y Examples,learning  sp model frank wolfe principal
165,"Many computer vision problems are formulated as the optimization of a cost
function. This approach faces two main challenges: (i) designing a cost
function with a local optimum at an acceptable solution, and (ii) developing an
efficient numerical method to search for one (or multiple) of these local
optima. While designing such functions is feasible in the noiseless case, the
stability and location of local optima are mostly unknown under noise,
occlusion, or missing data. In practice, this can result in undesirable local
optima or not having a local optimum in the expected place. On the other hand,
numerical optimization algorithms in high-dimensional spaces are typically
local and often rely on expensive first or second order information to guide
the search. To overcome these limitations, this paper proposes Discriminative
Optimization (DO), a method that learns search directions from data without the
need of a cost function. Specifically, DO explicitly learns a sequence of
updates in the search space that leads to stationary points that correspond to
desired solutions. We provide a formal analysis of DO and illustrate its
benefits in the problem of 3D point cloud registration, camera pose estimation,
and image denoising. We show that DO performed comparably or outperformed
state-of-the-art algorithms in terms of accuracy, robustness to perturbations,
and computational efficiency.","Discriminative Optimization: Theory and Applications to Computer Vision
  Problems",learning unseen model model <unk> proposed <unk> preferences
166,"Non-Negative Matrix Factorization, NMF, attempts to find a number of
archetypal response profiles, or parts, such that any sample profile in the
dataset can be approximated by a close profile among these archetypes or a
linear combination of these profiles. The non-negativity constraint is imposed
while estimating archetypal profiles, due to the non-negative nature of the
observed signal. Apart from non negativity, a volume constraint can be applied
on the Score matrix W to enhance the ability of learning parts of NMF. In this
report, we describe a very simple algorithm, which in effect achieves volume
minimization, although indirectly.","Permuted NMF: A Simple Algorithm Intended to Minimize the Volume of the
  Score Matrix",points agents hypothesis
167,"We propose a new algorithm for topic modeling, Vec2Topic, that identifies the
main topics in a corpus using semantic information captured via
high-dimensional distributed word embeddings. Our technique is unsupervised and
generates a list of topics ranked with respect to importance. We find that it
works better than existing topic modeling techniques such as Latent Dirichlet
Allocation for identifying key topics in user-generated content, such as
emails, chats, etc., where topics are diffused across the corpus. We also find
that Vec2Topic works equally well for non-user generated content, such as
papers, reports, etc., and for small corpora such as a single-document.",Topic Modeling Using Distributed Word Embeddings,empirical offers show proposed word for
168,"Reusable model design becomes desirable with the rapid expansion of computer
vision and machine learning applications. In this paper, we focus on the
reusability of pre-trained deep convolutional models. Specifically, different
from treating pre-trained models as feature extractors, we reveal more
treasures beneath convolutional layers, i.e., the convolutional activations
could act as a detector for the common object in the image co-localization
problem. We propose a simple yet effective method, termed Deep Descriptor
Transforming (DDT), for evaluating the correlations of descriptors and then
obtaining the category-consistent regions, which can accurately locate the
common object in a set of unlabeled images, i.e., unsupervised object
discovery. Empirical studies validate the effectiveness of the proposed DDT
method. On benchmark image co-localization datasets, DDT consistently
outperforms existing state-of-the-art methods by a large margin. Moreover, DDT
also demonstrates good generalization ability for unseen categories and
robustness for dealing with noisy data. Beyond those, DDT can be also employed
for harvesting web images into valid external data sources for improving
performance of both image recognition and object detection.","Unsupervised Object Discovery and Co-Localization by Deep Descriptor
  Transforming",words models proposed feature common
169,"Video classification has advanced tremendously over the recent years. A large
part of the improvements in video classification had to do with the work done
by the image classification community and the use of deep convolutional
networks (CNNs) which produce competitive results with hand- crafted motion
features. These networks were adapted to use video frames in various ways and
have yielded state of the art classification results. We present two methods
that build on this work, and scale it up to work with millions of videos and
hundreds of thousands of classes while maintaining a low computational cost. In
the context of large scale video processing, training CNNs on video frames is
extremely time consuming, due to the large number of frames involved. We
propose to avoid this problem by training CNNs on either YouTube thumbnails or
Flickr images, and then using these networks' outputs as features for other
higher level classifiers. We discuss the challenges of achieving this and
propose two models for frame-level and video-level classification. The first is
a highly efficient mixture of experts while the latter is based on long short
term memory neural networks. We present results on the Sports-1M video dataset
(1 million videos, 487 classes) and on a new dataset which has 12 million
videos and 150,000 labels.",Efficient Large Scale Video Classification,demonstrate show video adaptive combined
170,"In this paper, the Optional Prisoner's Dilemma game in a spatial environment,
with coevolutionary rules for both the strategy and network links between
agents, is studied. Using a Monte Carlo simulation approach, a number of
experiments are performed to identify favourable configurations of the
environment for the emergence of cooperation in adverse scenarios. Results show
that abstainers play a key role in the protection of cooperators against
exploitation from defectors. Scenarios of cyclic competition and of full
dominance of cooperation are also observed. This work provides insights towards
gaining an in-depth understanding of the emergence of cooperative behaviour in
real-world systems.","The Optional Prisoner's Dilemma in a Spatial Environment: Coevolving
  Game Strategy and Link Weights",drop discovery minimal
171,"This paper presents a new semi-supervised framework with convolutional neural
networks (CNNs) for text categorization. Unlike the previous approaches that
rely on word embeddings, our method learns embeddings of small text regions
from unlabeled data for integration into a supervised CNN. The proposed scheme
for embedding learning is based on the idea of two-view semi-supervised
learning, which is intended to be useful for the task of interest even though
the training is done on unlabeled data. Our models achieve better results than
previous approaches on sentiment classification and topic classification tasks.","Semi-supervised Convolutional Neural Networks for Text Categorization
  via Region Embedding",model <unk> using analyzed histogram set data 
172,"The problem of reinforcement learning in an unknown and discrete Markov
Decision Process (MDP) under the average-reward criterion is considered, when
the learner interacts with the system in a single stream of observations,
starting from an initial state without any reset. We revisit the minimax lower
bound for that problem by making appear the local variance of the bias function
in place of the diameter of the MDP. Furthermore, we provide a novel analysis
of the KL-UCRL algorithm establishing a high-probability regret bound scaling
as $\widetilde {\mathcal O}\Bigl({\textstyle \sqrt{S\sum_{s,a}{\bf
V}^\star_{s,a}T}}\Big)$ for this algorithm for ergodic MDPs, where $S$ denotes
the number of states and where ${\bf V}^\star_{s,a}$ is the variance of the
bias function with respect to the next-state distribution following action $a$
in state $s$. The resulting bound improves upon the best previously known
regret bound $\widetilde {\mathcal O}(DS\sqrt{AT})$ for that algorithm, where
$A$ and $D$ respectively denote the maximum number of actions (per state) and
the diameter of MDP. We finally compare the leading terms of the two bounds in
some benchmark MDPs indicating that the derived bound can provide an order of
magnitude improvement in some cases. Our analysis leverages novel variations of
the transportation lemma combined with Kullback-Leibler concentration
inequalities, that we believe to be of independent interest.","Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in
  MDPs",addressing for
173,"We study the problem of multivariate regression where the data are naturally
grouped, and a regression matrix is to be estimated for each group. We propose
an approach in which a dictionary of low rank parameter matrices is estimated
across groups, and a sparse linear combination of the dictionary elements is
estimated to form a model within each group. We refer to the method as
conditional sparse coding since it is a coding procedure for the response
vectors Y conditioned on the covariate vectors X. This approach captures the
shared information across the groups while adapting to the structure within
each group. It exploits the same intuition behind sparse coding that has been
successfully developed in computer vision and computational neuroscience. We
propose an algorithm for conditional sparse coding, analyze its theoretical
properties in terms of predictive accuracy, and present the results of
simulation and brain imaging experiments that compare the new technique to
reduced rank regression.",Conditional Sparse Coding and Grouped Multivariate Regression,based <unk> method  two
174,"Causal models defined in terms of structural equations have proved to be
quite a powerful way of representing knowledge regarding causality. However, a
number of authors have given examples that seem to show that the Halpern-Pearl
(HP) definition of causality gives intuitively unreasonable answers. Here it is
shown that, for each of these examples, we can give two stories consistent with
the description in the example, such that intuitions regarding causality are
quite different for each story. By adding additional variables, we can
disambiguate the stories. Moreover, in the resulting causal models, the HP
definition of causality gives the intuitively correct answer. It is also shown
that, by adding extra variables, a modification to the original HP definition
made to deal with an example of Hopkins and Pearl may not be necessary. Given
how much can be done by adding extra variables, there might be a concern that
the notion of causality is somewhat unstable. Can adding extra variables in a
""conservative"" way (i.e., maintaining all the relations between the variables
in the original model) cause the answer to the question ""Is X=x a cause of Y=y""
to alternate between ""yes"" and ""no""? It is shown that we can have such
alternation infinitely often, but if we take normality into consideration, we
cannot. Indeed, under appropriate normality assumptions. adding an extra
variable can change the answer from ""yes"" to ""no"", but after that, it cannot
cannot change back to ""yes"".",Appropriate Causal Models and the Stability of Causation,using model means show <unk> knowledge
175,"The ability to use a 2D map to navigate a complex 3D environment is quite
remarkable, and even difficult for many humans. Localization and navigation is
also an important problem in domains such as robotics, and has recently become
a focus of the deep reinforcement learning community. In this paper we teach a
reinforcement learning agent to read a map in order to find the shortest way
out of a random maze it has never seen before. Our system combines several
state-of-the-art methods such as A3C and incorporates novel elements such as a
recurrent localization cell. Our agent learns to localize itself based on 3D
first person images and an approximate orientation angle. The agent generalizes
well to bigger mazes, showing that it learned useful localization and
navigation capabilities.",Teaching a Machine to Read Maps with Deep Reinforcement Learning,typology axes show <unk> outdoor
176,"In many domains it is desirable to assess the preferences of users in a
qualitative rather than quantitative way. Such representations of qualitative
preference orderings form an importnat component of automated decision tools.
We propose a graphical representation of preferences that reflects conditional
dependence and independence of preference statements under a ceteris paribus
(all else being equal) interpretation. Such a representation is ofetn compact
and arguably natural. We describe several search algorithms for dominance
testing based on this representation; these algorithms are quite effective,
especially in specific network topologies, such as chain-and tree- structured
networks, as well as polytrees.",Reasoning With Conditional Ceteris Paribus Preference Statem,based <unk> <unk> qualitative
177,"This paper addresses the problem of detecting relevant motion caused by
objects of interest (e.g., person and vehicles) in large scale home
surveillance videos. The traditional method usually consists of two separate
steps, i.e., detecting moving objects with background subtraction running on
the camera, and filtering out nuisance motion events (e.g., trees, cloud,
shadow, rain/snow, flag) with deep learning based object detection and tracking
running on cloud. The method is extremely slow and therefore not cost
effective, and does not fully leverage the spatial-temporal redundancies with a
pre-trained off-the-shelf object detector. To dramatically speedup relevant
motion event detection and improve its performance, we propose a novel network
for relevant motion event detection, ReMotENet, which is a unified, end-to-end
data-driven method using spatial-temporal attention-based 3D ConvNets to
jointly model the appearance and motion of objects-of-interest in a video.
ReMotENet parses an entire video clip in one forward pass of a neural network
to achieve significant speedup. Meanwhile, it exploits the properties of home
surveillance videos, e.g., relevant motion is sparse both spatially and
temporally, and enhances 3D ConvNets with a spatial-temporal attention model
and reference-frame subtraction to encourage the network to focus on the
relevant moving objects. Experiments demonstrate that our method can achieve
comparable or event better performance than the object detection based method
but with three to four orders of magnitude speedup (up to 20k times) on GPU
devices. Our network is efficient, compact and light-weight. It can detect
relevant motion on a 15s surveillance video clip within 4-8 milliseconds on a
GPU and a fraction of second (0.17-0.39) on a CPU with a model size of less
than 1MB.","ReMotENet: Efficient Relevant Motion Event Detection for Large-scale
  Home Surveillance Videos",improve method data around
178,"Piecewise constant image approximations of sequential number of segments or
clusters of disconnected pixels are treated. The method of majorizing of
optimal approximation sequence by hierarchical sequence of image approximations
is proposed. A generalization for multidimensional case of color and
multispectral images is foreseen.","Image segmentation by optimal and hierarchical piecewise constant
  approximations",networks sentences 
179,"We propose a geometric model-free causality measurebased on multivariate
delay embedding that can efficiently detect linear and nonlinear causal
interactions between time series with no prior information. We then exploit the
proposed causal interaction measure in real MEG data analysis. The results are
used to construct effective connectivity maps of brain activity to decode
different categories of visual stimuli. Moreover, we discovered that the
MEG-based effective connectivity maps as a response to structured images
exhibit more geometric patterns, as disclosed by analyzing the evolution of
toplogical structures of the underlying networks using persistent homology.
Extensive simulation and experimental result have been carried out to
substantiate the capabilities of the proposed approach.","Effective Connectivity-Based Neural Decoding: A Causal
  Interaction-Driven Approach",completely encoder decoder show feature operator for
180,"Adaptive stochastic gradient methods such as AdaGrad have gained popularity
in particular for training deep neural networks. The most commonly used and
studied variant maintains a diagonal matrix approximation to second order
information by accumulating past gradients which are used to tune the step size
adaptively. In certain situations the full-matrix variant of AdaGrad is
expected to attain better performance, however in high dimensions it is
computationally impractical. We present Ada-LR and RadaGrad two computationally
efficient approximations to full-matrix AdaGrad based on randomized
dimensionality reduction. They are able to capture dependencies between
features and achieve similar performance to full-matrix AdaGrad but at a much
smaller computational cost. We show that the regret of Ada-LR is close to the
regret of full-matrix AdaGrad which can have an up-to exponentially smaller
dependence on the dimension than the diagonal variant. Empirically, we show
that Ada-LR and RadaGrad perform similarly to full-matrix AdaGrad. On the task
of training convolutional neural networks as well as recurrent neural networks,
RadaGrad achieves faster convergence than diagonal AdaGrad.",Scalable Adaptive Stochastic Optimization Using Random Projections,concepts made show show <unk>
181,"The important part of semantics of complex sentence is captured as relations
among semantic roles in subordinate and main clause respectively. However if
there can be relations between every pair of semantic roles, the amount of
computation to identify the relations that hold in the given sentence is
extremely large. In this paper, for semantics of Japanese complex sentence, we
introduce new pragmatic roles called `observer' and `motivated' respectively to
bridge semantic roles of subordinate and those of main clauses. By these new
roles constraints on the relations among semantic/pragmatic roles are known to
be almost local within subordinate or main clause. In other words, as for the
semantics of the whole complex sentence, the only role we should deal with is a
motivated.",Semantics of Complex Sentences in Japanese,paper  task  data <unk> number
182,"Gesture recognition is a challenging problem in the field of biometrics. In
this paper, we integrate Fisher criterion into Bidirectional Long-Short Term
Memory (BLSTM) network and Bidirectional Gated Recurrent Unit (BGRU),thus
leading to two new deep models termed as F-BLSTM and F-BGRU. BothFisher
discriminative deep models can effectively classify the gesture based on
analyzing the acceleration and angular velocity data of the human gestures.
Moreover, we collect a large Mobile Gesture Database (MGD) based on the
accelerations and angular velocities containing 5547 sequences of 12 gestures.
Extensive experiments are conducted to validate the superior performance of the
proposed networks as compared to the state-of-the-art BLSTM and BGRU on MGD
database and two benchmark databases (i.e. BUAA mobile gesture and SmartWatch
gesture).",Deep Fisher Discriminant Learning for Mobile Hand Gesture Recognition,layers model power high
183,"Reordering poses a major challenge in machine translation (MT) between two
languages with significant differences in word order. In this paper, we present
a novel reordering approach utilizing sparse features based on dependency word
pairs. Each instance of these features captures whether two words, which are
related by a dependency link in the source sentence dependency parse tree,
follow the same order or are swapped in the translation output. Experiments on
Chinese-to-English translation show a statistically significant improvement of
1.21 BLEU point using our approach, compared to a state-of-the-art statistical
MT system that incorporates prior reordering approaches.","To Swap or Not to Swap? Exploiting Dependency Word Pairs for Reordering
  in Statistical Machine Translation",paper  cues show significant
184,"As a rule, a quadratic functional depending on a great number of binary
variables has a lot of local minima. One of approaches allowing one to find in
averaged deeper local minima is aggregation of binary variables into larger
blocks/domains. To minimize the functional one has to change the states of
aggregated variables (domains). In the present publication we discuss methods
of domains formation. It is shown that the best results are obtained when
domains are formed by variables that are strongly connected with each other.",Cluster Approach to the Domains Formation,model word for show auc vector
185,"AUC is an important performance measure and many algorithms have been devoted
to AUC optimization, mostly by minimizing a surrogate convex loss on a training
data set. In this work, we focus on one-pass AUC optimization that requires
only going through the training data once without storing the entire training
dataset, where conventional online learning algorithms cannot be applied
directly because AUC is measured by a sum of losses defined over pairs of
instances from different classes. We develop a regression-based algorithm which
only needs to maintain the first and second order statistics of training data
in memory, resulting a storage requirement independent from the size of
training data. To efficiently handle high dimensional data, we develop a
randomized algorithm that approximates the covariance matrices by low rank
matrices. We verify, both theoretically and empirically, the effectiveness of
the proposed algorithm.",One-Pass AUC Optimization,models based document demonstrated <unk> loss
186,"This paper is concerned with paraphrase detection. The ability to detect
similar sentences written in natural language is crucial for several
applications, such as text mining, text summarization, plagiarism detection,
authorship authentication and question answering. Given two sentences, the
objective is to detect whether they are semantically identical. An important
insight from this work is that existing paraphrase systems perform well when
applied on clean texts, but they do not necessarily deliver good performance
against noisy texts. Challenges with paraphrase detection on user generated
short texts, such as Twitter, include language irregularity and noise. To cope
with these challenges, we propose a novel deep neural network-based approach
that relies on coarse-grained sentence modeling using a convolutional neural
network and a long short-term memory model, combined with a specific
fine-grained word-level similarity matching model. Our experimental results
show that the proposed approach outperforms existing state-of-the-art
approaches on user-generated noisy social media data, such as Twitter texts,
and achieves highly competitive performance on a cleaner corpus.",A Deep Network Model for Paraphrase Detection in Short Text Messages,based neural <unk>
187,"We consider a setting for Inverse Reinforcement Learning (IRL) where the
learner is extended with the ability to actively select multiple environments,
observing an agent's behavior on each environment. We first demonstrate that if
the learner can experiment with any transition dynamics on some fixed set of
states and actions, then there exists an algorithm that reconstructs the
agent's reward function to the fullest extent theoretically possible, and that
requires only a small (logarithmic) number of experiments. We contrast this
result to what is known about IRL in single fixed environments, namely that the
true reward function is fundamentally unidentifiable. We then extend this
setting to the more realistic case where the learner may not select any
transition dynamic, but rather is restricted to some fixed set of environments
that it may try. We connect the problem of maximizing the information derived
from experiments to submodular function maximization and demonstrate that a
greedy algorithm is near optimal (up to logarithmic factors). Finally, we
empirically validate our algorithm on an environment inspired by behavioral
psychology.",Towards Resolving Unidentifiability in Inverse Reinforcement Learning,learning reduced model set reduced
188,"Ezhil is a Tamil language based interpreted procedural programming language.
Tamil keywords and grammar are chosen to make the native Tamil speaker write
programs in the Ezhil system. Ezhil allows easy representation of computer
program closer to the Tamil language logical constructs equivalent to the
conditional, branch and loop statements in modern English based programming
languages. Ezhil is a compact programming language aimed towards Tamil speaking
novice computer users. Grammar for Ezhil and a few example programs are
reported here, from the initial proof-of-concept implementation using the
Python programming language1. To the best of our knowledge, Ezhil language is
the first freely available Tamil programming language.",Ezhil: A Tamil Programming Language,despite <unk> <unk> polynomial
189,"Image denoising techniques are essential to reducing noise levels and
enhancing diagnosis reliability in low-dose computed tomography (CT). Machine
learning based denoising methods have shown great potential in removing the
complex and spatial-variant noises in CT images. However, some residue
artifacts would appear in the denoised image due to complexity of noises. A
cascaded training network was proposed in this work, where the trained CNN was
applied on the training dataset to initiate new trainings and remove artifacts
induced by denoising. A cascades of convolutional neural networks (CNN) were
built iteratively to achieve better performance with simple CNN structures.
Experiments were carried out on 2016 Low-dose CT Grand Challenge datasets to
evaluate the method's performance.","A Cascaded Convolutional Neural Network for X-ray Low-dose CT Image
  Denoising",policy evaluate model two show <unk> based filters
190,"End-to-end models for strategic dialogue are challenging to train, because
linguistic and strategic aspects are entangled in latent state vectors. We
introduce an approach to generating latent representations of dialogue moves,
by inducing sentence representations to maximize the likelihood of subsequent
sentences and actions. The effect is to decouple much of the semantics of the
utterance from its linguistic realisation. We then use these latent sentence
representations for hierarchical language generation, planning and
reinforcement learning. Experiments show that using our message representations
increases the reward achieved by the model, improves the effectiveness of
long-term planning using rollouts, and allows self-play reinforcement learning
to improve decision making without diverging from human language. Our
hierarchical latent-variable model outperforms previous work both
linguistically and strategically.",Hierarchical Text Generation and Planning for Strategic Dialogue,using <unk> based achieved problem <unk>
191,"Sensors on mobile devices---accelerometers, gyroscopes, pressure meters, and
GPS---invite new applications in gesture recognition, gaming, and fitness
tracking. However, programming them remains challenging because human gestures
captured by sensors are noisy. This paper illustrates that noisy gestures
degrade training and classification accuracy for gesture recognition in
state-of-the-art deterministic Hidden Markov Models (HMM). We introduce a new
statistical quantization approach that mitigates these problems by (1) during
training, producing gesture-specific codebooks, HMMs, and error models for
gesture sequences; and (2) during classification, exploiting the error model to
explore multiple feasible HMM state sequences. We implement classification in
Uncertain<t>, a probabilistic programming system that encapsulates HMMs and
error models and then automates sampling and inference in the runtime.
Uncertain<T> developers directly express a choice of application-specific
trade-off between recall and precision at gesture recognition time, rather than
at training time. We demonstrate benefits in configurability, precision,
recall, and recognition on two data sets with 25 gestures from 28 people and
4200 total gestures. Incorporating gesture error more accurately in modeling
improves the average recognition rate of 20 gestures from 34\% in prior work to
62\%. Incorporating the error model during classification further improves the
average gesture recognition rate to 71\%. As far as we are aware, no prior work
shows how to generate an HMM error model during training and use it to improve
classification rates.",High Five: Improving Gesture Recognition by Embracing Uncertainty,using  bentley morphology introduction estimation
192,"Modeling emotional-cognition is in a nascent stage and therefore wide-open
for new ideas and discussions. In this paper the author looks at the modeling
problem by bringing in ideas from axiomatic mathematics, information theory,
computer science, molecular biology, non-linear dynamical systems and quantum
computing and explains how ideas from these disciplines may have applications
in modeling emotional-cognition.",A novice looks at emotional cognition,using preprint model compressed locations model
193,"Unsupervised domain adaptation of speech signal aims at adapting a
well-trained source-domain acoustic model to the unlabeled data from target
domain. This can be achieved by adversarial training of deep neural network
(DNN) acoustic models to learn an intermediate deep representation that is both
senone-discriminative and domain-invariant. Specifically, the DNN is trained to
jointly optimize the primary task of senone classification and the secondary
task of domain classification with adversarial objective functions. In this
work, instead of only focusing on learning a domain-invariant feature (i.e. the
shared component between domains), we also characterize the difference between
the source and target domain distributions by explicitly modeling the private
component of each domain through a private component extractor DNN. The private
component is trained to be orthogonal with the shared component and thus
implicitly increases the degree of domain-invariance of the shared component. A
reconstructor DNN is used to reconstruct the original speech feature from the
private and shared components as a regularization. This domain separation
framework is applied to the unsupervised environment adaptation task and
achieved 11.08% relative WER reduction from the gradient reversal layer
training, a representative adversarial training method, for automatic speech
recognition on CHiME-3 dataset.","Unsupervised Adaptation with Domain Separation Networks for Robust
  Speech Recognition",models model domain
194,"Statistical techniques that analyze texts, referred to as text analytics,
have departed from the use of simple word count statistics towards a new
paradigm. Text mining now hinges on a more sophisticated set of methods,
including the representations in terms of complex networks. While
well-established word-adjacency (co-occurrence) methods successfully grasp
syntactical features of written texts, they are unable to represent important
aspects of textual data, such as its topical structure, i.e. the sequence of
subjects developing at a mesoscopic level along the text. Such aspects are
often overlooked by current methodologies. In order to grasp the mesoscopic
characteristics of semantical content in written texts, we devised a network
model which is able to analyze documents in a multi-scale fashion. In the
proposed model, a limited amount of adjacent paragraphs are represented as
nodes, which are connected whenever they share a minimum semantical content. To
illustrate the capabilities of our model, we present, as a case example, a
qualitative analysis of ""Alice's Adventures in Wonderland"". We show that the
mesoscopic structure of a document, modeled as a network, reveals many semantic
traits of texts. Such an approach paves the way to a myriad of semantic-based
applications. In addition, our approach is illustrated in a machine learning
context, in which texts are classified among real texts and randomized
instances.",Representation of texts as complex networks: a mesoscopic approach,technique data  proposed therefore
195,"In this paper, we consider a popular model for collaborative filtering in
recommender systems where some users of a website rate some items, such as
movies, and the goal is to recover the ratings of some or all of the unrated
items of each user. In particular, we consider both the clustering model, where
only users (or items) are clustered, and the co-clustering model, where both
users and items are clustered, and further, we assume that some users rate many
items (information-rich users) and some users rate only a few items
(information-sparse users). When users (or items) are clustered, our algorithm
can recover the rating matrix with $\omega(MK \log M)$ noisy entries while $MK$
entries are necessary, where $K$ is the number of clusters and $M$ is the
number of items. In the case of co-clustering, we prove that $K^2$ entries are
necessary for recovering the rating matrix, and our algorithm achieves this
lower bound within a logarithmic factor when $K$ is sufficiently large. We
compare our algorithms with a well-known algorithms called alternating
minimization (AM), and a similarity score-based algorithm known as the
popularity-among-friends (PAF) algorithm by applying all three to the MovieLens
and Netflix data sets. Our co-clustering algorithm and AM have similar overall
error rates when recovering the rating matrix, both of which are lower than the
error rate under PAF. But more importantly, the error rate of our co-clustering
algorithm is significantly lower than AM and PAF in the scenarios of interest
in recommender systems: when recommending a few items to each user or when
recommending items to users who only rated a few items (these users are the
majority of the total user population). The performance difference increases
even more when noise is added to the datasets.","Collaborative Filtering with Information-Rich and Information-Sparse
  Entities",based problem  new planning new data negative size
196,"Mainly for the sake of solving the lack of keyword-specific data, we propose
one Keyword Spotting (KWS) system using Deep Neural Network (DNN) and
Connectionist Temporal Classifier (CTC) on power-constrained small-footprint
mobile devices, taking full advantage of general corpus from continuous speech
recognition which is of great amount. DNN is to directly predict the posterior
of phoneme units of any personally customized key-phrase, and CTC to produce a
confidence score of the given phoneme sequence as responsive decision-making
mechanism. The CTC-KWS has competitive performance in comparison with purely
DNN based keyword specific KWS, but not increasing any computational
complexity.","Small-footprint Keyword Spotting Using Deep Neural Network and
  Connectionist Temporal Classifier",<unk> show <unk>
197,"This paper provides a general result on controlling local Rademacher
complexities, which captures in an elegant form to relate the complexities with
constraint on the expected norm to the corresponding ones with constraint on
the empirical norm. This result is convenient to apply in real applications and
could yield refined local Rademacher complexity bounds for function classes
satisfying general entropy conditions. We demonstrate the power of our
complexity bounds by applying them to derive effective generalization error
bounds.",Local Rademacher Complexity Bounds based on Covering Numbers,data constraint two show two model practical networks
198,"Biometrics authentication is an effective method for automatically
recognizing individuals. The authentication consists of an enrollment phase and
an identification or verification phase. In the stages of enrollment known
(training) samples after the pre-processing stage are used for suitable feature
extraction to generate the template database. In the verification stage, the
test sample is similarly pre processed and subjected to feature extraction
modules, and then it is matched with the training feature templates to decide
whether it is a genuine or not. This paper presents use of a region of interest
(ROI) for palm print technology. First some of the existing methods for palm
print identification have been introduced. Then focus has been given on
extraction of a suitable smaller region from the acquired palm print to improve
the identification method accuracy. Several existing work in the topic of
region extraction have been examined. Subsequently, a simple and original
method has then proposed for locating the ROI that can be effectively used for
palm print analysis. The ROI extracted using this new technique is suitable for
different types of processing as it creates a rectangular or square area around
the center of activity represented by the lines, wrinkles and ridges of the
palm print. The effectiveness of the ROI approach has been tested by
integrating it with a texture based identification / authentication system
proposed earlier. The improvement has been shown by comparing the
identification accuracy rate before and after the ROI pre-processing.",Extracting Region of Interest for Palm Print Authentication,<unk> video various an
199,"In subspace clustering, a group of data points belonging to a union of
subspaces are assigned membership to their respective subspaces. This paper
presents a new approach dubbed Innovation Pursuit (iPursuit) to the problem of
subspace clustering using a new geometrical idea whereby subspaces are
identified based on their relative novelties. We present two frameworks in
which the idea of innovation pursuit is used to distinguish the subspaces.
Underlying the first framework is an iterative method that finds the subspaces
consecutively by solving a series of simple linear optimization problems, each
searching for a direction of innovation in the span of the data potentially
orthogonal to all subspaces except for the one to be identified in one step of
the algorithm. A detailed mathematical analysis is provided establishing
sufficient conditions for iPursuit to correctly cluster the data. The proposed
approach can provably yield exact clustering even when the subspaces have
significant intersections. It is shown that the complexity of the iterative
approach scales only linearly in the number of data points and subspaces, and
quadratically in the dimension of the subspaces. The second framework
integrates iPursuit with spectral clustering to yield a new variant of
spectral-clustering-based algorithms. The numerical simulations with both real
and synthetic data demonstrate that iPursuit can often outperform the
state-of-the-art subspace clustering algorithms, more so for subspaces with
significant intersections, and that it significantly improves the
state-of-the-art result for subspace-segmentation-based face clustering.",Innovation Pursuit: A New Approach to Subspace Clustering,superiority show video step
200,"An important problem in sequential decision-making under uncertainty is to
use limited data to compute a safe policy, i.e., a policy that is guaranteed to
perform at least as well as a given baseline strategy. In this paper, we
develop and analyze a new model-based approach to compute a safe policy when we
have access to an inaccurate dynamics model of the system with known accuracy
guarantees. Our proposed robust method uses this (inaccurate) model to directly
minimize the (negative) regret w.r.t. the baseline policy. Contrary to the
existing approaches, minimizing the regret allows one to improve the baseline
policy in states with accurate dynamics and seamlessly fall back to the
baseline policy, otherwise. We show that our formulation is NP-hard and propose
an approximate algorithm. Our empirical results on several domains show that
even this relatively simple approximate algorithm can significantly outperform
standard approaches.",Safe Policy Improvement by Minimizing Robust Baseline Regret,models model prohibit brain
201,"The fundamental role of hypernymy in NLP has motivated the development of
many methods for the automatic identification of this relation, most of which
rely on word distribution. We investigate an extensive number of such
unsupervised measures, using several distributional semantic models that differ
by context type and feature weighting. We analyze the performance of the
different methods based on their linguistic motivation. Comparison to the
state-of-the-art supervised methods shows that while supervised methods
generally outperform the unsupervised ones, the former are sensitive to the
distribution of training instances, hurting their reliability. Being based on
general linguistic hypotheses and independent from training data, unsupervised
measures are more robust, and therefore are still useful artillery for
hypernymy detection.","Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy
  Detection",networks <unk> show show
202,"Students in online courses generate large amounts of data that can be used to
personalize the learning process and improve quality of education. In this
paper, we present the Latent Skill Embedding (LSE), a probabilistic model of
students and educational content that can be used to recommend personalized
sequences of lessons with the goal of helping students prepare for specific
assessments. Akin to collaborative filtering for recommender systems, the
algorithm does not require students or content to be described by features, but
it learns a representation using access traces. We formulate this problem as a
regularized maximum-likelihood embedding of students, lessons, and assessments
from historical student-content interactions. An empirical evaluation on
large-scale data from Knewton, an adaptive learning technology company, shows
that this approach predicts assessment results competitively with benchmark
models and is able to discriminate between lesson sequences that lead to
mastery and failure.",Latent Skill Embedding for Personalized Lesson Sequence Recommendation,using <unk> model <unk> cross correlation data <unk> number
203,"We propose a finite-state transducer (FST) representation for the models used
to decode keyboard inputs on mobile devices. Drawing from learnings from the
field of speech recognition, we describe a decoding framework that can satisfy
the strict memory and latency constraints of keyboard input. We extend this
framework to support functionalities typically not present in speech
recognition, such as literal decoding, autocorrections, word completions, and
next word predictions.
  We describe the general framework of what we call for short the keyboard ""FST
decoder"" as well as the implementation details that are new compared to a
speech FST decoder. We demonstrate that the FST decoder enables new UX features
such as post-corrections. Finally, we sketch how this decoder can support
advanced features such as personalization and contextualization.",Mobile Keyboard Input Decoding with Finite-State Transducers,significant significant significant achieve achieved proposed known based
204,"For more than forty years now, modern theories of literature (Compagnon,
1979) insist on the role of paraphrases, rewritings, citations, reciprocal
borrowings and mutual contributions of any kinds. The notions of
intertextuality, transtextuality, hypertextuality/hypotextuality, were
introduced in the seventies and eighties to approach these phenomena. The
careful analysis of these references is of particular interest in evaluating
the distance that the creator voluntarily introduces with his/her masters.
Phoebus is collaborative project that makes computer scientists from the
University Pierre and Marie Curie (LIP6-UPMC) collaborate with the literary
teams of Paris-Sorbonne University with the aim to develop efficient tools for
literary studies that take advantage of modern computer science techniques. In
this context, we have developed a piece of software that automatically detects
and explores networks of textual reuses in classical literature. This paper
describes the principles on which is based this program, the significant
results that have already been obtained and the perspectives for the near
future.",Automatic Detection of Reuses and Citations in Literary Texts,programming stochastic
205,"In many recent applications, data is plentiful. By now, we have a rather
clear understanding of how more data can be used to improve the accuracy of
learning algorithms. Recently, there has been a growing interest in
understanding how more data can be leveraged to reduce the required training
runtime. In this paper, we study the runtime of learning as a function of the
number of available training examples, and underscore the main high-level
techniques. We provide some initial positive results showing that the runtime
can decrease exponentially while only requiring a polynomial growth of the
number of examples, and spell-out several interesting open problems.",Using More Data to Speed-up Training Time,models model next  research state model <unk>
206,"We consider a stochastic continuum armed bandit problem where the arms are
indexed by the $\ell_2$ ball $B_{d}(1+\nu)$ of radius $1+\nu$ in
$\mathbb{R}^d$. The reward functions $r :B_{d}(1+\nu) \rightarrow \mathbb{R}$
are considered to intrinsically depend on $k \ll d$ unknown linear parameters
so that $r(\mathbf{x}) = g(\mathbf{A} \mathbf{x})$ where $\mathbf{A}$ is a full
rank $k \times d$ matrix. Assuming the mean reward function to be smooth we
make use of results from low-rank matrix recovery literature and derive an
efficient randomized algorithm which achieves a regret bound of $O(C(k,d)
n^{\frac{1+k}{2+k}} (\log n)^{\frac{1}{2+k}})$ with high probability. Here
$C(k,d)$ is at most polynomial in $d$ and $k$ and $n$ is the number of rounds
or the sampling budget which is assumed to be known beforehand.","Stochastic continuum armed bandit problem of few linear parameters in
  high dimensions",suitable model data allows set linear
207,"Support Vector Machines, SVMs, and the Large Margin Nearest Neighbor
algorithm, LMNN, are two very popular learning algorithms with quite different
learning biases. In this paper we bring them into a unified view and show that
they have a much stronger relation than what is commonly thought. We analyze
SVMs from a metric learning perspective and cast them as a metric learning
problem, a view which helps us uncover the relations of the two algorithms. We
show that LMNN can be seen as learning a set of local SVM-like models in a
quadratic space. Along the way and inspired by the metric-based interpretation
of SVM s we derive a novel variant of SVMs, epsilon-SVM, to which LMNN is even
more similar. We give a unified view of LMNN and the different SVM variants.
Finally we provide some preliminary experiments on a number of benchmark
datasets in which show that epsilon-SVM compares favorably both with respect to
LMNN and SVM.",A metric learning perspective of SVM: on the relation of SVM and LMNN,models method strategy layer method <unk>
208,"Although a great methodological effort has been invested in proposing
competitive solutions to the class-imbalance problem, little effort has been
made in pursuing a theoretical understanding of this matter.
  In order to shed some light on this topic, we perform, through a novel
framework, an exhaustive analysis of the adequateness of the most commonly used
performance scores to assess this complex scenario. We conclude that using
unweighted H\""older means with exponent $p \leq 1$ to average the recalls of
all the classes produces adequate scores which are capable of determining
whether a classifier is competitive.
  Then, we review the major solutions presented in the class-imbalance
literature. Since any learning task can be defined as an optimisation problem
where a loss function, usually connected to a particular score, is minimised,
our goal, here, is to find whether the learning tasks found in the literature
are also oriented to maximise the previously detected adequate scores. We
conclude that they usually maximise the unweighted H\""older mean with $p = 1$
(a-mean).
  Finally, we provide bounds on the values of the studied performance scores
which guarantee a classifier with a higher recall than the random classifier in
each and every class.","Towards Competitive Classifiers for Unbalanced Classification Problems:
  A Study on the Performance Scores",along model <unk> image data <unk> knowledge
209,"Data quantization learns encoding results of data with certain requirements,
and provides a broad perspective of many real-world applications to data
handling. Nevertheless, the results of encoder is usually limited to
multivariate inputs with the random mapping, and side information of binary
codes are hardly to mostly depict the original data patterns as possible. In
the literature, cosine based random quantization has attracted much attentions
due to its intrinsic bounded results. Nevertheless, it usually suffers from the
uncertain outputs, and information of original data fails to be fully preserved
in the reduced codes. In this work, a novel binary embedding method, termed
adaptive training quantization (ATQ), is proposed to learn the ideal transform
of random encoder, where the limitation of cosine random mapping is tackled. As
an adaptive learning idea, the reduced mapping is adaptively calculated with
idea of data group, while the bias of random transform is to be improved to
hold most matching information. Experimental results show that the proposed
method is able to obtain outstanding performance compared with other random
quantization methods.",Adaptive Training of Random Mapping for Data Quantization,neural <unk> method <unk> online
210,"The progress in imaging techniques have allowed the study of various aspect
of cellular mechanisms. To isolate individual cells in live imaging data, we
introduce an elegant image segmentation framework that effectively extracts
cell boundaries, even in the presence of poor edge details. Our approach works
in two stages. First, we estimate pixel interior/border/exterior class
probabilities using random ferns. Then, we use an energy minimization framework
to compute boundaries whose localization is compliant with the pixel class
probabilities. We validate our approach on a manually annotated dataset.",Cell segmentation with random ferns and graph-cuts,data using based labeled using proposed data based
211,"Heuristic optimisers which search for an optimal configuration of variables
relative to an objective function often get stuck in local optima where the
algorithm is unable to find further improvement. The standard approach to
circumvent this problem involves periodically restarting the algorithm from
random initial configurations when no further improvement can be found. We
propose a method of partial reinitialization, whereby, in an attempt to find a
better solution, only sub-sets of variables are re-initialised rather than the
whole configuration. Much of the information gained from previous runs is hence
retained. This leads to significant improvements in the quality of the solution
found in a given time for a variety of optimisation problems in machine
learning.",Partial Reinitialisation for Optimisers,learning find model word for
212,"In this paper, an efficient implementation for a recognition system based on
the original HMAX model of the visual cortex is proposed. Various optimizations
targeted to increase accuracy at the so-called layers S1, C1, and S2 of the
HMAX model are proposed. At layer S1, all unimportant information such as
illumination and expression variations are eliminated from the images. Each
image is then convolved with 64 separable Gabor filters in the spatial domain.
At layer C1, the minimum scales values are exploited to be embedded into the
maximum ones using the additive embedding space. At layer S2, the prototypes
are generated in a more efficient way using Partitioning Around Medoid (PAM)
clustering algorithm. The impact of these optimizations in terms of accuracy
and computational complexity was evaluated on the Caltech101 database, and
compared with the baseline performance using support vector machine (SVM) and
nearest neighbor (NN) classifiers. The results show that our model provides
significant improvement in accuracy at the S1 layer by more than 10% where the
computational complexity is also reduced. The accuracy is slightly increased
for both approximations at the C1 and S2 layers.","Efficient Implementation of a Recognition System Using the Cortex
  Ventral Stream Model",paper  for based greedy online model <unk>
213,"Consider the binary classification problem of predicting a target variable
$Y$ from a discrete feature vector $X = (X_1,...,X_d)$. When the probability
distribution $\mathbb{P}(X,Y)$ is known, the optimal classifier, leading to the
minimum misclassification rate, is given by the Maximum A-posteriori
Probability decision rule. However, estimating the complete joint distribution
$\mathbb{P}(X,Y)$ is computationally and statistically impossible for large
values of $d$. An alternative approach is to first estimate some low order
marginals of $\mathbb{P}(X,Y)$ and then design the classifier based on the
estimated low order marginals. This approach is also helpful when the complete
training data instances are not available due to privacy concerns. In this
work, we consider the problem of finding the optimum classifier based on some
estimated low order marginals of $(X,Y)$. We prove that for a given set of
marginals, the minimum Hirschfeld-Gebelein-Renyi (HGR) correlation principle
introduced in [1] leads to a randomized classification rule which is shown to
have a misclassification rate no larger than twice the misclassification rate
of the optimal classifier. Then, under a separability condition, we show that
the proposed algorithm is equivalent to a randomized linear regression
approach. In addition, this method naturally results in a robust feature
selection method selecting a subset of features having the maximum worst case
HGR correlation with the target variable. Our theoretical upper-bound is
similar to the recent Discrete Chebyshev Classifier (DCC) approach [2], while
the proposed algorithm has significant computational advantages since it only
requires solving a least square optimization problem. Finally, we numerically
compare our proposed algorithm with the DCC classifier and show that the
proposed algorithm results in better misclassification rate over various
datasets.",Discrete Rényi Classifiers,rapid rapid size data robust images  role
214,"Structure from motion algorithms have an inherent limitation that the
reconstruction can only be determined up to the unknown scale factor. Modern
mobile devices are equipped with an inertial measurement unit (IMU), which can
be used for estimating the scale of the reconstruction. We propose a method
that recovers the metric scale given inertial measurements and camera poses. In
the process, we also perform a temporal and spatial alignment of the camera and
the IMU. Therefore, our solution can be easily combined with any existing
visual reconstruction software. The method can cope with noisy camera pose
estimates, typically caused by motion blur or rolling shutter artifacts, via
utilizing a Rauch-Tung-Striebel (RTS) smoother. Furthermore, the scale
estimation is performed in the frequency domain, which provides more robustness
to inaccurate sensor time stamps and noisy IMU samples than the previously used
time domain representation. In contrast to previous methods, our approach has
no parameters that need to be tuned for achieving a good performance. In the
experiments, we show that the algorithm outperforms the state-of-the-art in
both accuracy and convergence speed of the scale estimate. The accuracy of the
scale is around $1\%$ from the ground truth depending on the recording. We also
demonstrate that our method can improve the scale accuracy of the Project
Tango's build-in motion tracking.","Inertial-Based Scale Estimation for Structure from Motion on Mobile
  Devices",learning focus effective model languages loss
215,"The detection of small road hazards, such as lost cargo, is a vital
capability for self-driving cars. We tackle this challenging and rarely
addressed problem with a vision system that leverages appearance, contextual as
well as geometric cues. To utilize the appearance and contextual cues, we
propose a new deep learning-based obstacle detection framework. Here a variant
of a fully convolutional network is used to predict a pixel-wise semantic
labeling of (i) free-space, (ii) on-road unexpected obstacles, and (iii)
background. The geometric cues are exploited using a state-of-the-art detection
approach that predicts obstacles from stereo input images via model-based
statistical hypothesis tests. We present a principled Bayesian framework to
fuse the semantic and stereo-based detection results. The mid-level Stixel
representation is used to describe obstacles in a flexible, compact and robust
manner. We evaluate our new obstacle detection system on the Lost and Found
dataset, which includes very challenging scenes with obstacles of only 5 cm
height. Overall, we report a major improvement over the state-of-the-art, with
relative performance gains of up to 50%. In particular, we achieve a detection
rate of over 90% for distances of up to 50 m. Our system operates at 22 Hz on
our self-driving platform.","Detecting Unexpected Obstacles for Self-Driving Cars: Fusing Deep
  Learning and Geometric Modeling",networks image <unk>
216,"In this paper, we propose an effective scene text recognition method using
sparse coding based features, called Histograms of Sparse Codes (HSC) features.
For character detection, we use the HSC features instead of using the
Histograms of Oriented Gradients (HOG) features. The HSC features are extracted
by computing sparse codes with dictionaries that are learned from data using
K-SVD, and aggregating per-pixel sparse codes to form local histograms. For
word recognition, we integrate multiple cues including character detection
scores and geometric contexts in an objective function. The final recognition
results are obtained by searching for the words which correspond to the maximum
value of the objective function. The parameters in the objective function are
learned using the Minimum Classification Error (MCE) training method.
Experiments on several challenging datasets demonstrate that the proposed
HSC-based scene text recognition method outperforms HOG-based methods
significantly and outperforms most state-of-the-art methods.",Robust Scene Text Recognition Using Sparse Coding based Features,fractal <unk> use
217,"In this paper, researchers estimated the stock price of activated companies
in Tehran (Iran) stock exchange. It is used Linear Regression and Artificial
Neural Network methods and compared these two methods. In Artificial Neural
Network, of General Regression Neural Network method (GRNN) for architecture is
used. In this paper, first, researchers considered 10 macro economic variables
and 30 financial variables and then they obtained seven final variables
including 3 macro economic variables and 4 financial variables to estimate the
stock price using Independent components Analysis (ICA). So, we presented an
equation for two methods and compared their results which shown that artificial
neural network method is more efficient than linear regression method.","The Comparison of Methods Artificial Neural Network with Linear
  Regression Using Specific Variables for Prediction Stock Price in Tehran
  Stock Exchange",similarity using cross correlation known based numerals theory
218,"We draw a formal connection between using synthetic training data to optimize
neural network parameters and approximate, Bayesian, model-based reasoning. In
particular, training a neural network using synthetic data can be viewed as
learning a proposal distribution generator for approximate inference in the
synthetic-data generative model. We demonstrate this connection in a
recognition task where we develop a novel Captcha-breaking architecture and
train it using synthetic data, demonstrating both state-of-the-art performance
and a way of computing task-specific posterior uncertainty. Using a neural
network trained this way, we also demonstrate successful breaking of real-world
Captchas currently used by Facebook and Wikipedia. Reasoning from these
empirical results and drawing connections with Bayesian modeling, we discuss
the robustness of synthetic data results and suggest important considerations
for ensuring good neural network generalization when training with synthetic
data.",Using Synthetic Data to Train Neural Networks is Model-Based Reasoning,using offers shown
219,"Building a voice conversion (VC) system from non-parallel speech corpora is
challenging but highly valuable in real application scenarios. In most
situations, the source and the target speakers do not repeat the same texts or
they may even speak different languages. In this case, one possible, although
indirect, solution is to build a generative model for speech. Generative models
focus on explaining the observations with latent variables instead of learning
a pairwise transformation function, thereby bypassing the requirement of speech
frame alignment. In this paper, we propose a non-parallel VC framework with a
variational autoencoding Wasserstein generative adversarial network (VAW-GAN)
that explicitly considers a VC objective when building the speech model.
Experimental results corroborate the capability of our framework for building a
VC system from unaligned data, and demonstrate improved conversion quality.","Voice Conversion from Unaligned Corpora using Variational Autoencoding
  Wasserstein Generative Adversarial Networks",learning model model prove method word paper  for
220,"We study a specific \textit{combinatorial pure exploration stochastic bandit
problem} where the learner aims at finding the set of arms whose means are
above a given threshold, up to a given precision, and \textit{for a fixed time
horizon}. We propose a parameter-free algorithm based on an original heuristic,
and prove that it is optimal for this problem by deriving matching upper and
lower bounds. To the best of our knowledge, this is the first non-trivial pure
exploration setting with \textit{fixed budget} for which optimal strategies are
constructed.",An optimal algorithm for the Thresholding Bandit Problem,benchmark  noisy models
221,"RESTful services on the Web expose information through retrievable resource
representations that represent self-describing descriptions of resources, and
through the way how these resources are interlinked through the hyperlinks that
can be found in those representations. This basic design of RESTful services
means that for extracting the most useful information from a service, it is
necessary to understand a service's representations, which means both the
semantics in terms of describing a resource, and also its semantics in terms of
describing its linkage with other resources. Based on the Resource Linking
Language (ReLL), this paper describes a framework for how RESTful services can
be described, and how these descriptions can then be used to harvest
information from these services. Building on this framework, a layered model of
RESTful service semantics allows to represent a service's information in
RDF/OWL. Because REST is based on the linkage between resources, the same model
can be used for aggregating and interlinking multiple services for extracting
RDF data from sets of RESTful services.",From RESTful Services to RDF: Connecting the Web and the Semantic Web,models model common proposed poses first
222,"Plan Recognition algorithms require to recognize a complete hierarchy
explaining the agent's actions and goals. While the output of such algorithms
is informative to the recognizer, the cost of its calculation is high in
run-time, space, and completeness. Moreover, performing plan recognition online
requires the observing agent to reason about future actions that have not yet
been seen and maintain a set of hypotheses to support all possible options.
This paper presents a new and efficient algorithm for online plan recognition
called SLIM (Semi-Lazy Inference Mechanism). It combines both a bottom-up and
top-down parsing processes, which allow it to commit only to the minimum
necessary actions in real-time, but still provide complete hypotheses post
factum. We show both theoretically and empirically that although the
computational cost of this process is still exponential, there is a significant
improvement in run-time when compared to a state of the art of plan recognition
algorithm.",SLIM: Semi-Lazy Inference Mechanism for Plan Recognition,model data types polynomial
223,"We present a novel framework for learning to interpret and generate language
using only perceptual context as supervision. We demonstrate its capabilities
by developing a system that learns to sportscast simulated robot soccer games
in both English and Korean without any language-specific prior knowledge.
Training employs only ambiguous supervision consisting of a stream of
descriptive textual comments and a sequence of events extracted from the
simulation trace. The system simultaneously establishes correspondences between
individual comments and the events that they describe while building a
translation model that supports both parsing and generation. We also present a
novel algorithm for learning which events are worth describing. Human
evaluations of the generated commentaries indicate they are of reasonable
quality and in some cases even on par with those produced by humans for our
limited domain.","Training a Multilingual Sportscaster: Using Perceptual Context to Learn
  Language",simple show feature similarity estimated
224,"There have been two major lines of research aimed at capturing
resource-bounded players in game theory. The first, initiated by Rubinstein,
charges an agent for doing costly computation; the second, initiated by Neyman,
does not charge for computation, but limits the computation that agents can do,
typically by modeling agents as finite automata. We review recent work on
applying both approaches in the context of decision theory. For the first
approach, we take the objects of choice in a decision problem to be Turing
machines, and charge players for the ``complexity'' of the Turing machine
chosen (e.g., its running time). This approach can be used to explain
well-known phenomena like first-impression-matters biases (i.e., people tend to
put more weight on evidence they hear early on) and belief polarization (two
people with different prior beliefs, hearing the same evidence, can end up with
diametrically opposed conclusions) as the outcomes of quite rational decisions.
For the second approach, we model people as finite automata, and provide a
simple algorithm that, on a problem that captures a number of settings of
interest, provably performs optimally as the number of states in the automaton
increases.",Decision Theory with Resource-Bounded Agents,system <unk> show <unk>
225,"Causal inference deals with identifying which random variables ""cause"" or
control other random variables. Recent advances on the topic of causal
inference based on tools from statistical estimation and machine learning have
resulted in practical algorithms for causal inference. Causal inference has the
potential to have significant impact on medical research, prevention and
control of diseases, and identifying factors that impact economic changes to
name just a few. However, these promising applications for causal inference are
often ones that involve sensitive or personal data of users that need to be
kept private (e.g., medical records, personal finances, etc). Therefore, there
is a need for the development of causal inference methods that preserve data
privacy. We study the problem of inferring causality using the current, popular
causal inference framework, the additive noise model (ANM) while simultaneously
ensuring privacy of the users. Our framework provides differential privacy
guarantees for a variety of ANM variants. We run extensive experiments, and
demonstrate that our techniques are practical and easy to implement.",Private Causal Inference,impact impact for show applied prove
226,"Recently there has been an increasing interest in methods that deal with
multiple outputs. This has been motivated partly by frameworks like multitask
learning, multisensor networks or structured output data. From a Gaussian
processes perspective, the problem reduces to specifying an appropriate
covariance function that, whilst being positive semi-definite, captures the
dependencies between all the data points and across all the outputs. One
approach to account for non-trivial correlations between outputs employs
convolution processes. Under a latent function interpretation of the
convolution transform we establish dependencies between output variables. The
main drawbacks of this approach are the associated computational and storage
demands. In this paper we address these issues. We present different sparse
approximations for dependent output Gaussian processes constructed through the
convolution formalism. We exploit the conditional independencies present
naturally in the model. This leads to a form of the covariance similar in
spirit to the so called PITC and FITC approximations for a single output. We
show experimental results with synthetic and real data, in particular, we show
results in pollution prediction, school exams score prediction and gene
expression data.",Sparse Convolved Multiple Output Gaussian Processes,programming using handwriting simple method feature models
227,"The management and combination of uncertain, imprecise, fuzzy and even
paradoxical or high conflicting sources of information has always been, and
still remains today, of primal importance for the development of reliable
modern information systems involving artificial reasoning. In this chapter, we
present a survey of our recent theory of plausible and paradoxical reasoning,
known as Dezert-Smarandache Theory (DSmT) in the literature, developed for
dealing with imprecise, uncertain and paradoxical sources of information. We
focus our presentation here rather on the foundations of DSmT, and on the two
important new rules of combination, than on browsing specific applications of
DSmT available in literature. Several simple examples are given throughout the
presentation to show the efficiency and the generality of this new approach.
The last part of this chapter concerns the presentation of the neutrosophic
logic, the neutro-fuzzy inference and its connection with DSmT. Fuzzy logic and
neutrosophic logic are useful tools in decision making after fusioning the
information using the DSm hybrid rule of combination of masses.","The Combination of Paradoxical, Uncertain, and Imprecise Sources of
  Information based on DSmT and Neutro-Fuzzy Inference",learning mqc show rewards
228,"A simple method for finding the entropy and redundancy of a reasonable long
sample of English text by direct computer processing and from first principles
according to Shannon theory is presented. As an example, results on the entropy
of the English language have been obtained based on a total of 20.3 million
characters of written English, considering symbols from one to five hundred
characters in length. Besides a more realistic value of the entropy of English,
a new perspective on some classic entropy-related concepts is presented. This
method can also be extended to other Latin languages. Some implications for
practical applications such as plagiarism-detection software, and the minimum
number of words that should be used in social Internet network messaging, are
discussed.",A New Look at the Classical Entropy of Written English,structure this
229,"Epistemic logic with non-standard knowledge operators, especially the
""knowing-value"" operator, has recently gathered much attention. With the
""knowing-value"" operator, we can express knowledge of individual variables, but
not of the relations between them in general. In this paper, we propose a new
operator Kf to express knowledge of the functional dependencies between
variables. The semantics of this Kf operator uses a function domain which
imposes a constraint on what counts as a functional dependency relation. By
adjusting this function domain, different interesting logics arise, and in this
paper we axiomatize three such logics in a single agent setting. Then we show
how these three logics can be unified by allowing the function domain to vary
relative to different agents and possible worlds. A multiagent axiomatization
is given in this case.",Epistemic Logic with Functional Dependency Operator,using key principled model sense parametrize two
230,"The estimation of class prevalence, i.e., the fraction of a population that
belongs to a certain class, is a very useful tool in data analytics and
learning, and finds applications in many domains such as sentiment analysis,
epidemiology, etc. For example, in sentiment analysis, the objective is often
not to estimate whether a specific text conveys a positive or a negative
sentiment, but rather estimate the overall distribution of positive and
negative sentiments during an event window. A popular way of performing the
above task, often dubbed quantification, is to use supervised learning to train
a prevalence estimator from labeled data.
  Contemporary literature cites several performance measures used to measure
the success of such prevalence estimators. In this paper we propose the first
online stochastic algorithms for directly optimizing these
quantification-specific performance measures. We also provide algorithms that
optimize hybrid performance measures that seek to balance quantification and
classification performance. Our algorithms present a significant advancement in
the theory of multivariate optimization and we show, by a rigorous theoretical
analysis, that they exhibit optimal convergence. We also report extensive
experiments on benchmark and real data sets which demonstrate that our methods
significantly outperform existing optimization techniques used for these
performance measures.",Online Optimization Methods for the Quantification Problem,namely variables  based data <unk> two proposed
231,"We consider additive models built with trend filtering, i.e., additive models
whose components are each regularized by the (discrete) total variation of
their $(k+1)$st (discrete) derivative, for a chosen integer $k \geq 0$. This
results in $k$th degree piecewise polynomial components, (e.g., $k=0$ gives
piecewise constant components, $k=1$ gives piecewise linear, $k=2$ gives
piecewise quadratic, etc.). In univariate nonparametric regression, the
localized nature of the total variation regularizer used by trend filtering has
been shown to produce estimates with superior local adaptivity to those from
smoothing splines (and linear smoothers, more generally) (Tibshirani [2014]).
Further, the structured nature of this regularizer has been shown to lead to
highly efficient computational routines for trend filtering (Kim et al. [2009],
Ramdas and Tibshirani [2016]). In this paper, we argue that both of these
properties carry over to the additive models setting. We derive fast error
rates for additive trend filtering estimates, and prove that these rates are
minimax optimal when the underlying function is itself additive and has
component functions whose derivatives are of bounded variation. We show that
such rates are unattainable by additive smoothing splines (and by additive
models built from linear smoothers, in general). We argue that backfitting
provides an efficient algorithm for additive trend filtering, as it is built
around the fast univariate trend filtering solvers; moreover, we describe a
modified backfitting procedure whose iterations can be run in parallel.
Finally, we conduct experiments to examine the empirical properties of additive
trend filtering, and outline some possible extensions.",Additive Models with Trend Filtering,models model mammals tree  show pertinent biometric linear
232,"Dictionary learning for sparse representations is traditionally approached
with sequential atom updates, in which an optimized atom is used immediately
for the optimization of the next atoms. We propose instead a Jacobi version, in
which groups of atoms are updated independently, in parallel. Extensive
numerical evidence for sparse image representation shows that the parallel
algorithms, especially when all atoms are updated simultaneously, give better
dictionaries than their sequential counterparts.",Overcomplete Dictionary Learning with Jacobi Atom Updates,art different categorization sparse
233,"In this article, we explain in detail the internal structures and databases
of a smart health application. Moreover, we describe how to generate a
statistically sound synthetic dataset using real-world medical data.","Technical Report: Implementation and Validation of a Smart Health
  Application",preferences preferences
234,"In machine learning and optimization, one often wants to minimize a convex
objective function $F$ but can only evaluate a noisy approximation $\hat{F}$ to
it. Even though $F$ is convex, the noise may render $\hat{F}$ nonconvex, making
the task of minimizing $F$ intractable in general. As a consequence, several
works in theoretical computer science, machine learning and optimization have
focused on coming up with polynomial time algorithms to minimize $F$ under
conditions on the noise $F(x)-\hat{F}(x)$ such as its uniform-boundedness, or
on $F$ such as strong convexity. However, in many applications of interest,
these conditions do not hold. Here we show that, if the noise has magnitude
$\alpha F(x) + \beta$ for some $\alpha, \beta > 0$, then there is a polynomial
time algorithm to find an approximate minimizer of $F$. In particular, our
result allows for unbounded noise and generalizes those of Applegate and
Kannan, and Zhang, Liang and Charikar, who proved similar results for the
bounded noise case, and that of Belloni et al. who assume that the noise grows
in a very specific manner and that $F$ is strongly convex. Turning our result
on its head, one may also view our algorithm as minimizing a nonconvex function
$\hat{F}$ that is promised to be related to a convex function $F$ as above. Our
algorithm is a ""simulated annealing"" modification of the stochastic gradient
Langevin Markov chain and gradually decreases the temperature of the chain to
approach the global minimizer. Analyzing such an algorithm for the unbounded
noise model and a general convex function turns out to be challenging and
requires several technical ideas that might be of independent interest in
deriving non-asymptotic bounds for other simulated annealing based algorithms.",Convex Optimization with Nonconvex Oracles,systematic k without size noise noise models
235,"Event cameras are a paradigm shift in camera technology. Instead of full
frames, the sensor captures a sparse set of events caused by intensity changes.
Since only the changes are transferred, those cameras are able to capture quick
movements of objects in the scene or of the camera itself. In this work we
propose a novel method to perform camera tracking of event cameras in a
panoramic setting with three degrees of freedom. We propose a direct camera
tracking formulation, similar to state-of-the-art in visual odometry. We show
that the minimal information needed for simultaneous tracking and mapping is
the spatial position of events, without using the appearance of the imaged
scene point. We verify the robustness to fast camera movements and dynamic
objects in the scene on a recently proposed dataset and self-recorded
sequences.",Real-Time Panoramic Tracking for Event Cameras,models proposed <unk> model achieved model require networks
236,"Multimodal biometric identification has been grown a great attention in the
most interests in the security fields. In the real world there exist modern
system devices that are able to detect, recognize, and classify the human
identities with reliable and fast recognition rates. Unfortunately most of
these systems rely on one modality, and the reliability for two or more
modalities are further decreased. The variations of face images with respect to
different poses are considered as one of the important challenges in face
recognition systems. In this paper, we propose a multimodal biometric system
that able to detect the human face images that are not only one view face
image, but also multi-view face images. Each subject entered to the system
adjusted their face at front of the three cameras, and then the features of the
face images are extracted based on Speeded Up Robust Features (SURF) algorithm.
We utilize Multi-Layer Perceptron (MLP) and combined classifiers based on both
Learning Vector Quantization (LVQ), and Radial Basis Function (RBF) for
classification purposes. The proposed system has been tested using SDUMLA-HMT,
and CASIA datasets. Furthermore, we collected a database of multi-view face
images by which we take the additive white Gaussian noise into considerations.
The results indicated the reliability, robustness of the proposed system with
different poses and variations including noise images.",A Vision System for Multi-View Face Recognition,along proposed yielded accompanying propose system number
237,"The cyclic block coordinate descent-type (CBCD-type) methods, which performs
iterative updates for a few coordinates (a block) simultaneously throughout the
procedure, have shown remarkable computational performance for solving strongly
convex minimization problems. Typical applications include many popular
statistical machine learning methods such as elastic-net regression, ridge
penalized logistic regression, and sparse additive regression. Existing
optimization literature has shown that for strongly convex minimization, the
CBCD-type methods attain iteration complexity of
$\mathcal{O}(p\log(1/\epsilon))$, where $\epsilon$ is a pre-specified accuracy
of the objective value, and $p$ is the number of blocks. However, such
iteration complexity explicitly depends on $p$, and therefore is at least $p$
times worse than the complexity $\mathcal{O}(\log(1/\epsilon))$ of gradient
descent (GD) methods. To bridge this theoretical gap, we propose an improved
convergence analysis for the CBCD-type methods. In particular, we first show
that for a family of quadratic minimization problems, the iteration complexity
$\mathcal{O}(\log^2(p)\cdot\log(1/\epsilon))$ of the CBCD-type methods matches
that of the GD methods in term of dependency on $p$, up to a $\log^2 p$ factor.
Thus our complexity bounds are sharper than the existing bounds by at least a
factor of $p/\log^2(p)$. We also provide a lower bound to confirm that our
improved complexity bounds are tight (up to a $\log^2 (p)$ factor), under the
assumption that the largest and smallest eigenvalues of the Hessian matrix do
not scale with $p$. Finally, we generalize our analysis to other strongly
convex minimization problems beyond quadratic ones.","On Faster Convergence of Cyclic Block Coordinate Descent-type Methods
  for Strongly Convex Minimization",complexity for
238,"Predicting the binding free energy, or affinity, of a small molecule for a
protein target is frequently the first step along the arc of drug discovery.
High throughput experimental and virtual screening both suffer from low
accuracy, whereas more accurate approaches in both domains suffer from lack of
scale due to either financial or temporal constraints. While machine learning
(ML) has made immense progress in the fields of computer vision and natural
language processing, it has yet to offer comparable improvements over
domain-expertise driven algorithms in the molecular sciences. In this paper, we
propose new Deep Neural Network (DNN) architectures for affinity prediction.
The new model architectures are at least competitive with, and in many cases
state-of-the-art compared to previous knowledge-based and physics-based
approaches. In addition to more standard evaluation metrics, we also propose
the Regression Enrichment Factor $EF_\chi^{(R)}$ for the community to benchmark
against in future affinity prediction studies. Finally, we suggest the
adaptation of an agglomerative clustering cross-validation strategy to more
accurately reflect the generalization capacity of ML-based affinity models in
future works.",Spatial Graph Convolutions for Drug Discovery,making number show captures problem known based set
239,"A minimal solution using two affine correspondences is presented to estimate
the common focal length and the fundamental matrix between two semi-calibrated
cameras - known intrinsic parameters except a common focal length. To the best
of our knowledge, this problem is unsolved. The proposed approach extends point
correspondence-based techniques with linear constraints derived from local
affine transformations. The obtained multivariate polynomial system is
efficiently solved by the hidden-variable technique. Observing the geometry of
local affinities, we introduce novel conditions eliminating invalid roots. To
select the best one out of the remaining candidates, a root selection technique
is proposed outperforming the recent ones especially in case of high-level
noise. The proposed 2-point algorithm is validated on both synthetic data and
104 publicly available real image pairs. A Matlab implementation of the
proposed solution is included in the paper.","A Minimal Solution for Two-view Focal-length Estimation using Two Affine
  Correspondences",model presents train similarity estimated
240,"Evolutionary algorithms (EAs) are population-based general-purpose
optimization algorithms, and have been successfully applied in various
real-world optimization tasks. However, previous theoretical studies often
employ EAs with only a parent or offspring population and focus on specific
problems. Furthermore, they often only show upper bounds on the running time,
while lower bounds are also necessary to get a complete understanding of an
algorithm. In this paper, we analyze the running time of the
($\mu$+$\lambda$)-EA (a general population-based EA with mutation only) on the
class of pseudo-Boolean functions with a unique global optimum. By applying the
recently proposed switch analysis approach, we prove the lower bound $\Omega(n
\ln n+ \mu + \lambda n\ln\ln n/ \ln n)$ for the first time. Particularly on the
two widely-studied problems, OneMax and LeadingOnes, the derived lower bound
discloses that the ($\mu$+$\lambda$)-EA will be strictly slower than the
(1+1)-EA when the population size $\mu$ or $\lambda$ is above a moderate order.
Our results imply that the increase of population size, while usually desired
in practice, bears the risk of increasing the lower bound of the running time
and thus should be carefully considered.","A Lower Bound Analysis of Population-based Evolutionary Algorithms for
  Pseudo-Boolean Functions",theory for based design directions  model
241,"Discrete optimization is a central problem in artificial intelligence. The
optimization of the aggregated cost of a network of cost functions arises in a
variety of problems including (W)CSP, DCOP, as well as optimization in
stochastic variants such as the tasks of finding the most probable explanation
(MPE) in belief networks. Inference-based algorithms are powerful techniques
for solving discrete optimization problems, which can be used independently or
in combination with other techniques. However, their applicability is often
limited by their compute intensive nature and their space requirements. This
paper proposes the design and implementation of a novel inference-based
technique, which exploits modern massively parallel architectures, such as
those found in Graphical Processing Units (GPUs), to speed up the resolution of
exact and approximated inference-based algorithms for discrete optimization.
The paper studies the proposed algorithm in both centralized and distributed
optimization contexts. The paper demonstrates that the use of GPUs provides
significant advantages in terms of runtime and scalability, achieving up to two
orders of magnitude in speedups and showing a considerable reduction in
execution time (up to 345 times faster) with respect to a sequential version.","Accelerating Exact and Approximate Inference for (Distributed) Discrete
  Optimization with GPUs",models model <unk> set
242,"More and more processes governing our lives use in some part an automatic
decision step, where -- based on a feature vector derived from an applicant --
an algorithm has the decision power over the final outcome. Here we present a
simple idea which gives some of the power back to the applicant by providing
her with alternatives which would make the decision algorithm decide
differently. It is based on a formalization reminiscent of methods used for
evasion attacks, and consists in enumerating the subspaces where the
classifiers decides the desired output. This has been implemented for the
specific case of decision forests (ensemble methods based on decision trees),
mapping the problem to an iterative version of enumerating $k$-cliques.",What Can I Do Now? Guiding Users in a World of Automated Decisions,simple model mapping use show score model model
243,"This paper presents a new distributed computational model of distributed
systems called the phase web that extends V. Pratt's orthocurrence relation
from 1986. The model uses mutual-exclusion to express sequence, and a new kind
of hierarchy to replace event sequences, posets, and pomsets. The model
explicitly connects computation to a discrete Clifford algebra that is in turn
extended into homology and co-homology, wherein the recursive nature of objects
and boundaries becomes apparent and itself subject to hierarchical recursion.
Topsy, a programming environment embodying the phase web, is available from
www.cs.auc.dk/topsy.",Distributed Computation as Hierarchy,show data pixel landmark show objects sentence show
244,"The conjugate gradient (CG) method is an efficient iterative method for
solving large-scale strongly convex quadratic programming (QP). In this paper
we propose some generalized CG (GCG) methods for solving the
$\ell_1$-regularized (possibly not strongly) convex QP that terminate at an
optimal solution in a finite number of iterations. At each iteration, our
methods first identify a face of an orthant and then either perform an exact
line search along the direction of the negative projected minimum-norm
subgradient of the objective function or execute a CG subroutine that conducts
a sequence of CG iterations until a CG iterate crosses the boundary of this
face or an approximate minimizer of over this face or a subface is found. We
determine which type of step should be taken by comparing the magnitude of some
components of the minimum-norm subgradient of the objective function to that of
its rest components. Our analysis on finite convergence of these methods makes
use of an error bound result and some key properties of the aforementioned
exact line search and the CG subroutine. We also show that the proposed methods
are capable of finding an approximate solution of the problem by allowing some
inexactness on the execution of the CG subroutine. The overall arithmetic
operation cost of our GCG methods for finding an $\epsilon$-optimal solution
depends on $\epsilon$ in $O(\log(1/\epsilon))$, which is superior to the
accelerated proximal gradient method [2,23] that depends on $\epsilon$ in
$O(1/\sqrt{\epsilon})$. In addition, our GCG methods can be extended
straightforwardly to solve box-constrained convex QP with finite convergence.
Numerical results demonstrate that our methods are very favorable for solving
ill-conditioned problems.","Generalized Conjugate Gradient Methods for $\ell_1$ Regularized Convex
  Quadratic Programming with Finite Convergence",mechanism tensor approaches image feature convex models
245,"Topic models are one of the most popular methods for learning representations
of text, but a major challenge is that any change to the topic model requires
mathematically deriving a new inference algorithm. A promising approach to
address this problem is autoencoding variational Bayes (AEVB), but it has
proven diffi- cult to apply to topic models in practice. We present what is to
our knowledge the first effective AEVB based inference method for latent
Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For
Topic Model (AVITM). This model tackles the problems caused for AEVB by the
Dirichlet prior and by component collapsing. We find that AVITM matches
traditional methods in accuracy with much better inference time. Indeed,
because of the inference network, we find that it is unnecessary to pay the
computational cost of running variational optimization on test data. Because
AVITM is black box, it is readily applied to new topic models. As a dramatic
illustration of this, we present a new topic model called ProdLDA, that
replaces the mixture model in LDA with a product of experts. By changing only
one line of code from LDA, we find that ProdLDA yields much more interpretable
topics, even if LDA is trained via collapsed Gibbs sampling.",Autoencoding Variational Inference For Topic Models,systematic strategy models method data based
246,"Recent work has shown that recurrent neural networks (RNNs) can implicitly
capture and exploit hierarchical information when trained to solve common
natural language processing tasks such as language modeling (Linzen et al.,
2016) and neural machine translation (Shi et al., 2016). In contrast, the
ability to model structured data with non-recurrent neural networks has
received little attention despite their success in many NLP tasks (Gehring et
al., 2017; Vaswani et al., 2017). In this work, we compare the two
architectures---recurrent versus non-recurrent---with respect to their ability
to model hierarchical structure and find that recurrency is indeed important
for this purpose.",The Importance of Being Recurrent for Modeling Hierarchical Structure,identifying highest common
247,"We propose a new method of discovering causal structures, based on the
detection of local, spontaneous changes in the underlying data-generating
model. We analyze the classes of structures that are equivalent relative to a
stream of distributions produced by local changes, and devise algorithms that
output graphical representations of these equivalence classes. We present
experimental results, using simulated data, and examine the errors associated
with detection of changes and recovery of structures.",Causal Discovery from Changes,online model <unk> generalization method of proposed representations
248,"We present a method that can evaluate a RANSAC hypothesis in constant time,
i.e. independent of the size of the data. A key observation here is that
correct hypotheses are tightly clustered together in the latent parameter
domain. In a manner similar to the generalized Hough transform we seek to find
this cluster, only that we need as few as two votes for a successful detection.
Rapidly locating such pairs of similar hypotheses is made possible by adapting
the recent ""Random Grids"" range-search technique. We only perform the usual
(costly) hypothesis verification stage upon the discovery of a close pair of
hypotheses. We show that this event rarely happens for incorrect hypotheses,
enabling a significant speedup of the RANSAC pipeline. The suggested approach
is applied and tested on three robust estimation problems: camera localization,
3D rigid alignment and 2D-homography estimation. We perform rigorous testing on
both synthetic and real datasets, demonstrating an improvement in efficiency
without a compromise in accuracy. Furthermore, we achieve state-of-the-art 3D
alignment results on the challenging ""Redwood"" loop-closure challenge.",Latent RANSAC,model key produces <unk>
249,"This paper is concerned with the problem of top-$K$ ranking from pairwise
comparisons. Given a collection of $n$ items and a few pairwise binary
comparisons across them, one wishes to identify the set of $K$ items that
receive the highest ranks. To tackle this problem, we adopt the logistic
parametric model---the Bradley-Terry-Luce model, where each item is assigned a
latent preference score, and where the outcome of each pairwise comparison
depends solely on the relative scores of the two items involved. Recent works
have made significant progress towards characterizing the performance (e.g. the
mean square error for estimating the scores) of several classical methods,
including the spectral method and the maximum likelihood estimator (MLE).
However, where they stand regarding top-$K$ ranking remains unsettled.
  We demonstrate that under a random sampling model, the spectral method alone,
or the regularized MLE alone, is minimax optimal in terms of the sample
complexity---the number of paired comparisons needed to ensure exact top-$K$
identification. This is accomplished via optimal control of the entrywise error
of the score estimates. We complement our theoretical studies by numerical
experiments, confirming that both methods yield low entrywise errors for
estimating the underlying scores. Our theory is established based on a novel
leave-one-out trick, which proves effective for analyzing both iterative and
non-iterative optimization procedures. Along the way, we derive an elementary
eigenvector perturbation bound for probability transition matrices, which
parallels the Davis-Kahan $\sin\Theta$ theorem for symmetric matrices. This
further allows us to close the gap between the $\ell_2$ error upper bound for
the spectral method and the minimax lower limit.",Spectral Method and Regularized MLE Are Both Optimal for Top-$K$ Ranking,models model norm improve new
250,"Relation Extraction is an important sub-task of Information Extraction which
has the potential of employing deep learning (DL) models with the creation of
large datasets using distant supervision. In this review, we compare the
contributions and pitfalls of the various DL models that have been used for the
task, to help guide the path ahead.",A Survey of Deep Learning Methods for Relation Extraction,<unk> <unk> <unk> # ##
251,"This paper proposes a new method for solving Bayesian decision problems. The
method consists of representing a Bayesian decision problem as a
valuation-based system and applying a fusion algorithm for solving it. The
fusion algorithm is a hybrid of local computational methods for computation of
marginals of joint probability distributions and the local computational
methods for discrete optimization problems.",A Fusion Algorithm for Solving Bayesian Decision Problems,learning  role
252,"We study multi-turn response generation in chatbots where a response is
generated according to a conversation context. Existing work has modeled the
hierarchy of the context, but does not pay enough attention to the fact that
words and utterances in the context are differentially important. As a result,
they may lose important information in context and generate irrelevant
responses. We propose a hierarchical recurrent attention network (HRAN) to
model both aspects in a unified framework. In HRAN, a hierarchical attention
mechanism attends to important parts within and among utterances with word
level attention and utterance level attention respectively. With the word level
attention, hidden vectors of a word level encoder are synthesized as utterance
vectors and fed to an utterance level encoder to construct hidden
representations of the context. The hidden vectors of the context are then
processed by the utterance level attention and formed as context vectors for
decoding the response. Empirical studies on both automatic evaluation and human
judgment show that HRAN can significantly outperform state-of-the-art models
for multi-turn response generation.",Hierarchical Recurrent Attention Network for Response Generation,show learning conventional model feature word
253,"In computer vision, many problems such as image segmentation, pixel
labelling, and scene parsing can be formulated as binary quadratic programs
(BQPs). For submodular problems, cuts based methods can be employed to
efficiently solve large-scale problems. However, general nonsubmodular problems
are significantly more challenging to solve. Finding a solution when the
problem is of large size to be of practical interest, however, typically
requires relaxation. Two standard relaxation methods are widely used for
solving general BQPs--spectral methods and semidefinite programming (SDP), each
with their own advantages and disadvantages. Spectral relaxation is simple and
easy to implement, but its bound is loose. Semidefinite relaxation has a
tighter bound, but its computational complexity is high, especially for large
scale problems. In this work, we present a new SDP formulation for BQPs, with
two desirable properties. First, it has a similar relaxation bound to
conventional SDP formulations. Second, compared with conventional SDP methods,
the new SDP formulation leads to a significantly more efficient and scalable
dual optimization approach, which has the same degree of complexity as spectral
methods. We then propose two solvers, namely, quasi-Newton and smoothing Newton
methods, for the dual problem. Both of them are significantly more efficiently
than standard interior-point methods. In practice, the smoothing Newton solver
is faster than the quasi-Newton solver for dense or medium-sized problems,
while the quasi-Newton solver is preferable for large sparse/structured
problems. Our experiments on a few computer vision applications including
clustering, image segmentation, co-segmentation and registration show the
potential of our SDP formulation for solving large-scale BQPs.","Large-scale Binary Quadratic Optimization Using Semidefinite Relaxation
  and Applications",bandit method propose efficiency technique
254,"Research on multi-agent planning has been popular in recent years. While
previous research has been motivated by the understanding that, through
cooperation, multi-agent systems can achieve tasks that are unachievable by
single-agent systems, there are no formal characterizations of situations where
cooperation is required to achieve a goal, thus warranting the application of
multi-agent systems. In this paper, we provide such a formal discussion from
the planning aspect. We first show that determining whether there is required
cooperation (RC) is intractable is general. Then, by dividing the problems that
require cooperation (referred to as RC problems) into two classes -- problems
with heterogeneous and homogeneous agents, we aim to identify all the
conditions that can cause RC in these two classes. We establish that when none
of these identified conditions hold, the problem is single-agent solvable.
Furthermore, with a few assumptions, we provide an upper bound on the minimum
number of agents required for RC problems with homogeneous agents. This study
not only provides new insights into multi-agent planning, but also has many
applications. For example, in human-robot teaming, when a robot cannot achieve
a task, it may be due to RC. In such cases, the human teammate should be
informed and, consequently, coordinate with other available robots for a
solution.",A Formal Analysis of Required Cooperation in Multi-agent Planning,using <unk> show show systematic
255,"Recognizing fonts has become an important task in document analysis, due to
the increasing number of available digital documents in different fonts and
emphases. A generic font-recognition system independent of language, script and
content is desirable for processing various types of documents. At the same
time, categorizing calligraphy styles in handwritten manuscripts is important
for palaeographic analysis, but has not been studied sufficiently in the
literature. We address the font-recognition problem as analysis and
categorization of textures. We extract features using complex wavelet transform
and use support vector machines for classification. Extensive experimental
evaluations on different datasets in four languages and comparisons with
state-of-the-art studies show that our proposed method achieves higher
recognition accuracy while being computationally simpler. Furthermore, on a new
dataset generated from Ottoman manuscripts, we show that the proposed method
can also be used for categorizing Ottoman calligraphy with high accuracy.",Classifying Fonts and Calligraphy Styles Using Complex Wavelet Transform,programming model complexity slam
256,"We propose a novel approach to sufficient dimension reduction in regression,
based on estimating contour directions of negligible variation for the response
surface. These directions span the orthogonal complement of the minimal space
relevant for the regression, and can be extracted according to a measure of the
variation in the response, leading to General Contour Regression(GCR). In
comparison to exiisting sufficient dimension reduction techniques, this
sontour-based mothology guarantees exhaustive estimation of the central space
under ellipticity of the predictoor distribution and very mild additional
assumptions, while maintaining vn-consisytency and somputational ease.
Moreover, it proves to be robust to departures from ellipticity. We also
establish some useful population properties for GCR. Simulations to compare
performance with that of standard techniques such as ordinary least squares,
sliced inverse regression, principal hessian directions, and sliced average
variance estimation confirm the advntages anticipated by theoretical analyses.
We also demonstrate the use of contour-based methods on a data set concerning
grades of students from Massachusetts colleges.",Linear Contour Learning: A Method for Supervised Dimension Reduction,data  size extend data planning
257,"Recently there has been significant research on power generation,
distribution and transmission efficiency especially in the case of renewable
resources. The main objective is reduction of energy losses and this requires
improvements on data acquisition and analysis. In this paper we address these
concerns by using consumers' electrical smart meter readings to estimate
network loading and this information can then be used for better capacity
planning. We compare Deep Neural Network (DNN) methods with traditional methods
for load forecasting. Our results indicate that DNN methods outperform most
traditional methods. This comes at the cost of additional computational
complexity but this can be addressed with the use of cloud resources. We also
illustrate how these results can be used to better support dynamic pricing.",Improving Power Generation Efficiency using Deep Neural Networks,improve method learning ensemble
258,"Kernel alignment measures the degree of similarity between two kernels. In
this paper, inspired from kernel alignment, we propose a new Linear
Discriminant Analysis (LDA) formulation, kernel alignment LDA (kaLDA). We first
define two kernels, data kernel and class indicator kernel. The problem is to
find a subspace to maximize the alignment between subspace-transformed data
kernel and class indicator kernel. Surprisingly, the kernel alignment induced
kaLDA objective function is very similar to classical LDA and can be expressed
using between-class and total scatter matrices. This can be extended to
multi-label data. We use a Stiefel-manifold gradient descent algorithm to solve
this problem. We perform experiments on 8 single-label and 6 multi-label data
sets. Results show that kaLDA has very good performance on many single-label
and multi-label problems.",Kernel Alignment Inspired Linear Discriminant Analysis,two show tasks  tasks  approaches
259,"This report presents our submission to the MS COCO Captioning Challenge 2015.
The method uses Convolutional Neural Network activations as an embedding to
find semantically similar images. From these images, the most typical caption
is selected based on unigram frequencies. Although the method received low
scores with automated evaluation metrics and in human assessed average
correctness, it is competitive in the ratio of captions which pass the Turing
test and which are assessed as better or equal to human captions.",Technical Report: Image Captioning with Semantically Similar Images,role basis two
260,"It has been postulated that a good representation is one that disentangles
the underlying explanatory factors of variation. However, it remains an open
question what kind of training framework could potentially achieve that.
Whereas most previous work focuses on the static setting (e.g., with images),
we postulate that some of the causal factors could be discovered if the learner
is allowed to interact with its environment. The agent can experiment with
different actions and observe their effects. More specifically, we hypothesize
that some of these factors correspond to aspects of the environment which are
independently controllable, i.e., that there exists a policy and a learnable
feature for each such aspect of the environment, such that this policy can
yield changes in that feature with minimal changes to other features that
explain the statistical variations in the observed data. We propose a specific
objective function to find such factors, and verify experimentally that it can
indeed disentangle independently controllable aspects of the environment
without any extrinsic reward signal.","Disentangling the independently controllable factors of variation by
  interacting with the world",models model mammals tree  capabilities problem achieve based
261,"We analyze different notions of fairness in decision making when the
underlying model is not known with certainty. We argue that recent notions of
fairness in machine learning need to be modified to incorporate uncertainties
about model parameters. We introduce the notion of {\em subjective fairness} as
a suitable candidate for fair Bayesian decision making rules, relate this
definition with existing ones, and experimentally demonstrate the inherent
accuracy-fairness tradeoff under this definition.",Subjective fairness: Fairness is in the eye of the beholder, #  method process for
262,"Conversations allow the quick transfer of short bits of information and it is
reasonable to expect that changes in communication medium affect how we
converse. Using conversations in works of fiction and in an online social
networking platform, we show that the utterance length of conversations is
slowly shortening with time but adapts more strongly to the constraints of the
communication medium. This indicates that the introduction of any new medium of
communication can affect the way natural language evolves.",Adaptation of fictional and online conversations to communication media,uncertainty  features
263,"We propose a simple and straightforward way of creating powerful image
representations via cross-dimensional weighting and aggregation of deep
convolutional neural network layer outputs. We first present a generalized
framework that encompasses a broad family of approaches and includes
cross-dimensional pooling and weighting steps. We then propose specific
non-parametric schemes for both spatial- and channel-wise weighting that boost
the effect of highly active spatial responses and at the same time regulate
burstiness effects. We experiment on different public datasets for image search
and show that our approach outperforms the current state-of-the-art for
approaches based on pre-trained networks. We also provide an easy-to-use, open
source implementation that reproduces our results.",Cross-dimensional Weighting for Aggregated Deep Convolutional Features,knowledge show conventional problem observation
264,"Knowledge base construction (KBC) is the process of populating a knowledge
base, i.e., a relational database together with inference rules, with
information extracted from documents and structured sources. KBC blurs the
distinction between two traditional database problems, information extraction
and information integration. For the last several years, our group has been
building knowledge bases with scientific collaborators. Using our approach, we
have built knowledge bases that have comparable and sometimes better quality
than those constructed by human volunteers. In contrast to these knowledge
bases, which took experts a decade or more human years to construct, many of
our projects are constructed by a single graduate student.
  Our approach to KBC is based on joint probabilistic inference and learning,
but we do not see inference as either a panacea or a magic bullet: inference is
a tool that allows us to be systematic in how we construct, debug, and improve
the quality of such systems. In addition, inference allows us to construct
these systems in a more loosely coupled way than traditional approaches. To
support this idea, we have built the DeepDive system, which has the design goal
of letting the user ""think about features---not algorithms."" We think of
DeepDive as declarative in that one specifies what they want but not how to get
it. We describe our approach with a focus on feature engineering, which we
argue is an understudied problem relative to its importance to end-to-end
quality.",Feature Engineering for Knowledge Base Construction,learning ensemble model <unk>
265,"Collecting fully annotated image datasets is challenging and expensive. Many
types of weak supervision have been explored: weak manual annotations, web
search results, temporal continuity, ambient sound and others. We focus on one
particular unexplored mode: visual questions that are asked about images. The
key observation that inspires our work is that the question itself provides
useful information about the image (even without the answer being available).
For instance, the question ""what is the breed of the dog?"" informs the AI that
the animal in the scene is a dog and that there is only one dog present. We
make three contributions: (1) providing an extensive qualitative and
quantitative analysis of the information contained in human visual questions,
(2) proposing two simple but surprisingly effective modifications to the
standard visual question answering models that allow them to make use of weak
supervision in the form of unanswered questions associated with images and (3)
demonstrating that a simple data augmentation strategy inspired by our insights
results in a 7.1% improvement on the standard VQA benchmark.",What's in a Question: Using Visual Questions as a Form of Supervision,quality sharp show level transfer using cannot
266,"A gray-level image texture descriptors based on fractal dimension estimation
is proposed in this work. The proposed method estimates the fractal dimension
using probability (Voss) method. The descriptors are computed applying a
multiscale transform to the fractal dimension curves of the texture image. The
proposed texture descriptor method is evaluated in a classification task of
well known benchmark texture datasets. The results show the great performance
of the proposed method as a tool for texture images analysis and
characterization.","Texture Analysis And Characterization Using Probability Fractal
  Descriptors",exceeds handwritten based
267,"Understanding why a model makes a certain prediction can be as crucial as the
prediction's accuracy in many applications. However, the highest accuracy for
large modern datasets is often achieved by complex models that even experts
struggle to interpret, such as ensemble or deep learning models, creating a
tension between accuracy and interpretability. In response, various methods
have recently been proposed to help users interpret the predictions of complex
models, but it is often unclear how these methods are related and when one
method is preferable over another. To address this problem, we present a
unified framework for interpreting predictions, SHAP (SHapley Additive
exPlanations). SHAP assigns each feature an importance value for a particular
prediction. Its novel components include: (1) the identification of a new class
of additive feature importance measures, and (2) theoretical results showing
there is a unique solution in this class with a set of desirable properties.
The new class unifies six existing methods, notable because several recent
methods in the class lack the proposed desirable properties. Based on insights
from this unification, we present new methods that show improved computational
performance and/or better consistency with human intuition than previous
approaches.",A Unified Approach to Interpreting Model Predictions,model video <unk> problem offers online
268,"The models developed to date for knowledge base embedding are all based on
the assumption that the relations contained in knowledge bases are binary. For
the training and testing of these embedding models, multi-fold (or n-ary)
relational data are converted to triples (e.g., in FB15K dataset) and
interpreted as instances of binary relations. This paper presents a canonical
representation of knowledge bases containing multi-fold relations. We show that
the existing embedding models on the popular FB15K datasets correspond to a
sub-optimal modelling framework, resulting in a loss of structural information.
We advocate a novel modelling framework, which models multi-fold relations
directly using this canonical representation. Using this framework, the
existing TransH model is generalized to a new model, m-TransH. We demonstrate
experimentally that m-TransH outperforms TransH by a large margin, thereby
establishing a new state of the art.","On the representation and embedding of knowledge bases beyond binary
  relations",models model provides model <unk>
269,"Convolution neural network (CNN) has significantly pushed forward the
development of face recognition and analysis techniques. Current CNN models
tend to be deeper and larger to better fit large amounts of training data. When
training data are from internet, their labels are often ambiguous and
inaccurate. This paper presents a light CNN framework to learn a compact
embedding on the large-scale face data with massive noisy labels. First, we
introduce the concept of maxout activation into each convolutional layer of
CNN, which results in a Max-Feature-Map (MFM). Different from Rectified Linear
Unit that suppresses a neuron by a threshold (or bias), MFM suppresses a neuron
by a competitive relationship. MFM can not only separate noisy signals and
informative signals but also plays a role of feature selection. Second, a
network of five convolution layers and four Network in Network (NIN) layers are
implemented to reduce the number of parameters and improve performance. Lastly,
a semantic bootstrapping method is accordingly designed to make the prediction
of the models be better consistent with noisy labels. Experimental results show
that the proposed framework can utilize large-scale noisy data to learn a light
model in terms of both computational cost and storage space. The learnt single
model with a 256-D representation achieves state-of-the-art results on five
face benchmarks without fine-tuning. The light CNN model is released on
https://github.com/AlfredXiangWu/face_verification_experiment.",A Light CNN for Deep Face Representation with Noisy Labels,<unk> model <unk> objects simple
270,"Learning the parameters of graphical models using the maximum likelihood
estimation is generally hard which requires an approximation. Maximum composite
likelihood estimations are statistical approximations of the maximum likelihood
estimation which are higher-order generalizations of the maximum
pseudo-likelihood estimation. In this paper, we propose a composite likelihood
method and investigate its property. Furthermore, we apply our composite
likelihood method to restricted Boltzmann machines.",Composite Likelihood Estimation for Restricted Boltzmann machines,video theory fractal data around
271,"This paper presents an out-of-sample prediction comparison between major
machine learning models and the structural econometric model. Over the past
decade, machine learning has established itself as a powerful tool in many
prediction applications, but this approach is still not widely adopted in
empirical economic studies. To evaluate the benefits of this approach, I use
the most common machine learning algorithms, CART, C4.5, LASSO, random forest,
and adaboost, to construct prediction models for a cash transfer experiment
conducted by the Progresa program in Mexico, and I compare the prediction
results with those of a previous structural econometric study. Two prediction
tasks are performed in this paper: the out-of-sample forecast and the long-term
within-sample simulation. For the out-of-sample forecast, both the mean
absolute error and the root mean square error of the school attendance rates
found by all machine learning models are smaller than those found by the
structural model. Random forest and adaboost have the highest accuracy for the
individual outcomes of all subgroups. For the long-term within-sample
simulation, the structural model has better performance than do all of the
machine learning models. The poor within-sample fitness of the machine learning
model results from the inaccuracy of the income and pregnancy prediction
models. The result shows that the machine learning model performs better than
does the structural model when there are many data to learn; however, when the
data are limited, the structural model offers a more sensible prediction. The
findings of this paper show promise for adopting machine learning in economic
policy analyses in the era of big data.","Evaluating Conditional Cash Transfer Policies with Machine Learning
  Methods",programming theory model show columns encoder data 
272,"The design and analysis of communication systems typically rely on the
development of mathematical models that describe the underlying communication
channel, which dictates the relationship between the transmitted and the
received signals. However, in some systems, such as molecular communication
systems where chemical signals are used for transfer of information, it is not
possible to accurately model this relationship. In these scenarios, because of
the lack of mathematical channel models, a completely new approach to design
and analysis is required. In this work, we focus on one important aspect of
communication systems, the detection algorithms, and demonstrate that by
borrowing tools from deep learning, it is possible to train detectors that
perform well, without any knowledge of the underlying channel models. We
evaluate these algorithms using experimental data that is collected by a
chemical communication platform, where the channel model is unknown and
difficult to model analytically. We show that deep learning algorithms perform
significantly better than a simple detector that was used in previous works,
which also did not assume any knowledge of the channel.",Detection Algorithms for Communication Systems Using Deep Learning,sets  categorization updates present image dynamics scheme 
273,"Morphological Analysis is an important branch of linguistics for any Natural
Language Processing Technology. Morphology studies the word structure and
formation of word of a language. In current scenario of NLP research,
morphological analysis techniques have become more popular day by day. For
processing any language, morphology of the word should be first analyzed.
Assamese language contains very complex morphological structure. In our work we
have used Apertium based Finite-State-Transducers for developing morphological
analyzer for Assamese Language with some limited domain and we get 72.7%
accuracy",An implementation of Apertium based Assamese morphological analyzer,###  designed proposed potential unsupervised
274,"This paper presents a parallel memetic algorithm for solving the vehicle
routing problem with time windows (VRPTW). The VRPTW is a well-known NP-hard
discrete optimization problem with two objectives. The main objective is to
minimize the number of vehicles serving customers scattered on the map, and the
second one is to minimize the total distance traveled by the vehicles. Here,
the fleet size is minimized in the first phase of the proposed method using the
parallel heuristic algorithm (PHA), and the traveled distance is minimized in
the second phase by the parallel memetic algorithm (PMA). In both parallel
algorithms, the parallel components co-operate periodically in order to
exchange the best solutions found so far. An extensive experimental study
performed on the Gehring and Homberger's benchmark proves the high convergence
capabilities and robustness of both PHA and PMA. Also, we present the speedup
analysis of the PMA.","A Parallel Memetic Algorithm to Solve the Vehicle Routing Problem with
  Time Windows",based <unk>
275,"Many standard optimization methods for segmentation and reconstruction
compute ML model estimates for appearance or geometry of segments, e.g.
Zhu-Yuille 1996, Torr 1998, Chan-Vese 2001, GrabCut 2004, Delong et al. 2012.
We observe that the standard likelihood term in these formulations corresponds
to a generalized probabilistic K-means energy. In learning it is well known
that this energy has a strong bias to clusters of equal size, which can be
expressed as a penalty for KL divergence from a uniform distribution of
cardinalities. However, this volumetric bias has been mostly ignored in
computer vision. We demonstrate significant artifacts in standard segmentation
and reconstruction methods due to this bias. Moreover, we propose binary and
multi-label optimization techniques that either (a) remove this bias or (b)
replace it by a KL divergence term for any given target volume distribution.
Our general ideas apply to many continuous or discrete energy formulations in
segmentation, stereo, and other reconstruction problems.","Volumetric Bias in Segmentation and Reconstruction: Secrets and
  Solutions",data  model networks
276,"We develop fast spectral algorithms for tensor decomposition that match the
robustness guarantees of the best known polynomial-time algorithms for this
problem based on the sum-of-squares (SOS) semidefinite programming hierarchy.
  Our algorithms can decompose a 4-tensor with $n$-dimensional orthonormal
components in the presence of error with constant spectral norm (when viewed as
an $n^2$-by-$n^2$ matrix). The running time is $n^5$ which is close to linear
in the input size $n^4$.
  We also obtain algorithms with similar running time to learn sparsely-used
orthogonal dictionaries even when feature representations have constant
relative sparsity and non-independent coordinates.
  The only previous polynomial-time algorithms to solve these problem are based
on solving large semidefinite programs. In contrast, our algorithms are easy to
implement directly and are based on spectral projections and tensor-mode
rearrangements.
  Or work is inspired by recent of Hopkins, Schramm, Shi, and Steurer (STOC'16)
that shows how fast spectral algorithms can achieve the guarantees of SOS for
average-case problems. In this work, we introduce general techniques to capture
the guarantees of SOS for worst-case problems.","Fast and robust tensor decomposition with applications to dictionary
  learning",models model <unk> for
277,"Randomized experiments have been used to assist decision-making in many
areas. They help people select the optimal treatment for the test population
with certain statistical guarantee. However, subjects can show significant
heterogeneity in response to treatments. The problem of customizing treatment
assignment based on subject characteristics is known as uplift modeling,
differential response analysis, or personalized treatment learning in
literature. A key feature for uplift modeling is that the data is unlabeled. It
is impossible to know whether the chosen treatment is optimal for an individual
subject because response under alternative treatments is unobserved. This
presents a challenge to both the training and the evaluation of uplift models.
In this paper we describe how to obtain an unbiased estimate of the key
performance metric of an uplift model, the expected response. We present a new
uplift algorithm which creates a forest of randomized trees. The trees are
built with a splitting criterion designed to directly optimize their uplift
performance based on the proposed evaluation method. Both the evaluation method
and the algorithm apply to arbitrary number of treatments and general response
types. Experimental results on synthetic data and industry-provided data show
that our algorithm leads to significant performance improvement over other
applicable methods.",Uplift Modeling with Multiple Treatments and General Response Types,preferences show belief evaluation reconstruct
278,"For the last ten years, CAPTCHAs have been widely used by websites to prevent
their data being automatically updated by machines. By supposedly allowing only
humans to do so, CAPTCHAs take advantage of the reverse Turing test (TT),
knowing that humans are more intelligent than machines. Generally, CAPTCHAs
have defeated machines, but things are changing rapidly as technology improves.
Hence, advanced research into optical character recognition (OCR) is overtaking
attempts to strengthen CAPTCHAs against machine-based attacks. This paper
investigates the immunity of CAPTCHA, which was built on the failure of the TT.
We show that some CAPTCHAs are easily broken using a simple OCR machine built
for the purpose of this study. By reviewing other techniques, we show that even
more difficult CAPTCHAs can be broken using advanced OCR machines. Current
advances in OCR should enable machines to pass the TT in the image recognition
domain, which is exactly where machines are seeking to overcome CAPTCHAs. We
enhance traditional CAPTCHAs by employing not only characters, but also natural
language and multiple objects within the same CAPTCHA. The proposed CAPTCHAs
might be able to hold out against machines, at least until the advent of a
machine that passes the TT completely.",Bypassing Captcha By Machine A Proof For Passing The Turing Test,feature streams example show feature models
279,"Dense reconstructions often contain errors that prior work has so far
minimised using high quality sensors and regularising the output. Nevertheless,
errors still persist. This paper proposes a machine learning technique to
identify errors in three dimensional (3D) meshes. Beyond simply identifying
errors, our method quantifies both the magnitude and the direction of depth
estimate errors when viewing the scene. This enables us to improve the
reconstruction accuracy.
  We train a suitably deep network architecture with two 3D meshes: a
high-quality laser reconstruction, and a lower quality stereo image
reconstruction. The network predicts the amount of error in the lower quality
reconstruction with respect to the high-quality one, having only view the
former through its input. We evaluate our approach by correcting
two-dimensional (2D) inverse-depth images extracted from the 3D model, and show
that our method improves the quality of these depth reconstructions by up to a
relative 10% RMSE.",Meshed Up: Learnt Error Correction in 3D Reconstructions,the model signals  improve new <unk>
280,"We propose a method for estimating channel parameters from RSSI measurements
and the lost packet count, which can work in the presence of losses due to both
interference and signal attenuation below the noise floor. This is especially
important in the wireless networks, such as vehicular, where propagation model
changes with the density of nodes. The method is based on Stochastic
Expectation Maximization, where the received data is modeled as a mixture of
distributions (no/low interference and strong interference), incomplete
(censored) due to packet losses. The PDFs in the mixture are Gamma, according
to the commonly accepted model for wireless signal and interference power. This
approach leverages the loss count as additional information, hence
outperforming maximum likelihood estimation, which does not use this
information (ML-), for a small number of received RSSI samples. Hence, it
allows inexpensive on-line channel estimation from ad-hoc collected data. The
method also outperforms ML- on uncensored data mixtures, as ML- assumes that
samples are from a single-mode PDF.","EM-Based Channel Estimation from Crowd-Sourced RSSI Samples Corrupted by
  Noise and Interference",models proposed
281,"In depth from defocus (DFD), when images are captured with different camera
parameters, a relative magnification is induced between them. Image warping is
a simpler solution to account for magnification than seemingly more accurate
optical approaches. This work is an investigation into the effects of
magnification on the accuracy of DFD. We comment on issues regarding scaling
effect on relative blur computation. We statistically analyze accountability of
scale factor, commenting on the bias and efficiency of the estimator that does
not consider scale. We also discuss the effect of interpolation errors on blur
estimation in a warping based solution to handle magnification and carry out
experimental analysis to comment on the blur estimation accuracy.",Analysis of Magnification in Depth from Defocus,model data  model <unk> questions networks proposed less
282,"The posterior parietal cortex is believed to direct eye movements, especially
in regards to target tracking tasks, and a number of debates exist over the
precise nature of the computations performed by the parietal cortex, with each
side supported by different sets of biological evidence. In this paper I will
present my model which navigates a course between some of these debates,
towards the end of presenting a model which can explain some of the competing
interpretations among the data sets. In particular, rather than assuming that
proprioception or efference copies form the key source of information for
computing eye position information, I use a biological plausible implementation
of a Kalman filter to optimally combine the two signals, and a simple gain
control mechanism in order to accommodate the latency of the proprioceptive
signal. Fitting within the Bayesian brain hypothesis, the result is a Bayes
optimal solution to the eye control problem, with a range of data supporting
claims of biological plausibility.","A Biologically Realistic Model of Saccadic Eye Control with
  Probabilistic Population Codes",distributions estimate model <unk> conditions
283,"We investigate anomaly detection in an unsupervised framework and introduce
Long Short Term Memory (LSTM) neural network based algorithms. In particular,
given variable length data sequences, we first pass these sequences through our
LSTM based structure and obtain fixed length sequences. We then find a decision
function for our anomaly detectors based on the One Class Support Vector
Machines (OC-SVM) and Support Vector Data Description (SVDD) algorithms. As the
first time in the literature, we jointly train and optimize the parameters of
the LSTM architecture and the OC-SVM (or SVDD) algorithm using highly effective
gradient and quadratic programming based training methods. To apply the
gradient based training method, we modify the original objective criteria of
the OC-SVM and SVDD algorithms, where we prove the convergence of the modified
objective criteria to the original criteria. We also provide extensions of our
unsupervised formulation to the semi-supervised and fully supervised
frameworks. Thus, we obtain anomaly detection algorithms that can process
variable length data sequences while providing high performance, especially for
time series data. Our approach is generic so that we also apply this approach
to the Gated Recurrent Unit (GRU) architecture by directly replacing our LSTM
based structure with the GRU based structure. In our experiments, we illustrate
significant performance gains achieved by our algorithms with respect to the
conventional methods.","Unsupervised and Semi-supervised Anomaly Detection with LSTM Neural
  Networks",learning learning two model specifically method data task 
284,"Selectional preferences have long been claimed to be essential for
coreference resolution. However, they are mainly modeled only implicitly by
current coreference resolvers. We propose a dependency-based embedding model of
selectional preferences which allows fine-grained compatibility judgments with
high coverage. We show that the incorporation of our model improves coreference
resolution performance on the CoNLL dataset, matching the state-of-the-art
results of a more complex system. However, it comes with a cost that makes it
debatable how worthwhile such improvements are.",Revisiting Selectional Preferences for Coreference Resolution,kernel technique preferences
285,"We compare several ConvNets with different depth and regularization
techniques with multi-unit macaque IT cortex recordings and assess the impact
of the same on representational similarity with the primate visual cortex. We
find that with increasing depth and validation performance, ConvNet features
are closer to cortical IT representations.","Examining Representational Similarity in ConvNets and the Primate Visual
  Cortex",<unk> using visually modified
286,"We carry out a comprehensive analysis of letter frequencies in contemporary
written Marathi. We determine sets of letters which statistically predominate
any large generic Marathi text, and use these sets to estimate the entropy of
Marathi.",On the letter frequencies and entropy of written Marathi,time image specifically image scene online
287,"2-D complex Gabor filtering has found numerous applications in the fields of
computer vision and image processing. Especially, in some applications, it is
often needed to compute 2-D complex Gabor filter bank consisting of the 2-D
complex Gabor filtering outputs at multiple orientations and frequencies.
Although several approaches for fast 2-D complex Gabor filtering have been
proposed, they primarily focus on reducing the runtime of performing the 2-D
complex Gabor filtering once at specific orientation and frequency. To obtain
the 2-D complex Gabor filter bank output, existing methods are repeatedly
applied with respect to multiple orientations and frequencies. In this paper,
we propose a novel approach that efficiently computes the 2-D complex Gabor
filter bank by reducing the computational redundancy that arises when
performing the Gabor filtering at multiple orientations and frequencies. The
proposed method first decomposes the Gabor basis kernels to allow a fast
convolution with the Gaussian kernel in a separable manner. This enables
reducing the runtime of the 2-D complex Gabor filter bank by reusing
intermediate results of the 2-D complex Gabor filtering computed at a specific
orientation. Furthermore, we extend this idea into 2-D localized sliding
discrete Fourier transform (SDFT) using the Gaussian kernel in the DFT
computation, which lends a spatial localization ability as in the 2-D complex
Gabor filter. Experimental results demonstrate that our method runs faster than
state-of-the-arts methods for fast 2-D complex Gabor filtering, while
maintaining similar filtering quality.",Fast 2-D Complex Gabor Filter with Kernel Decomposition,ranking show video control search
288,"We describe a general framework for online adaptation of optimization
hyperparameters by `hot swapping' their values during learning. We investigate
this approach in the context of adaptive learning rate selection using an
explore-exploit strategy from the multi-armed bandit literature. Experiments on
a benchmark neural network show that the hot swapping approach leads to
consistently better solutions compared to well-known alternatives such as
AdaDelta and stochastic gradient with exhaustive hyperparameter search.",Hot Swapping for Online Adaptation of Optimization Hyperparameters,visualization problem profile
289,"Understanding the complexity of human language requires an appropriate
analysis of the statistical distribution of words in texts. We consider the
information retrieval problem of detecting and ranking the relevant words of a
text by means of statistical information referring to the ""spatial"" use of the
words. Shannon's entropy of information is used as a tool for automatic keyword
extraction. By using The Origin of Species by Charles Darwin as a
representative text sample, we show the performance of our detector and compare
it with another proposals in the literature. The random shuffled text receives
special attention as a tool for calibrating the ranking indices.",Statistical keyword detection in literary corpora,programming empirical two show news statistical online
290,"This paper aims for event recognition when video examples are scarce or even
completely absent. The key in such a challenging setting is a semantic video
representation. Rather than building the representation from individual
attribute detectors and their annotations, we propose to learn the entire
representation from freely available web videos and their descriptions using an
embedding between video features and term vectors. In our proposed embedding,
which we call VideoStory, the correlations between the terms are utilized to
learn a more effective representation by optimizing a joint objective balancing
descriptiveness and predictability.We show how learning the VideoStory using a
multimodal predictability loss, including appearance, motion and audio
features, results in a better predictable representation. We also propose a
variant of VideoStory to recognize an event in video from just the important
terms in a text query by introducing a term sensitive descriptiveness loss. Our
experiments on three challenging collections of web videos from the NIST
TRECVID Multimedia Event Detection and Columbia Consumer Videos datasets
demonstrate: i) the advantages of VideoStory over representations using
attributes or alternative embeddings, ii) the benefit of fusing video
modalities by an embedding over common strategies, iii) the complementarity of
term sensitive descriptiveness and multimodal predictability for event
recognition without examples. By it abilities to improve predictability upon
any underlying video feature while at the same time maximizing semantic
descriptiveness, VideoStory leads to state-of-the-art accuracy for both few-
and zero-example recognition of events in video.",VideoStory Embeddings Recognize Events when Examples are Scarce,video step size image word along
291,"This paper proposes to use probabilistic model checking to synthesize optimal
robot policies in multi-tasking autonomous systems that are subject to
human-robot interaction. Given the convincing empirical evidence that human
behavior can be related to reinforcement models, we take as input a
well-studied Q-table model of the human behavior for flexible scenarios. We
first describe an automated procedure to distill a Markov decision process
(MDP) for the human in an arbitrary but fixed scenario. The distinctive issue
is that -- in contrast to existing models -- under-specification of the human
behavior is included. Probabilistic model checking is used to predict the
human's behavior. Finally, the MDP model is extended with a robot model.
Optimal robot policies are synthesized by analyzing the resulting two-player
stochastic game. Experimental results with a prototypical implementation using
PRISM show promising results.","Probabilistic Model Checking for Complex Cognitive Tasks -- A case study
  in human-robot interaction",using self driving network exposing specifically 
292,"Recently hybrid evolutionary computation (EC) techniques are successfully
implemented for solving large sets of linear equations. All the recently
developed hybrid evolutionary algorithms, for solving linear equations, contain
both the recombination and the mutation operations. In this paper, two modified
hybrid evolutionary algorithms contained time-variant adaptive evolutionary
technique are proposed for solving linear equations in which recombination
operation is absent. The effectiveness of the recombination operator has been
studied for the time-variant adaptive hybrid algorithms for solving large set
of linear equations. Several experiments have been carried out using both the
proposed modified hybrid evolutionary algorithms (in which the recombination
operation is absent) and corresponding existing hybrid algorithms (in which the
recombination operation is present) to solve large set of linear equations. It
is found that the number of generations required by the existing hybrid
algorithms (i.e. the Gauss-Seidel-SR based time variant adaptive (GSBTVA)
hybrid algorithm and the Jacobi-SR based time variant adaptive (JBTVA) hybrid
algorithm) and modified hybrid algorithms (i.e. the modified Gauss-Seidel-SR
based time variant adaptive (MGSBTVA) hybrid algorithm and the modified
Jacobi-SR based time variant adaptive (MJBTVA) hybrid algorithm) are
comparable. Also the proposed modified algorithms require less amount of
computational time in comparison to the corresponding existing hybrid
algorithms. As the proposed modified hybrid algorithms do not contain
recombination operation, so they require less computational effort, and also
they are more efficient, effective and easy to implement.","For Solving Linear Equations Recombination is a Needless Operation in
  Time-Variant Adaptive Hybrid Algorithms",train similarity estimated
293,"Fully connected multilayer perceptrons are used for obtaining numerical
solutions of partial differential equations in various dimensions. Independent
variables are fed into the input layer, and the output is considered as
solution's value. To train such a network one can use square of equation's
residual as a cost function and minimize it with respect to weights by gradient
descent. Following previously developed method, derivatives of the equation's
residual along random directions in space of independent variables are also
added to cost function. Similar procedure is known to produce nearly machine
precision results using less than 8 grid points per dimension for 2D case. The
same effect is observed here for higher dimensions: solutions are obtained on
low density grids, but maintain their precision in the entire region. Boundary
value problems for linear and nonlinear Poisson equations are solved inside 2,
3, 4, and 5 dimensional balls. Grids for linear cases have 40, 159, 512 and
1536 points and for nonlinear 64, 350, 1536 and 6528 points respectively. In
all cases maximum error is less than $8.8\cdot10^{-6}$, and median error is
less than $2.4\cdot10^{-6}$. Very weak grid requirements enable neural networks
to obtain solution of 5D linear problem within 22 minutes, whereas projected
solving time for finite differences on the same hardware is 50 minutes. Method
is applied to second order equation, but requires little to none modifications
to solve systems or higher order PDEs.","Neural networks catching up with finite differences in solving partial
  differential equations in higher dimensions",based <unk>
294,"In recent years, the importance of deep learning has significantly increased
in pattern recognition, computer vision, and artificial intelligence research,
as well as in industry. However, despite the existence of multiple deep
learning frameworks, there is a lack of comprehensible and easy-to-use
high-level tools for the design, training, and testing of deep neural networks
(DNNs). In this paper, we introduce Barista, an open-source graphical
high-level interface for the Caffe deep learning framework. While Caffe is one
of the most popular frameworks for training DNNs, editing prototext files in
order to specify the net architecture and hyper parameters can become a
cumbersome and error-prone task. Instead, Barista offers a fully graphical user
interface with a graph-based net topology editor and provides an end-to-end
training facility for DNNs, which allows researchers to focus on solving their
problems without having to write code, edit text files, or manually parse
logged data.","Barista - a Graphical Tool for Designing and Training Deep Neural
  Networks",programming learning  incremental #### 
295,"In this paper, we propose a novel method to jointly solve scene layout
estimation and global registration problems for accurate indoor 3D
reconstruction. Given a sequence of range data, we first build a set of scene
fragments using KinectFusion and register them through pose graph optimization.
Afterwards, we alternate between layout estimation and layout-based global
registration processes in iterative fashion to complement each other. We
extract the scene layout through hierarchical agglomerative clustering and
energy-based multi-model fitting in consideration of noisy measurements. Having
the estimated scene layout in one hand, we register all the range data through
the global iterative closest point algorithm where the positions of 3D points
that belong to the layout such as walls and a ceiling are constrained to be
close to the layout. We experimentally verify the proposed method with the
publicly available synthetic and real-world datasets in both quantitative and
qualitative ways.","Joint Layout Estimation and Global Multi-View Registration for Indoor
  Reconstruction",demonstrate show significant high model
296,"We propose a probabilistic graphical model realizing a minimal encoding of
real variables dependencies based on possibly incomplete observation and an
empirical cumulative distribution function per variable. The target application
is a large scale partially observed system, like e.g. a traffic network, where
a small proportion of real valued variables are observed, and the other
variables have to be predicted. Our design objective is therefore to have good
scalability in a real-time setting. Instead of attempting to encode the
dependencies of the system directly in the description space, we propose a way
to encode them in a latent space of binary variables, reflecting a rough
perception of the observable (congested/non-congested for a traffic road). The
method relies in part on message passing algorithms, i.e. belief propagation,
but the core of the work concerns the definition of meaningful latent variables
associated to the variables of interest and their pairwise dependencies.
Numerical experiments demonstrate the applicability of the method in practice.","Using Latent Binary Variables for Online Reconstruction of Large Scale
  Systems",the model class action
297,"Accuracy and interpretability are two dominant features of successful
predictive models. Typically, a choice must be made in favor of complex black
box models such as recurrent neural networks (RNN) for accuracy versus less
accurate but more interpretable traditional models such as logistic regression.
This tradeoff poses challenges in medicine where both accuracy and
interpretability are important. We addressed this challenge by developing the
REverse Time AttentIoN model (RETAIN) for application to Electronic Health
Records (EHR) data. RETAIN achieves high accuracy while remaining clinically
interpretable and is based on a two-level neural attention model that detects
influential past visits and significant clinical variables within those visits
(e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR
data in a reverse time order so that recent clinical visits are likely to
receive higher attention. RETAIN was tested on a large health system EHR
dataset with 14 million visits completed by 263K patients over an 8 year period
and demonstrated predictive accuracy and computational scalability comparable
to state-of-the-art methods such as RNN, and ease of interpretability
comparable to traditional models.","RETAIN: An Interpretable Predictive Model for Healthcare using Reverse
  Time Attention Mechanism",results  <unk> show show concepts made search
298,"Unsupervised image segmentation and denoising are two fundamental tasks in
image processing. Usually, graph based models such as multicut are used for
segmentation and variational models are employed for denoising. Our approach
addresses both problems at the same time. We propose a novel ILP formulation of
the first derivative Potts model with the $\ell_1$ data term, where binary
variables are introduced to deal with the $\ell_0$ norm of the regularization
term. The ILP is then solved by a standard off-the-shelf MIP solver. Numerical
experiments are compared with the multicut problem.",A First Derivative Potts Model for Segmentation and Denoising Using ILP,train similarity the
299,"In the context of SAT solvers, Shatter is a popular tool for symmetry
breaking on CNF formulas. Nevertheless, little has been said about its use in
the context of AllSAT problems: problems where we are interested in listing all
the models of a Boolean formula. AllSAT has gained much popularity in recent
years due to its many applications in domains like model checking, data mining,
etc. One example of a particularly transparent application of AllSAT to other
fields of computer science is computational Ramsey theory. In this paper we
study the effect of incorporating Shatter to the workflow of using Boolean
formulas to generate all possible edge colorings of a graph avoiding prescribed
monochromatic subgraphs. Generating complete sets of colorings is an important
building block in computational Ramsey theory. We identify two drawbacks in the
na\""ive use of Shatter to break the symmetries of Boolean formulas encoding
Ramsey-type problems for graphs: a ""blow-up"" in the number of models and the
generation of incomplete sets of colorings. The issues presented in this work
are not intended to discourage the use of Shatter as a preprocessing tool for
AllSAT problems in combinatorial computing but to help researchers properly use
this tool by avoiding these potential pitfalls. To this end, we provide
strategies and additional tools to cope with the negative effects of using
Shatter for AllSAT. While the specific application addressed in this paper is
that of Ramsey-type problems, the analysis we carry out applies to many other
areas in which highly-symmetrical Boolean formulas arise and we wish to find
all of their models.",Exploring the Use of Shatter for AllSAT Through Ramsey-Type Problems,theory uses show observation theory organs
300,"A major challenge in brain tumor treatment planning and quantitative
evaluation is determination of the tumor extent. The noninvasive magnetic
resonance imaging (MRI) technique has emerged as a front-line diagnostic tool
for brain tumors without ionizing radiation. Manual segmentation of brain tumor
extent from 3D MRI volumes is a very time-consuming task and the performance is
highly relied on operator's experience. In this context, a reliable fully
automatic segmentation method for the brain tumor segmentation is necessary for
an efficient measurement of the tumor extent. In this study, we propose a fully
automatic method for brain tumor segmentation, which is developed using U-Net
based deep convolutional networks. Our method was evaluated on Multimodal Brain
Tumor Image Segmentation (BRATS 2015) datasets, which contain 220 high-grade
brain tumor and 54 low-grade tumor cases. Cross-validation has shown that our
method can obtain promising segmentation efficiently.","Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully
  Convolutional Networks",heterogeneity  kinect polynomial model show non exhaustive data 
301,"We introduce new online and batch algorithms that are robust to data with
missing features, a situation that arises in many practical applications. In
the online setup, we allow for the comparison hypothesis to change as a
function of the subset of features that is observed on any given round,
extending the standard setting where the comparison hypothesis is fixed
throughout. In the batch setup, we present a convex relation of a non-convex
problem to jointly estimate an imputation function, used to fill in the values
of missing features, along with the classification hypothesis. We prove regret
bounds in the online setting and Rademacher complexity bounds for the batch
i.i.d. setting. The algorithms are tested on several UCI datasets, showing
superior performance over baselines.",Online and Batch Learning Algorithms for Data with Missing Features,networks sender using significant contained improved model function 
302,"Query-by-example search often uses dynamic time warping (DTW) for comparing
queries and proposed matching segments. Recent work has shown that comparing
speech segments by representing them as fixed-dimensional vectors --- acoustic
word embeddings --- and measuring their vector distance (e.g., cosine distance)
can discriminate between words more accurately than DTW-based approaches. We
consider an approach to query-by-example search that embeds both the query and
database segments according to a neural model, followed by nearest-neighbor
search to find the matching segments. Earlier work on embedding-based
query-by-example, using template-based acoustic word embeddings, achieved
competitive performance. We find that our embeddings, based on recurrent neural
networks trained to optimize word discrimination, achieve substantial
improvements in performance and run-time efficiency over the previous
approaches.","Query-by-Example Search with Discriminative Neural Acoustic Word
  Embeddings",camera models show feature word for
303,"When reasoning with uncertainty there are many situations where evidences are
not only uncertain but their propositions may also be weakly specified in the
sense that it may not be certain to which event a proposition is referring. It
is then crucial not to combine such evidences in the mistaken belief that they
are referring to the same event. This situation would become manageable if the
evidences could be clustered into subsets representing events that should be
handled separately. In an earlier article we established within Dempster-Shafer
theory a criterion function called the metaconflict function. With this
criterion we can partition a set of evidences into subsets. Each subset
representing a separate event. In this article we will not only find the most
plausible subset for each piece of evidence, we will also find the plausibility
for every subset that the evidence belongs to the subset. Also, when the number
of subsets are uncertain we aim to find a posterior probability distribution
regarding the number of subsets.",Cluster-based Specification Techniques in Dempster-Shafer Theory,find show distribution rapid
304,"The Recurrent Neural Networks and their variants have shown promising
performances in sequence modeling tasks such as Natural Language Processing.
These models, however, turn out to be impractical and difficult to train when
exposed to very high-dimensional inputs due to the large input-to-hidden weight
matrix. This may have prevented RNNs' large-scale application in tasks that
involve very high input dimensions such as video modeling; current approaches
reduce the input dimensions using various feature extractors. To address this
challenge, we propose a new, more general and efficient approach by factorizing
the input-to-hidden weight matrix using Tensor-Train decomposition which is
trained simultaneously with the weights themselves. We test our model on
classification tasks using multiple real-world video datasets and achieve
competitive performances with state-of-the-art models, even though our model
architecture is orders of magnitude less complex. We believe that the proposed
approach provides a novel and fundamental building block for modeling
high-dimensional sequential data with RNN architectures and opens up many
possibilities to transfer the expressive and advanced architectures from other
domains such as NLP to modeling high-dimensional sequential data.",Tensor-Train Recurrent Neural Networks for Video Classification,various show show deal agreement
305,"A modification of OWL-S regarding parameter description is proposed. It is
strictly based on Description Logic. In addition to class description of
parameters it also allows the modelling of relations between parameters and the
precise description of the size of data to be supplied to a service. In
particular, it solves two major issues identified within current proposals for
a Semantic Web Service annotation standard.",Semantic Description of Parameters in Web Service Annotations,strategy models show blackhole a for
306,"Many image-to-image translation problems are ambiguous, as a single input
image may correspond to multiple possible outputs. In this work, we aim to
model a \emph{distribution} of possible outputs in a conditional generative
modeling setting. The ambiguity of the mapping is distilled in a
low-dimensional latent vector, which can be randomly sampled at test time. A
generator learns to map the given input, combined with this latent code, to the
output. We explicitly encourage the connection between output and the latent
code to be invertible. This helps prevent a many-to-one mapping from the latent
code to the output during training, also known as the problem of mode collapse,
and produces more diverse results. We explore several variants of this approach
by employing different training objectives, network architectures, and methods
of injecting the latent code. Our proposed method encourages bijective
consistency between the latent encoding and output modes. We present a
systematic comparison of our method and other variants on both perceptual
realism and diversity.",Toward Multimodal Image-to-Image Translation,learning metric model known image <unk> experience  using
307,"Uterine Cervical Cancer is one of the most common forms of cancer in women
worldwide. Most cases of cervical cancer can be prevented through screening
programs aimed at detecting precancerous lesions. During Digital Colposcopy,
colposcopic images or cervigrams are acquired in raw form. They contain
specular reflections which appear as bright spots heavily saturated with white
light and occur due to the presence of moisture on the uneven cervix surface
and. The cervix region occupies about half of the raw cervigram image. Other
parts of the image contain irrelevant information, such as equipment, frames,
text and non-cervix tissues. This irrelevant information can confuse automatic
identification of the tissues within the cervix. Therefore we focus on the
cervical borders, so that we have a geometric boundary on the relevant image
area. Our novel technique eliminates the SR, identifies the region of interest
and makes the cervigram ready for segmentation algorithms.",Preprocessing for Automating Early Detection of Cervical Cancer,show significant need using labeling automated the
308,"We present a layered Boltzmann machine (BM) that can better exploit the
advantages of a distributed representation. It is widely believed that deep BMs
(DBMs) have far greater representational power than its shallow counterpart,
restricted Boltzmann machines (RBMs). However, this expectation on the
supremacy of DBMs over RBMs has not ever been validated in a theoretical
fashion. In this paper, we provide both theoretical and empirical evidences
that the representational power of DBMs can be actually rather limited in
taking advantages of distributed representations. We propose an approximate
measure for the representational power of a BM regarding to the efficiency of a
distributed representation. With this measure, we show a surprising fact that
DBMs can make inefficient use of distributed representations. Based on these
observations, we propose an alternative BM architecture, which we dub soft-deep
BMs (sDBMs). We show that sDBMs can more efficiently exploit the distributed
representations in terms of the measure. Experiments demonstrate that sDBMs
outperform several state-of-the-art models, including DBMs, in generative tasks
on binarized MNIST and Caltech-101 silhouettes.",Soft-Deep Boltzmann Machines,for proposed logistic word for
309,"Most existing person re-identification algorithms either extract robust
visual features or learn discriminative metrics for person images. However, the
underlying manifold which those images reside on is rarely investigated. That
raises a problem that the learned metric is not smooth with respect to the
local geometry structure of the data manifold.
  In this paper, we study person re-identification with manifold-based affinity
learning, which did not receive enough attention from this area. An
unconventional manifold-preserving algorithm is proposed, which can 1) make the
best use of supervision from training data, whose label information is given as
pairwise constraints; 2) scale up to large repositories with low on-line time
complexity; and 3) be plunged into most existing algorithms, serving as a
generic postprocessing procedure to further boost the identification
accuracies. Extensive experimental results on five popular person
re-identification benchmarks consistently demonstrate the effectiveness of our
method. Especially, on the largest CUHK03 and Market-1501, our method
outperforms the state-of-the-art alternatives by a large margin with high
efficiency, which is more appropriate for practical applications.",Scalable Person Re-identification on Supervised Smoothed Manifold,#  dictionary  <unk> show first 
310,"We propose a nonrigid registration approach for diffusion tensor images using
a multicomponent information-theoretic measure. Explicit orientation
optimization is enabled by incorporating tensor reorientation, which is
necessary for wrapping diffusion tensor images. Experimental results on
diffusion tensor images indicate the feasibility of the proposed approach and a
much better performance compared to the affine registration method based on
mutual information in terms of registration accuracy in the presence of
geometric distortion.","A Multicomponent Approach to Nonrigid Registration of Diffusion Tensor
  Images",gain learning planning model learning discuss
311,"We summarize the potential impact that the European Union's new General Data
Protection Regulation will have on the routine use of machine learning
algorithms. Slated to take effect as law across the EU in 2018, it will
restrict automated individual decision-making (that is, algorithms that make
decisions based on user-level predictors) which ""significantly affect"" users.
The law will also effectively create a ""right to explanation,"" whereby a user
can ask for an explanation of an algorithmic decision that was made about them.
We argue that while this law will pose large challenges for industry, it
highlights opportunities for computer scientists to take the lead in designing
algorithms and evaluation frameworks which avoid discrimination and enable
explanation.","European Union regulations on algorithmic decision-making and a ""right
  to explanation""",linear model categorization images  ensemble
312,"We show that parametric models trained by a stochastic gradient method (SGM)
with few iterations have vanishing generalization error. We prove our results
by arguing that SGM is algorithmically stable in the sense of Bousquet and
Elisseeff. Our analysis only employs elementary tools from convex and
continuous optimization. We derive stability bounds for both convex and
non-convex optimization under standard Lipschitz and smoothness assumptions.
  Applying our results to the convex case, we provide new insights for why
multiple epochs of stochastic gradient methods generalize well in practice. In
the non-convex case, we give a new interpretation of common practices in neural
networks, and formally show that popular techniques for training large deep
models are indeed stability-promoting. Our findings conceptually underscore the
importance of reducing training time beyond its obvious benefit.","Train faster, generalize better: Stability of stochastic gradient
  descent",user models
313,"This paper presents the Intelligent Voice (IV) system submitted to the NIST
2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this
year was on developing speaker recognition technology which is robust for novel
languages that are much more heterogeneous than those used in the current
state-of-the-art, using significantly less training data, that does not contain
meta-data from those languages. The system is based on the state-of-the-art
i-vector/PLDA which is developed on the fixed training condition, and the
results are reported on the protocol defined on the development set of the
challenge.",The Intelligent Voice 2016 Speaker Recognition System,probability version search different <unk>
314,"It is a challenging task to select correlated variables in a high dimensional
space. To address this challenge, the elastic net has been developed and
successfully applied to many applications. Despite its great success, the
elastic net does not explicitly use correlation information embedded in data to
select correlated variables. To overcome this limitation, we present a novel
Bayesian hybrid model, the EigenNet, that uses the eigenstructures of data to
guide variable selection. Specifically, it integrates a sparse conditional
classification model with a generative model capturing variable correlations in
a principled Bayesian framework. We reparameterize the hybrid model in the
eigenspace to avoid overfiting and to increase the computational efficiency of
its MCMC sampler. Furthermore, we provide an alternative view to the EigenNet
from a regularization perspective: the EigenNet has an adaptive
eigenspace-based composite regularizer, which naturally generalizes the
$l_{1/2}$ regularizer used by the elastic net. Experiments on synthetic and
real data show that the EigenNet significantly outperforms the lasso, the
elastic net, and the Bayesian lasso in terms of prediction accuracy, especially
when the number of training samples is smaller than the number of variables.","EigenNet: A Bayesian hybrid of generative and conditional models for
  sparse learning",setting model semi supervised
315,"In video games, virtual characters' decision systems often use a simplified
representation of the world. To increase both their autonomy and believability
we want those characters to be able to learn this representation from human
players. We propose to use a model called growing neural gas to learn by
imitation the topology of the environment. The implementation of the model, the
modifications and the parameters we used are detailed. Then, the quality of the
learned representations and their evolution during the learning are studied
using different measures. Improvements for the growing neural gas to give more
information to the character's model are given in the conclusion.","Learning a Representation of a Believable Virtual Character's
  Environment with an Imitation Algorithm",role improve show significant deal agreement
316,"Artificial Neural Network (ANN) s has widely been used for recognition of
optically scanned character, which partially emulates human thinking in the
domain of the Artificial Intelligence. But prior to recognition, it is
necessary to segment the character from the text to sentences, words etc.
Segmentation of words into individual letters has been one of the major
problems in handwriting recognition. Despite several successful works all over
the work, development of such tools in specific languages is still an ongoing
process especially in the Indian context. This work explores the application of
ANN as an aid to segmentation of handwritten characters in Assamese- an
important language in the North Eastern part of India. The work explores the
performance difference obtained in applying an ANN-based dynamic segmentation
algorithm compared to projection- based static segmentation. The algorithm
involves, first training of an ANN with individual handwritten characters
recorded from different individuals. Handwritten sentences are separated out
from text using a static segmentation method. From the segmented line,
individual characters are separated out by first over segmenting the entire
line. Each of the segments thus obtained, next, is fed to the trained ANN. The
point of segmentation at which the ANN recognizes a segment or a combination of
several segments to be similar to a handwritten character, a segmentation
boundary for the character is assumed to exist and segmentation performed. The
segmented character is next compared to the best available match and the
segmentation boundary confirmed.","ANN-based Innovative Segmentation Method for Handwritten text in
  Assamese",empirical model empirical kernels
317,"Automated segmentation of retinal blood vessels in label-free fundus images
entails a pivotal role in computed aided diagnosis of ophthalmic pathologies,
viz., diabetic retinopathy, hypertensive disorders and cardiovascular diseases.
The challenge remains active in medical image analysis research due to varied
distribution of blood vessels, which manifest variations in their dimensions of
physical appearance against a noisy background.
  In this paper we formulate the segmentation challenge as a classification
task. Specifically, we employ unsupervised hierarchical feature learning using
ensemble of two level of sparsely trained denoised stacked autoencoder. First
level training with bootstrap samples ensures decoupling and second level
ensemble formed by different network architectures ensures architectural
revision. We show that ensemble training of auto-encoders fosters diversity in
learning dictionary of visual kernels for vessel segmentation. SoftMax
classifier is used for fine tuning each member auto-encoder and multiple
strategies are explored for 2-level fusion of ensemble members. On DRIVE
dataset, we achieve maximum average accuracy of 95.33\% with an impressively
low standard deviation of 0.003 and Kappa agreement coefficient of 0.708 .
Comparison with other major algorithms substantiates the high efficacy of our
model.","Deep Neural Ensemble for Retinal Vessel Segmentation in Fundus Images
  towards Achieving Label-free Angiography",concepts step proposed data center unsupervised
318,"Object Detection is critical for automatic military operations. However, the
performance of current object detection algorithms is deficient in terms of the
requirements in military scenarios. This is mainly because the object presence
is hard to detect due to the indistinguishable appearance and dramatic changes
of object's size which is determined by the distance to the detection sensors.
Recent advances in deep learning have achieved promising results in many
challenging tasks. The state-of-the-art in object detection is represented by
convolutional neural networks (CNNs), such as the fast R-CNN algorithm. These
CNN-based methods improve the detection performance significantly on several
public generic object detection datasets. However, their performance on
detecting small objects or undistinguishable objects in visible spectrum images
is still insufficient. In this study, we propose a novel detection algorithm
for military objects by fusing multi-channel CNNs. We combine spatial, temporal
and thermal information by generating a three-channel image, and they will be
fused as CNN feature maps in an unsupervised manner. The backbone of our object
detection framework is from the fast R-CNN algorithm, and we utilize
cross-domain transfer learning technique to fine-tune the CNN model on
generated multi-channel images. In the experiments, we validated the proposed
method with the images from SENSIAC (Military Sensing Information Analysis
Centre) database and compared it with the state-of-the-art. The experimental
results demonstrated the effectiveness of the proposed method on both accuracy
and computational efficiency.","Multi-Channel CNN-based Object Detection for Enhanced Situation
  Awareness",produces scientists 
319,"PDDL+ is an extension of PDDL that enables modelling planning domains with
mixed discrete-continuous dynamics. In this paper we present a new approach to
PDDL+ planning based on Constraint Answer Set Programming (CASP), i.e. ASP
rules plus numerical constraints. To the best of our knowledge, ours is the
first attempt to link PDDL+ planning and logic programming. We provide an
encoding of PDDL+ models into CASP problems. The encoding can handle non-linear
hybrid domains, and represents a solid basis for applying logic programming to
PDDL+ planning. As a case study, we consider the EZCSP CASP solver and obtain
promising results on a set of PDDL+ benchmark problems.",PDDL+ Planning via Constraint Answer Set Programming,programming set <unk>
320,"We show that given an estimate $\widehat{A}$ that is close to a general
high-rank positive semi-definite (PSD) matrix $A$ in spectral norm (i.e.,
$\|\widehat{A}-A\|_2 \leq \delta$), the simple truncated SVD of $\widehat{A}$
produces a multiplicative approximation of $A$ in Frobenius norm. This
observation leads to many interesting results on general high-rank matrix
estimation problems, which we briefly summarize below ($A$ is an $n\times n$
high-rank PSD matrix and $A_k$ is the best rank-$k$ approximation of $A$):
  (1) High-rank matrix completion: By observing
$\Omega(\frac{n\max\{\epsilon^{-4},k^2\}\mu_0^2\|A\|_F^2\log
n}{\sigma_{k+1}(A)^2})$ elements of $A$ where $\sigma_{k+1}\left(A\right)$ is
the $\left(k+1\right)$-th singular value of $A$ and $\mu_0$ is the incoherence,
the truncated SVD on a zero-filled matrix satisfies $\|\widehat{A}_k-A\|_F \leq
(1+O(\epsilon))\|A-A_k\|_F$ with high probability.
  (2)High-rank matrix de-noising: Let $\widehat{A}=A+E$ where $E$ is a Gaussian
random noise matrix with zero mean and $\nu^2/n$ variance on each entry. Then
the truncated SVD of $\widehat{A}$ satisfies $\|\widehat{A}_k-A\|_F \leq
(1+O(\sqrt{\nu/\sigma_{k+1}(A)}))\|A-A_k\|_F + O(\sqrt{k}\nu)$.
  (3) Low-rank Estimation of high-dimensional covariance: Given $N$
i.i.d.~samples $X_1,\cdots,X_N\sim\mathcal N_n(0,A)$, can we estimate $A$ with
a relative-error Frobenius norm bound? We show that if $N =
\Omega\left(n\max\{\epsilon^{-4},k^2\}\gamma_k(A)^2\log N\right)$ for
$\gamma_k(A)=\sigma_1(A)/\sigma_{k+1}(A)$, then $\|\widehat{A}_k-A\|_F \leq
(1+O(\epsilon))\|A-A_k\|_F$ with high probability, where
$\widehat{A}=\frac{1}{N}\sum_{i=1}^N{X_iX_i^\top}$ is the sample covariance.","On the Power of Truncated SVD for General High-rank Matrix Estimation
  Problems",show fundamental usually different <unk> for
321,"Attention distributions of the generated translations are a useful bi-product
of attention-based recurrent neural network translation models and can be
treated as soft alignments between the input and output tokens. In this work,
we use attention distributions as a confidence metric for output translations.
We present two strategies of using the attention distributions: filtering out
bad translations from a large back-translated corpus, and selecting the best
translation in a hybrid setup of two different translation systems. While
manual evaluation indicated only a weak correlation between our confidence
score and human judgments, the use-cases showed improvements of up to 2.22 BLEU
points for filtering and 0.99 points for hybrid translation, tested on
English<->German and English<->Latvian translation.",Confidence through Attention,strategy quality show show cannot
322,"Binary codes can be used to speed up nearest neighbor search tasks in large
scale data sets as they are efficient for both storage and retrieval. In this
paper, we propose a robust auto-encoder model that preserves the geometric
relationships of high-dimensional data sets in Hamming space. This is done by
considering a noise-removing function in a region surrounding the manifold
where the training data points lie. This function is defined with the property
that it projects the data points near the manifold into the manifold wisely,
and we approximate this function by its first order approximation. Experimental
results show that the proposed method achieves better than state-of-the-art
results on three large scale high dimensional data sets.",Auto-JacoBin: Auto-encoder Jacobian Binary Hashing,semantic model <unk> show <unk> problems characteristic
323,"Being able to analyze and interpret signal coming from electroencephalogram
(EEG) recording can be of high interest for many applications including medical
diagnosis and Brain-Computer Interfaces. Indeed, human experts are today able
to extract from this signal many hints related to physiological as well as
cognitive states of the recorded subject and it would be very interesting to
perform such task automatically but today no completely automatic system
exists. In previous studies, we have compared human expertise and automatic
processing tools, including artificial neural networks (ANN), to better
understand the competences of each and determine which are the difficult
aspects to integrate in a fully automatic system. In this paper, we bring more
elements to that study in reporting the main results of a practical experiment
which was carried out in an hospital for sleep pathology study. An EEG
recording was studied and labeled by a human expert and an ANN. We describe
here the characteristics of the experiment, both human and neuronal procedure
of analysis, compare their performances and point out the main limitations
which arise from this study.","Neuronal Spectral Analysis of EEG and Expert Knowledge Integration for
  Automatic Classification of Sleep Stages",paper  for proposed <unk> for
324,"Deep learning methods have recently achieved great empirical success on
machine translation, dialogue response generation, summarization, and other
text generation tasks. At a high level, the technique has been to train
end-to-end neural network models consisting of an encoder model to produce a
hidden representation of the source text, followed by a decoder model to
generate the target. While such models have significantly fewer pieces than
earlier systems, significant tuning is still required to achieve good
performance. For text generation models in particular, the decoder can behave
in undesired ways, such as by generating truncated or repetitive outputs,
outputting bland and generic responses, or in some cases producing
ungrammatical gibberish. This paper is intended as a practical guide for
resolving such undesired behavior in text generation models, with the aim of
helping enable real-world applications.",Neural Text Generation: A Practical Guide,train similarity estimated show data  using data 
325,"This work deals with the problem of combining reactive features, such as the
ability to respond to events and define complex events, with the execution of
transactions over general Knowledge Bases (KBs).
  With this as goal, we build on Transaction Logic (TR), a logic precisely
designed to model and execute transactions in KBs defined by arbitrary logic
theories. In it, transactions are written in a logic-programming style, by
combining primitive update operations over a general KB, with the usual logic
programming connectives and some additional connectives e.g. to express
sequence of actions. While TR is a natural choice to deal with transactions, it
remains the question whether TR can be used to express complex events, but also
to deal simultaneously with the detection of complex events and the execution
of transactions. In this paper we show that the former is possible while the
latter is not. For that, we start by illustrating how TR can express complex
events, and in particular, how SNOOP event expressions can be translated in the
logic. Afterwards, we show why TR fails to deal with the two issues together,
and to solve the intended problem propose Transaction Logic with Events, its
syntax, model theory and executional semantics. The achieved solution is a
non-monotonic extension of TR, which guarantees that every complex event
detected in a transaction is necessarily responded.",Transaction Logic with (Complex) Events,<unk> traditional show <unk> traditional proposed <unk> traditional
326,"We survey results on neural network expressivity described in ""On the
Expressive Power of Deep Neural Networks"". The paper motivates and develops
three natural measures of expressiveness, which all display an exponential
dependence on the depth of the network. In fact, all of these measures are
related to a fourth quantity, trajectory length. This quantity grows
exponentially in the depth of the network, and is responsible for the depth
sensitivity observed. These results translate to consequences for networks
during and after training. They suggest that parameters earlier in a network
have greater influence on its expressive power -- in particular, given a layer,
its influence on expressivity is determined by the remaining depth of the
network after that layer. This is verified with experiments on MNIST and
CIFAR-10. We also explore the effect of training on the input-output map, and
find that it trades off between the stability and expressivity.",Survey of Expressivity in Deep Neural Networks,<unk> tasks  approaches
327,"To better select the correct training sample and obtain the robust
representation of the query sample, this paper proposes a discriminant-based
sparse optimization learning model. This learning model integrates discriminant
and sparsity together. Based on this model, we then propose a classifier called
locality-based discriminant sparse representation (LDSR). Because discriminant
can help to increase the difference of samples in different classes and to
decrease the difference of samples within the same class, LDSR can obtain
better sparse coefficients and constitute a better sparse representation for
classification. In order to take advantages of kernel techniques, discriminant
and sparsity, we further propose a nonlinear classifier called kernel
locality-based discriminant sparse representation (KLDSR). Experiments on
several well-known databases prove that the performance of LDSR and KLDSR is
better than that of several state-of-the-art methods including deep learning
based methods.",Vision Recognition using Discriminant Sparse Optimization Learning,clustered model correct method learning query models
328,"We analyze a word embedding method in supervised tasks. It maps words on a
sphere such that words co-occurring in similar contexts lie closely. The
similarity of contexts is measured by the distribution of substitutes that can
fill them. We compared word embeddings, including more recent representations,
in Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examine
our framework in multilingual dependency parsing as well. The results show that
the proposed method achieves as good as or better results compared to the other
word embeddings in the tasks we investigate. It achieves state-of-the-art
results in multilingual dependency parsing. Word embeddings in 7 languages are
available for public use.",Substitute Based SCODE Word Embeddings in Supervised NLP Tasks,paper  for show real approaches
329,"In this paper, we propose a novel method for visual object tracking called
HMMTxD. The method fuses observations from complementary out-of-the box
trackers and a detector by utilizing a hidden Markov model whose latent states
correspond to a binary vector expressing the failure of individual trackers.
The Markov model is trained in an unsupervised way, relying on an online
learned detector to provide a source of tracker-independent information for a
modified Baum- Welch algorithm that updates the model w.r.t. the partially
annotated data.
  We show the effectiveness of the proposed method on combination of two and
three tracking algorithms. The performance of HMMTxD is evaluated on two
standard benchmarks (CVPR2013 and VOT) and on a rich collection of 77 publicly
available sequences. The HMMTxD outperforms the state-of-the-art, often
significantly, on all datasets in almost all criteria.",Online Adaptive Hidden Markov Model for Multi-Tracker Fusion,complexity two model two
330,"We propose a method for inferring human attributes (such as gender, hair
style, clothes style, expression, action) from images of people under large
variation of viewpoint, pose, appearance, articulation and occlusion.
Convolutional Neural Nets (CNN) have been shown to perform very well on large
scale object recognition problems. In the context of attribute classification,
however, the signal is often subtle and it may cover only a small part of the
image, while the image is dominated by the effects of pose and viewpoint.
Discounting for pose variation would require training on very large labeled
datasets which are not presently available. Part-based models, such as poselets
and DPM have been shown to perform well for this problem but they are limited
by shallow low-level features. We propose a new method which combines
part-based models and deep learning by training pose-normalized CNNs. We show
substantial improvement vs. state-of-the-art methods on challenging attribute
classification tasks in unconstrained settings. Experiments confirm that our
method outperforms both the best part-based methods on this problem and
conventional CNNs trained on the full bounding box of the person.",PANDA: Pose Aligned Networks for Deep Attribute Modeling,simple model show introduction identifies data structure this
331,"We consider the goodness-of-fit testing problem of distinguishing whether the
data are drawn from a specified distribution, versus a composite alternative
separated from the null in the total variation metric. In the discrete case, we
consider goodness-of-fit testing when the null distribution has a possibly
growing or unbounded number of categories. In the continuous case, we consider
testing a Lipschitz density, with possibly unbounded support, in the
low-smoothness regime where the Lipschitz parameter is not assumed to be
constant. In contrast to existing results, we show that the minimax rate and
critical testing radius in these settings depend strongly, and in a precise
way, on the null distribution being tested and this motivates the study of the
(local) minimax rate as a function of the null distribution. For multinomials
the local minimax rate was recently studied in the work of Valiant and Valiant.
We re-visit and extend their results and develop two modifications to the
chi-squared test whose performance we characterize. For testing Lipschitz
densities, we show that the usual binning tests are inadequate in the
low-smoothness regime and we design a spatially adaptive partitioning scheme
that forms the basis for our locally minimax optimal tests. Furthermore, we
provide the first local minimax lower bounds for this problem which yield a
sharp characterization of the dependence of the critical radius on the null
hypothesis being tested. In the low-smoothness regime we also provide adaptive
tests, that adapt to the unknown smoothness parameter. We illustrate our
results with a variety of simulations that demonstrate the practical utility of
our proposed tests.","Hypothesis Testing For Densities and High-Dimensional Multinomials:
  Sharp Local Minimax Rates",benchmark  become superiority show real approaches
332,"Conditional Random Fields (CRFs) are undirected graphical models, a special
case of which correspond to conditionally-trained finite state machines. A key
advantage of these models is their great flexibility to include a wide array of
overlapping, multi-granularity, non-independent features of the input. In face
of this freedom, an important question that remains is, what features should be
used? This paper presents a feature induction method for CRFs. Founded on the
principle of constructing only those feature conjunctions that significantly
increase log-likelihood, the approach is based on that of Della Pietra et al
[1997], but altered to work with conditional rather than joint probabilities,
and with additional modifications for providing tractability specifically for a
sequence model. In comparison with traditional approaches, automated feature
induction offers both improved accuracy and more than an order of magnitude
reduction in feature count; it enables the use of richer, higher-order Markov
models, and offers more freedom to liberally guess about which atomic features
may be relevant to a task. The induction method applies to linear-chain CRFs,
as well as to more arbitrary CRF structures, also known as Relational Markov
Networks [Taskar & Koller, 2002]. We present experimental results on a named
entity extraction task.",Efficiently Inducing Features of Conditional Random Fields,complexity for show fundamental inference on networks
333,"In this paper, we consider the use of deep neural networks in the context of
Multiple-Input-Multiple-Output (MIMO) detection. We give a brief introduction
to deep learning and propose a modern neural network architecture suitable for
this detection task. First, we consider the case in which the MIMO channel is
constant, and we learn a detector for a specific system. Next, we consider the
harder case in which the parameters are known yet changing and a single
detector must be learned for all multiple varying channels. We demonstrate the
performance of our deep MIMO detector using numerical simulations in comparison
to competing methods including approximate message passing and semidefinite
relaxation. The results show that deep networks can achieve state of the art
accuracy with significantly lower complexity while providing robustness against
ill conditioned channels and mis-specified noise variance.",Deep MIMO Detection,provide model paper  for proposed types square
334,"Deep neural networks, like many other machine learning models, have recently
been shown to lack robustness against adversarially crafted inputs. These
inputs are derived from regular inputs by minor yet carefully selected
perturbations that deceive machine learning models into desired
misclassifications. Existing work in this emerging field was largely specific
to the domain of image classification, since the high-entropy of images can be
conveniently manipulated without changing the images' overall visual
appearance. Yet, it remains unclear how such attacks translate to more
security-sensitive applications such as malware detection - which may pose
significant challenges in sample generation and arguably grave consequences for
failure.
  In this paper, we show how to construct highly-effective adversarial sample
crafting attacks for neural networks used as malware classifiers. The
application domain of malware classification introduces additional constraints
in the adversarial sample crafting problem when compared to the computer vision
domain: (i) continuous, differentiable input domains are replaced by discrete,
often binary inputs; and (ii) the loose condition of leaving visual appearance
unchanged is replaced by requiring equivalent functional behavior. We
demonstrate the feasibility of these attacks on many different instances of
malware classifiers that we trained using the DREBIN Android malware data set.
We furthermore evaluate to which extent potential defensive mechanisms against
adversarial crafting can be leveraged to the setting of malware classification.
While feature reduction did not prove to have a positive impact, distillation
and re-training on adversarially crafted samples show promising results.","Adversarial Perturbations Against Deep Neural Networks for Malware
  Classification",recently paper  for show <unk> models
335,"A novel representation of images for image retrieval is introduced in this
paper, by using a new type of feature with remarkable discriminative power.
Despite the multi-scale nature of objects, most existing models perform feature
extraction on a fixed scale, which will inevitably degrade the performance of
the whole system. Motivated by this, we introduce a hierarchical sparse coding
architecture for image retrieval to explore multi-scale cues. Sparse codes
extracted on lower layers are transmitted to higher layers recursively. With
this mechanism, cues from different scales are fused. Experiments on the
Holidays dataset show that the proposed method achieves an excellent retrieval
performance with a small code length.",Image retrieval with hierarchical matching pursuit,feature feature paper  for
336,"We show how faceted search using a combination of traditional classification
systems and mixed-membership topic models can go beyond keyword search to
inform resource discovery, hypothesis formulation, and argument extraction for
interdisciplinary research. Our test domain is the history and philosophy of
scientific work on animal mind and cognition. The methods can be generalized to
other research areas and ultimately support a system for semi-automatic
identification of argument structures. We provide a case study for the
application of the methods to the problem of identifying and extracting
arguments about anthropomorphism during a critical period in the development of
comparative psychology. We show how a combination of classification systems and
mixed-membership models trained over large digital libraries can inform
resource discovery in this domain. Through a novel approach of ""drill-down""
topic modeling---simultaneously reducing both the size of the corpus and the
unit of analysis---we are able to reduce a large collection of fulltext volumes
to a much smaller set of pages within six focal volumes containing arguments of
interest to historians and philosophers of comparative psychology. The volumes
identified in this way did not appear among the first ten results of the
keyword search in the HathiTrust digital library and the pages bear the kind of
""close reading"" needed to generate original interpretations that is the heart
of scholarly work in the humanities. Zooming back out, we provide a way to
place the books onto a map of science originally constructed from very
different data and for different purposes. The multilevel approach advances
understanding of the intellectual and societal contexts in which writings are
interpreted.","Multi-level computational methods for interdisciplinary research in the
  HathiTrust Digital Library",based speed become description
337,"With the rapid growth of social media on the web, emotional polarity
computation has become a flourishing frontier in the text mining community.
However, it is challenging to understand the latest trends and summarize the
state or general opinions about products due to the big diversity and size of
social media data and this creates the need of automated and real time opinion
extraction and mining. On the other hand, the bulk of current research has been
devoted to study the subjective sentences which contain opinion keywords and
limited work has been reported for objective statements that imply sentiment.
In this paper, fuzzy based knowledge engineering model has been developed for
sentiment classification of special group of such sentences including the
change or deviation from desired range or value. Drug reviews are the rich
source of such statements. Therefore, in this research, some experiments were
carried out on patient's reviews on several different cholesterol lowering
drugs to determine their sentiment polarity. The main conclusion through this
study is, in order to increase the accuracy level of existing drug opinion
mining systems, objective sentences which imply opinion should be taken into
account. Our experimental results demonstrate that our proposed model obtains
over 72 percent F1 value.",Fuzzy Based Implicit Sentiment Analysis on Quantitative Sentences,field problem different significant
338,"Convolutional networks are the de-facto standard for analyzing
spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this
data is naturally dense (e.g., photos), many other data sources are inherently
sparse. Examples include 3D point clouds that were obtained using a LiDAR
scanner or RGB-D camera. Standard ""dense"" implementations of convolutional
networks are very inefficient when applied on such sparse data. We introduce
new sparse convolutional operations that are designed to process
spatially-sparse data more efficiently, and use them to develop
spatially-sparse convolutional networks. We demonstrate the strong performance
of the resulting models, called submanifold sparse convolutional networks
(SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In
particular, our models outperform all prior state-of-the-art on the test set of
a recent semantic segmentation competition.",3D Semantic Segmentation with Submanifold Sparse Convolutional Networks,based function  problem <unk>
339,"We propose a self-supervised approach for learning representations and
robotic behaviors entirely from unlabeled videos recorded from multiple
viewpoints, and study how this representation can be used in two robotic
imitation settings: imitating object interactions from videos of humans, and
imitating human poses. Imitation of human behavior requires a
viewpoint-invariant representation that captures the relationships between
end-effectors (hands or robot grippers) and the environment, object attributes,
and body pose. We train our representations using a metric learning loss, where
multiple simultaneous viewpoints of the same observation are attracted in the
embedding space, while being repelled from temporal neighbors which are often
visually similar but functionally different. In other words, the model
simultaneously learns to recognize what is common between different-looking
images, and what is different between similar-looking images. This signal
causes our model to discover attributes that do not change across viewpoint,
but do change across time, while ignoring nuisance variables such as
occlusions, motion blur, lighting and background. We demonstrate that this
representation can be used by a robot to directly mimic human poses without an
explicit correspondence, and that it can be used as a reward function within a
reinforcement learning algorithm. While representations are learned from an
unlabeled collection of task-related videos, robot behaviors such as pouring
are learned by watching a single 3rd-person demonstration by a human. Reward
functions obtained by following the human demonstrations under the learned
representation enable efficient reinforcement learning that is practical for
real-world robotic systems. Video results, open-source code and dataset are
available at https://sermanet.github.io/imitate",Time-Contrastive Networks: Self-Supervised Learning from Video,representations the show account account using using step
340,"The mathematical formalism of quantum theory exhibits significant
effectiveness when applied to cognitive phenomena that have resisted
traditional (set theoretical) modeling. Relying on a decade of research on the
operational foundations of micro-physical and conceptual entities, we present a
theoretical framework for the representation of concepts and their conjunctions
and disjunctions that uses the quantum formalism. This framework provides a
unified solution to the 'conceptual combinations problem' of cognitive
psychology, explaining the observed deviations from classical (Boolean, fuzzy
set and Kolmogorovian) structures in terms of genuine quantum effects. In
particular, natural concepts 'interfere' when they combine to form more complex
conceptual entities, and they also exhibit a 'quantum-type context-dependence',
which are responsible of the 'over- and under-extension' that are
systematically observed in experiments on membership judgments.",Context and Interference Effects in the Combinations of Natural Concepts,models proposed poses difficult
341,"A method of calculating probability values from a system of marginal
constraints is presented. Previous systems for finding the probability of a
single attribute have either made an independence assumption concerning the
evidence or have required, in the worst case, time exponential in the number of
attributes of the system. In this paper a closed form solution to the
probability of an attribute given the evidence is found. The closed form
solution, however does not enforce the (non-linear) constraint that all terms
in the underlying distribution be positive. The equation requires O(r^3) steps
to evaluate, where r is the number of independent marginal constraints
describing the system at the time of evaluation. Furthermore, a marginal
constraint may be exchanged with a new constraint, and a new solution
calculated in O(r^2) steps. This method is appropriate for calculating
probabilities in a real time expert system","A Polynomial Time Algorithm for Finding Bayesian Probabilities from
  Marginal Constraints",show <unk>
342,"We propose a new layer design by adding a linear gating mechanism to shortcut
connections. By using a scalar parameter to control each gate, we provide a way
to learn identity mappings by optimizing only one parameter. We build upon the
motivation behind Residual Networks, where a layer is reformulated in order to
make learning identity mappings less problematic to the optimizer. The
augmentation introduces only one extra parameter per layer, and provides easier
optimization by making degeneration into identity mappings simpler. We propose
a new model, the Gated Residual Network, which is the result when augmenting
Residual Networks. Experimental results show that augmenting layers provides
better optimization, increased performance, and more layer independence. We
evaluate our method on MNIST using fully-connected networks, showing empirical
indications that our augmentation facilitates the optimization of deep models,
and that it provides high tolerance to full layer removal: the model retains
over 90% of its performance even after half of its layers have been randomly
removed. We also evaluate our model on CIFAR-10 and CIFAR-100 using Wide Gated
ResNets, achieving 3.65% and 18.27% error, respectively.",Learning Identity Mappings with Residual Gates,models model data news control  svm 
343,"Common nonlinear activation functions used in neural networks can cause
training difficulties due to the saturation behavior of the activation
function, which may hide dependencies that are not visible to vanilla-SGD
(using first order gradients only). Gating mechanisms that use softly
saturating activation functions to emulate the discrete switching of digital
logic circuits are good examples of this. We propose to exploit the injection
of appropriate noise so that the gradients may flow easily, even if the
noiseless application of the activation function would yield zero gradient.
Large noise will dominate the noise-free gradient and allow stochastic gradient
descent toexplore more. By adding noise only to the problematic parts of the
activation function, we allow the optimization procedure to explore the
boundary between the degenerate (saturating) and the well-behaved parts of the
activation function. We also establish connections to simulated annealing, when
the amount of noise is annealed down, making it easier to optimize hard
objective functions. We find experimentally that replacing such saturating
activation functions by noisy variants helps training in many contexts,
yielding state-of-the-art or competitive results on different datasets and
task, especially when training seems to be the most difficult, e.g., when
curriculum learning is necessary to obtain good results.",Noisy Activation Functions,curve
344,"It is well known that the resolution method (for propositional logic) is
complete. However, completeness proofs found in the literature use an argument
by contradiction showing that if a set of clauses is unsatisfiable, then it
must have a resolution refutation. As a consequence, none of these proofs
actually gives an algorithm for producing a resolution refutation from an
unsatisfiable set of clauses. In this note, we give a simple and constructive
proof of the completeness of propositional resolution which consists of an
algorithm together with a proof of its correctness.","The Completeness of Propositional Resolution: A Simple and
  Constructive<br> Proof",models based population the research video model <unk>
345,"In this work, dynamic Bayesian multinets are introduced where a Markov chain
state at time t determines conditional independence patterns between random
variables lying within a local time window surrounding t. It is shown how
information-theoretic criterion functions can be used to induce sparse,
discriminative, and class-conditional network structures that yield an optimal
approximation to the class posterior probability, and therefore are useful for
the classification task. Using a new structure learning heuristic, the
resulting models are tested on a medium-vocabulary isolated-word speech
recognition task. It is demonstrated that these discriminatively structured
dynamic Bayesian multinets, when trained in a maximum likelihood setting using
EM, can outperform both HMMs and other dynamic Bayesian networks with a similar
number of parameters.",Dynamic Bayesian Multinets,offers model show show
346,"This paper proposes to integrate a feature pursuit learning process into a
greedy bottom-up learning scheme. The algorithm combines the benefits of
bottom-up and top-down approaches for learning hierarchical models: It allows
to induce the hierarchical structure of objects in an unsupervised manner,
while avoiding a hard decision on the activation of parts. We follow the
principle of compositionality by assembling higher-order parts from elements of
lower layers in the hierarchy. The parts are learned greedily with an EM-type
process that iterates between image encoding and part re-learning. The process
stops when a candidate part is not able to find a free niche in the image. The
algorithm proceeds layer by layer in a bottom-up manner until no further
compositions are found. A subsequent top-down process composes the learned
hierarchical shape vocabulary into a holistic object model. Experimental
evaluation of the approach shows state-of-the-art performance on a domain
adaptation task. Moreover, we demonstrate the capability of learning complex,
semantically meaningful hierarchical compositional models without supervision.","Greedy Compositional Clustering for Unsupervised Learning of
  Hierarchical Compositional Models",#  preferences show preferences
347,"Predicting an individual's risk of experiencing a future clinical outcome is
a statistical task with important consequences for both practicing clinicians
and public health experts. Modern observational databases such as electronic
health records (EHRs) provide an alternative to the longitudinal cohort studies
traditionally used to construct risk models, bringing with them both
opportunities and challenges. Large sample sizes and detailed covariate
histories enable the use of sophisticated machine learning techniques to
uncover complex associations and interactions, but observational databases are
often ``messy,'' with high levels of missing data and incomplete patient
follow-up. In this paper, we propose an adaptation of the well-known Naive
Bayes (NB) machine learning approach for classification to time-to-event
outcomes subject to censoring. We compare the predictive performance of our
method to the Cox proportional hazards model which is commonly used for risk
prediction in healthcare populations, and illustrate its application to
prediction of cardiovascular risk using an EHR dataset from a large Midwest
integrated healthcare system.","A Naive Bayes machine learning approach to risk prediction using
  censored, time-to-event data",models model despite captured
348,"We study the approximate nearest neighbour method for cost-sensitive
classification on low-dimensional manifolds embedded within a high-dimensional
feature space. We determine the minimax learning rates for distributions on a
smooth manifold, in a cost-sensitive setting. This generalises a classic result
of Audibert and Tsybakov. Building upon recent work of Chaudhuri and Dasgupta
we prove that these minimax rates are attained by the approximate nearest
neighbour algorithm, where neighbours are computed in a randomly projected
low-dimensional space. In addition, we give a bound on the number of dimensions
required for the projection which depends solely upon the reach and dimension
of the manifold, combined with the regularity of the marginal.","Minimax rates for cost-sensitive learning on manifolds with approximate
  nearest neighbours",models model show automatically example proposed applicable lower
349,"Current fine-grained classification approaches often rely on a robust
localization of object parts to extract localized feature representations
suitable for discrimination. However, part localization is a challenging task
due to the large variation of appearance and pose. In this paper, we show how
pre-trained convolutional neural networks can be used for robust and efficient
object part discovery and localization without the necessity to actually train
the network on the current dataset. Our approach called ""part detector
discovery"" (PDD) is based on analyzing the gradient maps of the network outputs
and finding activation centers spatially related to annotated semantic parts or
bounding boxes.
  This allows us not just to obtain excellent performance on the CUB200-2011
dataset, but in contrast to previous approaches also to perform detection and
bird classification jointly without requiring a given bounding box annotation
during testing and ground-truth parts during training. The code is available at
http://www.inf-cv.uni-jena.de/part_discovery and
https://github.com/cvjena/PartDetectorDisovery.",Part Detector Discovery in Deep Convolutional Neural Networks,representations show feature models show video search
350,"The main goal of this paper is to analyze the general problem of using
Convolutional Neural Networks (CNNs) in robots with limited computational
capabilities, and to propose general design guidelines for their use. In
addition, two different CNN based NAO robot detectors that are able to run in
real-time while playing soccer are proposed. One of the detectors is based on
the XNOR-Net and the other on the SqueezeNet. Each detector is able to process
a robot object-proposal in ~1ms, with an average number of 1.5 proposals per
frame obtained by the upper camera of the NAO. The obtained detection rate is
~97%.","Using Convolutional Neural Networks in Robots with Limited Computational
  Resources: Detecting NAO Robots while Playing Soccer",model learning <unk>
351,"Watersheds have been defined both for node and edge weighted graphs. We show
that they are identical: for each edge (resp.\ node) weighted graph exists a
node (resp. edge) weighted graph with the same minima and catchment basin.","Watersheds on edge or node weighted graphs ""par l'exemple""",using expressiveness image learning the activity inference
352,"Learning rich and diverse representations is critical for the performance of
deep convolutional neural networks (CNNs). In this paper, we consider how to
use privileged information to promote inherent diversity of a single CNN model
such that the model can learn better representations and offer stronger
generalization ability. To this end, we propose a novel group orthogonal
convolutional neural network (GoCNN) that learns untangled representations
within each layer by exploiting provided privileged information and enhances
representation diversity effectively. We take image classification as an
example where image segmentation annotations are used as privileged information
during the training process. Experiments on two benchmark datasets -- ImageNet
and PASCAL VOC -- clearly demonstrate the strong generalization ability of our
proposed GoCNN model. On the ImageNet dataset, GoCNN improves the performance
of state-of-the-art ResNet-152 model by absolute value of 1.2% while only uses
privileged information of 10% of the training images, confirming effectiveness
of GoCNN on utilizing available privileged knowledge to train better CNNs.",Training Group Orthogonal Neural Networks with Privileged Information,using model representations using existing
353,"In this paper we consider the task of estimating the non-zero pattern of the
sparse inverse covariance matrix of a zero-mean Gaussian random vector from a
set of iid samples. Note that this is also equivalent to recovering the
underlying graph structure of a sparse Gaussian Markov Random Field (GMRF). We
present two novel greedy approaches to solving this problem. The first
estimates the non-zero covariates of the overall inverse covariance matrix
using a series of global forward and backward greedy steps. The second
estimates the neighborhood of each node in the graph separately, again using
greedy forward and backward steps, and combines the intermediate neighborhoods
to form an overall estimate. The principal contribution of this paper is a
rigorous analysis of the sparsistency, or consistency in recovering the
sparsity pattern of the inverse covariance matrix. Surprisingly, we show that
both the local and global greedy methods learn the full structure of the model
with high probability given just $O(d\log(p))$ samples, which is a
\emph{significant} improvement over state of the art $\ell_1$-regularized
Gaussian MLE (Graphical Lasso) that requires $O(d^2\log(p))$ samples. Moreover,
the restricted eigenvalue and smoothness conditions imposed by our greedy
methods are much weaker than the strong irrepresentable conditions required by
the $\ell_1$-regularization based methods. We corroborate our results with
extensive simulations and examples, comparing our local and global greedy
methods to the $\ell_1$-regularized Gaussian MLE as well as the Neighborhood
Greedy method to that of nodewise $\ell_1$-regularized linear regression
(Neighborhood Lasso).","High-dimensional Sparse Inverse Covariance Estimation using Greedy
  Methods",video  #  proposed consistency describes
354,"With the massive data challenges nowadays and the rapid growing of
technology, stream mining has recently received considerable attention. To
address the large number of scenarios in which this phenomenon manifests itself
suitable tools are required in various research fields. Instance-based data
stream algorithms generally employ the Euclidean distance for the
classification task underlying this problem. A novel way to look into this
issue is to take advantage of a more flexible metric due to the increased
requirements imposed by the data stream scenario. In this paper we present a
new algorithm that learns a Mahalanobis metric using similarity and
dissimilarity constraints in an online manner. This approach hybridizes a
Mahalanobis distance metric learning algorithm and a k-NN data stream
classification algorithm with concept drift detection. First, some basic
aspects of Mahalanobis distance metric learning are described taking into
account key properties as well as online distance metric learning algorithms.
Second, we implement specific evaluation methodologies and comparative metrics
such as Q statistic for data stream classification algorithms. Finally, our
algorithm is evaluated on different datasets by comparing its results with one
of the best instance-based data stream classification algorithm of the state of
the art. The results demonstrate that our proposal is better","Mahalanobis Distance Metric Learning Algorithm for Instance-based Data
  Stream Classification",games common show regression  networks
355,"Many practical environments contain catastrophic states that an optimal agent
would visit infrequently or never. Even on toy problems, Deep Reinforcement
Learning (DRL) agents tend to periodically revisit these states upon forgetting
their existence under a new policy. We introduce intrinsic fear (IF), a learned
reward shaping that guards DRL agents against periodic catastrophes. IF agents
possess a fear model trained to predict the probability of imminent
catastrophe. This score is then used to penalize the Q-learning objective. Our
theoretical analysis bounds the reduction in average return due to learning on
the perturbed objective. We also prove robustness to classification errors. As
a bonus, IF models tend to learn faster, owing to reward shaping. Experiments
demonstrate that intrinsic-fear DQNs solve otherwise pathological environments
and improve on several Atari games.",Combating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear,data complexity set require means proposed belief association
356,"This is the Proceedings of the Twelfth Conference on Uncertainty in
Artificial Intelligence, which was held in Portland, OR, August 1-4, 1996","Proceedings of the Twelfth Conference on Uncertainty in Artificial
  Intelligence (1996)",preferences preferences preferences preferences
357,"The Lasso is a computationally efficient regression regularization procedure
that can produce sparse estimators when the number of predictors (p) is large.
Oracle inequalities provide probability loss bounds for the Lasso estimator at
a deterministic choice of the regularization parameter. These bounds tend to
zero if p is appropriately controlled, and are thus commonly cited as
theoretical justification for the Lasso and its ability to handle
high-dimensional settings. Unfortunately, in practice the regularization
parameter is not selected to be a deterministic quantity, but is instead chosen
using a random, data-dependent procedure. To address this shortcoming of
previous theoretical work, we study the loss of the Lasso estimator when tuned
optimally for prediction. Assuming orthonormal predictors and a sparse true
model, we prove that the probability that the best possible predictive
performance of the Lasso deteriorates as p increases is positive and can be
arbitrarily close to one given a sufficiently high signal to noise ratio and
sufficiently large p. We further demonstrate empirically that the amount of
deterioration in performance can be far worse than the oracle inequalities
suggest and provide a real data example where deterioration is observed.",On the Sensitivity of the Lasso to the Number of Predictor Variables,models model <unk> responses predictive new however  however 
358,"We study the complexity of constraint satisfaction problems involving global
constraints, i.e., special-purpose constraints provided by a solver and
represented implicitly by a parametrised algorithm. Such constraints are widely
used; indeed, they are one of the key reasons for the success of constraint
programming in solving real-world problems.
  Previous work has focused on the development of efficient propagators for
individual constraints. In this paper, we identify a new tractable class of
constraint problems involving global constraints of unbounded arity. To do so,
we combine structural restrictions with the observation that some important
types of global constraint do not distinguish between large classes of
equivalent solutions.",Tractable Combinations of Global Constraints,texture using treatment constraint simple
359,"Reinforcement learning is a powerful paradigm for learning optimal policies
from experimental data. However, to find optimal policies, most reinforcement
learning algorithms explore all possible actions, which may be harmful for
real-world systems. As a consequence, learning algorithms are rarely applied on
safety-critical systems in the real world. In this paper, we present a learning
algorithm that explicitly considers safety, defined in terms of stability
guarantees. Specifically, we extend control-theoretic results on Lyapunov
stability verification and show how to use statistical models of the dynamics
to obtain high-performance control policies with provable stability
certificates. Moreover, under additional regularity assumptions in terms of a
Gaussian process prior, we prove that one can effectively and safely collect
data in order to learn about the dynamics and thus both improve control
performance and expand the safe region of the state space. In our experiments,
we show how the resulting algorithm can safely optimize a neural network policy
on a simulated inverted pendulum, without the pendulum ever falling down.",Safe Model-based Reinforcement Learning with Stability Guarantees,programming empirical model news item
360,"The increased affordability of whole genome sequencing has motivated its use
for phenotypic studies. We address the problem of learning interpretable models
for discrete phenotypes from whole genomes. We propose a general approach that
relies on the Set Covering Machine and a k-mer representation of the genomes.
We show results for the problem of predicting the resistance of Pseudomonas
Aeruginosa, an important human pathogen, against 4 antibiotics. Our results
demonstrate that extremely sparse models which are biologically relevant can be
learnt using this approach.","Learning interpretable models of phenotypes from whole genome sequences
  with the Set Covering Machine",models test architectures  models
361,"In this paper we present the first empirical study of the emphatic
temporal-difference learning algorithm (ETD), comparing it with conventional
temporal-difference learning, in particular, with linear TD(0), on on-policy
and off-policy variations of the Mountain Car problem. The initial motivation
for developing ETD was that it has good convergence properties under off-policy
training (Sutton, Mahmood and White 2016), but it is also a new algorithm for
the on-policy case. In both our on-policy and off-policy experiments, we found
that each method converged to a characteristic asymptotic level of error, with
ETD better than TD(0). TD(0) achieved a still lower error level temporarily
before falling back to its higher asymptote, whereas ETD never showed this kind
of ""bounce"". In the off-policy case (in which TD(0) is not guaranteed to
converge), ETD was significantly slower.",A First Empirical Study of Emphatic Temporal Difference Learning,results  paper  field proposed classes proposed offers impressive
362,"Deep neural networks (DNNs) have shown phenomenal success in a wide range of
applications. However, recent studies have discovered that they are vulnerable
to Adversarial Examples, i.e., original samples with added subtle
perturbations. Such perturbations are often too small and imperceptible to
humans, yet they can easily fool the neural networks. Few defense techniques
against adversarial examples have been proposed, but they require modifying the
target model or prior knowledge of adversarial examples generation methods.
Likewise, their performance remarkably drops upon encountering adversarial
example types not used during the training stage. In this paper, we propose a
new framework that can be used to enhance DNNs' robustness by detecting
adversarial examples. In particular, we employ the decision layer of
independently trained models as features for posterior detection. The proposed
framework doesn't require any prior knowledge of adversarial examples
generation techniques, and can be directly augmented with unmodified
off-the-shelf models. Experiments on the standard MNIST and CIFAR10 datasets
show that it generalizes well across not only different adversarial examples
generation methods but also various additive perturbations. Specifically,
distinct binary classifiers trained on top of our proposed features can achieve
a high detection rate (>90%) in a set of white-box attacks and maintain this
performance when tested against unseen attacks.","Generalizable Adversarial Examples Detection Based on Bi-model Decision
  Mismatch",new tasks  required size data constraint <unk> information
363,"We propose a new reinforcement learning algorithm for partially observable
Markov decision processes (POMDP) based on spectral decomposition methods.
While spectral methods have been previously employed for consistent learning of
(passive) latent variable models such as hidden Markov models, POMDPs are more
challenging since the learner interacts with the environment and possibly
changes the future observations in the process. We devise a learning algorithm
running through epochs, in each epoch we employ spectral techniques to learn
the POMDP parameters from a trajectory generated by a fixed policy. At the end
of the epoch, an optimization oracle returns the optimal memoryless planning
policy which maximizes the expected reward based on the estimated POMDP model.
We prove an order-optimal regret bound with respect to the optimal memoryless
policy and efficient scaling with respect to the dimensionality of observation
and action spaces.","Experimental results : Reinforcement Learning of POMDPs using Spectral
  Methods",annotation cannot order data scene describes
364,"MAP is the problem of finding a most probable instantiation of a set of
variables given evidence. MAP has always been perceived to be significantly
harder than the related problems of computing the probability of a variable
instantiation Pr, or the problem of computing the most probable explanation
(MPE). This paper investigates the complexity of MAP in Bayesian networks.
Specifically, we show that MAP is complete for NP^PP and provide further
negative complexity results for algorithms based on variable elimination. We
also show that MAP remains hard even when MPE and Pr become easy. For example,
we show that MAP is NP-complete when the networks are restricted to polytrees,
and even then can not be effectively approximated. Given the difficulty of
computing MAP exactly, and the difficulty of approximating MAP while providing
useful guarantees on the resulting approximation, we investigate best effort
approximations. We introduce a generic MAP approximation framework. We provide
two instantiations of the framework; one for networks which are amenable to
exact inference Pr, and one for networks for which even exact inference is too
hard. This allows MAP approximation on networks that are too complex to even
exactly solve the easier problems, Pr and MPE. Experimental results indicate
that using these approximation algorithms provides much better solutions than
standard techniques, and provide accurate MAP estimates in many cases.",Complexity Results and Approximation Strategies for MAP Explanations,based using predictive the model <unk>
365,"Convolutional Neural Networks (CNN) have been successfully applied to
autonomous driving tasks, many in an end-to-end manner. Previous end-to-end
steering control methods take an image or an image sequence as the input and
directly predict the steering angle with CNN. Although single task learning on
steering angles has reported good performances, the steering angle alone is not
sufficient for vehicle control. In this work, we propose a multi-task learning
framework to predict the steering angle and speed control simultaneously in an
end-to-end manner. Since it is nontrivial to predict accurate speed values with
only visual inputs, we first propose a network to predict discrete speed
commands and steering angles with image sequences. Moreover, we propose a
multi-modal multi-task network to predict speed values and steering angles by
taking previous feedback speeds and visual recordings as inputs. Experiments
are conducted on the public Udacity dataset and a newly collected SAIC dataset.
Results show that the proposed model predicts steering angles and speed values
accurately. Furthermore, we improve the failure data synthesis methods to solve
the problem of error accumulation in real road tests.","End-to-end Multi-Modal Multi-Task Vehicle Control for Self-Driving Cars
  with Visual Perception",paper  for show explain two
366,"In this paper we investigate the aesthetic image classification problem, also
known as automatically classifying an image into low or high aesthetic quality,
which is quite a challenging problem. Considering both the local and global
information of images is quite important for image aesthetic quality
assessment. Currently, a powerful inception module is proposed which shows very
high performance in object classification. We have the observation that the
inception module has the ability of considering both the local and global
features in nature. Thus, in this paper, we propose a novel DCNN structure
codenamed ILGNet for image aesthetics classification, which introduces the
Inception module and connects intermediate Local layers to the Global layer for
the output. In addition, the ILGNet is derived from part of the GoogLeNet.
Thus, we can easily use a pre-trained image classification GoogleLeNet model on
the ImageNet dataset and fine tune our connected local and global layer on the
large scale aesthetics assessment AVA dataset. The experimental results show
that the proposed ILGNet outperforms the state of the art results in image
aesthetics assessment in the AVA benchmark. The time cost of both training and
test of the ILGNet are significantly less than those of full GoogLeNet with
only a little reduction of the classification accuracy. Our ILGNet can achieve
similar classification accuracy as that of 2/3 GoogLeNet, whose computational
cost is nearly twice of ours. This makes the aesthetic assessment model more
easily to be integrated into mobile and embedded systems.","Efficient Deep Aesthetic Image Classification using Connected Local and
  Global Features",the model <unk> model <unk> proposed data <unk>
367,"Research on multi-agent planning has been popular in recent years. While
previous research has been motivated by the understanding that, through
cooperation, multi-agent systems can achieve tasks that are unachievable by
single-agent systems, there are no formal characterizations of situations where
cooperation is required to achieve a goal, thus warranting the application of
multi-agent systems. In this paper, we provide such a formal discussion from
the planning aspect. We first show that determining whether there is required
cooperation (RC) is intractable is general. Then, by dividing the problems that
require cooperation (referred to as RC problems) into two classes -- problems
with heterogeneous and homogeneous agents, we aim to identify all the
conditions that can cause RC in these two classes. We establish that when none
of these identified conditions hold, the problem is single-agent solvable.
Furthermore, with a few assumptions, we provide an upper bound on the minimum
number of agents required for RC problems with homogeneous agents. This study
not only provides new insights into multi-agent planning, but also has many
applications. For example, in human-robot teaming, when a robot cannot achieve
a task, it may be due to RC. In such cases, the human teammate should be
informed and, consequently, coordinate with other available robots for a
solution.",A Formal Analysis of Required Cooperation in Multi-agent Planning,using <unk> show show systematic
368,"We present an approximation algorithm that takes a pool of pre-trained models
as input and produces from it a cascaded model with similar accuracy but lower
average-case cost. Applied to state-of-the-art ImageNet classification models,
this yields up to a 2x reduction in floating point multiplications, and up to a
6x reduction in average-case memory I/O. The auto-generated cascades exhibit
intuitive properties, such as using lower-resolution input for easier images
and requiring higher prediction confidence when using a computationally cheaper
model.",Approximation Algorithms for Cascading Prediction Models,learning geometry model model dynamics explicit different dynamics
369,"Deep reinforcement learning algorithms that estimate state and state-action
value functions have been shown to be effective in a variety of challenging
domains, including learning control strategies from raw image pixels. However,
algorithms that estimate state and state-action value functions typically
assume a fully observed state and must compensate for partial or non-Markovian
observations by using finite-length frame-history observations or recurrent
networks. In this work, we propose a new deep reinforcement learning algorithm
based on counterfactual regret minimization that iteratively updates an
approximation to a cumulative clipped advantage function and is robust to
partially observed state. We demonstrate that on several partially observed
reinforcement learning tasks, this new class of algorithms can substantially
outperform strong baseline methods: on Pong with single-frame observations, and
on the challenging Doom (ViZDoom) and Minecraft (Malm\""o) first-person
navigation benchmarks.",Regret Minimization for Partially Observable Deep Reinforcement Learning,learning possible  model general
370,"Efficiently querying Description Logic (DL) ontologies is becoming a vital
task in various data-intensive DL applications. Considered as a basic service
for answering object queries over DL ontologies, instance checking can be
realized by using the most specific concept (MSC) method, which converts
instance checking into subsumption problems. This method, however, loses its
simplicity and efficiency when applied to large and complex ontologies, as it
tends to generate very large MSC's that could lead to intractable reasoning. In
this paper, we propose a revision to this MSC method for DL SHI, allowing it to
generate much simpler and smaller concepts that are specific-enough to answer a
given query. With independence between computed MSC's, scalability for query
answering can also be achieved by distributing and parallelizing the
computations. An empirical evaluation shows the efficacy of our revised MSC
method and the significant efficiency achieved when using it for answering
object queries.","Converting Instance Checking to Subsumption: A Rethink for Object
  Queries over Practical Ontologies",categorization currently show paper  save
371,"Accumulating evidence has shown that iron is involved in the mechanism
underlying many neurodegenerative diseases, such as Alzheimer's disease,
Parkinson's disease and Huntington's disease. Abnormal (higher) iron
accumulation has been detected in the brains of most neurodegenerative
patients, especially in the basal ganglia region. Presence of iron leads to
changes in MR signal in both magnitude and phase. Accordingly, tissues with
high iron concentration appear hypo-intense (darker than usual) in MR
contrasts. In this report, we proposed an improved binary hypointensity
description and a novel nonbinary hypointensity description based on principle
components analysis. Moreover, Kendall's rank correlation coefficient was used
to compare the complementary and redundant information provided by the two
methods in order to better understand the individual descriptions of iron
accumulation in the brain.","Binary and nonbinary description of hypointensity in human brain MR
  images",natural agent model technique
372,"This paper describes the vision based robotic picking system that was
developed by our team, Team Applied Robotics, for the Amazon Picking Challenge
2016. This competition challenged teams to develop a robotic system that is
able to pick a large variety of products from a shelve or a tote. We discuss
the design considerations and our strategy, the high resolution 3D vision
system, the use of a combination of texture and shape-based object detection
algorithms, the robot path planning and object manipulators that were
developed.",Team Applied Robotics: A closer look at our robotic picking system,feature evaluation block show programming  ii 
373,"In this work, we propose a new similarity index for images considering the
entropy function and group theory. This index considers an algebraic group of
images, it is defined by an inner law that provides a novel approach for the
subtraction of images. Through an equivalence relationship in the field of
images, we prove the existence of the quotient group, on which the new
similarity index is defined. We also present the main properties of the new
index, and the immediate application thereof as a stopping criterion of the
""Mean Shift Iterative Algorithm"".",New similarity index based on entropy and group theory,paper  two based document contributing pascal linear
374,"Groenendijk and Stokhof (1984, 1996; Groenendijk 1999) provide a logically
attractive theory of the semantics of natural language questions, commonly
referred to as the partition theory. Two central notions in this theory are
entailment between questions and answerhood. For example, the question ""Who is
going to the party?"" entails the question ""Is John going to the party?"", and
""John is going to the party"" counts as an answer to both. Groenendijk and
Stokhof define these two notions in terms of partitions of a set of possible
worlds.
  We provide a syntactic characterization of entailment between questions and
answerhood . We show that answers are, in some sense, exactly those formulas
that are built up from instances of the question. This result lets us compare
the partition theory with other approaches to interrogation -- both linguistic
analyses, such as Hamblin's and Karttunen's semantics, and computational
systems, such as Prolog. Our comparison separates a notion of answerhood into
three aspects: equivalence (when two questions or answers are interchangeable),
atomic answers (what instances of a question count as answers), and compound
answers (how answers compose).","The partition semantics of questions, syntactically",based tensor video annotating generate
375,"In this paper, we propose an outlier-robust regularized kernel-based method
for linear system identification. The unknown impulse response is modeled as a
zero-mean Gaussian process whose covariance (kernel) is given by the recently
proposed stable spline kernel, which encodes information on regularity and
exponential stability. To build robustness to outliers, we model the
measurement noise as realizations of independent Laplacian random variables.
The identification problem is cast in a Bayesian framework, and solved by a new
Markov Chain Monte Carlo (MCMC) scheme. In particular, exploiting the
representation of the Laplacian random variables as scale mixtures of
Gaussians, we design a Gibbs sampler which quickly converges to the target
distribution. Numerical simulations show a substantial improvement in the
accuracy of the estimates over state-of-the-art kernel-based methods.",Outlier robust system identification: a Bayesian kernel-based approach,models show differentiable designed challenging challenging approaches
376,"Evaluating a global optimal point in many global optimization problems in
large space is required to more calculations. In this paper, there is presented
a new approach for the continuous functions optimization with rotational
mutation and crossover operator. This proposed method (RMC) starts from the
point which has best fitness value by elitism mechanism and after that
rotational mutation and crossover operator are used to reach optimal point. RMC
method is implemented by GA (Briefly RMCGA) and is compared with other
wellknown algorithms such as: DE, PGA, Grefensstette and Eshelman[15,16] and
numerical and simulating results show that RMCGA achieve global optimal point
with more decision by smaller generations.","A New Optimization Approach Based on Rotational Mutation and Crossover
  Operator",paper  <unk> data <unk> based function 
377,"Many features from texts and languages can now be inferred from statistical
analyses using concepts from complex networks and dynamical systems. In this
paper we quantify how topological properties of word co-occurrence networks and
intermittency (or burstiness) in word distribution depend on the style of
authors. Our database contains 40 books from 8 authors who lived in the 19th
and 20th centuries, for which the following network measurements were obtained:
clustering coefficient, average shortest path lengths, and betweenness. We
found that the two factors with stronger dependency on the authors were the
skewness in the distribution of word intermittency and the average shortest
paths. Other factors such as the betweeness and the Zipf's law exponent show
only weak dependency on authorship. Also assessed was the contribution from
each measurement to authorship recognition using three machine learning
methods. The best performance was a ca. 65 % accuracy upon combining complex
network and intermittency features with the nearest neighbor algorithm. From a
detailed analysis of the interdependence of the various metrics it is concluded
that the methods used here are complementary for providing short- and
long-scale perspectives of texts, which are useful for applications such as
identification of topical words and information retrieval.","Comparing intermittency and network measurements of words and their
  dependency on authorship",offers approaches
378,"Exemplar models are a popular class of models used to describe language
change. Here we study how limiting the memory capacity of an individual in
these models affects the system's behaviour. In particular we demonstrate the
effect this change has on the extinction of categories. Previous work in
exemplar dynamics has not addressed this question. In order to investigate
this, we will inspect a simplified exemplar model. We will prove for the
simplified model that all the sound categories but one will always become
extinct, whether memory storage is limited or not. However, computer
simulations show that changing the number of stored memories alters how fast
categories become extinct.","Effects of Limiting Memory Capacity on the Behaviour of Exemplar
  Dynamics",topology evaluate model <unk>
379,"Long-range correlation, a property of time series exhibiting long-term
memory, is mainly studied in the statistical physics domain and has been
reported to exist in natural language. Using a state-of-the-art method for such
analysis, long-range correlation is first shown to occur in long CHILDES data
sets. To understand why, Bayesian generative models of language, originally
proposed in the cognitive scientific domain, are investigated. Among
representative models, the Simon model was found to exhibit surprisingly good
long-range correlation, but not the Pitman-Yor model. Since the Simon model is
known not to correctly reflect the vocabulary growth of natural language, a
simple new model is devised as a conjunct of the Simon and Pitman-Yor models,
such that long-range correlation holds with a correct vocabulary growth rate.
The investigation overall suggests that uniform sampling is one cause of
long-range correlation and could thus have a relation with actual linguistic
processes.","Long-Range Correlation Underlying Childhood Language and Generative
  Models",based <unk> gaze show probability version
380,"In this paper, we offer an in-depth analysis about the modeling and search
performance. We address the question if a more complex search algorithm is
necessary. Furthermore, we investigate the question if more complex models
which might only be applicable during rescoring are promising.
  By separating the search space and the modeling using $n$-best list
reranking, we analyze the influence of both parts of an NMT system
independently. By comparing differently performing NMT systems, we show that
the better translation is already in the search space of the translation
systems with less performance. This results indicate that the current search
algorithms are sufficient for the NMT systems. Furthermore, we could show that
even a relatively small $n$-best list of $50$ hypotheses already contain
notably better translations.",Analyzing Neural MT Search and Model Performance,prove approaches proposed user noise for
381,"This paper considers global optimization with a black-box unknown objective
function that can be non-convex and non-differentiable. Such a difficult
optimization problem arises in many real-world applications, such as parameter
tuning in machine learning, engineering design problem, and planning with a
complex physics simulator. This paper proposes a new global optimization
algorithm, called Locally Oriented Global Optimization (LOGO), to aim for both
fast convergence in practice and finite-time error bound in theory. The
advantage and usage of the new algorithm are illustrated via theoretical
analysis and an experiment conducted with 11 benchmark test functions. Further,
we modify the LOGO algorithm to specifically solve a planning problem via
policy search with continuous state/action space and long time horizon while
maintaining its finite-time error bound. We apply the proposed planning method
to accident management of a nuclear power plant. The result of the application
study demonstrates the practical utility of our method.",Global Continuous Optimization with Error Bound and Fast Convergence,using model categorization code example different data scene
382,"We describe Swapout, a new stochastic training method, that outperforms
ResNets of identical network structure yielding impressive results on CIFAR-10
and CIFAR-100. Swapout samples from a rich set of architectures including
dropout, stochastic depth and residual architectures as special cases. When
viewed as a regularization method swapout not only inhibits co-adaptation of
units in a layer, similar to dropout, but also across network layers. We
conjecture that swapout achieves strong regularization by implicitly tying the
parameters across layers. When viewed as an ensemble training method, it
samples a much richer set of architectures than existing methods such as
dropout or stochastic depth. We propose a parameterization that reveals
connections to exiting architectures and suggests a much richer set of
architectures to be explored. We show that our formulation suggests an
efficient training method and validate our conclusions on CIFAR-10 and
CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider
model performs similar to a 1001 layer ResNet model.",Swapout: Learning an ensemble of deep architectures,potential  ii  proposed potential largely
383,"We review several of the most widely used techniques for training recurrent
neural networks to approximate dynamical systems, then describe a novel
algorithm for this task. The algorithm is based on an earlier theoretical
result that guarantees the quality of the network approximation. We show that a
feedforward neural network can be trained on the vector field representation of
a given dynamical system using backpropagation, then recast, using matrix
manipulations, as a recurrent network that replicates the original system's
dynamics. After detailing this algorithm and its relation to earlier
approaches, we present numerical examples that demonstrate its capabilities.
One of the distinguishing features of our approach is that both the original
dynamical systems and the recurrent networks that simulate them operate in
continuous time.",Synthesis of recurrent neural networks for dynamical system simulation,annotation cannot show distinguish inference
384,"We investigate the task of building open domain, conversational dialogue
systems based on large dialogue corpora using generative models. Generative
models produce system responses that are autonomously generated word-by-word,
opening up the possibility for realistic, flexible interactions. In support of
this goal, we extend the recently proposed hierarchical recurrent
encoder-decoder neural network to the dialogue domain, and demonstrate that
this model is competitive with state-of-the-art neural language models and
back-off n-gram models. We investigate the limitations of this and similar
approaches, and show how its performance can be improved by bootstrapping the
learning from a larger question-answer pair corpus and from pretrained word
embeddings.","Building End-To-End Dialogue Systems Using Generative Hierarchical
  Neural Network Models",models show feature for
385,"In this work we propose a multi-valued extension of logic programs under the
stable models semantics where each true atom in a model is associated with a
set of justifications. These justifications are expressed in terms of causal
graphs formed by rule labels and edges that represent their application
ordering. For positive programs, we show that the causal justifications
obtained for a given atom have a direct correspon- dence to (relevant)
syntactic proofs of that atom using the program rules involved in the graphs.
The most interesting contribution is that this causal information is obtained
in a purely semantic way, by algebraic op- erations (product, sum and
application) on a lattice of causal values whose ordering relation expresses
when a justification is stronger than another. Finally, for programs with
negation, we define the concept of causal stable model by introducing an
analogous transformation to Gelfond and Lifschitz's program reduct. As a
result, default negation behaves as ""absence of proof"" and no justification is
derived from negative liter",Causal Graph Justifications of Logic Programs,programming distribution limited
386,An updated version will be uploaded later.,Weak Evolvability Equals Strong Evolvability,concepts  ca  compositional association association one
387,"This paper describes a process for constructing situation-specific belief
networks from a knowledge base of network fragments. A situation-specific
network is a minimal query complete network constructed from a knowledge base
in response to a query for the probability distribution on a set of target
variables given evidence and context variables. We present definitions of query
completeness and situation-specific networks. We describe conditions on the
knowledge base that guarantee query completeness. The relationship of our work
to earlier work on KBMC is also discussed.",Constructing Situation Specific Belief Networks,knowledge show set present image word paper  for
388,"SNePS is a logic- and network- based knowledge representation, reasoning, and
acting system, based on a monotonic, paraconsistent, first-order term logic,
with compositional intensional semantics. It has an ATMS-style facility for
belief contraction, and an acting component, including a well-defined syntax
and semantics for primitive and composite acts, as well as for ``rules'' that
allow for acting in support of reasoning and reasoning in support of acting.
SNePS has been designed to support natural language competent cognitive agents.
  When the current version of SNePS detects an explicit contradiction, it
interacts with the user, providing information that helps the user decide what
to remove from the knowledge base in order to remove the contradiction. The
forthcoming SNePS 2.6 will also do automatic belief contraction if the
information in the knowledge base warrents it.",Automatic Belief Revision in SNePS,objects criterion show significant suited step
389,"Lensless imaging is an important and challenging problem. One notable
solution to lensless imaging is a single pixel camera which benefits from ideas
central to compressive sampling. However, traditional single pixel cameras
require many illumination patterns which result in a long acquisition process.
Here we present a method for lensless imaging based on compressive ultrafast
sensing. Each sensor acquisition is encoded with a different illumination
pattern and produces a time series where time is a function of the photon's
origin in the scene. Currently available hardware with picosecond time
resolution enables time tagging photons as they arrive to an omnidirectional
sensor. This allows lensless imaging with significantly fewer patterns compared
to regular single pixel imaging. To that end, we develop a framework for
designing lensless imaging systems that use ultrafast detectors. We provide an
algorithm for ideal sensor placement and an algorithm for optimized active
illumination patterns. We show that efficient lensless imaging is possible with
ultrafast measurement and compressive sensing. This paves the way for novel
imaging architectures and remote sensing in extreme situations where imaging
with a lens is not possible.",Lensless Imaging with Compressive Ultrafast Sensing,using topology spectral show provides ##
390,"Theory of graphical models has matured over more than three decades to
provide the backbone for several classes of models that are used in a myriad of
applications such as genetic mapping of diseases, credit risk evaluation,
reliability and computer security, etc. Despite of their generic applicability
and wide adoptance, the constraints imposed by undirected graphical models and
Bayesian networks have also been recognized to be unnecessarily stringent under
certain circumstances. This observation has led to the proposal of several
generalizations that aim at more relaxed constraints by which the models can
impose local or context-specific dependence structures. Here we consider an
additional class of such models, termed as stratified graphical models. We
develop a method for Bayesian learning of these models by deriving an
analytical expression for the marginal likelihood of data under a specific
subclass of decomposable stratified models. A non-reversible Markov chain Monte
Carlo approach is further used to identify models that are highly supported by
the posterior distribution over the model space. Our method is illustrated and
compared with ordinary graphical models through application to several real and
synthetic datasets.","Stratified Graphical Models - Context-Specific Independence in Graphical
  Models",proposed efficiency simultaneously models
391,"A model of the mechanisms underlying exploratory behaviour, based on
empirical research and refined using a computer simulation, is presented. The
behaviour of killifish from two lakes, one with killifish predators and one
without, was compared in the laboratory. Plotting average activity in a novel
environment versus time resulted in an inverted-U-shaped curve for both groups;
however, the curve for killifish from the lake without predators was (1)
steeper, (2) reached a peak value earlier, (S) reached a higher peak value, and
(4) subsumed less area than the curve for killifish from the lake with
predators. We hypothesize that the shape of the exploration curve reflects a
competition between motivational subsystems that excite and inhibit exploratory
behaviour in a way that is tuned to match the affordance probabilities of the
animal's environment. A computer implementation of this model produced curves
which differed along the same four dimensions as differentiate the two
killifish curves. All four differences were reproduced in the model by tuning a
single parameter: the time-dependent component of the decay-rate of the
exploration-inhibiting subsystem.",A Model of the Mechanisms Underlying Exploratory Behaviour,for proposed potential networks
392,"Deep Neural Networks are becoming increasingly popular in always-on IoT edge
devices performing data analytics right at the source, reducing latency as well
as energy consumption for data communication. This paper presents CMSIS-NN,
efficient kernels developed to maximize the performance and minimize the memory
footprint of neural network (NN) applications on Arm Cortex-M processors
targeted for intelligent IoT edge devices. Neural network inference based on
CMSIS-NN kernels achieves 4.6X improvement in runtime/throughput and 4.9X
improvement in energy efficiency.",CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs,engine alone proposed distribution planning superiority
393,"This paper proposes a generalized Hybrid Real-coded Quantum Evolutionary
Algorithm (HRCQEA) for optimizing complex functions as well as combinatorial
optimization. The main idea of HRCQEA is to devise a new technique for mutation
and crossover operators. Using the evolutionary equation of PSO a
Single-Multiple gene Mutation (SMM) is designed and the concept of Arithmetic
Crossover (AC) is used in the new Crossover operator. In HRCQEA, each triploid
chromosome represents a particle and the position of the particle is updated
using SMM and Quantum Rotation Gate (QRG), which can make the balance between
exploration and exploitation. Crossover is employed to expand the search space,
Hill Climbing Selection (HCS) and elitism help to accelerate the convergence
speed. Simulation results on Knapsack Problem and five benchmark complex
functions with high dimension show that HRCQEA performs better in terms of
ability to discover the global optimum and convergence speed.","A Generalized Hybrid Real-Coded Quantum Evolutionary Algorithm Based on
  Particle Swarm Theory with Arithmetic Crossover",using conditional approaches model feature real breast
394,"This paper considers the multi-task learning problem and in the setting where
some relevant features could be shared across few related tasks. Most of the
existing methods assume the extent to which the given tasks are related or
share a common feature space to be known apriori. In real-world applications
however, it is desirable to automatically discover the groups of related tasks
that share a feature space. In this paper we aim at searching the exponentially
large space of all possible groups of tasks that may share a feature space. The
main contribution is a convex formulation that employs a graph-based
regularizer and simultaneously discovers few groups of related tasks, having
close-by task parameters, as well as the feature space shared within each
group. The regularizer encodes an important structure among the groups of tasks
leading to an efficient algorithm for solving it: if there is no feature space
under which a group of tasks has close-by task parameters, then there does not
exist such a feature space for any of its supersets. An efficient active set
algorithm that exploits this simplification and performs a clever search in the
exponentially large space is presented. The algorithm is guaranteed to solve
the proposed formulation (within some precision) in a time polynomial in the
number of groups of related tasks discovered. Empirical results on benchmark
datasets show that the proposed formulation achieves good generalization and
outperforms state-of-the-art multi-task learning algorithms in some cases.","A Convex Feature Learning Formulation for Latent Task Structure
  Discovery",paper  model <unk> knowledge
395,"Control Strategies for hierarchical tree-like probabilistic inference
networks are formulated and investigated. Strategies that utilize staged
look-ahead and temporary focus on subgoals are formalized and refined using the
Depth Vector concept that serves as a tool for defining the 'virtual tree'
regarded by the control strategy. The concept is illustrated by four types of
control strategies for three-level trees that are characterized according to
their Depth Vector, and according to the way they consider intermediate nodes
and the role that they let these nodes play. INFERENTI is a computerized
inference system written in Prolog, which provides tools for exercising a
variety of control strategies. The system also provides tools for simulating
test data and for comparing the relative average performance under different
strategies.",A Framework for Control Strategies in Uncertain Inference Networks,models show show control search
396,"Inference in the presence of outliers is an important field of research as
outliers are ubiquitous and may arise across a variety of problems and domains.
Bayesian optimization is method that heavily relies on probabilistic inference.
This allows outstanding sample efficiency because the probabilistic machinery
provides a memory of the whole optimization process. However, that virtue
becomes a disadvantage when the memory is populated with outliers, inducing
bias in the estimation. In this paper, we present an empirical evaluation of
Bayesian optimization methods in the presence of outliers. The empirical
evidence shows that Bayesian optimization with robust regression often produces
suboptimal results. We then propose a new algorithm which combines robust
regression (a Gaussian process with Student-t likelihood) with outlier
diagnostics to classify data points as outliers or inliers. By using an
scheduler for the classification of outliers, our method is more efficient and
has better convergence over the standard robust regression. Furthermore, we
show that even in controlled situations with no expected outliers, our method
is able to produce better results.",Practical Bayesian optimization in the presence of outliers,<unk> model mammals tree  capabilities gpu using jpeg
397,"Planning is a notoriously difficult computational problem of high worst-case
complexity. Researchers have been investing significant efforts to develop
heuristics or restrictions to make planning practically feasible. Case-based
planning is a heuristic approach where one tries to reuse previous experience
when solving similar problems in order to avoid some of the planning effort.
Plan reuse may offer an interesting alternative to plan generation in some
settings.
  We provide theoretical results that identify situations in which plan reuse
is provably tractable. We perform our analysis in the framework of
parameterized complexity, which supports a rigorous worst-case complexity
analysis that takes structural properties of the input into account in terms of
parameters. A central notion of parameterized complexity is fixed-parameter
tractability which extends the classical notion of polynomial-time tractability
by utilizing the effect of structural properties of the problem input.
  We draw a detailed map of the parameterized complexity landscape of several
variants of problems that arise in the context of case-based planning. In
particular, we consider the problem of reusing an existing plan, imposing
various restrictions in terms of parameters, such as the number of steps that
can be added to the existing plan to turn it into a solution of the planning
instance at hand.",Parameterized Complexity Results for Plan Reuse,for show slight role
398,"This article describes a method to build syntactical dependencies starting
from the phrase structure parsing process. The goal is to obtain all the
information needed for a detailled semantical analysis. Interaction Grammars
are used for parsing; the saturation of polarities which is the core of this
formalism can be mapped to dependency relation. Formally, graph patterns are
used to express the set of constraints which control dependency creations.",Motifs de graphe pour le calcul de dépendances syntaxiques complètes,data programming efforts show significant require take
399,"Particle swarm optimization comes under lot of changes after James Kennedy
and Russell Eberhart first proposes the idea in 1995. The changes has been done
mainly on Inertia parameters in velocity updating equation so that the
convergence rate will be higher. We are proposing a novel approach where
particles movement will not be depend on its velocity rather it will be decided
by constrained biased random walk of particles. In random walk every particles
movement based on two significant parameters, one is random process like toss
of a coin and other is how much displacement a particle should have. In our
approach we exploit this idea by performing a biased random operation and based
on the outcome of that random operation, PSO particles choose the direction of
the path and move non-uniformly into the solution space. This constrained,
non-uniform movement helps the random walking particle to converge quicker then
classical PSO. In our constrained biased random walking approach, we no longer
needed velocity term (Vi), rather we introduce a new parameter (K) which is a
probabilistic function. No global best particle (PGbest), local best particle
(PLbest), Constriction parameter (W) are required rather we use a new term
called Ptarg which is loosely influenced by PGbest.We test our algorithm on
five different benchmark functions, and also compare its performance with
classical PSO and Quantum Particle Swarm Optimization (QPSO).This new approach
have been shown significantly better than basic PSO and sometime outperform
QPSO in terms of convergence, search space, number of iterations.",A Modification of Particle Swarm Optimization using Random Walk,using video step problem limited model prove
400,"PCANet was proposed as a lightweight deep learning network that mainly
leverages Principal Component Analysis (PCA) to learn multistage filter banks
followed by binarization and block-wise histograming. PCANet was shown worked
surprisingly well in various image classification tasks. However, PCANet is
data-dependence hence inflexible. In this paper, we proposed a
data-independence network, dubbed DCTNet for face recognition in which we adopt
Discrete Cosine Transform (DCT) as filter banks in place of PCA. This is
motivated by the fact that 2D DCT basis is indeed a good approximation for high
ranked eigenvectors of PCA. Both 2D DCT and PCA resemble a kind of modulated
sine-wave patterns, which can be perceived as a bandpass filter bank. DCTNet is
free from learning as 2D DCT bases can be computed in advance. Besides that, we
also proposed an effective method to regulate the block-wise histogram feature
vector of DCTNet for robustness. It is shown to provide surprising performance
boost when the probe image is considerably different in appearance from the
gallery image. We evaluate the performance of DCTNet extensively on a number of
benchmark face databases and being able to achieve on par with or often better
accuracy performance than PCANet.",DCTNet : A Simple Learning-free Approach for Face Recognition,set present mathematical characteristic show theory theory
401,"We consider the problem of learning general-purpose, paraphrastic sentence
embeddings, revisiting the setting of Wieting et al. (2016b). While they found
LSTM recurrent networks to underperform word averaging, we present several
developments that together produce the opposite conclusion. These include
training on sentence pairs rather than phrase pairs, averaging states to
represent sequences, and regularizing aggressively. These improve LSTMs in both
transfer learning and supervised settings. We also introduce a new recurrent
architecture, the Gated Recurrent Averaging Network, that is inspired by
averaging and LSTMs while outperforming them both. We analyze our learned
models, finding evidence of preferences for particular parts of speech and
dependency relations.",Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings,based function  al  show research estimate
402,"The recent success of deep neural networks is powered in part by large-scale
well-labeled training data. However, it is a daunting task to laboriously
annotate an ImageNet-like dateset. On the contrary, it is fairly convenient,
fast, and cheap to collect training images from the Web along with their noisy
labels. This signifies the need of alternative approaches to training deep
neural networks using such noisy labels. Existing methods tackling this problem
either try to identify and correct the wrong labels or reweigh the data terms
in the loss function according to the inferred noisy rates. Both strategies
inevitably incur errors for some of the data points. In this paper, we contend
that it is actually better to ignore the labels of some of the data points than
to keep them if the labels are incorrect, especially when the noisy rate is
high. After all, the wrong labels could mislead a neural network to a bad local
optimum. We suggest a two-stage framework for the learning from noisy labels.
In the first stage, we identify a small portion of images from the noisy
training set of which the labels are correct with a high probability. The noisy
labels of the other images are ignored. In the second stage, we train a deep
neural network in a semi-supervised manner. This framework effectively takes
advantage of the whole training set and yet only a portion of its labels that
are most likely correct. Experiments on three datasets verify the effectiveness
of our approach especially when the noisy rate is high.",A Semi-Supervised Two-Stage Approach to Learning from Noisy Labels,studied model matches problem architectures  made
403,"We analyze differences between two information-theoretically motivated
approaches to statistical inference and model selection: the Minimum
Description Length (MDL) principle, and the Minimum Message Length (MML)
principle. Based on this analysis, we present two revised versions of MML: a
pointwise estimator which gives the MML-optimal single parameter model, and a
volumewise estimator which gives the MML-optimal region in the parameter space.
Our empirical results suggest that with small data sets, the MDL approach
yields more accurate predictions than the MML estimators. The empirical results
also demonstrate that the revised MML estimators introduced here perform better
than the original MML estimator suggested by Wallace and Freeman.",Minimum Encoding Approaches for Predictive Modeling,models model brain
404,"Research is taking place to find effective algorithms for content-based image
representation and description. There is a substantial amount of algorithms
available that use visual features (color, shape, texture). Shape feature has
attracted much attention from researchers that there are many shape
representation and description algorithms in literature. These shape image
representation and description algorithms are usually not application
independent or robust, making them undesirable for generic shape description.
This paper presents an object shape representation using Kernel Density Feature
Points Estimator (KDFPE). In this method, the density of feature points within
defined rings around the centroid of the image is obtained. The KDFPE is then
applied to the vector of the image. KDFPE is invariant to translation, scale
and rotation. This method of image representation shows improved retrieval rate
when compared to Density Histogram Feature Points (DHFP) method. Analytic
analysis is done to justify our method, which was compared with the DHFP to
prove its robustness.","Kernel Density Feature Points Estimator for Content-Based Image
  Retrieval",linear model research feature paper  for
405,"While imitation learning is becoming common practice in robotics, this
approach often suffers from data mismatch and compounding errors. DAgger is an
iterative algorithm that addresses these issues by continually aggregating
training data from both the expert and novice policies, but does not consider
the impact of safety. We present a probabilistic extension to DAgger, which
uses the distribution over actions provided by the novice policy, for a given
observation. Our method, which we call DropoutDAgger, uses dropout to train the
novice as a Bayesian neural network that provides insight to its confidence.
Using the distribution over the novice's actions, we estimate a probabilistic
measure of safety with respect to the expert action, tuned to balance
exploration and exploitation. The utility of this approach is evaluated on the
MuJoCo HalfCheetah and in a simple driving experiment, demonstrating improved
performance and safety compared to other DAgger variants and classic imitation
learning.",DropoutDAgger: A Bayesian Approach to Safe Imitation Learning,predictive <unk> method <unk> minimal parametrize two
406,"Machine vision applications are low cost and high precision measurement
systems which are frequently used in production lines. With these systems that
provide contactless control and measurement, production facilities are able to
reach high production numbers without errors. Machine vision operations such as
product counting, error control, dimension measurement can be performed through
a camera. In this paper, a machine vision application is proposed, which can
perform object-independent product counting. The proposed approach is based on
Otsu thresholding and Hough transformation and performs automatic counting
independently of product type and color. Basically one camera is used in the
system. Through this camera, an image of the products passing through a
conveyor is taken and various image processing algorithms are applied to these
images. In this approach using images obtained from a real experimental setup,
a real-time machine vision application was installed. As a result of the
experimental studies performed, it has been determined that the proposed
approach gives fast, accurate and reliable results.","An Image Processing based Object Counting Approach for Machine Vision
  Application",columns encoder model regression  networks
407,"K-means Fast Learning Artificial Neural Network (K-FLANN) is an unsupervised
neural network requires two parameters: tolerance and vigilance. Best
Clustering results are feasible only by finest parameters specified to the
neural network. Selecting optimal values for these parameters is a major
problem. To solve this issue, Genetic Algorithm (GA) is used to determine
optimal parameters of K-FLANN for finding groups in multidimensional data.
K-FLANN is a simple topological network, in which output nodes grows
dynamically during the clustering process on receiving input patterns. Original
K-FLANN is enhanced to select winner unit out of the matched nodes so that
stable clusters are formed with in a less number of epochs. The experimental
results show that the GA is efficient in finding optimal values of parameters
from the large search space and is tested using artificial and synthetic data
sets.","Optimal parameter selection for unsupervised neural network using
  genetic algorithm",consistent model method <unk>
408,"Document segmentation is one of the critical phases in machine recognition of
any language. Correct segmentation of individual symbols decides the accuracy
of character recognition technique. It is used to decompose image of a sequence
of characters into sub images of individual symbols by segmenting lines and
words. Devnagari is the most popular script in India. It is used for writing
Hindi, Marathi, Sanskrit and Nepali languages. Moreover, Hindi is the third
most popular language in the world. Devnagari documents consist of vowels,
consonants and various modifiers. Hence proper segmentation of Devnagari word
is challenging. A simple histogram based approach to segment Devnagari
documents is proposed in this paper. Various challenges in segmentation of
Devnagari script are also discussed.",Devnagari document segmentation using histogram approach,domain  model class inference on
409,"Purpose: In this paper, we investigate a framework for interactive brain
tumor segmentation which, at its core, treats the problem of interactive brain
tumor segmentation as a machine learning problem.
  Methods: This method has an advantage over typical machine learning methods
for this task where generalization is made across brains. The problem with
these methods is that they need to deal with intensity bias correction and
other MRI-specific noise. In this paper, we avoid these issues by approaching
the problem as one of within brain generalization. Specifically, we propose a
semi-automatic method that segments a brain tumor by training and generalizing
within that brain only, based on some minimum user interaction.
  Conclusion: We investigate how adding spatial feature coordinates (i.e. $i$,
$j$, $k$) to the intensity features can significantly improve the performance
of different classification methods such as SVM, kNN and random forests. This
would only be possible within an interactive framework. We also investigate the
use of a more appropriate kernel and the adaptation of hyper-parameters
specifically for each brain.
  Results: As a result of these experiments, we obtain an interactive method
whose results reported on the MICCAI-BRATS 2013 dataset are the second most
accurate compared to published methods, while using significantly less memory
and processing power than most state-of-the-art methods.",Within-Brain Classification for Brain Tumor Segmentation,uses show signals  improve new numerals non parametric
410,"As the field of data science continues to grow, there will be an
ever-increasing demand for tools that make machine learning accessible to
non-experts. In this paper, we introduce the concept of tree-based pipeline
optimization for automating one of the most tedious parts of machine
learning---pipeline design. We implement an open source Tree-based Pipeline
Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a
series of simulated and real-world benchmark data sets. In particular, we show
that TPOT can design machine learning pipelines that provide a significant
improvement over a basic machine learning analysis while requiring little to no
input nor prior knowledge from the user. We also address the tendency for TPOT
to design overly complex pipelines by integrating Pareto optimization, which
produces compact pipelines without sacrificing classification accuracy. As
such, this work represents an important step toward fully automating machine
learning pipeline design.","Evaluation of a Tree-based Pipeline Optimization Tool for Automating
  Data Science",models learning <unk> account account account
411,"We aim to study the modeling limitations of the commonly employed boosted
decision trees classifier. Inspired by the success of large, data-hungry visual
recognition models (e.g. deep convolutional neural networks), this paper
focuses on the relationship between modeling capacity of the weak learners,
dataset size, and dataset properties. A set of novel experiments on the Caltech
Pedestrian Detection benchmark results in the best known performance among
non-CNN techniques while operating at fast run-time speed. Furthermore, the
performance is on par with deep architectures (9.71% log-average miss rate),
while using only HOG+LUV channels as features. The conclusions from this study
are shown to generalize over different object detection domains as demonstrated
on the FDDB face detection benchmark (93.37% accuracy). Despite the impressive
performance, this study reveals the limited modeling capacity of the common
boosted trees model, motivating a need for architectural changes in order to
compete with multi-level and very deep architectures.","To Boost or Not to Boost? On the Limits of Boosted Trees for Object
  Detection",proposed observation
412,"In this work, a study on Variable Neighborhood Search algorithms for
multi-depot dial-a-ride problems is presented. In dial-a-ride problems patients
need to be transported from pre-specified pickup locations to pre-specified
delivery locations, under different considerations. The addressed problem
presents several constraints and features, such as heterogeneous vehicles,
distributed in different depots, and heterogeneous patients. The aim is of
minimizing the total routing cost, while respecting time-window, ride-time,
capacity and route duration constraints. The objective of the study is of
determining the best algorithm configuration in terms of initial solution,
neighborhood and local search procedures. At this aim, two different procedures
for the computation of an initial solution, six different type of neighborhoods
and five local search procedures, where only intra-route changes are made, have
been considered and compared.
  We have also evaluated an ""adjusting procedure"" that aims to produce feasible
solutions from infeasible solutions with small constraints violations. The
different VNS algorithms have been tested on instances from literature as well
as on random instances arising from a real-world healthcare application.","Variable Neighborhood Search Algorithms for the multi-depot dial-a-ride
  problem with heterogeneous vehicles and users",mechanism instances data 
413,"To model combinatorial decision problems involving uncertainty and
probability, we extend the stochastic constraint programming framework proposed
in [Walsh, 2002] along a number of important dimensions (e.g. to multiple
chance constraints and to a range of new objectives). We also provide a new
(but equivalent) semantics based on scenarios. Using this semantics, we can
compile stochastic constraint programs down into conventional (nonstochastic)
constraint programs. This allows us to exploit the full power of existing
constraint solvers. We have implemented this framework for decision making
under uncertainty in stochastic OPL, a language which is based on the OPL
constraint modelling language [Hentenryck et al., 1999]. To illustrate the
potential of this framework, we model a wide range of problems in areas as
diverse as finance, agriculture and production.",Scenario-based Stochastic Constraint Programming,data constraint stochastic efficient show <unk>
414,"Regularized empirical risk minimization with constrained labels (in contrast
to fixed labels) is a remarkably general abstraction of learning. For common
loss and regularization functions, this optimization problem assumes the form
of a mixed integer program (MIP) whose objective function is non-convex. In
this form, the problem is resistant to standard optimization techniques. We
construct MIPs with the same solutions whose objective functions are convex.
Specifically, we characterize the tightest convex extension of the objective
function, given by the Legendre-Fenchel biconjugate. Computing values of this
tightest convex extension is NP-hard. However, by applying our characterization
to every function in an additive decomposition of the objective function, we
obtain a class of looser convex extensions that can be computed efficiently.
For some decompositions, common loss and regularization functions, we derive a
closed form.",Convexification of Learning from Constraints,size <unk> preferences
415,"Artificial Bee Colony (ABC) is a distinguished optimization strategy that can
resolve nonlinear and multifaceted problems. It is comparatively a
straightforward and modern population based probabilistic approach for
comprehensive optimization. In the vein of the other population based
algorithms, ABC is moreover computationally classy due to its slow nature of
search procedure. The solution exploration equation of ABC is extensively
influenced by a arbitrary quantity which helps in exploration at the cost of
exploitation of the better search space. In the solution exploration equation
of ABC due to the outsized step size the chance of skipping the factual
solution is high. Therefore, here this paper improve onlooker bee phase with
help of a local search strategy inspired by memetic algorithm to balance the
diversity and convergence capability of the ABC. The proposed algorithm is
named as Improved Onlooker Bee Phase in ABC (IoABC). It is tested over 12 well
known un-biased test problems of diverse complexities and two engineering
optimization problems; results show that the anticipated algorithm go one
better than the basic ABC and its recent deviations in a good number of the
experiments.",Improved Onlooker Bee Phase in Artificial Bee Colony Algorithm,mechanism strategy shown method feature domain 
416,"This paper describes an an open-source software system for the automatic
conversion of NLP event representations to system biology structured data
interchange formats such as SBML and BioPAX. It is part of a larger effort to
make results of the NLP community available for system biology pathway
modelers.",Extracting Biological Pathway Models From NLP Event Representations,<unk> <unk>
417,"The biological immune system is a robust, complex, adaptive system that
defends the body from foreign pathogens. It is able to categorize all cells (or
molecules) within the body as self or non-self substances. It does this with
the help of a distributed task force that has the intelligence to take action
from a local and also a global perspective using its network of chemical
messengers for communication. There are two major branches of the immune
system. The innate immune system is an unchanging mechanism that detects and
destroys certain invading organisms, whilst the adaptive immune system responds
to previously unknown foreign cells and builds a response to them that can
remain in the body over a long period of time. This remarkable information
processing biological system has caught the attention of computer science in
recent years.
  A novel computational intelligence technique, inspired by immunology, has
emerged, called Artificial Immune Systems. Several concepts from the immune
system have been extracted and applied for solution to real world science and
engineering problems. In this tutorial, we briefly describe the immune system
metaphors that are relevant to existing Artificial Immune Systems methods. We
will then show illustrative real-world problems suitable for Artificial Immune
Systems and give a step-by-step algorithm walkthrough for one such problem. A
comparison of the Artificial Immune Systems to other well-known algorithms,
areas for future work, tips & tricks and a list of resources will round this
tutorial off. It should be noted that as Artificial Immune Systems is still a
young and evolving field, there is not yet a fixed algorithm template and hence
actual implementations might differ somewhat from time to time and from those
examples given here.",Artificial Immune Systems (INTROS 2),method data researchers high models
418,"In Artificial Intelligence with Coalition Structure Generation (CSG) one
refers to those cooperative complex problems that require to find an optimal
partition, maximising a social welfare, of a set of entities involved in a
system into exhaustive and disjoint coalitions. The solution of the CSG problem
finds applications in many fields such as Machine Learning (covering machines,
clustering), Data Mining (decision tree, discretization), Graph Theory, Natural
Language Processing (aggregation), Semantic Web (service composition), and
Bioinformatics. The problem of finding the optimal coalition structure is
NP-complete. In this paper we present a greedy adaptive search procedure
(GRASP) with path-relinking to efficiently search the space of coalition
structures. Experiments and comparisons to other algorithms prove the validity
of the proposed method in solving this hard combinatorial problem.",GRASP and path-relinking for Coalition Structure Generation,improve often using significant seems model tracking 
419,"We present a method for generating synthetic versions of Twitter data using
neural generative models. The goal is to protect individuals in the source data
from stylometric re-identification attacks while still releasing data that
carries research value. To generate tweet corpora that maintain user-level word
distributions, our proposed approach augments powerful neural language models
with local parameters that weight user-specific inputs. We compare our work to
two standard text data protection methods: redaction and iterative translation.
We evaluate the three methods on risk and utility. We define risk following the
stylometric models of re-identification, and we define utility based on two
general language measures and two common text analysis tasks. We find that
neural models are able to significantly lower risk over previous methods at the
cost of some utility. More importantly, we show that the risk utility trade-off
depends on how the neural model's logits (or the unscaled pre-activation values
of the output layer) are scaled. This work presents promising results for a new
tool addressing the problem of privacy for free text and sharing social media
data in a way that respects privacy and is ethically responsible.","Using Neural Generative Models to Release Synthetic Twitter Corpora with
  Reduced Stylometric Identifiability of Users",models image  textual new open suitable networks
420,"Deep Q-learning is investigated as an end-to-end solution to estimate the
optimal strategies for acting on time series input. Experiments are conducted
on two idealized trading games. 1) Univariate: the only input is a wave-like
price time series, and 2) Bivariate: the input includes a random stepwise price
time series and a noisy signal time series, which is positively correlated with
future price changes. The Univariate game tests whether the agent can capture
the underlying dynamics, and the Bivariate game tests whether the agent can
utilize the hidden relation among the inputs. Stacked Gated Recurrent Unit
(GRU), Long Short-Term Memory (LSTM) units, Convolutional Neural Network (CNN),
and multi-layer perceptron (MLP) are used to model Q values. For both games,
all agents successfully find a profitable strategy. The GRU-based agents show
best overall performance in the Univariate game, while the MLP-based agents
outperform others in the Bivariate game.","Deep reinforcement learning for time series: playing idealized trading
  games",research demonstrate show research <unk>
421,"Photoplethysmography (PPG) devices are widely used for monitoring
cardiovascular function. However, these devices require skin contact, which
restrict their use to at-rest short-term monitoring using single-point
measurements. Photoplethysmographic imaging (PPGI) has been recently proposed
as a non-contact monitoring alternative by measuring blood pulse signals across
a spatial region of interest. Existing systems operate in reflectance mode, of
which many are limited to short-distance monitoring and are prone to temporal
changes in ambient illumination. This paper is the first study to investigate
the feasibility of long-distance non-contact cardiovascular monitoring at the
supermeter level using transmittance PPGI. For this purpose, a novel PPGI
system was designed at the hardware and software level using ambient correction
via temporally coded illumination (TCI) and signal processing for PPGI signal
extraction. Experimental results show that the processing steps yield a
substantially more pulsatile PPGI signal than the raw acquired signal,
resulting in statistically significant increases in correlation to ground-truth
PPG in both short- ($p \in [<0.0001, 0.040]$) and long-distance ($p \in
[<0.0001, 0.056]$) monitoring. The results support the hypothesis that
long-distance heart rate monitoring is feasible using transmittance PPGI,
allowing for new possibilities of monitoring cardiovascular function in a
non-contact manner.","Non-contact transmittance photoplethysmographic imaging (PPGI) for
  long-distance cardiovascular monitoring",layers model distributions estimate model data uncertainty  different
422,"Character identification plays a vital role in the contemporary world of
Image processing. It can solve many composite problems and makes humans work
easier. An instance is Handwritten Character detection. Handwritten recognition
is not a novel expertise, but it has not gained community notice until Now. The
eventual aim of designing Handwritten Character recognition structure with an
accurateness rate of 100% is pretty illusionary. Tamil Handwritten Character
recognition system uses the Neural Networks to distinguish them. Neural Network
and structural characteristics are used to instruct and recognize written
characters. After training and testing the exactness rate reached 99%. This
correctness rate is extremely high. In this paper we are exploring image
processing through the Hilditch algorithm foundation and structural
characteristics of a character in the image. And we recognized some character
of the Tamil language, and we are trying to identify all the character of Tamil
In our future works.",Hilditchs Algorithm Based Tamil Character Recognition,model <unk>  such hard new learning <unk>
423,"3D Morphable Models (3DMMs) are powerful statistical models of 3D facial
shape and texture, and among the state-of-the-art methods for reconstructing
facial shape from single images. With the advent of new 3D sensors, many 3D
facial datasets have been collected containing both neutral as well as
expressive faces. However, all datasets are captured under controlled
conditions. Thus, even though powerful 3D facial shape models can be learnt
from such data, it is difficult to build statistical texture models that are
sufficient to reconstruct faces captured in unconstrained conditions
(""in-the-wild""). In this paper, we propose the first, to the best of our
knowledge, ""in-the-wild"" 3DMM by combining a powerful statistical model of
facial shape, which describes both identity and expression, with an
""in-the-wild"" texture model. We show that the employment of such an
""in-the-wild"" texture model greatly simplifies the fitting procedure, because
there is no need to optimize with regards to the illumination parameters.
Furthermore, we propose a new fast algorithm for fitting the 3DMM in arbitrary
images. Finally, we have captured the first 3D facial database with relatively
unconstrained conditions and report quantitative evaluations with
state-of-the-art performance. Complementary qualitative reconstruction results
are demonstrated on standard ""in-the-wild"" facial databases. An open source
implementation of our technique is released as part of the Menpo Project.","3D Face Morphable Models ""In-the-Wild""",models method facial demonstrate
424,"Training deep feature hierarchies to solve supervised learning tasks has
achieved state of the art performance on many problems in computer vision.
However, a principled way in which to train such hierarchies in the
unsupervised setting has remained elusive. In this work we suggest a new
architecture and loss for training deep feature hierarchies that linearize the
transformations observed in unlabeled natural video sequences. This is done by
training a generative model to predict video frames. We also address the
problem of inherent uncertainty in prediction by introducing latent variables
that are non-deterministic functions of the input into the network
architecture.",Learning to Linearize Under Uncertainty,models proposed feature terminating models
425,"Here we show, contrary to the classical supposition, that a process for
generating symbols according to some probability distribution need not, with
any likelihood, produce a given finite text in any finite time, even if it is
guaranteed to produce the text in infinite time. The result extends to
target-free text generation and has implications for simulations of
probabilistic processes.",Infinity in computable probability,scoring model potential units multimodal units function  scoring
426,"Nonnegative matrix factorization (NMF) has attracted much attention in the
last decade as a dimension reduction method in many applications. Due to the
explosion in the size of data, naturally the samples are collected and stored
distributively in local computational nodes. Thus, there is a growing need to
develop algorithms in a distributed memory architecture. We propose a novel
distributed algorithm, called \textit{distributed incremental block coordinate
descent} (DID), to solve the problem. By adapting the block coordinate descent
framework, closed-form update rules are obtained in DID. Moreover, DID performs
updates incrementally based on the most recently updated residual matrix. As a
result, only one communication step per iteration is required. The correctness,
efficiency, and scalability of the proposed algorithm are verified in a series
of numerical experiments.","DID: Distributed Incremental Block Coordinate Descent for Nonnegative
  Matrix Factorization",model <unk> gps rule
427,"We consider the problem of low canonical polyadic (CP) rank tensor
completion. A completion is a tensor whose entries agree with the observed
entries and its rank matches the given CP rank. We analyze the manifold
structure corresponding to the tensors with the given rank and define a set of
polynomials based on the sampling pattern and CP decomposition. Then, we show
that finite completability of the sampled tensor is equivalent to having a
certain number of algebraically independent polynomials among the defined
polynomials. Our proposed approach results in characterizing the maximum number
of algebraically independent polynomials in terms of a simple geometric
structure of the sampling pattern, and therefore we obtain the deterministic
necessary and sufficient condition on the sampling pattern for finite
completability of the sampled tensor. Moreover, assuming that the entries of
the tensor are sampled independently with probability $p$ and using the
mentioned deterministic analysis, we propose a combinatorial method to derive a
lower bound on the sampling probability $p$, or equivalently, the number of
sampled entries that guarantees finite completability with high probability. We
also show that the existing result for the matrix completion problem can be
used to obtain a loose lower bound on the sampling probability $p$. In
addition, we obtain deterministic and probabilistic conditions for unique
completability. It is seen that the number of samples required for finite or
unique completability obtained by the proposed analysis on the CP manifold is
orders-of-magnitude lower than that is obtained by the existing analysis on the
Grassmannian manifold.",Fundamental Conditions for Low-CP-Rank Tensor Completion,rank model show maintaining method <unk>
428,"We propose a general Bayesian network model for application in a wide class
of problems of therapy monitoring. We discuss the use of stochastic simulation
as a computational approach to inference on the proposed class of models. As an
illustration we present an application to the monitoring of cytotoxic
chemotherapy in breast cancer.",Bayesian Networks Aplied to Therapy Monitoring,average generation show constrained data 
429,"We present a system for generating parsers based directly on the metaphor of
parsing as deduction. Parsing algorithms can be represented directly as
deduction systems, and a single deduction engine can interpret such deduction
systems so as to implement the corresponding parser. The method generalizes
easily to parsers for augmented phrase structure formalisms, such as
definite-clause grammars and other logic grammar formalisms, and has been used
for rapid prototyping of parsing algorithms for a variety of formalisms
including variants of tree-adjoining grammars, categorial grammars, and
lexicalized context-free grammars.",Principles and Implementation of Deductive Parsing,the model best rapid
430,"In this work, we propose a novel approach to prioritize the depth map
computation of multi-view stereo (MVS) to obtain compact 3D point clouds of
high quality and completeness at low computational cost. Our prioritization
approach operates before the MVS algorithm is executed and consists of two
steps. In the first step, we aim to find a good set of matching partners for
each view. In the second step, we rank the resulting view clusters (i.e. key
views with matching partners) according to their impact on the fulfillment of
desired quality parameters such as completeness, ground resolution and
accuracy. Additional to geometric analysis, we use a novel machine learning
technique for training a confidence predictor. The purpose of this confidence
predictor is to estimate the chances of a successful depth reconstruction for
each pixel in each image for one specific MVS algorithm based on the RGB images
and the image constellation. The underlying machine learning technique does not
require any ground truth or manually labeled data for training, but instead
adapts ideas from depth map fusion for providing a supervision signal. The
trained confidence predictor allows us to evaluate the quality of image
constellations and their potential impact to the resulting 3D reconstruction
and thus builds a solid foundation for our prioritization approach. In our
experiments, we are thus able to reach more than 70% of the maximal reachable
quality fulfillment using only 5% of the available images as key views. For
evaluating our approach within and across different domains, we use two
completely different scenarios, i.e. cultural heritage preservation and
reconstruction of single family houses.","Prioritized Multi-View Stereo Depth Map Generation Using Confidence
  Prediction",based variables  based level method  ensemble
431,"We consider the problem of Robust PCA in the fully and partially observed
settings. Without corruptions, this is the well-known matrix completion
problem. From a statistical standpoint this problem has been recently
well-studied, and conditions on when recovery is possible (how many
observations do we need, how many corruptions can we tolerate) via
polynomial-time algorithms is by now understood. This paper presents and
analyzes a non-convex optimization approach that greatly reduces the
computational complexity of the above problems, compared to the best available
algorithms. In particular, in the fully observed case, with $r$ denoting rank
and $d$ dimension, we reduce the complexity from
$\mathcal{O}(r^2d^2\log(1/\varepsilon))$ to
$\mathcal{O}(rd^2\log(1/\varepsilon))$ -- a big savings when the rank is big.
For the partially observed case, we show the complexity of our algorithm is no
more than $\mathcal{O}(r^4d \log d \log(1/\varepsilon))$. Not only is this the
best-known run-time for a provable algorithm under partial observation, but in
the setting where $r$ is small compared to $d$, it also allows for
near-linear-in-$d$ run-time that can be exploited in the fully-observed case as
well, by simply running our algorithm on a subset of the observations.",Fast Algorithms for Robust PCA via Gradient Descent,complexity stochastic show distributions complexity method
432,"Transforming constraint models is an important task in re- cent constraint
programming systems. User-understandable models are defined during the modeling
phase but rewriting or tuning them is manda- tory to get solving-efficient
models. We propose a new architecture al- lowing to define bridges between any
(modeling or solver) languages and to implement model optimizations. This
architecture follows a model- driven approach where the constraint modeling
process is seen as a set of model transformations. Among others, an interesting
feature is the def- inition of transformations as concept-oriented rules, i.e.
based on types of model elements where the types are organized into a hierarchy
called a metamodel.","Using ATL to define advanced and flexible constraint model
  transformations",based <unk> squared noisy show
433,"Hand detection is essential for many hand related tasks, e.g. parsing hand
pose, understanding gesture, which are extremely useful for robotics and
human-computer interaction. However, hand detection in uncontrolled
environments is challenging due to the flexibility of wrist joint and cluttered
background. We propose a deep learning based approach which detects hands and
calibrates in-plane rotation under supervision at the same time. To guarantee
the recall, we propose a context aware proposal generation algorithm which
significantly outperforms the selective search. We then design a convolutional
neural network(CNN) which handles object rotation explicitly to jointly solve
the object detection and rotation estimation tasks. Experiments show that our
method achieves better results than state-of-the-art detection models on
widely-used benchmarks such as Oxford and Egohands database. We further show
that rotation estimation and classification can mutually benefit each other.",Joint Hand Detection and Rotation Estimation by Using CNN,selection model preparation using <unk>
434,"This paper considers the problem of information capacity of a random neural
network. The network is represented by matrices that are square and
symmetrical. The matrices have a weight which determines the highest and lowest
possible value found in the matrix. The examined matrices are randomly
generated and analyzed by a computer program. We find the surprising result
that the capacity of the network is a maximum for the binary random neural
network and it does not change as the number of quantization levels associated
with the weights increases.",Memory Capacity of a Random Neural Network,using self driving using research inference
435,"We develop a new collaborative filtering (CF) method that combines both
previously known users' preferences, i.e. standard CF, as well as product/user
attributes, i.e. classical function approximation, to predict a given user's
interest in a particular product. Our method is a generalized low rank matrix
completion problem, where we learn a function whose inputs are pairs of vectors
-- the standard low rank matrix completion problem being a special case where
the inputs to the function are the row and column indices of the matrix. We
solve this generalized matrix completion problem using tensor product kernels
for which we also formally generalize standard kernel properties. Benchmark
experiments on movie ratings show the advantages of our generalized matrix
completion method over the standard matrix completion one with no information
about movies or people, as well as over standard multi-task or single task
learning methods.",Low-rank matrix factorization with attributes,complexity preferences show show loss
436,"We present a novel attribute learning framework named Hypergraph-based
Attribute Predictor (HAP). In HAP, a hypergraph is leveraged to depict the
attribute relations in the data. Then the attribute prediction problem is
casted as a regularized hypergraph cut problem in which HAP jointly learns a
collection of attribute projections from the feature space to a hypergraph
embedding space aligned with the attribute space. The learned projections
directly act as attribute classifiers (linear and kernelized). This formulation
leads to a very efficient approach. By considering our model as a multi-graph
cut task, our framework can flexibly incorporate other available information,
in particular class label. We apply our approach to attribute prediction,
Zero-shot and $N$-shot learning tasks. The results on AWA, USAA and CUB
databases demonstrate the value of our methods in comparison with the
state-of-the-art approaches.",Learning Hypergraph-regularized Attribute Predictors,learning dictionary  bandit model simple
437,"Recurrent neural networks have achieved excellent performance in many
applications. However, on portable devices with limited resources, the models
are often too large to deploy. For applications on the server with large scale
concurrent requests, the latency during inference can also be very critical for
costly computing resources. In this work, we address these problems by
quantizing the network, both weights and activations, into multiple binary
codes {-1,+1}. We formulate the quantization as an optimization problem. Under
the key observation that once the quantization coefficients are fixed the
binary codes can be derived efficiently by binary search tree, alternating
minimization is then applied. We test the quantization for two well-known RNNs,
i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the
language models. Compared with the full-precision counter part, by 2-bit
quantization we can achieve ~16x memory saving and ~6x real inference
acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit
quantization, we can achieve almost no loss in the accuracy or even surpass the
original model, with ~10.5x memory saving and ~3x real inference acceleration.
Both results beat the exiting quantization works with large margins. We extend
our alternating quantization to image classification tasks. In both RNNs and
feedforward neural networks, the method also achieves excellent performance.",Alternating Multi-bit Quantization for Recurrent Neural Networks,programming using reflects show distribution using size consistency
438,"The paper gives an artificial neural network (ANN) approach to time series
modeling, the data being instance versus notes (characterized by pitch)
depicting the structure of a North Indian raga, namely, Bageshree. Respecting
the sentiments of the artists' community, the paper argues why it is more
ethical to model a structure than try and ""manufacture"" an artist by training
the neural network to copy performances of artists. Indian Classical Music
centers on the ragas, where emotion and devotion are both important and neither
can be substituted by such ""calculated artistry"" which the ANN generated copies
are ultimately up to.","On an Ethical Use of Neural Networks: A Case Study on a North Indian
  Raga",channel distribution set layer different convex models
439,"While variational methods have been among the most powerful tools for solving
linear inverse problems in imaging, deep (convolutional) neural networks have
recently taken the lead in many challenging benchmarks. A remaining drawback of
deep learning approaches is their requirement for an expensive retraining
whenever the specific problem, the noise level, noise type, or desired measure
of fidelity changes. On the contrary, variational methods have a plug-and-play
nature as they usually consist of separate data fidelity and regularization
terms.
  In this paper we study the possibility of replacing the proximal operator of
the regularization used in many convex energy minimization algorithms by a
denoising neural network. The latter therefore serves as an implicit natural
image prior, while the data term can still be chosen independently. Using a
fixed denoising neural network in exemplary problems of image deconvolution
with different blur kernels and image demosaicking, we obtain state-of-the-art
reconstruction results. These indicate the high generalizability of our
approach and a reduction of the need for problem-specific training.
Additionally, we discuss novel results on the analysis of possible optimization
algorithms to incorporate the network into, as well as the choices of algorithm
parameters and their relation to the noise level the neural network is trained
on.","Learning Proximal Operators: Using Denoising Networks for Regularizing
  Inverse Imaging Problems",significant <unk> linear model significant noise for
440,"A Bayesian Belief Network (BN) is a model of a joint distribution over a
setof n variables, with a DAG structure to represent the immediate
dependenciesbetween the variables, and a set of parameters (aka CPTables) to
represent thelocal conditional probabilities of a node, given each assignment
to itsparents. In many situations, these parameters are themselves random
variables - this may reflect the uncertainty of the domain expert, or may come
from atraining sample used to estimate the parameter values. The distribution
overthese ""CPtable variables"" induces a distribution over the response the
BNwill return to any ""What is Pr(H | E)?"" query. This paper investigates
thevariance of this response, showing first that it is asymptotically
normal,then providing its mean and asymptotical variance. We then present
aneffective general algorithm for computing this variance, which has the
samecomplexity as simply computing the (mean value of) the response itself -
ie,O(n 2^w), where n is the number of variables and w is the effective
treewidth. Finally, we provide empirical evidence that this algorithm,
whichincorporates assumptions and approximations, works effectively in
practice,given only small samples.",Bayesian Error-Bars for Belief Net Inference,models tasks  required proposed distributions rapid
441,"With the availability of more powerful computers, iterative reconstruction
algorithms are the subject of an ongoing work in the design of more efficient
reconstruction algorithms for X-ray computed tomography. In this work, we show
how two analytical reconstruction algorithms can be improved by correcting the
corresponding reconstructions using a randomized iterative reconstruction
algorithm. The combined analytical reconstruction followed by randomized
iterative reconstruction can also be viewed as a reconstruction algorithm
which, in the experiments we have conducted, uses up to $35\%$ less projection
angles as compared to the analytical reconstruction algorithms and produces the
same results in terms of quality of reconstruction, without increasing the
execution time significantly.","Randomized Iterative Reconstruction for Sparse View X-ray Computed
  Tomography",continual using networks layer different evaluation reconstruct
442,"We introduce contextual explanation networks (CENs)---a class of models that
learn to predict by generating and leveraging intermediate explanations. CENs
are deep networks that generate parameters for context-specific probabilistic
graphical models which are further used for prediction and play the role of
explanations. Contrary to the existing post-hoc model-explanation tools, CENs
learn to predict and to explain jointly. Our approach offers two major
advantages: (i) for each prediction, valid instance-specific explanations are
generated with no computational overhead and (ii) prediction via explanation
acts as a regularization and boosts performance in low-resource settings. We
prove that local approximations to the decision boundary of our networks are
consistent with the generated explanations. Our results on image and text
classification and survival analysis tasks demonstrate that CENs are
competitive with the state-of-the-art while offering additional insights behind
each prediction, valuable for decision support.",Contextual Explanation Networks,concepts model proposed <unk> focus logic 
443,"In this paper, we consider the problem of finding dense intrinsic
correspondence between manifolds using the recently introduced functional
framework. We pose the functional correspondence problem as matrix completion
with manifold geometric structure and inducing functional localization with the
$L_1$ norm. We discuss efficient numerical procedures for the solution of our
problem. Our method compares favorably to the accuracy of state-of-the-art
correspondence algorithms on non-rigid shape matching benchmarks, and is
especially advantageous in settings when only scarce data is available.",Functional correspondence by matrix completion,learning model model <unk>
444,"The interest in statistical machine translation systems increases currently
due to political and social events in the world. A proposed Statistical Machine
Translation (SMT) based model that can be used to translate a sentence from the
source Language (English) to the target language (Arabic) automatically through
efficiently incorporating different statistical and Natural Language Processing
(NLP) models such as language model, alignment model, phrase based model,
reordering model, and translation model. These models are combined to enhance
the performance of statistical machine translation (SMT). Many implementation
tools have been used in this work such as Moses, Gizaa++, IRSTLM, KenLM, and
BLEU. Based on the implementation, evaluation of this model, and comparing the
generated translation with other implemented machine translation systems like
Google Translate, it was proved that this proposed model has enhanced the
results of the statistical machine translation, and forms a reliable and
efficient model in this field of research.","A Hybrid Model for Enhancing Lexical Statistical Machine Translation
  (SMT)",empirical naturally
445,"Wireless sensor networks usually comprise a large number of sensors
monitoring changes in variables. These changes in variables represent changes
in physical quantities. The changes can occur for various reasons; these
reasons are highlighted in this work. Outliers are unusual measurements.
Outliers are important; they are information-bearing occurrences. This work
seeks to identify them based on an approach presented in [1]. A critical review
of most previous works in this area has been presented in [2], and few more are
considered here just to set the stage. The main work can be described as this;
given a set of measurements from sensors that represent a normal situation, [1]
proceeds by first estimating the probability density function (pdf) of the set
using a data-split approach, then estimate the entropy of the set using the
arithmetic mean as an approximation for the expectation. The increase in
entropy that occurs when strange data is recorded is used to detect unusual
measurements in the test set depending on the desired confidence interval or
false alarm rate. The results presented in [1] have been confirmed for
different test signals such as the Gaussian, Beta, in one dimension and beta in
two dimensions, and a beta and uniform mixture distribution in two dimensions.
Finally, the method was confirmed on real data and the results are presented.
The major drawbacks of [1] were identified, and a method that seeks to mitigate
this using the Bhattacharyya distance is presented. This method detects more
subtle anomalies, especially the type that would pass as normal in [1].
Finally, recommendations for future research are presented: the subject of
interpretability, especially for subtle measurements, being the most elusive as
of today.",Anomaly Detection in Wireless Sensor Networks,produces video set ## 
446,"Plane model extraction from three-dimensional point clouds is a necessary
step in many different applications such as planar object reconstruction,
indoor mapping and indoor localization. Different RANdom SAmple Consensus
(RANSAC)-based methods have been proposed for this purpose in recent years. In
this study, we propose a novel method-based on RANSAC called Multiplane Model
Estimation, which can estimate multiple plane models simultaneously from a
noisy point cloud using the knowledge extracted from a scene (or an object) in
order to reconstruct it accurately. This method comprises two steps: first, it
clusters the data into planar faces that preserve some constraints defined by
knowledge related to the object (e.g., the angles between faces); and second,
the models of the planes are estimated based on these data using a novel
multi-constraint RANSAC. We performed experiments in the clustering and RANSAC
stages, which showed that the proposed method performed better than
state-of-the-art methods.","Three-dimensional planar model estimation using multi-constraint
  knowledge based on k-means and RANSAC",learning domain  model categorization stochastic method high models
447,"We study the problem of learning to choose from m discrete treatment options
(e.g., news item or medical drug) the one with best causal effect for a
particular instance (e.g., user or patient) where the training data consists of
passive observations of covariates, treatment, and the outcome of the
treatment. The standard approach to this problem is regress and compare: split
the training data by treatment, fit a regression model in each split, and, for
a new instance, predict all m outcomes and pick the best. By reformulating the
problem as a single learning task rather than m separate ones, we propose a new
approach based on recursively partitioning the data into regimes where
different treatments are optimal. We extend this approach to an optimal
partitioning approach that finds a globally optimal partition, achieving a
compact, interpretable, and impactful personalization model. We develop new
tools for validating and evaluating personalization models on observational
data and use these to demonstrate the power of our novel approaches in a
personalized medicine and a job training application.",Recursive Partitioning for Personalization using Observational Data,programming uses show show feature data concept dynamic
448,"The performance of modern face recognition systems is a function of the
dataset on which they are trained. Most datasets are largely biased toward
""near-frontal"" views with benign lighting conditions, negatively effecting
recognition performance on images that do not meet these criteria. The proposed
approach demonstrates how a baseline training set can be augmented to increase
pose and lighting variability using semi-synthetic images with simulated pose
and lighting conditions. The semi-synthetic images are generated using a fast
and robust 3-d shape estimation and rendering pipeline which includes the full
head and background. Various methods of incorporating the semi-synthetic
renderings into the training procedure of a state of the art deep neural
network-based recognition system without modifying the structure of the network
itself are investigated. Quantitative results are presented on the challenging
IJB-A identification dataset using a state of the art recognition pipeline as a
baseline.",Dataset Augmentation for Pose and Lighting Invariant Face Recognition,networks model <unk> visualizations
449,"Most image instance retrieval pipelines are based on comparison of vectors
known as global image descriptors between a query image and the database
images. Due to their success in large scale image classification,
representations extracted from Convolutional Neural Networks (CNN) are quickly
gaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptors
for image instance retrieval. While CNN-based descriptors are generally
remarked for good retrieval performance at lower bitrates, they nevertheless
present a number of drawbacks including the lack of robustness to common object
transformations such as rotations compared with their interest point based FV
counterparts.
  In this paper, we propose a method for computing invariant global descriptors
from CNNs. Our method implements a recently proposed mathematical theory for
invariance in a sensory cortex modeled as a feedforward neural network. The
resulting global descriptors can be made invariant to multiple arbitrary
transformation groups while retaining good discriminativeness.
  Based on a thorough empirical evaluation using several publicly available
datasets, we show that our method is able to significantly and consistently
improve retrieval results every time a new type of invariance is incorporated.
We also show that our method which has few parameters is not prone to
overfitting: improvements generalize well across datasets with different
properties with regard to invariances. Finally, we show that our descriptors
are able to compare favourably to other state-of-the-art compact descriptors in
similar bitranges, exceeding the highest retrieval results reported in the
literature on some datasets. A dedicated dimensionality reduction step
--quantization or hashing-- may be able to further improve the competitiveness
of the descriptors.",Group Invariant Deep Representations for Image Instance Retrieval,new model model researchers using model networks
450,"From concentration inequalities for the suprema of Gaussian or Rademacher
processes an inequality is derived. It is applied to sharpen existing and to
derive novel bounds on the empirical Rademacher complexities of unit balls in
various norms appearing in the context of structured sparsity and multitask
dictionary learning or matrix factorization. A key role is played by the
largest eigenvalue of the data covariance matrix.","An Inequality with Applications to Structured Sparsity and Multitask
  Dictionary Learning",using meaningful variational show new tensor
451,"The field of Distributed Constraint Optimization has gained momentum in
recent years thanks to its ability to address various applications related to
multi-agent cooperation. While techniques to solve Distributed Constraint
Optimization Problems (DCOPs) are abundant and have matured substantially since
the field inception, the number of DCOP realistic applications and benchmark
used to asses the performance of DCOP algorithms is lagging behind. To contrast
this background we (i) introduce the Smart Home Device Scheduling (SHDS)
problem, which describe the problem of coordinating smart devices schedules
across multiple homes as a multi-agent system, (ii) detail the physical models
adopted to simulate smart sensors, smart actuators, and homes environments, and
(iii) introduce a DCOP realistic benchmark for SHDS problems.","A Realistic Dataset for the Smart Home Device Scheduling Problem for
  DCOPs",nearly using high models
452,"Flower pollination algorithm is a new nature-inspired algorithm, based on the
characteristics of flowering plants. In this paper, we extend this flower
algorithm to solve multi-objective optimization problems in engineering. By
using the weighted sum method with random weights, we show that the proposed
multi-objective flower algorithm can accurately find the Pareto fronts for a
set of test functions. We then solve a bi-objective disc brake design problem,
which indeed converges quickly.",Multi-objective Flower Algorithm for Optimization,paper  for
453,"Max-cut, clustering, and many other partitioning problems that are of
significant importance to machine learning and other scientific fields are
NP-hard, a reality that has motivated researchers to develop a wealth of
approximation algorithms and heuristics. Although the best algorithm to use
typically depends on the specific application domain, a worst-case analysis is
often used to compare algorithms. This may be misleading if worst-case
instances occur infrequently, and thus there is a demand for optimization
methods which return the algorithm configuration best suited for the given
application's typical inputs. We address this problem for clustering, max-cut,
and other partitioning problems, such as integer quadratic programming, by
designing computationally efficient and sample efficient learning algorithms
which receive samples from an application-specific distribution over problem
instances and learn a partitioning algorithm with high expected performance.
Our algorithms learn over common integer quadratic programming and clustering
algorithm families: SDP rounding algorithms and agglomerative clustering
algorithms with dynamic programming. For our sample complexity analysis, we
provide tight bounds on the pseudodimension of these algorithm classes, and
show that surprisingly, even for classes of algorithms parameterized by a
single parameter, the pseudo-dimension is superconstant. In this way, our work
both contributes to the foundations of algorithm configuration and pushes the
boundaries of learning theory, since the algorithm classes we analyze consist
of multi-stage optimization procedures and are significantly more complex than
classes typically studied in learning theory.","Learning-Theoretic Foundations of Algorithm Configuration for
  Combinatorial Partitioning Problems",complexity services demonstrate model correlations
454,"Before the operation of a motor imagery based brain-computer interface (BCI)
adopting machine learning techniques, a cumbersome training procedure is
unavoidable. The development of a practical BCI posed the challenge of
classifying single-trial EEG with a small training set. In this letter, we
addressed this problem by employing a series of signal processing and machine
learning approaches to alleviate overfitting and obtained test accuracy similar
to training accuracy on the datasets from BCI Competition III and our own
experiments.","Classifying Single-Trial EEG during Motor Imagery with a Small Training
  Set",video faces model data user two
455,"In this paper, we use a fully convolutional neural network (FCNN) for the
segmentation of gliomas from Magnetic Resonance Images (MRI). A fully
automatic, voxel based classification was achieved by training a 23 layer deep
FCNN on 2-D slices extracted from patient volumes. The network was trained on
slices extracted from 130 patients and validated on 50 patients. For the task
of survival prediction, texture and shape based features were extracted from T1
post contrast volume to train an XGBoost regressor. On BraTS 2017 validation
set, the proposed scheme achieved a mean whole tumor, tumor core and active
dice score of 0.83, 0.69 and 0.69 respectively and an accuracy of 52% for the
overall survival prediction.","Automatic Segmentation and Overall Survival Prediction in Gliomas using
  Fully Convolutional Neural Network and Texture Analysis",bandit show geometry linear
456,"CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerful
technique for tensor completion through explicitly capturing the multilinear
latent factors. The existing CP algorithms require the tensor rank to be
manually specified, however, the determination of tensor rank remains a
challenging problem especially for CP rank. In addition, existing approaches do
not take into account uncertainty information of latent factors, as well as
missing entries. To address these issues, we formulate CP factorization using a
hierarchical probabilistic model and employ a fully Bayesian treatment by
incorporating a sparsity-inducing prior over multiple latent factors and the
appropriate hyperpriors over all hyperparameters, resulting in automatic rank
determination. To learn the model, we develop an efficient deterministic
Bayesian inference algorithm, which scales linearly with data size. Our method
is characterized as a tuning parameter-free approach, which can effectively
infer underlying multilinear factors with a low-rank constraint, while also
providing predictive distributions over missing entries. Extensive simulations
on synthetic data illustrate the intrinsic capability of our method to recover
the ground-truth of CP rank and prevent the overfitting problem, even when a
large amount of entries are missing. Moreover, the results from real-world
applications, including image inpainting and facial image synthesis,
demonstrate that our method outperforms state-of-the-art approaches for both
tensor factorization and tensor completion in terms of predictive performance.","Bayesian CP Factorization of Incomplete Tensors with Automatic Rank
  Determination",produces require the problem <unk>
457,"We stabilize the activations of Recurrent Neural Networks (RNNs) by
penalizing the squared distance between successive hidden states' norms.
  This penalty term is an effective regularizer for RNNs including LSTMs and
IRNNs, improving performance on character-level language modeling and phoneme
recognition, and outperforming weight noise and dropout.
  We achieve competitive performance (18.6\% PER) on the TIMIT phoneme
recognition task for RNNs evaluated without beam search or an RNN transducer.
  With this penalty term, IRNN can achieve similar performance to LSTM on
language modeling, although adding the penalty term to the LSTM results in
superior performance.
  Our penalty term also prevents the exponential growth of IRNN's activations
outside of their training horizon, allowing them to generalize to much longer
sequences.",Regularizing RNNs by Stabilizing Activations,noise rating image learning version
458,"Swarm intelligence and bio-inspired algorithms form a hot topic in the
developments of new algorithms inspired by nature. These nature-inspired
metaheuristic algorithms can be based on swarm intelligence, biological
systems, physical and chemical systems. Therefore, these algorithms can be
called swarm-intelligence-based, bio-inspired, physics-based and
chemistry-based, depending on the sources of inspiration. Though not all of
them are efficient, a few algorithms have proved to be very efficient and thus
have become popular tools for solving real-world problems. Some algorithms are
insufficiently studied. The purpose of this review is to present a relatively
comprehensive list of all the algorithms in the literature, so as to inspire
further research.",A Brief Review of Nature-Inspired Algorithms for Optimization,theory ranking show theory ranking
459,"Program specialisation aims at improving the overall performance of programs
by performing source to source transformations. A common approach within
functional and logic programming, known respectively as partial evaluation and
partial deduction, is to exploit partial knowledge about the input. It is
achieved through a well-automated application of parts of the
Burstall-Darlington unfold/fold transformation framework. The main challenge in
developing systems is to design automatic control that ensures correctness,
efficiency, and termination. This survey and tutorial presents the main
developments in controlling partial deduction over the past 10 years and
analyses their respective merits and shortcomings. It ends with an assessment
of current achievements and sketches some remaining research challenges.",Logic program specialisation through partial deduction: Control issues,train similarity estimated
460,"The production system is a theoretical model of computation relevant to the
artificial intelligence field allowing for problem solving procedures such as
hierarchical tree search. In this work we explore some of the connections
between artificial intelligence and quantum computation by presenting a model
for a quantum production system. Our approach focuses on initially developing a
model for a reversible production system which is a simple mapping of Bennett's
reversible Turing machine. We then expand on this result in order to
accommodate for the requirements of quantum computation. We present the details
of how our proposition can be used alongside Grover's algorithm in order to
yield a speedup comparatively to its classical counterpart. We discuss the
requirements associated with such a speedup and how it compares against a
similar quantum hierarchical search approach.",A Quantum Production Model,scheme  model <unk>
461,"During the past decade, several areas of speech and language understanding
have witnessed substantial breakthroughs from the use of data-driven models. In
the area of dialogue systems, the trend is less obvious, and most practical
systems are still built through significant engineering and expert knowledge.
Nevertheless, several recent results suggest that data-driven approaches are
feasible and quite promising. To facilitate research in this area, we have
carried out a wide survey of publicly available datasets suitable for
data-driven learning of dialogue systems. We discuss important characteristics
of these datasets, how they can be used to learn diverse dialogue strategies,
and their other potential uses. We also examine methods for transfer learning
between datasets and the use of external knowledge. Finally, we discuss
appropriate choice of evaluation metrics for the learning objective.",A Survey of Available Corpora for Building Data-Driven Dialogue Systems,models models show significant video step
462,"Episodic control has been proposed as a third approach to reinforcement
learning, besides model-free and model-based control, by analogy with the three
types of human memory. i.e. episodic, procedural and semantic memory. But the
theoretical properties of episodic control are not well investigated. Here I
show that in deterministic tree Markov decision processes, episodic control is
equivalent to a form of prioritized sweeping in terms of sample efficiency as
well as memory and computation demands. For general deterministic and
stochastic environments, prioritized sweeping performs better even when memory
and computation demands are restricted to be equal to those of episodic
control. These results suggest generalizations of prioritized sweeping to
partially observable environments, its combined use with function approximation
and the search for possible implementations of prioritized sweeping in brains.",Is prioritized sweeping the better episodic control?,<unk> model filters <unk>
463,"We propose an efficient approach to exploiting motion information from
consecutive frames of a video sequence to recover the 3D pose of people.
Previous approaches typically compute candidate poses in individual frames and
then link them in a post-processing step to resolve ambiguities. By contrast,
we directly regress from a spatio-temporal volume of bounding boxes to a 3D
pose in the central frame.
  We further show that, for this approach to achieve its full potential, it is
essential to compensate for the motion in consecutive frames so that the
subject remains centered. This then allows us to effectively overcome
ambiguities and improve upon the state-of-the-art by a large margin on the
Human3.6m, HumanEva, and KTH Multiview Football 3D human pose estimation
benchmarks.",Direct Prediction of 3D Body Poses from Motion Compensated Sequences,online present differential minimization using differential  ii 
464,"In the recent literature the important role of depth in deep learning has
been emphasized. In this paper we argue that sufficient width of a feedforward
network is equally important by answering the simple question under which
conditions the decision regions of a neural network are connected. It turns out
that for a class of activation functions including leaky ReLU, neural networks
having a pyramidal structure, that is no layer has more hidden units than the
input dimension, produce necessarily connected decision regions. This implies
that a sufficiently wide layer is necessary to produce disconnected decision
regions. We discuss the implications of this result for the construction of
neural networks, in particular the relation to the problem of adversarial
manipulation of classifiers.","Neural Networks Should Be Wide Enough to Learn Disconnected Decision
  Regions",<unk> proposed paper  for
465,"In this paper, we report a knowledge-based method for Word Sense
Disambiguation in the domains of biomedical and clinical text. We combine word
representations created on large corpora with a small number of definitions
from the UMLS to create concept representations, which we then compare to
representations of the context of ambiguous terms. Using no relational
information, we obtain comparable performance to previous approaches on the
MSH-WSD dataset, which is a well-known dataset in the biomedical domain.
Additionally, our method is fast and easy to set up and extend to other
domains. Supplementary materials, including source code, can be found at https:
//github.com/clips/yarn","Using Distributed Representations to Disambiguate Biomedical and
  Clinical Concepts",operator models show <unk> <unk> using fundamental usually
466,"We present a stochastic setting for optimization problems with nonsmooth
convex separable objective functions over linear equality constraints. To solve
such problems, we propose a stochastic Alternating Direction Method of
Multipliers (ADMM) algorithm. Our algorithm applies to a more general class of
nonsmooth convex functions that does not necessarily have a closed-form
solution by minimizing the augmented function directly. We also demonstrate the
rates of convergence for our algorithm under various structural assumptions of
the stochastic functions: $O(1/\sqrt{t})$ for convex functions and $O(\log
t/t)$ for strongly convex functions. Compared to previous literature, we
establish the convergence rate of ADMM algorithm, for the first time, in terms
of both the objective value and the feasibility violation.",Stochastic ADMM for Nonsmooth Optimization,#  model kernel <unk>
467,"How should we present training examples to learners to teach them
classification rules? This is a natural problem when training workers for
crowdsourcing labeling tasks, and is also motivated by challenges in
data-driven online education. We propose a natural stochastic model of the
learners, modeling them as randomly switching among hypotheses based on
observed feedback. We then develop STRICT, an efficient algorithm for selecting
examples to teach to workers. Our solution greedily maximizes a submodular
surrogate objective function in order to select examples to show to the
learners. We prove that our strategy is competitive with the optimal teaching
policy. Moreover, for the special case of linear separators, we prove that an
exponential reduction in error probability can be achieved. Our experiments on
simulated workers as well as three real image annotation tasks on Amazon
Mechanical Turk show the effectiveness of our teaching algorithm.",Near-Optimally Teaching the Crowd to Classify,method process echo
468,"In Multimodal Neural Machine Translation (MNMT), a neural model generates a
translated sentence that describes an image, given the image itself and one
source descriptions in English. This is considered as the multimodal image
caption translation task. The images are processed with Convolutional Neural
Network (CNN) to extract visual features exploitable by the translation model.
So far, the CNNs used are pre-trained on object detection and localization
task. We hypothesize that richer architecture, such as dense captioning models,
may be more suitable for MNMT and could lead to improved translations. We
extend this intuition to the word-embeddings, where we compute both linguistic
and visual representation for our corpus vocabulary. We combine and compare
different confi","Visually Grounded Word Embeddings and Richer Visual Features for
  Improving Multimodal Neural Machine Translation",learning means model implications
469,"Proximity Distribution Kernel is an effective method for bag-of-featues based
image representation. In this paper, we investigate the soft assignment of
visual words to image features for proximity distribution. Visual word
contribution function is proposed to model ambiguous proximity distributions.
Three ambiguous proximity distributions is developed by three ambiguous
contribution functions. The experiments are conducted on both classification
and retrieval of medical image data sets. The results show that the performance
of the proposed methods, Proximity Distribution Kernel (PDK), is better or
comparable to the state-of-the-art bag-of-features based image representation
methods.",Ambiguous Proximity Distribution,complexity number based two <unk>
470,"The area under the ROC curve is widely used as a measure of performance of
classification rules. However, it has recently been shown that the measure is
fundamentally incoherent, in the sense that it treats the relative severities
of misclassifications differently when different classifiers are used. To
overcome this, Hand (2009) proposed the $H$ measure, which allows a given
researcher to fix the distribution of relative severities to a
classifier-independent setting on a given problem. This note extends the
discussion, and proposes a modified standard distribution for the $H$ measure,
which better matches the requirements of researchers, in particular those faced
with heavily unbalanced datasets, the $Beta(\pi_1+1,\pi_0+1)$ distribution.
[Preprint submitted at Pattern Recognition Letters]",A better Beta for the H measure of classification performance,points significant <unk>
471,"Recognising objects according to a pre-defined fixed set of class labels has
been well studied in the Computer Vision. There are a great many practical
applications where the subjects that may be of interest are not known
beforehand, or so easily delineated, however. In many of these cases natural
language dialog is a natural way to specify the subject of interest, and the
task achieving this capability (a.k.a, Referring Expression Comprehension) has
recently attracted attention. To this end we propose a unified framework, the
ParalleL AttentioN (PLAN) network, to discover the object in an image that is
being referred to in variable length natural expression descriptions, from
short phrases query to long multi-round dialogs. The PLAN network has two
attention mechanisms that relate parts of the expressions to both the global
visual content and also directly to object candidates. Furthermore, the
attention mechanisms are recurrent, making the referring process visualizable
and explainable. The attended information from these dual sources are combined
to reason about the referred object. These two attention mechanisms can be
trained in parallel and we find the combined system outperforms the
state-of-art on several benchmarked datasets with different length language
input, such as RefCOCO, RefCOCO+ and GuessWhat?!.","Parallel Attention: A Unified Framework for Visual Object Discovery
  through Dialogs and Queries",new new evaluation networks show process discuss
472,"Precise 3D segmentation of infant brain tissues is an essential step towards
comprehensive volumetric studies and quantitative analysis of early brain
developement. However, computing such segmentations is very challenging,
especially for 6-month infant brain, due to the poor image quality, among other
difficulties inherent to infant brain MRI, e.g., the isointense contrast
between white and gray matter and the severe partial volume effect due to small
brain sizes. This study investigates the problem with an ensemble of semi-dense
fully convolutional neural networks (CNNs), which employs T1-weighted and
T2-weighted MR images as input. We demonstrate that the ensemble agreement is
highly correlated with the segmentation errors. Therefore, our method provides
measures that can guide local user corrections. To the best of our knowledge,
this work is the first ensemble of 3D CNNs for suggesting annotations within
images. Furthermore, inspired by the very recent success of dense networks, we
propose a novel architecture, SemiDenseNet, which connects all convolutional
layers directly to the end of the network. Our architecture allows the
efficient propagation of gradients during training, while limiting the number
of parameters, requiring one order of magnitude less parameters than popular
medical image segmentation networks such as 3D U-Net. Another contribution of
our work is the study of the impact that early or late fusions of multiple
image modalities might have on the performances of deep architectures. We
report evaluations of our method on the public data of the MICCAI iSEG-2017
Challenge on 6-month infant brain MRI segmentation, and show very competitive
results among 21 teams, ranking first or second in most metrics.","Deep CNN ensembles and suggestive annotations for infant brain MRI
  segmentation",methods we data with achieve texture
473,"The proposal is to use clusters, graphs and networks as models in order to
analyse the Web structure. Clusters, graphs and networks provide knowledge
representation and organization. Clusters were generated by co-site analysis.
The sample is a set of academic Web sites from the countries belonging to the
European Union. These clusters are here revisited from the point of view of
graph theory and social network analysis. This is a quantitative and structural
analysis. In fact, the Internet is a computer network that connects people and
organizations. Thus we may consider it to be a social network. The set of Web
academic sites represents an empirical social network, and is viewed as a
virtual community. The network structural properties are here analysed applying
together cluster analysis, graph theory and social network analysis.","Clusters, Graphs, and Networks for Analysing Internet-Web-Supported
  Communication within a Virtual Community",data  model <unk> for
474,"Emotional and physical well-being at workplace is important for a positive
work environment and higher productivity. Jobs such as software programming
lead to a sedentary lifestyle and require high interaction with computers.
Working at the same job for years can cause a feeling of intellectual
stagnation and lack of drive. Many employees experience lack of motivation,
mild to extreme depression due to reasons such as aversion towards job
responsibilities and incompatibility with coworkers or boss. This research
proposed an affect monitoring system EmoFit that would play the role of
psychological and physical health trainer. The day to day computer activity and
body language was analyzed to detect the physical and emotional well-being of
the user. Keystrokes, activity interruptions, eye tracking, facial expressions,
body posture and speech were monitored to gauge the users health. The system
also provided activities such as at-desk exercise and stress relief game and
motivational quotes in an attempt to promote users well-being. The experimental
results and positive feedback from test subjects showed that EmoFit would help
improve emotional and physical well-being at jobs that involve significant
computer usage.",EmoFit: Affect Monitoring System for Sedentary Jobs,improve method greedy estimate  technique precision
475,"Learning compact and interpretable representations is a very natural task,
which has not been solved satisfactorily even for simple binary datasets. In
this paper, we review various ways of composing experts for binary data and
argue that competitive forms of interaction are best suited to learn
low-dimensional representations. We propose a new composition rule that
discourages experts from focusing on similar structures and that penalizes
opposing votes strongly so that abstaining from voting becomes more attractive.
We also introduce a novel sequential initialization procedure, which is based
on a process of oversimplification and correction. Experiments show that with
our approach very intuitive models can be learned.",Compact Compositional Models,representations complexity for show require representations using paper 
476,"The Software Development Life Cycle (SDLC) starts with eliciting requirements
of the customers in the form of Software Requirement Specification (SRS). SRS
document needed for software development is mostly written in Natural
Language(NL) convenient for the client. From the SRS document only, the class
name, its attributes and the functions incorporated in the body of the class
are traced based on pre-knowledge of analyst. The paper intends to present a
review on Object Oriented (OO) analysis using Natural Language Processing (NLP)
techniques. This analysis can be manual where domain expert helps to generate
the required diagram or automated system, where the system generates the
required diagram, from the input in the form of SRS.","Object Oriented Analysis using Natural Language Processing concepts: A
  Review",natural models show significant set
477,"Advances in image restoration and enhancement techniques have led to
discussion about how such algorithmscan be applied as a pre-processing step to
improve automatic visual recognition. In principle, techniques like deblurring
and super-resolution should yield improvements by de-emphasizing noise and
increasing signal in an input image. But the historically divergent goals of
the computational photography and visual recognition communities have created a
significant need for more work in this direction. To facilitate new research,
we introduce a new benchmark dataset called UG^2, which contains three
difficult real-world scenarios: uncontrolled videos taken by UAVs and manned
gliders, as well as controlled videos taken on the ground. Over 160,000
annotated frames forhundreds of ImageNet classes are available, which are used
for baseline experiments that assess the impact of known and unknown image
artifacts and other conditions on common deep learning-based object
classification approaches. Further, current image restoration and enhancement
techniques are evaluated by determining whether or not theyimprove baseline
classification performance. Results showthat there is plenty of room for
algorithmic innovation, making this dataset a useful tool going forward.","UG^2: a Video Benchmark for Assessing the Impact of Image Restoration
  and Enhancement on Automatic Visual Recognition",<unk>
478,"Efficient Human Epithelial-2 (HEp-2) cell image classification can facilitate
the diagnosis of many autoimmune diseases. This paper presents an automatic
framework for this classification task, by utilizing the deep convolutional
neural networks (CNNs) which have recently attracted intensive attention in
visual recognition. This paper elaborates the important components of this
framework, discusses multiple key factors that impact the efficiency of
training a deep CNN, and systematically compares this framework with the
well-established image classification models in the literature. Experiments on
benchmark datasets show that i) the proposed framework can effectively
outperform existing models by properly applying data augmentation; ii) our
CNN-based framework demonstrates excellent adaptability across different
datasets, which is highly desirable for classification under varying laboratory
settings. Our system is ranked high in the cell image classification
competition hosted by ICPR 2014.",HEp-2 Cell Image Classification with Deep Convolutional Neural Networks,segmentation data efficient show models proposed account using
479,"Deep dictionary learning seeks multiple dictionaries at different image
scales to capture complementary coherent characteristics. We propose a method
for learning a hierarchy of synthesis dictionaries with an image classification
goal. The dictionaries and classification parameters are trained by a
classification objective, and the sparse features are extracted by reducing a
reconstruction loss in each layer. The reconstruction objectives in some sense
regularize the classification problem and inject source signal information in
the extracted features. The performance of the proposed hierarchical method
increases by adding more layers, which consequently makes this model easier to
tune and adapt. The proposed algorithm furthermore, shows remarkably lower
fooling rate in presence of adversarial perturbation. The validation of the
proposed approach is based on its classification performance using four
benchmark datasets and is compared to a CNN of similar size.",Deep Dictionary Learning: A PARametric NETwork Approach,method maximal generated method learning users model utility
480,"We proposed a kind of naturally combined shape-color affine moment invariants
(SCAMI), which consider both shape and color affine transformations
simultaneously in one single system. In the real scene, color and shape
deformations always exist in images simultaneously. Simple shape invariants or
color invariants can not be qualified for this situation. The conventional
method is just to make a simple linear combination of the two factors.
Meanwhile, the manual selection of weights is a complex issue. Our construction
method is based on the multiple integration framework. The integral kernel is
assigned as the continued product of the shape and color invariant cores. It is
the first time to directly derive an invariant to dual affine transformations
of shape and color. The manual selection of weights is no longer necessary, and
both the shape and color transformations are extended to affine transformation
group. With the various of invariant cores, a set of lower-order invariants are
constructed and the completeness and independence are discussed detailedly. A
set of SCAMIs, which called SCAMI24, are recommended, and the effectiveness and
robustness have been evaluated on both synthetic and real datasets.","Naturally Combined Shape-Color Moment Invariants under Affine
  Transformations",data measure  highlighting problem <unk>
481,"For a classification problem described by the joint density $P(\omega,x)$,
models of $P(\omega\eq\omega'|x,x')$ (the ``Bayesian similarity measure'') have
been shown to be an optimal similarity measure for nearest neighbor
classification. This paper analyzes demonstrates several additional properties
of that conditional distribution. The paper first shows that we can
reconstruct, up to class labels, the class posterior distribution $P(\omega|x)$
given $P(\omega\eq\omega'|x,x')$, gives a procedure for recovering the class
labels, and gives an asymptotically Bayes-optimal classification procedure. It
also shows, given such an optimal similarity measure, how to construct a
classifier that outperforms the nearest neighbor classifier and achieves
Bayes-optimal classification rates. The paper then analyzes Bayesian similarity
in a framework where a classifier faces a number of related classification
tasks (multitask learning) and illustrates that reconstruction of the class
posterior distribution is not possible in general. Finally, the paper
identifies a distinct class of classification problems using
$P(\omega\eq\omega'|x,x')$ and shows that using $P(\omega\eq\omega'|x,x')$ to
solve those problems is the Bayes optimal solution.",On the Relationship between the Posterior and Optimal Similarity,increase two show different differential first
482,"Multilinear Grammar provides a framework for integrating the many different
syntagmatic structures of language into a coherent semiotically based Rank
Interpretation Architecture, with default linear grammars at each rank. The
architecture defines a Sui Generis Condition on ranks, from discourse through
utterance and phrasal structures to the word, with its sub-ranks of morphology
and phonology. Each rank has unique structures and its own semantic-pragmatic
and prosodic-phonetic interpretation models. Default computational models for
each rank are proposed, based on a Procedural Plausibility Condition:
incremental processing in linear time with finite working memory. We suggest
that the Rank Interpretation Architecture and its multilinear properties
provide systematic design features of human languages, contrasting with
unordered lists of key properties or single structural properties at one rank,
such as recursion, which have previously been been put forward as language
design features. The framework provides a realistic background for the gradual
development of complexity in the phylogeny and ontogeny of language, and
clarifies a range of challenges for the evaluation of realistic linguistic
theories and applications. The empirical objective of the paper is to
demonstrate unique multilinear properties at each rank and thereby motivate the
Multilinear Grammar and Rank Interpretation Architecture framework as a
coherent approach to capturing the complexity of human languages in the
simplest possible way.",Multilinear Grammar: Ranks and Interpretations,complexity models show state of the art  categorization ensemble
483,"This paper presents a new method to learn online policies in continuous
state, continuous action, model-free Markov decision processes, with two
properties that are crucial for practical applications. First, the policies are
implementable with a very low computational cost: once the policy is computed,
the action corresponding to a given state is obtained in logarithmic time with
respect to the number of samples used. Second, our method is versatile: it does
not rely on any a priori knowledge of the structure of optimal policies. We
build upon the Fitted Q-iteration algorithm which represents the $Q$-value as
the average of several regression trees. Our algorithm, the Fitted Policy
Forest algorithm (FPF), computes a regression forest representing the Q-value
and transforms it into a single tree representing the policy, while keeping
control on the size of the policy using resampling and leaf merging. We
introduce an adaptation of Multi-Resolution Exploration (MRE) which is
particularly suited to FPF. We assess the performance of FPF on three classical
benchmarks for reinforcement learning: the ""Inverted Pendulum"", the ""Double
Integrator"" and ""Car on the Hill"" and show that FPF equals or outperforms other
algorithms, although these algorithms rely on the use of particular
representations of the policies, especially chosen in order to fit each of the
three problems. Finally, we exhibit that the combination of FPF and MRE allows
to find nearly optimal solutions in problems where $\epsilon$-greedy approaches
would fail.","Online Reinforcement Learning for Real-Time Exploration in Continuous
  State and Action Markov Decision Processes",accurate demonstrate show fundamental one
484,"Even todays most advanced machine learning models are easily fooled by almost
imperceptible perturbations of their inputs. Foolbox is a new Python package to
generate such adversarial perturbations and to quantify and compare the
robustness of machine learning models. It is build around the idea that the
most comparable robustness measure is the minimum perturbation needed to craft
an adversarial example. To this end, Foolbox provides reference implementations
of most published adversarial attack methods alongside some new ones, all of
which perform internal hyperparameter tuning to find the minimum adversarial
perturbation. Additionally, Foolbox interfaces with most popular deep learning
frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows
different adversarial criteria such as targeted misclassification and top-k
misclassification as well as different distance measures. The code is licensed
under the MIT license and is openly available at
https://github.com/bethgelab/foolbox . The most up-to-date documentation can be
found at http://foolbox.readthedocs.io .","Foolbox: A Python toolbox to benchmark the robustness of machine
  learning models",simple show significant made search
485,"In this paper we present the initial development of a general theory for
mapping inference in predicate logic to computation over Tensor Product
Representations (TPRs; Smolensky (1990), Smolensky & Legendre (2006)). After an
initial brief synopsis of TPRs (Section 0), we begin with particular examples
of inference with TPRs in the 'bAbI' question-answering task of Weston et al.
(2015) (Section 1). We then present a simplification of the general analysis
that suffices for the bAbI task (Section 2). Finally, we lay out the general
treatment of inference over TPRs (Section 3). We also show the simplification
in Section 2 derives the inference methods described in Lee et al. (2016); this
shows how the simple methods of Lee et al. (2016) can be formally extended to
more general reasoning tasks.",Basic Reasoning with Tensor Product Representations,increase model show mapping  #  randomization
486,"Most optimal routing problems focus on minimizing travel time or distance
traveled. Oftentimes, a more useful objective is to maximize the probability of
on-time arrival, which requires statistical distributions of travel times,
rather than just mean values. We propose a method to estimate travel time
distributions on large-scale road networks, using probe vehicle data collected
from GPS. We present a framework that works with large input of data, and
scales linearly with the size of the network. Leveraging the planar topology of
the graph, the method computes efficiently the time correlations between
neighboring streets. First, raw probe vehicle traces are compressed into pairs
of travel times and number of stops for each traversed road segment using a
`stop-and-go' algorithm developed for this work. The compressed data is then
used as input for training a path travel time model, which couples a Markov
model along with a Gaussian Markov random field. Finally, scalable inference
algorithms are developed for obtaining path travel time distributions from the
composite MM-GMRF model. We illustrate the accuracy and scalability of our
model on a 505,000 road link network spanning the San Francisco Bay Area.","Arriving on time: estimating travel time distributions on large-scale
  road networks",new <unk> based function  proposed data require state
487,"In complex, high dimensional and unstructured data it is often difficult to
extract meaningful patterns. This is especially the case when dealing with
textual data. Recent studies in machine learning, information theory and
network science have developed several novel instruments to extract the
semantics of unstructured data, and harness it to build a network of relations.
Such approaches serve as an efficient tool for dimensionality reduction and
pattern detection. This paper applies semantic network science to extract
ideological proximity in the international arena, by focusing on the data from
General Debates in the UN General Assembly on the topics of high salience to
international community. UN General Debate corpus (UNGDC) covers all high-level
debates in the UN General Assembly from 1970 to 2014, covering all UN member
states. The research proceeds in three main steps. First, Latent Dirichlet
Allocation (LDA) is used to extract the topics of the UN speeches, and
therefore semantic information. Each country is then assigned a vector
specifying the exposure to each of the topics identified. This intermediate
output is then used in to construct a network of countries based on information
theoretical metrics where the links capture similar vectorial patterns in the
topic distributions. Topology of the networks is then analyzed through network
properties like density, path length and clustering. Finally, we identify
specific topological features of our networks using the map equation framework
to detect communities in our networks of countries.","Topology Analysis of International Networks Based on Debates in the
  United Nations",show significant agent model objects agent
488,"Quantum annealing is a heuristic quantum algorithm which exploits quantum
resources to minimize an objective function embedded as the energy levels of a
programmable physical system. To take advantage of a potential quantum
advantage, one needs to be able to map the problem of interest to the native
hardware with reasonably low overhead. Because experimental considerations
constrain our objective function to take the form of a low degree PUBO
(polynomial unconstrained binary optimization), we employ non-convex loss
functions which are polynomial functions of the margin. We show that these loss
functions are robust to label noise and provide a clear advantage over convex
methods. These loss functions may also be useful for classical approaches as
they compile to regularized risk expressions which can be evaluated in constant
time with respect to the number of training examples.","Construction of non-convex polynomial loss functions for training a
  binary classifier with quantum annealing",online model online
489,"A martingale framework for concept change detection based on testing data
exchangeability was recently proposed (Ho, 2005). In this paper, we describe
the proposed change-detection test based on the Doob's Maximal Inequality and
show that it is an approximation of the sequential probability ratio test
(SPRT). The relationship between the threshold value used in the proposed test
and its size and power is deduced from the approximation. The mean delay time
before a change is detected is estimated using the average sample number of a
SPRT. The performance of the test using various threshold values is examined on
five different data stream scenarios simulated using two synthetic data sets.
Finally, experimental results show that the test is effective in detecting
changes in time-varying data streams simulated using three benchmark data sets.","On the Detection of Concept Changes in Time-Varying Data Stream by
  Testing Exchangeability",improve the different data learning  required
490,"Content based video retrieval is an approach for facilitating the searching
and browsing of large image collections over World Wide Web. In this approach,
video analysis is conducted on low level visual properties extracted from video
frame. We believed that in order to create an effective video retrieval system,
visual perception must be taken into account. We conjectured that a technique
which employs multiple features for indexing and retrieval would be more
effective in the discrimination and search tasks of videos. In order to
validate this claim, content based indexing and retrieval systems were
implemented using color histogram, various texture features and other
approaches. Videos were stored in Oracle 9i Database and a user study measured
correctness of response.",Content based video retrieval,<unk> set
491,"We propose a method for hand pose estimation based on a deep regressor
trained on two different kinds of input. Raw depth data is fused with an
intermediate representation in the form of a segmentation of the hand into
parts. This intermediate representation contains important topological
information and provides useful cues for reasoning about joint locations. The
mapping from raw depth to segmentation maps is learned in a
semi/weakly-supervised way from two different datasets: (i) a synthetic dataset
created through a rendering pipeline including densely labeled ground truth
(pixelwise segmentations); and (ii) a dataset with real images for which ground
truth joint positions are available, but not dense segmentations. Loss for
training on real images is generated from a patch-wise restoration process,
which aligns tentative segmentation maps with a large dictionary of synthetic
poses. The underlying premise is that the domain shift between synthetic and
real data is smaller in the intermediate representation, where labels carry
geometric and topological meaning, than in the raw input domain. Experiments on
the NYU dataset show that the proposed training method decreases error on
joints over direct regression of joints from depth data by 15.7%.","Hand Pose Estimation through Semi-Supervised and Weakly-Supervised
  Learning",mechanism differential fundamental set conventional
492,"In this paper, we demonstrate the application of Fuzzy Markup Language (FML)
to construct an FML-based Dynamic Assessment Agent (FDAA), and we present an
FML-based Human-Machine Cooperative System (FHMCS) for the game of Go. The
proposed FDAA comprises an intelligent decision-making and learning mechanism,
an intelligent game bot, a proximal development agent, and an intelligent
agent. The intelligent game bot is based on the open-source code of Facebook
Darkforest, and it features a representational state transfer application
programming interface mechanism. The proximal development agent contains a
dynamic assessment mechanism, a GoSocket mechanism, and an FML engine with a
fuzzy knowledge base and rule base. The intelligent agent contains a GoSocket
engine and a summarization agent that is based on the estimated win rate,
real-time simulation number, and matching degree of predicted moves.
Additionally, the FML for player performance evaluation and linguistic
descriptions for game results commentary are presented. We experimentally
verify and validate the performance of the FDAA and variants of the FHMCS by
testing five games in 2016 and 60 games of Google Master Go, a new version of
the AlphaGo program, in January 2017. The experimental results demonstrate that
the proposed FDAA can work effectively for Go applications.","FML-based Dynamic Assessment Agent for Human-Machine Cooperative System
  on Game of Go",paper  system number show linked knowledge
493,"A negative result is when the outcome of an experiment or a model is not what
is expected or when a hypothesis does not hold. Despite being often overlooked
in the scientific community, negative results are results and they carry value.
While this topic has been extensively discussed in other fields such as social
sciences and biosciences, less attention has been paid to it in the computer
vision community. The unique characteristics of computer vision, particularly
its experimental aspect, call for a special treatment of this matter. In this
paper, I will address what makes negative results important, how they should be
disseminated and incentivized, and what lessons can be learned from cognitive
vision research in this regard. Further, I will discuss issues such as computer
vision and human vision interaction, experimental design and statistical
hypothesis testing, explanatory versus predictive modeling, performance
evaluation, model comparison, as well as computer vision research culture.",Negative Results in Computer Vision: A Perspective,new key theory show show set data 
494,"Hash codes are a very efficient data representation needed to be able to cope
with the ever growing amounts of data. We introduce a random forest semantic
hashing scheme with information-theoretic code aggregation, showing for the
first time how random forest, a technique that together with deep learning have
shown spectacular results in classification, can also be extended to
large-scale retrieval. Traditional random forest fails to enforce the
consistency of hashes generated from each tree for the same class data, i.e.,
to preserve the underlying similarity, and it also lacks a principled way for
code aggregation across trees. We start with a simple hashing scheme, where
independently trained random trees in a forest are acting as hashing functions.
We the propose a subspace model as the splitting function, and show that it
enforces the hash consistency in a tree for data from the same class. We also
introduce an information-theoretic approach for aggregating codes of individual
trees into a single hash code, producing a near-optimal unique hash for each
class. Experiments on large-scale public datasets are presented, showing that
the proposed approach significantly outperforms state-of-the-art hashing
methods for retrieval tasks.",Random Forests Can Hash,using limited show predictive texture
495,"In this paper, we use known camera motion associated to a video sequence of a
static scene in order to estimate and incrementally refine the surrounding
depth field. We exploit the SO(3)-invariance of brightness and depth fields
dynamics to customize standard image processing techniques. Inspired by the
Horn-Schunck method, we propose a SO(3)-invariant cost to estimate the depth
field. At each time step, this provides a diffusion equation on the unit
Riemannian sphere that is numerically solved to obtain a real time depth field
estimation of the entire field of view. Two asymptotic observers are derived
from the governing equations of dynamics, respectively based on optical flow
and depth estimations: implemented on noisy sequences of synthetic images as
well as on real data, they perform a more robust and accurate depth estimation.
This approach is complementary to most methods employing state observers for
range estimation, which uniquely concern single or isolated feature points.","SO(3)-invariant asymptotic observers for dense depth field estimation
  based on visual data and known camera motion",signals  categorization cross correlation new <unk>
496,"We consider the problem of minimizing the sum of a smooth function $h$ with a
bounded Hessian, and a nonsmooth function. We assume that the latter function
is a composition of a proper closed function $P$ and a surjective linear map
$\cal M$, with the proximal mappings of $\tau P$, $\tau > 0$, simple to
compute. This problem is nonconvex in general and encompasses many important
applications in engineering and machine learning. In this paper, we examined
two types of splitting methods for solving this nonconvex optimization problem:
alternating direction method of multipliers and proximal gradient algorithm.
For the direct adaptation of the alternating direction method of multipliers,
we show that, if the penalty parameter is chosen sufficiently large and the
sequence generated has a cluster point, then it gives a stationary point of the
nonconvex problem. We also establish convergence of the whole sequence under an
additional assumption that the functions $h$ and $P$ are semi-algebraic.
Furthermore, we give simple sufficient conditions to guarantee boundedness of
the sequence generated. These conditions can be satisfied for a wide range of
applications including the least squares problem with the $\ell_{1/2}$
regularization. Finally, when $\cal M$ is the identity so that the proximal
gradient algorithm can be efficiently applied, we show that any cluster point
is stationary under a slightly more flexible constant step-size rule than what
is known in the literature for a nonconvex $h$.","Global convergence of splitting methods for nonconvex composite
  optimization",concepts learning  a show feature paper  for
497,"Deep Neural nets (NNs) with millions of parameters are at the heart of many
state-of-the-art computer vision systems today. However, recent works have
shown that much smaller models can achieve similar levels of performance. In
this work, we address the problem of pruning parameters in a trained NN model.
Instead of removing individual weights one at a time as done in previous works,
we remove one neuron at a time. We show how similar neurons are redundant, and
propose a systematic way to remove them. Our experiments in pruning the densely
connected layers show that we can remove upto 85\% of the total parameters in
an MNIST-trained network, and about 35\% for AlexNet without significantly
affecting performance. Our method can be applied on top of most networks with a
fully connected layer to give a smaller network.",Data-free parameter pruning for Deep Neural Networks,present model data <unk>
498,"In this paper, we propose an approach to learn hierarchical features for
visual object tracking. First, we offline learn features robust to diverse
motion patterns from auxiliary video sequences. The hierarchical features are
learned via a two-layer convolutional neural network. Embedding the temporal
slowness constraint in the stacked architecture makes the learned features
robust to complicated motion transformations, which is important for visual
object tracking. Then, given a target video sequence, we propose a domain
adaptation module to online adapt the pre-learned features according to the
specific target object. The adaptation is conducted in both layers of the deep
feature learning module so as to include appearance information of the specific
target object. As a result, the learned hierarchical features can be robust to
both complicated motion transformations and appearance changes of target
objects. We integrate our feature learning algorithm into three tracking
methods. Experimental results demonstrate that significant improvement can be
achieved using our learned hierarchical features, especially on video sequences
with complicated motion transformations.",Video Tracking Using Learned Hierarchical Features,using unlabeled texture
499,"Collecting large labeled data sets is a laborious and expensive task, whose
scaling up requires division of the labeling workload between many teachers.
When the number of classes is large, miscorrespondences between the labels
given by the different teachers are likely to occur, which, in the extreme
case, may reach total inconsistency. In this paper we describe how globally
consistent labels can be obtained, despite the absence of teacher coordination,
and discuss the possible efficiency of this process in terms of human labor. We
define a notion of label efficiency, measuring the ratio between the number of
globally consistent labels obtained and the number of labels provided by
distributed teachers. We show that the efficiency depends critically on the
ratio alpha between the number of data instances seen by a single teacher, and
the number of classes. We suggest several algorithms for the distributed
labeling problem, and analyze their efficiency as a function of alpha. In
addition, we provide an upper bound on label efficiency for the case of
completely uncoordinated teachers, and show that efficiency approaches 0 as the
ratio between the number of labels each teacher provides and the number of
classes drops (i.e. alpha goes to 0).",Efficient Human Computation,significant natural approaches approaches consider algorithms  involves using
500,"We present a novel detection method using a deep convolutional neural network
(CNN), named AttentionNet. We cast an object detection problem as an iterative
classification problem, which is the most suitable form of a CNN. AttentionNet
provides quantized weak directions pointing a target object and the ensemble of
iterative predictions from AttentionNet converges to an accurate object
boundary box. Since AttentionNet is a unified network for object detection, it
detects objects without any separated models from the object proposal to the
post bounding-box regression. We evaluate AttentionNet by a human detection
task and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC
2007/2012 with an 8-layered architecture only.",AttentionNet: Aggregating Weak Directions for Accurate Object Detection,studied model signals  improve online different feature models
501,"High-throughput sequencing allows the detection and quantification of
frequencies of somatic single nucleotide variants (SNV) in heterogeneous tumor
cell populations. In some cases, the evolutionary history and population
frequency of the subclonal lineages of tumor cells present in the sample can be
reconstructed from these SNV frequency measurements. However, automated methods
to do this reconstruction are not available and the conditions under which
reconstruction is possible have not been described.
  We describe the conditions under which the evolutionary history can be
uniquely reconstructed from SNV frequencies from single or multiple samples
from the tumor population and we introduce a new statistical model, PhyloSub,
that infers the phylogeny and genotype of the major subclonal lineages
represented in the population of cancer cells. It uses a Bayesian nonparametric
prior over trees that groups SNVs into major subclonal lineages and
automatically estimates the number of lineages and their ancestry. We sample
from the joint posterior distribution over trees to identify evolutionary
histories and cell population frequencies that have the highest probability of
generating the observed SNV frequency data. When multiple phylogenies are
consistent with a given set of SNV frequencies, PhyloSub represents the
uncertainty in the tumor phylogeny using a partial order plot. Experiments on a
simulated dataset and two real datasets comprising tumor samples from acute
myeloid leukemia and chronic lymphocytic leukemia patients demonstrate that
PhyloSub can infer both linear (or chain) and branching lineages and its
inferences are in good agreement with ground truth, where it is available.","Inferring clonal evolution of tumors from single nucleotide somatic
  mutations",models show significant referred
502,"Graph-based representations of images have recently acquired an important
role for classification purposes within the context of machine learning
approaches. The underlying idea is to consider that relevant information of an
image is implicitly encoded into the relationships between more basic entities
that compose by themselves the whole image. The classification problem is then
reformulated in terms of an optimization problem usually solved by a
gradient-based search procedure. Vario-eta through structure is an approximate
second order stochastic optimization technique that achieves a good trade-off
between speed of convergence and the computational effort required. However,
the robustness of this technique for large scale problems has not been yet
assessed. In this paper we firstly provide a theoretical justification of the
assumptions made by this optimization procedure. Secondly, a complexity
analysis of the algorithm is performed to prove its suitability for large scale
learning problems.",Complexity Analysis of Vario-eta through Structure,based <unk>
503,"We introduce a learning-based approach to detect repeatable keypoints under
drastic imaging changes of weather and lighting conditions to which
state-of-the-art keypoint detectors are surprisingly sensitive. We first
identify good keypoint candidates in multiple training images taken from the
same viewpoint. We then train a regressor to predict a score map whose maxima
are those points so that they can be found by simple non-maximum suppression.
As there are no standard datasets to test the influence of these kinds of
changes, we created our own, which we will make publicly available. We will
show that our method significantly outperforms the state-of-the-art methods in
such challenging conditions, while still achieving state-of-the-art performance
on the untrained standard Oxford dataset.",TILDE: A Temporally Invariant Learned DEtector,review show show preferences
504,"Cephalometric analysis has an important role in dentistry and especially in
orthodontics as a treatment planning tool to gauge the size and special
relationships of the teeth, jaws and cranium. The first step of using such
analyses is localizing some important landmarks known as cephalometric
landmarks on craniofacial in x-ray image. The past decade has seen a growing
interest in automating this process. In this paper, a novel hybrid approach is
proposed for automatic detection of cephalometric landmarks. Here, the
landmarks are categorized into three main sets according to their anatomical
characteristics and usage in well-known cephalometric analyses. Consequently,
to have a reliable and accurate detection system, three methods named edge
tracing, weighted template matching, and analysis based estimation are
designed, each of which is consistent and well-suited for one category. Edge
tracing method is suggested to predict those landmarks which are located on
edges. Weighted template matching method is well-suited for landmarks located
in an obvious and specific structure which can be extracted or searchable in a
given x-ray image. The last but not the least method is named analysis based
estimation. This method is based on the fact that in cephalometric analyses the
relations between landmarks are used and the locations of some landmarks are
never used individually. Therefore the third suggested method has a novelty in
estimating the desired relations directly. The effectiveness of the proposed
approach is compared with the state of the art methods and the results were
promising especially in real world applications.",A Novel Hybrid Approach for Cephalometric Landmark Detection,video <unk> number based word paper  system
505,"We consider a neural network architecture with randomized features, a
sign-splitter, followed by rectified linear units (ReLU). We prove that our
architecture exhibits robustness to the input perturbation: the output feature
of the neural network exhibits a Lipschitz continuity in terms of the input
perturbation. We further show that the network output exhibits a discrimination
ability that inputs that are not arbitrarily close generate output vectors
which maintain distance between each other obeying a certain lower bound. This
ensures that two different inputs remain discriminable while contracting the
distance in the output feature space.","R3Net: Random Weights, Rectifier Linear Units and Robustness for
  Artificial Neural Network",systematic visual
506,"The main goal of this work is to analyze the behaviour of the FA quantifier
fuzzification mechanism. As we prove in the paper, this model has a very solid
theorethical behaviour, superior to most of the models defined in the
literature. Moreover, we show that the underlying probabilistic interpretation
has very interesting consequences.","The probatilistic Quantifier Fuzzification Mechanism FA: A theoretical
  analysis",efficiency models show significant means model neighbourhood
507,"Bipartite matching markets pair agents on one side of a market with agents,
items, or contracts on the opposing side. Prior work addresses online bipartite
matching markets, where agents arrive over time and are dynamically matched to
a known set of disposable resources. In this paper, we propose a new model,
Online Matching with (offline) Reusable Resources under Known Adversarial
Distributions (OM-RR-KAD), in which resources on the offline side are reusable
instead of disposable; that is, once matched, resources become available again
at some point in the future. We show that our model is tractable by presenting
an LP-based adaptive algorithm that achieves an online competitive ratio of 1/2
- eps for any given eps greater than 0. We also show that no non-adaptive
algorithm can achieve a ratio of 1/2 + o(1) based on the same benchmark LP.
Through a data-driven analysis on a massive openly-available dataset, we show
our model is robust enough to capture the application of taxi dispatching
services and ride-sharing systems. We also present heuristics that perform well
in practice.","Allocation Problems in Ride-Sharing Platforms: Online Matching with
  Offline Reusable Resources",step problem feature models
508,"The web plays an important role in people's social lives since the emergence
of Web 2.0. It facilitates the interaction between users, gives them the
possibility to freely interact, share and collaborate through social networks,
online communities forums, blogs, wikis and other online collaborative media.
However, an other side of the web is negatively taken such as posting
inflammatory messages. Thus, when dealing with the online communities forums,
the managers seek to always enhance the performance of such platforms. In fact,
to keep the serenity and prohibit the disturbance of the normal atmosphere,
managers always try to novice users against these malicious persons by posting
such message (DO NOT FEED TROLLS). But, this kind of warning is not enough to
reduce this phenomenon. In this context we propose a new approach for detecting
malicious people also called 'Trolls' in order to allow community managers to
take their ability to post online. To be more realistic, our proposal is
defined within an uncertain framework. Based on the assumption consisting on
the trolls' integration in the successful discussion threads, we try to detect
the presence of such malicious users. Indeed, this method is based on a
conflict measure of the belief function theory applied between the different
messages of the thread. In order to show the feasibility and the result of our
approach, we test it in different simulated data.",Trolls Identification within an Uncertain Framework,along model <unk> model types different different potential
509,"Supervised linear feature extraction can be achieved by fitting a reduced
rank multivariate model. This paper studies rank penalized and rank constrained
vector generalized linear models. From the perspective of thresholding rules,
we build a framework for fitting singular value penalized models and use it for
feature extraction. Through solving the rank constraint form of the problem, we
propose progressive feature space reduction for fast computation in high
dimensions with little performance loss. A novel projective cross-validation is
proposed for parameter tuning in such nonconvex setups. Real data applications
are given to show the power of the methodology in supervised dimension
reduction and feature extraction.",Reduced Rank Vector Generalized Linear Models for Feature Extraction,feature user demonstrate show resistive set
510,"We are at an exciting time for machine lipreading. Traditional research
stemmed from the adaptation of audio recognition systems. But now, the computer
vision community is also participating. This joining of two previously
disparate areas with different perspectives on computer lipreading is creating
opportunities for collaborations, but in doing so the literature is
experiencing challenges in knowledge sharing due to multiple uses of terms and
phrases and the range of methods for scoring results.
  In particular we highlight three areas with the intention to improve
communication between those researching lipreading; the effects of
interchanging between speech reading and lipreading; speaker dependence across
train, validation, and test splits; and the use of accuracy, correctness,
errors, and varying units (phonemes, visemes, words, and sentences) to measure
system performance. We make recommendations as to how we can be more
consistent.","Visual speech recognition: aligning terminologies for better
  understanding",models show research show model online
511,"As machine learning systems become ubiquitous, there has been a surge of
interest in interpretable machine learning: systems that provide explanation
for their outputs. These explanations are often used to qualitatively assess
other criteria such as safety or non-discrimination. However, despite the
interest in interpretability, there is very little consensus on what
interpretable machine learning is and how it should be measured. In this
position paper, we first define interpretability and describe when
interpretability is needed (and when it is not). Next, we suggest a taxonomy
for rigorous evaluation and expose open questions towards a more rigorous
science of interpretable machine learning.",Towards A Rigorous Science of Interpretable Machine Learning, mil  number
512,"Generalizing empirical findings to new environments, settings, or populations
is essential in most scientific explorations. This article treats a particular
problem of generalizability, called ""transportability"", defined as a license to
transfer information learned in experimental studies to a different population,
on which only observational studies can be conducted. Given a set of
assumptions concerning commonalities and differences between the two
populations, Pearl and Bareinboim (2011) derived sufficient conditions that
permit such transfer to take place. This article summarizes their findings and
supplements them with an effective procedure for deciding when and how
transportability is feasible. It establishes a necessary and sufficient
condition for deciding when causal effects in the target population are
estimable from both the statistical information available and the causal
information transferred from the experiments. The article further provides a
complete algorithm for computing the transport formula, that is, a way of
combining observational and experimental information to synthesize bias-free
estimate of the desired causal relation. Finally, the article examines the
differences between transportability and other variants of generalizability.","A General Algorithm for Deciding Transportability of Experimental
  Results",<unk> based design <unk> demonstrate
513,"In dialogical argumentation it is often assumed that the involved parties
always correctly identify the intended statements posited by each other,
realize all of the associated relations, conform to the three acceptability
states (accepted, rejected, undecided), adjust their views when new and correct
information comes in, and that a framework handling only attack relations is
sufficient to represent their opinions. Although it is natural to make these
assumptions as a starting point for further research, removing them or even
acknowledging that such removal should happen is more challenging for some of
these concepts than for others. Probabilistic argumentation is one of the
approaches that can be harnessed for more accurate user modelling. The
epistemic approach allows us to represent how much a given argument is believed
by a given person, offering us the possibility to express more than just three
agreement states. It is equipped with a wide range of postulates, including
those that do not make any restrictions concerning how initial arguments should
be viewed, thus potentially being more adequate for handling beliefs of the
people that have not fully disclosed their opinions in comparison to Dung's
semantics. The constellation approach can be used to represent the views of
different people concerning the structure of the framework we are dealing with,
including cases in which not all relations are acknowledged or when they are
seen differently than intended. Finally, bipolar argumentation frameworks can
be used to express both positive and negative relations between arguments. In
this paper we describe the results of an experiment in which participants
judged dialogues in terms of agreement and structure. We compare our findings
with the aforementioned assumptions as well as with the constellation and
epistemic approaches to probabilistic argumentation and bipolar argumentation.","Empirical Evaluation of Abstract Argumentation: Supporting the Need for
  Bipolar and Probabilistic Approaches",word for show packing online
514,"Recently, fundamental conditions on the sampling patterns have been obtained
for finite completability of low-rank matrices or tensors given the
corresponding ranks. In this paper, we consider the scenario where the rank is
not given and we aim to approximate the unknown rank based on the location of
sampled entries and some given completion. We consider a number of data models,
including single-view matrix, multi-view matrix, CP tensor, tensor-train tensor
and Tucker tensor. For each of these data models, we provide an upper bound on
the rank when an arbitrary low-rank completion is given. We characterize these
bounds both deterministically, i.e., with probability one given that the
sampling pattern satisfies certain combinatorial properties, and
probabilistically, i.e., with high probability given that the sampling
probability is above some threshold. Moreover, for both single-view matrix and
CP tensor, we are able to show that the obtained upper bound is exactly equal
to the unknown rank if the lowest-rank completion is given. Furthermore, we
provide numerical experiments for the case of single-view matrix, where we use
nuclear norm minimization to find a low-rank completion of the sampled data and
we observe that in most of the cases the proposed upper bound on the rank is
equal to the true rank.",Rank Determination for Low-Rank Data Completion,specifically method exogenous
515,"Neural Machine Translation models have replaced the conventional phrase based
statistical translation methods since the former takes a generic, scalable,
data-driven approach rather than relying on manual, hand-crafted features. The
neural machine translation system is based on one neural network that is
composed of two parts, one that is responsible for input language sentence and
other part that handles the desired output language sentence. This model based
on encoder-decoder architecture also takes as input the distributed
representations of the source language which enriches the learnt dependencies
and gives a warm start to the network. In this work, we transform Roman-Urdu to
Urdu transliteration into sequence to sequence learning problem. To this end,
we make the following contributions. We create the first ever parallel corpora
of Roman-Urdu to Urdu, create the first ever distributed representation of
Roman-Urdu and present the first neural machine translation model that
transliterates text from Roman-Urdu to Urdu language. Our model has achieved
the state-of-the-art results using BLEU as the evaluation metric. Precisely,
our model is able to correctly predict sentences up to length 10 while
achieving BLEU score of 48.6 on the test set. We are hopeful that our model and
our results shall serve as the baseline for further work in the domain of
neural machine translation for Roman-Urdu to Urdu using distributed
representation.",Sequence to Sequence Networks for Roman-Urdu to Urdu Transliteration,reduces show show words <unk>
516,"We explore the efficiency of the CRF inference module beyond image level
semantic segmentation. The key idea is to combine the best of two worlds of
semantic co-labeling and exploiting more expressive models. Similar to
[Alvarez14] our formulation enables us perform inference over ten thousand
images within seconds. On the other hand, it can handle higher-order clique
potentials similar to [vineet2014] in terms of region-level label consistency
and context in terms of co-occurrences. We follow the mean-field updates for
higher order potentials similar to [vineet2014] and extend the spatial
smoothness and appearance kernels [DenseCRF13] to address video data inspired
by [Alvarez14]; thus making the system amenable to perform video semantic
segmentation most effectively.","Beyond Semantic Image Segmentation : Exploring Efficient Inference in
  Video",models model <unk> model <unk> different word paper 
517,"Radiofrequency (RF) catheter ablation has transformed treatment for
tachyarrhythmias and has become first-line therapy for some tachycardias. The
precise localization of the arrhythmogenic site and the positioning of the RF
catheter over that site are problematic: they can impair the efficiency of the
procedure and are time consuming (several hours). Electroanatomic mapping
technologies are available that enable the display of the cardiac chambers and
the relative position of ablation lesions. However, these are expensive and use
custom-made catheters. The proposed methodology makes use of standard catheters
and inexpensive technology in order to create a 3D volume of the heart chamber
affected by the arrhythmia. Further, we propose a novel method that uses a
priori 3D information of the mapping catheter in order to estimate the 3D
locations of multiple electrodes across single view C-arm images. The monoplane
algorithm is tested for feasibility on computer simulations and initial canine
data.","3D/2D Registration of Mapping Catheter Images for Arrhythmia
  Interventional Assistance",models based function  show inference on
518,"In this paper we present a language for finite state continuous time Bayesian
networks (CTBNs), which describe structured stochastic processes that evolve
over continuous time. The state of the system is decomposed into a set of local
variables whose values change over time. The dynamics of the system are
described by specifying the behavior of each local variable as a function of
its parents in a directed (possibly cyclic) graph. The model specifies, at any
given point in time, the distribution over two aspects: when a local variable
changes its value and the next value it takes. These distributions are
determined by the variable s CURRENT value AND the CURRENT VALUES OF its
parents IN the graph.More formally, each variable IS modelled AS a finite state
continuous time Markov process whose transition intensities are functions OF
its parents.We present a probabilistic semantics FOR the language IN terms OF
the generative model a CTBN defines OVER sequences OF events.We list types OF
queries one might ask OF a CTBN, discuss the conceptual AND computational
difficulties associated WITH exact inference, AND provide an algorithm FOR
approximate inference which takes advantage OF the structure within the
process.",Continuous Time Bayesian Networks,critical model show brain show empirical data 
519,"We present pomegranate, an open source machine learning package for
probabilistic modeling in Python. Probabilistic modeling encompasses a wide
range of methods that explicitly describe uncertainty using probability
distributions. Three widely used probabilistic models implemented in
pomegranate are general mixture models, hidden Markov models, and Bayesian
networks. A primary focus of pomegranate is to abstract away the complexities
of training models from their definition. This allows users to focus on
specifying the correct model for their application instead of being limited by
their understanding of the underlying algorithms. An aspect of this focus
involves the collection of additive sufficient statistics from data sets as a
strategy for training models. This approach trivially enables many useful
learning strategies, such as out-of-core learning, minibatch learning, and
semi-supervised learning, without requiring the user to consider how to
partition data or modify the algorithms to handle these tasks themselves.
pomegranate is written in Cython to speed up calculations and releases the
global interpreter lock to allow for built-in multithreaded parallelism, making
it competitive with---or outperform---other implementations of similar
algorithms. This paper presents an overview of the design choices in
pomegranate, and how they have enabled complex features to be supported by
simple code.",Pomegranate: fast and flexible probabilistic modeling in python,learning verb problem <unk> means
520,"The recent, remarkable growth of machine learning has led to intense interest
in the privacy of the data on which machine learning relies, and to new
techniques for preserving privacy. However, older ideas about privacy may well
remain valid and useful. This note reviews two recent works on privacy in the
light of the wisdom of some of the early literature, in particular the
principles distilled by Saltzer and Schroeder in the 1970s.","On the Protection of Private Information in Machine Learning Systems:
  Two Recent Approaches",using significant referred new information different architectures  robustness
521,"Global voting schemes based on the Hough transform (HT) have been widely used
to robustly detect lines in images. However, since the votes do not take line
connectivity into account, these methods do not deal well with cluttered
images. In opposition, the so-called local methods enforce connectivity but
lack robustness to deal with challenging situations that occur in many
realistic scenarios, e.g., when line segments cross or when long segments are
corrupted. In this paper, we address the critical limitations of the HT as a
line segment extractor by incorporating connectivity in the voting process.
This is done by only accounting for the contributions of edge points lying in
increasingly larger neighborhoods and whose position and directional content
agree with potential line segments. As a result, our method, which we call
STRAIGHT (Segment exTRAction by connectivity-enforcInG HT), extracts the
longest connected segments in each location of the image, thus also integrating
into the HT voting process the usually separate step of individual segment
extraction. The usage of the Hough space mapping and a corresponding
hierarchical implementation make our approach computationally feasible. We
present experiments that illustrate, with synthetic and real images, how
STRAIGHT succeeds in extracting complete segments in several situations where
current methods fail.","Connectivity-Enforcing Hough Transform for the Robust Extraction of Line
  Segments",theory improvements  show significant shown different extraction evaluation
522,"An algorithm for pose and motion estimation using corresponding features in
images and a digital terrain map is proposed. Using a Digital Terrain (or
Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the
absolute position and orientation of the camera. In order to do this, the DTM
is used to formulate a constraint between corresponding features in two
consecutive frames. The utilization of data is shown to improve the robustness
and accuracy of the inertial navigation algorithm. Extended Kalman filter was
used to combine results of inertial navigation algorithm and proposed
vision-based navigation algorithm. The feasibility of this algorithms is
established through numerical simulations.","Vision-Based Navigation I: A navigation filter for fusing
  DTM/correspondence updates",edges segments tasks developing show various various
523,"In recent years there has been growing interest in solutions for the delivery
of clinical care for the elderly, due to the large increase in aging
population. Monitoring a patient in his home environment is necessary to ensure
continuity of care in home settings, but, to be useful, this activity must not
be too invasive for patients and a burden for caregivers. We prototyped a
system called SINDI (Secure and INDependent lIving), focused on i) collecting a
limited amount of data about the person and the environment through Wireless
Sensor Networks (WSN), and ii) inferring from these data enough information to
support caregivers in understanding patients' well being and in predicting
possible evolutions of their health. Our hierarchical logic-based model of
health combines data from different sources, sensor data, tests results,
common-sense knowledge and patient's clinical profile at the lower level, and
correlation rules between health conditions across upper levels. The logical
formalization and the reasoning process are based on Answer Set Programming.
The expressive power of this logic programming paradigm makes it possible to
reason about health evolution even when the available information is incomplete
and potentially incoherent, while declarativity simplifies rules specification
by caregivers and allows automatic encoding of knowledge. This paper describes
how these issues have been targeted in the application scenario of the SINDI
system.","Reasoning Support for Risk Prediction and Prevention in Independent
  Living",data influence <unk> data <unk> science  two
524,"A large fraction of the electronic health records consists of clinical
measurements collected over time, such as blood tests, which provide important
information about the health status of a patient. These sequences of clinical
measurements are naturally represented as time series, characterized by
multiple variables and the presence of missing data, which complicate analysis.
In this work, we propose a surgical site infection detection framework for
patients undergoing colorectal cancer surgery that is completely unsupervised,
hence alleviating the problem of getting access to labelled training data. The
framework is based on powerful kernels for multivariate time series that
account for missing data when computing similarities. Our approach show
superior performance compared to baselines that have to resort to imputation
techniques and performs comparable to a supervised classification baseline.","An Unsupervised Multivariate Time Series Kernel Approach for Identifying
  Patients with Surgical Site Infection from Blood Samples",various role show show offers approaches
525,"The positive-unlabeled (PU) classification is a common scenario in real-world
applications such as healthcare, text classification, and bioinformatics, in
which we only observe a few samples labeled as ""positive"" together with a large
volume of ""unlabeled"" samples that may contain both positive and negative
samples. Building robust classifier for the PU problem is very challenging,
especially for complex data where the negative samples overwhelm and mislabeled
samples or corrupted features exist. To address these three issues, we propose
a robust learning framework that unifies AUC maximization (a robust metric for
biased labels), outlier detection (for excluding wrong labels), and feature
selection (for excluding corrupted features). The generalization error bounds
are provided for the proposed model that give valuable insight into the
theoretical performance of the method and lead to useful practical guidance,
e.g., to train a model, we find that the included unlabeled samples are
sufficient as long as the sample size is comparable to the number of positive
samples in the training process. Empirical comparisons and two real-world
applications on surgical site infection (SSI) and EEG seizure detection are
also conducted to show the effectiveness of the proposed model.","A Robust AUC Maximization Framework with Simultaneous Outlier Detection
  and Feature Selection for Positive-Unlabeled Classification",word show show set set
526,"This paper presents a database of human faces for persons wearing spectacles.
The database consists of images of faces having significant variations with
respect to illumination, head pose, skin color, facial expressions and sizes,
and nature of spectacles. The database contains data of 60 subjects. This
database is expected to be a precious resource for the development and
evaluation of algorithms for face detection, eye detection, head tracking, eye
gaze tracking, etc., for subjects wearing spectacles. As such, this can be a
valuable contribution to the computer vision community.",SPECFACE - A Dataset of Human Faces Wearing Spectacles,<unk> model regression  may data around
527,"Service discovery requests' messages have a vital role in sharing and
locating resources in many of service discovery protocols. Sending more
messages than a link can handle may cause congestion and loss of messages which
dramatically influences the performance of these protocols. Re-send the lost
messages result in latency and inefficiency in performing the tasks which
user(s) require from the connected nodes. This issue become a serious problem
in two cases: first, when the number of clients which performs a service
discovery request is increasing, as this result in increasing in the number of
sent discovery messages; second, when the network resources such as bandwidth
capacity are consumed by other applications. These two cases lead to network
congestion and loss of messages. This paper propose an algorithm to improve the
services discovery protocols performance by separating each consecutive burst
of messages with a specific period of time which calculated regarding the
available network resources. It was tested when the routers were connected in
two configurations; decentralised and centralised .In addition, this paper
explains the impact of increasing the number of clients and the consumed
network resources on the proposed algorithm.",Networks Utilization Improvements for Service Discovery Performance,models show sequential images  traditional
528,"I propose the purpose our concept of actual causation serves is minimizing
various cost in intervention practice. Actual causation has three features:
nonredundant sufficiency, continuity and abnormality; these features correspond
to the minimization of exploitative cost, exploratory cost and risk cost in
intervention practice. Incorporating these three features, a definition of
actual causation is given. I test the definition in 66 causal cases from actual
causation literature and show that this definition's application fit intuition
better than some other causal modelling based definitions.",Cost and Actual Causation,<unk> estimate model data despite using
529,"In this paper, we focus on general-purpose Distributed Stream Data Processing
Systems (DSDPSs), which deal with processing of unbounded streams of continuous
data at scale distributedly in real or near-real time. A fundamental problem in
a DSDPS is the scheduling problem with the objective of minimizing average
end-to-end tuple processing time. A widely-used solution is to distribute
workload evenly over machines in the cluster in a round-robin manner, which is
obviously not efficient due to lack of consideration for communication delay.
Model-based approaches do not work well either due to the high complexity of
the system environment. We aim to develop a novel model-free approach that can
learn to well control a DSDPS from its experience rather than accurate and
mathematically solvable system models, just as a human learns a skill (such as
cooking, driving, swimming, etc). Specifically, we, for the first time, propose
to leverage emerging Deep Reinforcement Learning (DRL) for enabling model-free
control in DSDPSs; and present design, implementation and evaluation of a novel
and highly effective DRL-based control framework, which minimizes average
end-to-end tuple processing time by jointly learning the system environment via
collecting very limited runtime statistics data and making decisions under the
guidance of powerful Deep Neural Networks. To validate and evaluate the
proposed framework, we implemented it based on a widely-used DSDPS, Apache
Storm, and tested it with three representative applications. Extensive
experimental results show 1) Compared to Storm's default scheduler and the
state-of-the-art model-based method, the proposed framework reduces average
tuple processing by 33.5% and 14.0% respectively on average. 2) The proposed
framework can quickly reach a good scheduling solution during online learning,
which justifies its practicability for online control in DSDPSs.","Model-Free Control for Distributed Stream Data Processing using Deep
  Reinforcement Learning",programming biological show scene without
530,"Gene and protein networks are very important to model complex large-scale
systems in molecular biology. Inferring or reverseengineering such networks can
be defined as the process of identifying gene/protein interactions from
experimental data through computational analysis. However, this task is
typically complicated by the enormously large scale of the unknowns in a rather
small sample size. Furthermore, when the goal is to study causal relationships
within the network, tools capable of overcoming the limitations of correlation
networks are required. In this work, we make use of Bayesian Graphical Models
to attach this problem and, specifically, we perform a comparative study of
different state-of-the-art heuristics, analyzing their performance in inferring
the structure of the Bayesian Network from breast cancer data.","Combining Bayesian Approaches and Evolutionary Techniques for the
  Inference of Breast Cancer Networks",addition resulting  ##  two evolution
531,"Logic programs with aggregates (LPA) are one of the major linguistic
extensions to Logic Programming (LP). In this work, we propose a generalization
of the notions of unfounded set and well-founded semantics for programs with
monotone and antimonotone aggregates (LPAma programs). In particular, we
present a new notion of unfounded set for LPAma programs, which is a sound
generalization of the original definition for standard (aggregate-free) LP. On
this basis, we define a well-founded operator for LPAma programs, the fixpoint
of which is called well-founded model (or well-founded semantics) for LPAma
programs. The most important properties of unfounded sets and the well-founded
semantics for standard LP are retained by this generalization, notably
existence and uniqueness of the well-founded model, together with a strong
relationship to the answer set semantics for LPAma programs. We show that one
of the D-well-founded semantics, defined by Pelov, Denecker, and Bruynooghe for
a broader class of aggregates using approximating operators, coincides with the
well-founded model as defined in this work on LPAma programs. We also discuss
some complexity issues, most importantly we give a formal proof of tractable
computation of the well-founded model for LPA programs. Moreover, we prove that
for general LPA programs, which may contain aggregates that are neither
monotone nor antimonotone, deciding satisfaction of aggregate expressions with
respect to partial interpretations is coNP-complete. As a consequence, a
well-founded semantics for general LPA programs that allows for tractable
computation is unlikely to exist, which justifies the restriction on LPAma
programs. Finally, we present a prototype system extending DLV, which supports
the well-founded semantics for LPAma programs, at the time of writing the only
implemented system that does so. Experiments with this prototype show
significant computational advantages of aggregate constructs over equivalent
aggregate-free encodings.","Unfounded Sets and Well-Founded Semantics of Answer Set Programs with
  Aggregates",<unk> search
532,"Recent advances in depth imaging sensors provide easy access to the
synchronized depth with color, called RGB-D image. In this paper, we propose an
unsupervised method for indoor RGB-D image segmentation and analysis. We
consider a statistical image generation model based on the color and geometry
of the scene. Our method consists of a joint color-spatial-directional
clustering method followed by a statistical planar region merging method. We
evaluate our method on the NYU depth database and compare it with existing
unsupervised RGB-D segmentation methods. Results show that, it is comparable
with the state of the art methods and it needs less computation time. Moreover,
it opens interesting perspectives to fuse color and geometry in an unsupervised
manner.","Joint Color-Spatial-Directional clustering and Region Merging (JCSD-RM)
  for unsupervised RGB-D image segmentation",using self driving different significant <unk>
533,"The primary theme of this investigation is a decision theoretic account of
conditional ought statements (e.g., ""You ought to do A, if C"") that rectifies
glaring deficiencies in classical deontic logic. The resulting account forms a
sound basis for qualitative decision theory, thus providing a framework for
qualitative planning under uncertainty. In particular, we show that adding
causal relationships (in the form of a single graph) as part of an epistemic
state is sufficient to facilitate the analysis of action sequences, their
consequences, their interaction with observations, their expected utilities
and, hence, the synthesis of plans and strategies under uncertainty.",From Conditional Oughts to Qualitative Decision Theory,<unk> problem qualitative size camera al 
534,"Causal models defined in terms of structural equations have proved to be
quite a powerful way of representing knowledge regarding causality. However, a
number of authors have given examples that seem to show that the Halpern-Pearl
(HP) definition of causality gives intuitively unreasonable answers. Here it is
shown that, for each of these examples, we can give two stories consistent with
the description in the example, such that intuitions regarding causality are
quite different for each story. By adding additional variables, we can
disambiguate the stories. Moreover, in the resulting causal models, the HP
definition of causality gives the intuitively correct answer. It is also shown
that, by adding extra variables, a modification to the original HP definition
made to deal with an example of Hopkins and Pearl may not be necessary. Given
how much can be done by adding extra variables, there might be a concern that
the notion of causality is somewhat unstable. Can adding extra variables in a
""conservative"" way (i.e., maintaining all the relations between the variables
in the original model) cause the answer to the question ""Is X=x a cause of Y=y""
to alternate between ""yes"" and ""no""? It is shown that we can have such
alternation infinitely often, but if we take normality into consideration, we
cannot. Indeed, under appropriate normality assumptions. adding an extra
variable can change the answer from ""yes"" to ""no"", but after that, it cannot
cannot change back to ""yes"".",Appropriate Causal Models and the Stability of Causation,using model means show <unk> knowledge
535,"The Traveling Salesman Problem (TSP) is one of the most famous optimization
problems. Greedy crossover designed by Greffenstette et al, can be used while
Symmetric TSP (STSP) is resolved by Genetic Algorithm (GA). Researchers have
proposed several versions of greedy crossover. Here we propose improved version
of it. We compare our greedy crossover with some of recent crossovers, we use
our greedy crossover and some recent crossovers in GA then compare crossovers
on speed and accuracy.","Developing Improved Greedy Crossover to Solve Symmetric Traveling
  Salesman Problem",al  strategy uses show services online
536,"Based on the Aristotelian concept of potentiality vs. actuality allowing for
the study of energy and dynamics in language, we propose a field approach to
lexical analysis. Falling back on the distributional hypothesis to
statistically model word meaning, we used evolving fields as a metaphor to
express time-dependent changes in a vector space model by a combination of
random indexing and evolving self-organizing maps (ESOM). To monitor semantic
drifts within the observation period, an experiment was carried out on the term
space of a collection of 12.8 million Amazon book reviews. For evaluation, the
semantic consistency of ESOM term clusters was compared with their respective
neighbourhoods in WordNet, and contrasted with distances among term vectors by
random indexing. We found that at 0.05 level of significance, the terms in the
clusters showed a high level of semantic consistency. Tracking the drift of
distributional patterns in the term space across time periods, we found that
consistency decreased, but not at a statistically significant level. Our method
is highly scalable, with interpretations in philosophy.","Monitoring Term Drift Based on Semantic Consistency in an Evolving
  Vector Field",models model image <unk> concise density using density
537,"This paper explores the Coevolutionary Optional Prisoner's Dilemma (COPD)
game, which is a simple model to coevolve game strategy and link weights of
agents playing the Optional Prisoner's Dilemma game. We consider a population
of agents placed in a lattice grid with boundary conditions. A number of Monte
Carlo simulations are performed to investigate the impacts of the COPD game on
the emergence of cooperation. Results show that the coevolutionary rules enable
cooperators to survive and even dominate, with the presence of abstainers in
the population playing a key role in the protection of cooperators against
exploitation from defectors. We observe that in adverse conditions such as when
the initial population of abstainers is too scarce/abundant, or when the
temptation to defect is very high, cooperation has no chance of emerging.
However, when the simple coevolutionary rules are applied, cooperators
flourish.",The Impact of Coevolution and Abstention on the Emergence of Cooperation,using accurate paper  for different accurate for
538,"We present a first procedure that can estimate -- with statistical
consistency guarantees -- any local-maxima of a density, under benign
distributional conditions. The procedure estimates all such local maxima, or
$\textit{modal-sets}$, of any bounded shape or dimension, including usual
point-modes. In practice, modal-sets can arise as dense low-dimensional
structures in noisy data, and more generally serve to better model the rich
variety of locally-high-density structures in data.
  The procedure is then shown to be competitive on clustering applications, and
moreover is quite stable to a wide range of settings of its tuning parameter.",Modal-set estimation with an application to clustering,significant state present image impact qualitative
539,"Human communication typically has an underlying structure. This is reflected
in the fact that in many user generated videos, a starting point, ending, and
certain objective steps between these two can be identified. In this paper, we
propose a method for parsing a video into such semantic steps in an
unsupervised way. The proposed method is capable of providing a semantic
""storyline"" of the video composed of its objective steps. We accomplish this
using both visual and language cues in a joint generative model. The proposed
method can also provide a textual description for each of the identified
semantic steps and video segments. We evaluate this method on a large number of
complex YouTube videos and show results of unprecedented quality for this
intricate and impactful problem.",Unsupervised Semantic Parsing of Video Collections,set data  different train similarity estimated
540,"Recognizing materials in real-world images is a challenging task. Real-world
materials have rich surface texture, geometry, lighting conditions, and
clutter, which combine to make the problem particularly difficult. In this
paper, we introduce a new, large-scale, open dataset of materials in the wild,
the Materials in Context Database (MINC), and combine this dataset with deep
learning to achieve material recognition and segmentation of images in the
wild.
  MINC is an order of magnitude larger than previous material databases, while
being more diverse and well-sampled across its 23 categories. Using MINC, we
train convolutional neural networks (CNNs) for two tasks: classifying materials
from patches, and simultaneous material recognition and segmentation in full
images. For patch-based classification on MINC we found that the best
performing CNN architectures can achieve 85.2% mean class accuracy. We convert
these trained CNN classifiers into an efficient fully convolutional framework
combined with a fully connected conditional random field (CRF) to predict the
material at every pixel in an image, achieving 73.1% mean class accuracy. Our
experiments demonstrate that having a large, well-sampled dataset such as MINC
is crucial for real-world material recognition and segmentation.",Material Recognition in the Wild with the Materials in Context Database,using model <unk>
541,"Refer to the literature of lung nodule classification, many studies adopt
Convolutional Neural Networks (CNN) to directly predict the malignancy of lung
nodules with original thoracic Computed Tomography (CT) and nodule location.
However, these studies cannot tell how the CNN works in terms of predicting the
malignancy of the given nodule, e.g., it's hard to conclude that whether the
region within the nodule or the contextual information matters according to the
output of the CNN. In this paper, we propose an interpretable and multi-task
learning CNN -- Joint learning for \textbf{P}ulmonary \textbf{N}odule
\textbf{S}egmentation \textbf{A}ttributes and \textbf{M}alignancy
\textbf{P}rediction (PN-SAMP). It is able to not only accurately predict the
malignancy of lung nodules, but also provide semantic high-level attributes as
well as the areas of detected nodules. Moreover, the combination of nodule
segmentation, attributes and malignancy prediction is helpful to improve the
performance of each single task. In addition, inspired by the fact that
radiologists often change window widths and window centers to help to make
decision on uncertain nodules, PN-SAMP mixes multiple WW/WC together to gain
information for the raw CT input images. To verify the effectiveness of the
proposed method, the evaluation is implemented on the public LIDC-IDRI dataset,
which is one of the largest dataset for lung nodule malignancy prediction.
Experiments indicate that the proposed PN-SAMP achieves significant improvement
with respect to lung nodule classification, and promising performance on lung
nodule segmentation and attribute learning, compared with the-state-of-the-art
methods.","Joint Learning for Pulmonary Nodule Segmentation, Attributes and
  Malignancy Prediction",paper  system facial the proposed function  based function 
542,"People have information needs of varying complexity, which can be solved by
an intelligent agent able to answer questions formulated in a proper way,
eventually considering user context and preferences. In a scenario in which the
user profile can be considered as a question, intelligent agents able to answer
questions can be used to find the most relevant answers for a given user. In
this work we propose a novel model based on Artificial Neural Networks to
answer questions with multiple answers by exploiting multiple facts retrieved
from a knowledge base. The model is evaluated on the factoid Question Answering
and top-n recommendation tasks of the bAbI Movie Dialog dataset. After
assessing the performance of the model on both tasks, we try to define the
long-term goal of a conversational recommender system able to interact using
natural language and to support users in their information seeking processes in
a personalized way.",Iterative Multi-document Neural Attention for Multiple Answer Prediction,predictive for
543,"Modern machine learning systems such as image classifiers rely heavily on
large scale data sets for training. Such data sets are costly to create, thus
in practice a small number of freely available, open source data sets are
widely used. We suggest that examining the geo-diversity of open data sets is
critical before adopting a data set for use cases in the developing world. We
analyze two large, publicly available image data sets to assess geo-diversity
and find that these data sets appear to exhibit an observable amerocentric and
eurocentric representation bias. Further, we analyze classifiers trained on
these data sets to assess the impact of these training distributions and find
strong differences in the relative performance on images from different
locales. These results emphasize the need to ensure geo-representation when
constructing data sets for use in the developing world.","No Classification without Representation: Assessing Geodiversity Issues
  in Open Data Sets for the Developing World",for model <unk>
544,"We consider the problem of computing accurate point-to-point correspondences
among a set of human face scans with varying expressions. Our fully automatic
approach does not require any manually placed markers on the scan. Instead, the
approach learns the locations of a set of landmarks present in a database and
uses this knowledge to automatically predict the locations of these landmarks
on a newly available scan. The predicted landmarks are then used to compute
point-to-point correspondences between a template model and the newly available
scan. To accurately fit the expression of the template to the expression of the
scan, we use as template a blendshape model. Our algorithm was tested on a
database of human faces of different ethnic groups with strongly varying
expressions. Experimental results show that the obtained point-to-point
correspondence is both highly accurate and consistent for most of the tested 3D
face models.",Fully Automatic Expression-Invariant Face Correspondence,astronomy  two show significant instances linear
545,"Do object part localization methods produce bilaterally symmetric results on
mirror images? Surprisingly not, even though state of the art methods augment
the training set with mirrored images. In this paper we take a closer look into
this issue. We first introduce the concept of mirrorability as the ability of a
model to produce symmetric results in mirrored images and introduce a
corresponding measure, namely the \textit{mirror error} that is defined as the
difference between the detection result on an image and the mirror of the
detection result on its mirror image. We evaluate the mirrorability of several
state of the art algorithms in two of the most intensively studied problems,
namely human pose estimation and face alignment. Our experiments lead to
several interesting findings: 1) Surprisingly, most of state of the art methods
struggle to preserve the mirror symmetry, despite the fact that they do have
very similar overall performance on the original and mirror images; 2) the low
mirrorability is not caused by training or testing sample bias - all algorithms
are trained on both the original images and their mirrored versions; 3) the
mirror error is strongly correlated to the localization/alignment error (with
correlation coefficients around 0.7). Since the mirror error is calculated
without knowledge of the ground truth, we show two interesting applications -
in the first it is used to guide the selection of difficult samples and in the
second to give feedback in a popular Cascaded Pose Regression method for face
alignment.","Mirror, mirror on the wall, tell me, is the error small?",domain  model <unk> graphics models test #  role
546,"The problem of object localization has become one of the mainstream problems
of vision. Most of the algorithms proposed involve the design for the model to
be specifically for localizing objects. In this paper, we explore whether a
pre-trained canonical ConvNet (without fine-tuning) trained purely for object
classification on one dataset with global image level labels can be used to
localize objects in images containing a single instance on a separate dataset
while generalizing to novel classes. We propose a simple algorithm involving
cropping and blackening out regions in the image space called Explicit Image
Space based Search (EISS) for locating the most responsive regions in an image
in the context of object localization. EISS brings to light the interesting
phenomenon of a ConvNets responding more to features within objects as opposed
to object level descriptors, as the classes in the training data get more
correlated (visually/semantically similar).","Class Correlation affects Single Object Localization using Pre-trained
  ConvNets",accurate biological method scene without
547,"We consider the problem of learning object arrangements in a 3D scene. The
key idea here is to learn how objects relate to human poses based on their
affordances, ease of use and reachability. In contrast to modeling
object-object relationships, modeling human-object relationships scales
linearly in the number of objects. We design appropriate density functions
based on 3D spatial features to capture this. We learn the distribution of
human poses in a scene using a variant of the Dirichlet process mixture model
that allows sharing of the density function parameters across the same object
types. Then we can reason about arrangements of the objects in the room based
on these meaningful human poses. In our extensive experiments on 20 different
rooms with a total of 47 objects, our algorithm predicted correct placements
with an average error of 1.6 meters from ground truth. In arranging five real
scenes, it received a score of 4.3/5 compared to 3.7 for the best baseline
method.",Learning Object Arrangements in 3D Scenes using Human Context,two uses method query models
548,"We propose a frequency domain method based on robust independent component
analysis (RICA) to address the multichannel Blind Source Separation (BSS)
problem of convolutive speech mixtures in highly reverberant environments. We
impose regularization processes to tackle the ill-conditioning problem of the
covariance matrix and to mitigate the performance degradation in the frequency
domain. We apply an algorithm to separate the source signals in adverse
conditions, i.e. high reverberation conditions when short observation signals
are available. Furthermore, we study the impact of several parameters on the
performance of separation, e.g. overlapping ratio and window type of the
frequency domain method. We also compare different techniques to solve the
frequency-domain permutation ambiguity. Through simulations and real world
experiments, we verify the superiority of the presented convolutive algorithm
among other BSS algorithms, including recursive regularized ICA (RR ICA),
independent vector analysis (IVA).",A RobustICA Based Algorithm for Blind Separation of Convolutive Mixtures,impact texture show distribution using <unk>
549,"We introduce three generalizations of homotopy equivalence in digital images,
to allow us to express whether a finite and an infinite digital image are
similar with respect to homotopy.
  We show that these three generalizations are not equivalent to ordinary
homotopy equivalence, and give several examples. We show that, like homotopy
equivalence, our three generalizations imply isomorphism of fundamental groups,
and are preserved under wedges and Cartesian products.",Homotopy relations for digital images,networks proposed <unk> for
550,"Our goal is to design architectures that retain the groundbreaking
performance of CNNs for landmark localization and at the same time are
lightweight, compact and suitable for applications with limited computational
resources. To this end, we make the following contributions: (a) we are the
first to study the effect of neural network binarization on localization tasks,
namely human pose estimation and face alignment. We exhaustively evaluate
various design choices, identify performance bottlenecks, and more importantly
propose multiple orthogonal ways to boost performance. (b) Based on our
analysis, we propose a novel hierarchical, parallel and multi-scale residual
architecture that yields large performance improvement over the standard
bottleneck block while having the same number of parameters, thus bridging the
gap between the original network and its binarized counterpart. (c) We perform
a large number of ablation studies that shed light on the properties and the
performance of the proposed block. (d) We present results for experiments on
the most challenging datasets for human pose estimation and face alignment,
reporting in many cases state-of-the-art performance. Code can be downloaded
from https://www.adrianbulat.com/binary-cnn-landmarks","Binarized Convolutional Landmark Localizers for Human Pose Estimation
  and Face Alignment with Limited Resources",<unk> method video <unk>
551,"This paper investigates the concept of digital city. First, a functional
analysis of a digital city is made in the light of the modern study of
urbanism; similarities between the virtual and urban constructions are pointed
out. Next, a semiotic perspective on the subject matter is elaborated, and a
terminological basis is introduced to treat a digital city as a self-organizing
meaning-producing system intended to support social or spatial navigation. An
explicit definition of a digital city is formulated. Finally, the proposed
approach is discussed, conclusions are given, and future work is outlined.","Communication of Social Agents and the Digital City - A Semiotic
  Perspective",estimate model #  required
552,"We present a unified framework to study graph kernels, special cases of which
include the random walk graph kernel \citep{GaeFlaWro03,BorOngSchVisetal05},
marginalized graph kernel \citep{KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04},
and geometric kernel on graphs \citep{Gaertner02}. Through extensions of linear
algebra to Reproducing Kernel Hilbert Spaces (RKHS) and reduction to a
Sylvester equation, we construct an algorithm that improves the time complexity
of kernel computation from $O(n^6)$ to $O(n^3)$. When the graphs are sparse,
conjugate gradient solvers or fixed-point iterations bring our algorithm into
the sub-cubic domain. Experiments on graphs from bioinformatics and other
application domains show that it is often more than a thousand times faster
than previous approaches. We then explore connections between diffusion kernels
\citep{KonLaf02}, regularization on graphs \citep{SmoKon03}, and graph kernels,
and use these connections to propose new graph kernels. Finally, we show that
rational kernels \citep{CorHafMoh02,CorHafMoh03,CorHafMoh04} when specialized
to graphs reduce to the random walk graph kernel.",Graph Kernels,planning search proposed free different <unk>
553,"In this paper, we propose a general framework to accelerate the universal
histogram-based image contrast enhancement (CE) algorithms. Both spatial and
gray-level selective down- sampling of digital images are adopted to decrease
computational cost, while the visual quality of enhanced images is still
preserved and without apparent degradation. Mapping function calibration is
novelly proposed to reconstruct the pixel mapping on the gray levels missed by
downsampling. As two case studies, accelerations of histogram equalization (HE)
and the state-of-the-art global CE algorithm, i.e., spatial mutual information
and PageRank (SMIRANK), are presented detailedly. Both quantitative and
qualitative assessment results have verified the effectiveness of our proposed
CE acceleration framework. In typical tests, computational efficiencies of HE
and SMIRANK have been speeded up by about 3.9 and 13.5 times, respectively.","Acceleration of Histogram-Based Contrast Enhancement via Selective
  Downsampling",learning <unk> model <unk> systematic inference on
554,"Only a few studies have been reported regarding human ear recognition in long
wave infrared band. Thus, we have created ear database based on long wave
infrared band. We have called that the database is long wave infrared band
MIDAS consisting of 2430 records of 81 subjects. Thermal band provides seamless
operation both night and day, robust against spoofing with understanding live
ear and invariant to illumination conditions for human ear recognition. We have
proposed to use different algorithms to reveal the distinctive features. Then,
we have reduced the number of dimensions using subspace methods. Finally, the
dimension of data is reduced in accordance with the classifier methods. After
this, the decision is determined by the best sores or combining some of the
best scores with matching fusion. The results have showed that the fusion
technique was successful. We have reached 97.71% for rank-1 with 567 test
probes. Furthermore, we have defined the perfect rank which is rank number when
recognition rate reaches 100% in cumulative matching curve. This evaluation is
important for especially forensics, for example corpse identification, criminal
investigation etc.","Ear Recognition With Score-Level Fusion Based On CMC In Long-Wave
  Infrared Spectrum",self driving using <unk>
555,"Many engineering optimization problems can be considered as linear
programming problems where all or some of the parameters involved are
linguistic in nature. These can only be quantified using fuzzy sets. The aim of
this paper is to solve a fuzzy linear programming problem in which the
parameters involved are fuzzy quantities with logistic membership functions. To
explore the applicability of the method a numerical example is considered to
determine the monthly production planning quotas and profit of a home textile
group.","Application of a Fuzzy Programming Technique to Production Planning in
  the Textile Industry",types model distribution sense model <unk>
556,"We present a new online boosting algorithm for adapting the weights of a
boosted classifier, which yields a closer approximation to Freund and
Schapire's AdaBoost algorithm than previous online boosting algorithms. We also
contribute a new way of deriving the online algorithm that ties together
previous online boosting work. We assume that the weak hypotheses were selected
beforehand, and only their weights are updated during online boosting. The
update rule is derived by minimizing AdaBoost's loss when viewed in an
incremental form. The equations show that optimization is computationally
expensive. However, a fast online approximation is possible. We compare
approximation error to batch AdaBoost on synthetic datasets and generalization
error on face datasets and the MNIST dataset.",Online Coordinate Boosting,weights demonstrate using weights two show
557,"Positive-definite kernel functions are fundamental elements of kernel methods
and Gaussian processes. A well-known construction of such functions comes from
Bochner's characterization, which connects a positive-definite function with a
probability distribution. Another construction, which appears to have attracted
less attention, is Polya's criterion that characterizes a subset of these
functions. In this paper, we study the latter characterization and derive a
number of novel kernels little known previously.
  In the context of large-scale kernel machines, Rahimi and Recht (2007)
proposed a random feature map (random Fourier) that approximates a kernel
function, through independent sampling of the probability distribution in
Bochner's characterization. The authors also suggested another feature map
(random binning), which, although not explicitly stated, comes from Polya's
characterization. We show that with the same number of random samples, the
random binning map results in an Euclidean inner product closer to the kernel
than does the random Fourier map. The superiority of the random binning map is
confirmed empirically through regressions and classifications in the
reproducing kernel Hilbert space.","On Bochner's and Polya's Characterizations of Positive-Definite Kernels
  and the Respective Random Feature Maps",causal similar show kernel the
558,"In this paper we study the application of convolutional neural networks for
jointly detecting objects depicted in still images and estimating their 3D
pose. We identify different feature representations of oriented objects, and
energies that lead a network to learn this representations. The choice of the
representation is crucial since the pose of an object has a natural, continuous
structure while its category is a discrete variable. We evaluate the different
approaches on the joint object detection and pose estimation task of the
Pascal3D+ benchmark using Average Viewpoint Precision. We show that a
classification approach on discretized viewpoints achieves state-of-the-art
performance for joint object detection and pose estimation, and significantly
outperforms existing baselines on this benchmark.","Convolutional Neural Networks for joint object detection and pose
  estimation: A comparative study",models show <unk> data  proposed objects annotations feedforward
559,"We propose a definition of causality for time series in terms of the effect
of an intervention in one component of a multivariate time series on another
component at some later point in time. Conditions for identifiability,
comparable to the back-door and front-door criteria, are presented and can also
be verified graphically. Computation of the causal effect is derived and
illustrated for the linear case.",Causal Reasoning in Graphical Time Series Models,annotation syntactic show <unk> significant
560,"Echo state networks (ESN), a type of reservoir computing (RC) architecture,
are efficient and accurate artificial neural systems for time series processing
and learning. An ESN consists of a core of recurrent neural networks, called a
reservoir, with a small number of tunable parameters to generate a
high-dimensional representation of an input, and a readout layer which is
easily trained using regression to produce a desired output from the reservoir
states. Certain computational tasks involve real-time calculation of high-order
time correlations, which requires nonlinear transformation either in the
reservoir or the readout layer. Traditional ESN employs a reservoir with
sigmoid or tanh function neurons. In contrast, some types of biological neurons
obey response curves that can be described as a product unit rather than a sum
and threshold. Inspired by this class of neurons, we introduce a RC
architecture with a reservoir of product nodes for time series computation. We
find that the product RC shows many properties of standard ESN such as
short-term memory and nonlinear capacity. On standard benchmarks for chaotic
prediction tasks, the product RC maintains the performance of a standard
nonlinear ESN while being more amenable to mathematical analysis. Our study
provides evidence that such networks are powerful in highly nonlinear tasks
owing to high-order statistics generated by the recurrent product node
reservoir.","Product Reservoir Computing: Time-Series Computation with Multiplicative
  Neurons",<unk> interfaces <unk>
561,"We present a transition-based dependency parser that uses a convolutional
neural network to compose word representations from characters. The character
composition model shows great improvement over the word-lookup model,
especially for parsing agglutinative languages. These improvements are even
better than using pre-trained word embeddings from extra data. On the SPMRL
data sets, our system outperforms the previous best greedy parser (Ballesteros
et al., 2015) by a margin of 3% on average.","Character Composition Model with Convolutional Neural Networks for
  Dependency Parsing on Morphologically Rich Languages",natural role require different different square
562,"Hidden Markov Models (HMMs) can be accurately approximated using
co-occurrence frequencies of pairs and triples of observations by using a fast
spectral method in contrast to the usual slow methods like EM or Gibbs
sampling. We provide a new spectral method which significantly reduces the
number of model parameters that need to be estimated, and generates a sample
complexity that does not depend on the size of the observation vocabulary. We
present an elementary proof giving bounds on the relative accuracy of
probability estimates from our model. (Correlaries show our bounds can be
weakened to provide either L1 bounds or KL bounds which provide easier direct
comparisons to previous work.) Our theorem uses conditions that are checkable
from the data, instead of putting conditions on the unobservable Markov
transition matrix.",Spectral dimensionality reduction for HMMs,representations feature models estimated
563,"Convolutional Neural Network (CNN) techniques are applied to the problem of
determining the depth from a single camera image (monocular depth). Fully
connected CNN topologies preserve all details of the input images, enabling the
detection of fine details, but miss larger features; networks that employ 2x2,
4x4 and 8x8 max-pooling operators can determine larger features at the expense
of finer details. After designing, training and optimising a set of topologies,
these networks may be combined into a single network topology using graph
optimization techniques. This ""Semi Parallel Deep Neural Network (SPDNN)""
eliminates duplicate common network layers, reducing network size and
computational effort significantly, and can be further optimized by retraining
to achieve an improved level of convergence over the individual topologies. In
this study, four models are trained and have been evaluated at 2 stages on the
KITTI dataset. The ground truth images in the first part of the experiment come
from the benchmark, and for the second part, the ground truth images are the
depth map results from applying a state-of-the-art stereo matching method. The
results of this evaluation demonstrate that using post-processing techniques to
refine the target of the network increases the accuracy of depth estimation on
individual mono images. The second evaluation shows that using segmentation
data as the input can improve the depth estimation results to a point where
performance is comparable with stereo depth estimation. The computational time
is also discussed in this study.","Depth from Monocular Images using a Semi-Parallel Deep Neural Network
  (SPDNN) Hybrid Architecture",university model uncertainty data using improve model concepts
564,"While being it extremely important, many Exploratory Data Analysis (EDA)
systems have the inhability to perform classification and visualization in a
continuous basis or to self-organize new data-items into the older ones
(evenmore into new labels if necessary), which can be crucial in KDD -
Knowledge Discovery, Retrieval and Data Mining Systems (interactive and online
forms of Web Applications are just one example). This disadvantge is also
present in more recent approaches using Self-Organizing Maps. On the present
work, and exploiting past sucesses in recently proposed Stigmergic Ant Systems
a robust online classifier is presented, which produces class decisions on a
continuous stream data, allowing for continuous mappings. Results show that
increasingly better results are achieved, as demonstraded by other authors in
different areas. KEYWORDS: Swarm Intelligence, Ant Systems, Stigmergy,
Data-Mining, Exploratory Data Analysis, Image Retrieval, Continuous
Classification.",Swarms on Continuous Data,model means method hand crafted successful unsupervised
565,"Judgments about personality based on facial appearance are strong effectors
in social decision making, and are known to have impact on areas from
presidential elections to jury decisions. Recent work has shown that it is
possible to predict perception of memorability, trustworthiness, intelligence
and other attributes in human face images. The most successful of these
approaches require face images expertly annotated with key facial landmarks. We
demonstrate a Convolutional Neural Network (CNN) model that is able to perform
the same task without the need for landmark features, thereby greatly
increasing efficiency. The model has high accuracy, surpassing human-level
performance in some cases. Furthermore, we use a deconvolutional approach to
visualize important features for perception of 22 attributes and demonstrate a
new method for separately visualizing positive and negative features.","Predicting and visualizing psychological attributions with a deep neural
  network",theory model show aging common
566,"Lipreading is the task of decoding text from the movement of a speaker's
mouth. Traditional approaches separated the problem into two stages: designing
or learning visual features, and prediction. More recent deep lipreading
approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman,
2016a). However, existing work on models trained end-to-end perform only word
classification, rather than sentence-level sequence prediction. Studies have
shown that human lipreading performance increases for longer words (Easton &
Basala, 1982), indicating the importance of features capturing temporal context
in an ambiguous communication channel. Motivated by this observation, we
present LipNet, a model that maps a variable-length sequence of video frames to
text, making use of spatiotemporal convolutions, a recurrent network, and the
connectionist temporal classification loss, trained entirely end-to-end. To the
best of our knowledge, LipNet is the first end-to-end sentence-level lipreading
model that simultaneously learns spatiotemporal visual features and a sequence
model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level,
overlapped speaker split task, outperforming experienced human lipreaders and
the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).",LipNet: End-to-End Sentence-level Lipreading,level  rnn  texture show tensor
567,"In this work several semantic approaches to concept-based query expansion and
reranking schemes are studied and compared with different ontology-based
expansion methods in web document search and retrieval. In particular, we focus
on concept-based query expansion schemes, where, in order to effectively
increase the precision of web document retrieval and to decrease the users
browsing time, the main goal is to quickly provide users with the most suitable
query expansion. Two key tasks for query expansion in web document retrieval
are to find the expansion candidates, as the closest concepts in web document
domain, and to rank the expanded queries properly. The approach we propose aims
at improving the expansion phase for better web document retrieval and
precision. The basic idea is to measure the distance between candidate concepts
using the PMING distance, a collaborative semantic proximity measure, i.e. a
measure which can be computed by using statistical results from web search
engine. Experiments show that the proposed technique can provide users with
more satisfying expansion results and improve the quality of web document
retrieval.","Semantic Evolutionary Concept Distances for Effective Information
  Retrieval in Query Expansion",programming models show video step
568,"Novelty detection in news events has long been a difficult problem. A number
of models performed well on specific data streams but certain issues are far
from being solved, particularly in large data streams from the WWW where
unpredictability of new terms requires adaptation in the vector space model. We
present a novel event detection system based on the Incremental Term
Frequency-Inverse Document Frequency (TF-IDF) weighting incorporated with
Locality Sensitive Hashing (LSH). Our system could efficiently and effectively
adapt to the changes within the data streams of any new terms with continual
updates to the vector space model. Regarding miss probability, our proposed
novelty detection framework outperforms a recognised baseline system by
approximately 16% when evaluating a benchmark dataset from Google News.","An Improved System for Sentence-level Novelty Detection in Textual
  Streams",models ranking show generation time
569,"In this paper, we propose a model for simulating search operators whose
behaviour often changes continuously during the search. In these scenarios, the
performance of the operators decreases when they are applied. This is motivated
by the fact that operators for optimization problems are often roughly
classified into exploitation operators and exploration operators. Our
simulation model is used to compare the different performances of operator
selection policies and clearly identify their ability to adapt to such specific
operators behaviours. The experimental study provides interesting results on
the respective behaviours of operator selection policies when faced to such non
stationary search scenarios.",Simulating Non Stationary Operators in Search Algorithms,using key theory equation different squared
570,"Depth estimation is a fundamental problem for light field photography
applications. Numerous methods have been proposed in recent years, which either
focus on crafting cost terms for more robust matching, or on analyzing the
geometry of scene structures embedded in the epipolar-plane images. Significant
improvements have been made in terms of overall depth estimation error;
however, current state-of-the-art methods still show limitations in handling
intricate occluding structures and complex scenes with multiple occlusions. To
address these challenging issues, we propose a very effective depth estimation
framework which focuses on regularizing the initial label confidence map and
edge strength weights. Specifically, we first detect partially occluded
boundary regions (POBR) via superpixel based regularization. Series of
shrinkage/reinforcement operations are then applied on the label confidence map
and edge strength weights over the POBR. We show that after weight
manipulations, even a low-complexity weighted least squares model can produce
much better depth estimation than state-of-the-art methods in terms of average
disparity error rate, occlusion boundary precision-recall rate, and the
preservation of intricate visual features.","Accurate Light Field Depth Estimation with Superpixel Regularization
  over Partially Occluded Regions",uncertainty  show show the
571,"Visual recognition requires rich representations that span levels from low to
high, scales from small to large, and resolutions from fine to coarse. Even
with the depth of features in a convolutional network, a layer in isolation is
not enough: compounding and aggregating these representations improves
inference of what and where. Architectural efforts are exploring many
dimensions for network backbones, designing deeper or wider architectures, but
how to best aggregate layers and blocks across a network deserves further
attention. Although skip connections have been incorporated to combine layers,
these connections have been ""shallow"" themselves, and only fuse by simple,
one-step operations. We augment standard architectures with deeper aggregation
to better fuse information across layers. Our deep layer aggregation structures
iteratively and hierarchically merge the feature hierarchy to make networks
with better accuracy and fewer parameters. Experiments across architectures and
tasks show that deep layer aggregation improves recognition and resolution
compared to existing branching and merging schemes.",Deep Layer Aggregation,significant improve correspondence effectively demonstrate
572,"We study the Bipartite Boolean Quadratic Programming Problem (BBQP) which is
an extension of the well known Boolean Quadratic Programming Problem (BQP).
Applications of the BBQP include mining discrete patterns from binary data,
approximating matrices by rank-one binary matrices, computing the cut-norm of a
matrix, and solving optimisation problems such as maximum weight biclique,
bipartite maximum weight cut, maximum weight induced sub-graph of a bipartite
graph, etc. For the BBQP, we first present several algorithmic components,
specifically, hill climbers and mutations, and then show how to combine them in
a high-performance metaheuristic. Instead of hand-tuning a standard
metaheuristic to test the efficiency of the hybrid of the components, we chose
to use an automated generation of a multi-component metaheuristic to save human
time, and also improve objectivity in the analysis and comparisons of
components. For this we designed a new metaheuristic schema which we call
Conditional Markov Chain Search (CMCS). We show that CMCS is flexible enough to
model several standard metaheuristics; this flexibility is controlled by
multiple numeric parameters, and so is convenient for automated generation. We
study the configurations revealed by our approach and show that the best of
them outperforms the previous state-of-the-art BBQP algorithm by several orders
of magnitude. In our experiments we use benchmark instances introduced in the
preliminary version of this paper and described here, which have already become
the de facto standard in the BBQP literature.","Markov Chain methods for the bipartite Boolean quadratic programming
  problem",art model show facial the
573,"We propose the use of incomplete dot products (IDP) to dynamically adjust the
number of input channels used in each layer of a convolutional neural network
during feedforward inference. IDP adds monotonically non-increasing
coefficients, referred to as a ""profile"", to the channels during training. The
profile orders the contribution of each channel in non-increasing order. At
inference time, the number of channels used can be dynamically adjusted to
trade off accuracy for lowered power consumption and reduced latency by
selecting only a beginning subset of channels. This approach allows for a
single network to dynamically scale over a computation range, as opposed to
training and deploying multiple networks to support different levels of
computation scaling. Additionally, we extend the notion to multiple profiles,
each optimized for some specific range of computation scaling. We present
experiments on the computation and accuracy trade-offs of IDP for popular image
classification models and datasets. We demonstrate that, for MNIST and
CIFAR-10, IDP reduces computation significantly, e.g., by 75%, without
significantly compromising accuracy. We argue that IDP provides a convenient
and effective means for devices to lower computation costs dynamically to
reflect the current computation budget of the system. For example, VGG-16 with
50% IDP (using only the first 50% of channels) achieves 70% in accuracy on the
CIFAR-10 dataset compared to the standard network which achieves only 35%
accuracy when using the reduced channel set.","Incomplete Dot Products for Dynamic Computation Scaling in Neural
  Network Inference",video model show consistency achieved
574,"Decision trees have been a very popular class of predictive models for
decades due to their interpretability and good performance on categorical
features. However, they are not always robust and tend to overfit the data.
Additionally, if allowed to grow large, they lose interpretability. In this
paper, we present a novel mixed integer programming formulation to construct
optimal decision trees of specified size. We take special structure of
categorical features into account and allow combinatorial decisions (based on
subsets of values of such a feature) at each node. We show that very good
accuracy can be achieved with small trees using moderately-sized training sets.
The optimization problems we solve are easily tractable with modern solvers.",Optimal Generalized Decision Trees via Integer Programming,models model prohibit brain
575,"Although the traits emerged in a mass gathering are often non-deliberative,
the act of mass impulse may lead to irre- vocable crowd disasters. The two-fold
increase of carnage in crowd since the past two decades has spurred significant
advances in the field of computer vision, towards effective and proactive crowd
surveillance. Computer vision stud- ies related to crowd are observed to
resonate with the understanding of the emergent behavior in physics (complex
systems) and biology (animal swarm). These studies, which are inspired by
biology and physics, share surprisingly common insights, and interesting
contradictions. However, this aspect of discussion has not been fully explored.
Therefore, this survey provides the readers with a review of the
state-of-the-art methods in crowd behavior analysis from the physics and
biologically inspired perspectives. We provide insights and comprehensive
discussions for a broader understanding of the underlying prospect of blending
physics and biology studies in computer vision.",Crowd Behavior Analysis: A Review where Physics meets Biology,fundamental model segmentation propose words complexity number
576,"For words, rank-frequency distributions have long been heralded for adherence
to a potentially-universal phenomenon known as Zipf's law. The hypothetical
form of this empirical phenomenon was refined by Ben\^{i}ot Mandelbrot to that
which is presently referred to as the Zipf-Mandelbrot law. Parallel to this,
Herbet Simon proposed a selection model potentially explaining Zipf's law.
However, a significant dispute between Simon and Mandelbrot, notable empirical
exceptions, and the lack of a strong empirical connection between Simon's model
and the Zipf-Mandelbrot law have left the questions of universality and
mechanistic generation open. We offer a resolution to these issues by
exhibiting how the dark matter of word segmentation, i.e., space, punctuation,
etc., connect the Zipf-Mandelbrot law to Simon's mechanistic process. This
explains Mandelbrot's refinement as no more than a fudge factor, accommodating
the effects of the exclusion of the rank-frequency dark matter. Thus,
integrating these non-word objects resolves a more-generalized rank-frequency
law. Since this relies upon the integration of space, etc., we find support for
the hypothesis that $all$ are generated by common processes, indicating from a
physical perspective that space is a word, too.","Is space a word, too?",types theory kernels
577,"We developed a machine vision system to automatically capture the dynamics of
pedestrians under four different traffic scenarios. By considering the overhead
view of each pedestrian as a digital object, the system processes the image
sequences to track the pedestrians. Considering the perspective effect of the
camera lens and the projected area of the hallway at the top-view scene, the
distance of each tracked object from its original position to its current
position is approximated every video frame. Using the approximated distance and
the video frame rate (30 frames per second), the respective velocity and
acceleration of each tracked object are later derived. The quantified motion
characteristics of the pedestrians are displayed by the system through
2-dimensional graphs of the kinematics of motion. The system also outputs video
images of the pedestrians with superimposed markers for tracking. These visual
markers were used to visually describe and quantify the behavior of the
pedestrians under different traffic scenarios.","Capturing the Dynamics of Pedestrian Traffic Using a Machine Vision
  System",video step
578,"We tackle the problem of constructive preference elicitation, that is the
problem of learning user preferences over very large decision problems,
involving a combinatorial space of possible outcomes. In this setting, the
suggested configuration is synthesized on-the-fly by solving a constrained
optimization problem, while the preferences are learned itera tively by
interacting with the user. Previous work has shown that Coactive Learning is a
suitable method for learning user preferences in constructive scenarios. In
Coactive Learning the user provides feedback to the algorithm in the form of an
improvement to a suggested configuration. When the problem involves many
decision variables and constraints, this type of interaction poses a
significant cognitive burden on the user. We propose a decomposition technique
for large preference-based decision problems relying exclusively on inference
and feedback over partial configurations. This has the clear advantage of
drastically reducing the user cognitive load. Additionally, part-wise inference
can be (up to exponentially) less computationally demanding than inference over
full configurations. We discuss the theoretical implications of working with
parts and present promising empirical results on one synthetic and two
realistic constructive problems.",Decomposition Strategies for Constructive Preference Elicitation,<unk> show
579,"Low-shot learning methods for image classification support learning from
sparse data. We extend these techniques to support dense semantic image
segmentation. Specifically, we train a network that, given a small set of
annotated images, produces parameters for a Fully Convolutional Network (FCN).
We use this FCN to perform dense pixel-level prediction on a test image for the
new semantic class. Our architecture shows a 25% relative meanIoU improvement
compared to the best baseline methods for one-shot segmentation on unseen
classes in the PASCAL VOC 2012 dataset and is at least 3 times faster.",One-Shot Learning for Semantic Segmentation,images  <unk> two show algebraic
580,"This paper presents a theoretical analysis of sample selection bias
correction. The sample bias correction technique commonly used in machine
learning consists of reweighting the cost of an error on each training point of
a biased sample to more closely reflect the unbiased distribution. This relies
on weights derived by various estimation techniques based on finite samples. We
analyze the effect of an error in that estimation on the accuracy of the
hypothesis returned by the learning algorithm for two estimation techniques: a
cluster-based estimation technique and kernel mean matching. We also report the
results of sample bias correction experiments with several data sets using
these techniques. Our analysis is based on the novel concept of distributional
stability which generalizes the existing concept of point-based stability. Much
of our work and proof techniques can be used to analyze other importance
weighting techniques and their effect on accuracy when using a distributionally
stable algorithm.",Sample Selection Bias Correction Theory,rate  model paper  for
581,"We introduce a novel RGB-D patch descriptor designed for detecting coplanar
surfaces in SLAM reconstruction. The core of our method is a deep convolutional
neural net that takes in RGB, depth, and normal information of a planar patch
in an image and outputs a descriptor that can be used to find coplanar patches
from other images.We train the network on 10 million triplets of coplanar and
non-coplanar patches, and evaluate on a new coplanarity benchmark created from
commodity RGB-D scans. Experiments show that our learned descriptor outperforms
alternatives extended for this new task by a significant margin. In addition,
we demonstrate the benefits of coplanarity matching in a robust RGBD
reconstruction formulation.We find that coplanarity constraints detected with
our method are sufficient to get reconstruction results comparable to
state-of-the-art frameworks on most scenes, but outperform other methods on
standard benchmarks when combined with a simple keypoint method.",PlaneMatch: Patch Coplanarity Prediction for Robust RGB-D Reconstruction,specifically model tasks diffusion proposed
582,"We present a powerful method to extract per-point semantic class labels from
aerialphotogrammetry data. Labeling this kind of data is important for tasks
such as environmental modelling, object classification and scene understanding.
Unlike previous point cloud classification methods that rely exclusively on
geometric features, we show that incorporating color information yields a
significant increase in accuracy in detecting semantic classes. We test our
classification method on three real-world photogrammetry datasets that were
generated with Pix4Dmapper Pro, and with varying point densities. We show that
off-the-shelf machine learning techniques coupled with our new features allow
us to train highly accurate classifiers that generalize well to unseen data,
processing point clouds containing 10 million points in less than 3 minutes on
a desktop computer.",Classification of Aerial Photogrammetric 3D Point Clouds,the proposed <unk> unlabeled <unk>
583,"We present an unsupervised representation learning approach using videos
without semantic labels. We leverage the temporal coherence as a supervisory
signal by formulating representation learning as a sequence sorting task. We
take temporally shuffled frames (i.e., in non-chronological order) as inputs
and train a convolutional neural network to sort the shuffled sequences.
Similar to comparison-based sorting algorithms, we propose to extract features
from all frame pairs and aggregate them to predict the correct order. As
sorting shuffled image sequence requires an understanding of the statistical
temporal structure of images, training with such a proxy task allows us to
learn rich and generalizable visual representation. We validate the
effectiveness of the learned representation using our method as pre-training on
high-level recognition problems. The experimental results show that our method
compares favorably against state-of-the-art methods on action recognition,
image classification and object detection tasks.",Unsupervised Representation Learning by Sorting Sequences,models model <unk> previously auto encoder paper  for
584,"The classification of high dimensional data with kernel methods is considered
in this article. Exploit- ing the emptiness property of high dimensional
spaces, a kernel based on the Mahalanobis distance is proposed. The computation
of the Mahalanobis distance requires the inversion of a covariance matrix. In
high dimensional spaces, the estimated covariance matrix is ill-conditioned and
its inversion is unstable or impossible. Using a parsimonious statistical
model, namely the High Dimensional Discriminant Analysis model, the specific
signal and noise subspaces are estimated for each considered class making the
inverse of the class specific covariance matrix explicit and stable, leading to
the definition of a parsimonious Mahalanobis kernel. A SVM based framework is
used for selecting the hyperparameters of the parsimonious Mahalanobis kernel
by optimizing the so-called radius-margin bound. Experimental results on three
high dimensional data sets show that the proposed kernel is suitable for
classifying high dimensional data, providing better classification accuracies
than the conventional Gaussian kernel.","Parsimonious Mahalanobis Kernel for the Classification of High
  Dimensional Data",model model kernel paper  for
585,"We propose a method that infers whether linear relations between two
high-dimensional variables X and Y are due to a causal influence from X to Y or
from Y to X. The earlier proposed so-called Trace Method is extended to the
regime where the dimension of the observed variables exceeds the sample size.
Based on previous work, we postulate conditions that characterize a causal
relation between X and Y. Moreover, we describe a statistical test and argue
that both causal directions are typically rejected if there is a common cause.
A full theoretical analysis is presented for the deterministic case but our
approach seems to be valid for the noisy case, too, for which we additionally
present an approach based on a sparsity constraint. The discussed method yields
promising results for both simulated and real world data.","Testing whether linear equations are causal: A free probability theory
  approach",tractable demonstrate
586,"We consider the problem of semantic image segmentation using deep
convolutional neural networks. We propose a novel network architecture called
the label refinement network that predicts segmentation labels in a
coarse-to-fine fashion at several resolutions. The segmentation labels at a
coarse resolution are used together with convolutional features to obtain finer
resolution segmentation labels. We define loss functions at several stages in
the network to provide supervisions at different stages. Our experimental
results on several standard datasets demonstrate that the proposed model
provides an effective way of producing pixel-wise dense image labeling.",Label Refinement Network for Coarse-to-Fine Semantic Segmentation,<unk> neural method <unk>
587,"In this paper, we propose a novel framework with 3D convolutional networks
(ConvNets) for automated detection of pulmonary nodules from low-dose CT scans,
which is a challenging yet crucial task for lung cancer early diagnosis and
treatment. Different from previous standard ConvNets, we try to tackle the
severe hard/easy sample imbalance problem in medical datasets and explore the
benefits of localized annotations to regularize the learning, and hence boost
the performance of ConvNets to achieve more accurate detections. Our proposed
framework consists of two stages: 1) candidate screening, and 2) false positive
reduction. In the first stage, we establish a 3D fully convolutional network,
effectively trained with an online sample filtering scheme, to sensitively and
rapidly screen the nodule candidates. In the second stage, we design a
hybrid-loss residual network which harnesses the location and size information
as important cues to guide the nodule recognition procedure. Experimental
results on the public large-scale LUNA16 dataset demonstrate superior
performance of our proposed method compared with state-of-the-art approaches
for the pulmonary nodule detection task.","Automated Pulmonary Nodule Detection via 3D ConvNets with Online Sample
  Filtering and Hybrid-Loss Residual Learning",predictive model new <unk>
588,"One of the challenges in evaluating multi-object video detection, tracking
and classification systems is having publically available data sets with which
to compare different systems. However, the measures of performance for tracking
and classification are different. Data sets that are suitable for evaluating
tracking systems may not be appropriate for classification. Tracking video data
sets typically only have ground truth track IDs, while classification video
data sets only have ground truth class-label IDs. The former identifies the
same object over multiple frames, while the latter identifies the type of
object in individual frames. This paper describes an advancement of the ground
truth meta-data for the DARPA Neovision2 Tower data set to allow both the
evaluation of tracking and classification. The ground truth data sets presented
in this paper contain unique object IDs across 5 different classes of object
(Car, Bus, Truck, Person, Cyclist) for 24 videos of 871 image frames each. In
addition to the object IDs and class labels, the ground truth data also
contains the original bounding box coordinates together with new bounding boxes
in instances where un-annotated objects were present. The unique IDs are
maintained during occlusions between multiple objects or when objects re-enter
the field of view. This will provide: a solid foundation for evaluating the
performance of multi-object tracking of different types of objects, a
straightforward comparison of tracking system performance using the standard
Multi Object Tracking (MOT) framework, and classification performance using the
Neovision2 metrics. These data have been hosted publically.","A data set for evaluating the performance of multi-class multi-object
  video tracking",state of the art show combining without
589,"We generalize the stochastic block model to the important case in which edges
are annotated with weights drawn from an exponential family distribution. This
generalization introduces several technical difficulties for model estimation,
which we solve using a Bayesian approach. We introduce a variational algorithm
that efficiently approximates the model's posterior distribution for dense
graphs. In specific numerical experiments on edge-weighted networks, this
weighted stochastic block model outperforms the common approach of first
applying a single threshold to all weights and then applying the classic
stochastic block model, which can obscure latent block structure in networks.
This model will enable the recovery of latent structure in a broader range of
network data than was previously possible.",Adapting the Stochastic Block Model to Edge-Weighted Networks,set observation problem applied stochastic
590,"We present a new optimization-theoretic approach to analyzing
Follow-the-Leader style algorithms, particularly in the setting where
perturbations are used as a tool for regularization. We show that adding a
strongly convex penalty function to the decision rule and adding stochastic
perturbations to data correspond to deterministic and stochastic smoothing
operations, respectively. We establish an equivalence between ""Follow the
Regularized Leader"" and ""Follow the Perturbed Leader"" up to the smoothness
properties. This intuition leads to a new generic analysis framework that
recovers and improves the previous known regret bounds of the class of
algorithms commonly known as Follow the Perturbed Leader.",Online Linear Optimization via Smoothing,programming stochastic show kernel demonstrate
591,"We introduce a near-linear complexity (geometric and meshless/algebraic)
multigrid/multiresolution method for PDEs with rough ($L^\infty$) coefficients
with rigorous a-priori accuracy and performance estimates. The method is
discovered through a decision/game theory formulation of the problems of (1)
identifying restriction and interpolation operators (2) recovering a signal
from incomplete measurements based on norm constraints on its image under a
linear operator (3) gambling on the value of the solution of the PDE based on a
hierarchy of nested measurements of its solution or source term. The resulting
elementary gambles form a hierarchy of (deterministic) basis functions of
$H^1_0(\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbands
with respect to the scalar product induced by the energy norm of the PDE (2)
enable sparse compression of the solution space in $H^1_0(\Omega)$ (3) induce
an orthogonal multiresolution operator decomposition. The operating diagram of
the multigrid method is that of an inverted pyramid in which gamblets are
computed locally (by virtue of their exponential decay), hierarchically (from
fine to coarse scales) and the PDE is decomposed into a hierarchy of
independent linear systems with uniformly bounded condition numbers. The
resulting algorithm is parallelizable both in space (via localization) and in
bandwith/subscale (subscales can be computed independently from each other).
Although the method is deterministic it has a natural Bayesian interpretation
under the measure of probability emerging (as a mixed strategy) from the
information game formulation and multiresolution approximations form a
martingale with respect to the filtration induced by the hierarchy of nested
measurements.","Multigrid with rough coefficients and Multiresolution operator
  decomposition from Hierarchical Information Games",model model using feature models method function  defining
592,"Political speeches and debates play an important role in shaping the images
of politicians, and the public often relies on media outlets to select bits of
political communication from a large pool of utterances. It is an important
research question to understand what factors impact this selection process.
  To quantitatively explore the selection process, we build a three- decade
dataset of presidential debate transcripts and post-debate coverage. We first
examine the effect of wording and propose a binary classification framework
that controls for both the speaker and the debate situation. We find that
crowdworkers can only achieve an accuracy of 60% in this task, indicating that
media choices are not entirely obvious. Our classifiers outperform crowdworkers
on average, mainly in primary debates. We also compare important factors from
crowdworkers' free-form explanations with those from data-driven methods and
find interesting differences. Few crowdworkers mentioned that ""context
matters"", whereas our data show that well-quoted sentences are more distinct
from the previous utterance by the same speaker than less-quoted sentences.
Finally, we examine the aggregate effect of media preferences towards different
wordings to understand the extent of fragmentation among media outlets. By
analyzing a bipartite graph built from quoting behavior in our data, we observe
a decreasing trend in bipartisan coverage.","""You are no Jack Kennedy"": On Media Selection of Highlights from
  Presidential Debates",model show model networks proposed <unk> predictive for
593,"In order to study the application of artificial intelligence (AI) to dental
imaging, we applied AI technology to classify a set of panoramic radiographs
using (a) a convolutional neural network (CNN) which is a form of an artificial
neural network (ANN), (b) representative image cognition algorithms that
implement scale-invariant feature transform (SIFT), and (c) histogram of
oriented gradients (HOG).",Machine Learning for Dental Image Analysis,tasks sorting using methods  pose refine
594,"It is often the case that, within an online recommender system, multiple
users share a common account. Can such shared accounts be identified solely on
the basis of the userprovided ratings? Once a shared account is identified, can
the different users sharing it be identified as well? Whenever such user
identification is feasible, it opens the way to possible improvements in
personalized recommendations, but also raises privacy concerns. We develop a
model for composite accounts based on unions of linear subspaces, and use
subspace clustering for carrying out the identification task. We show that a
significant fraction of such accounts is identifiable in a reliable manner, and
illustrate potential uses for personalized recommendation.","Guess Who Rated This Movie: Identifying Users Through Subspace
  Clustering",meaningful facial the
595,"In this position paper we present a novel approach to neurobiologically
plausible implementation of emotional reactions and behaviors for real-time
autonomous robotic systems. The working metaphor we use is the ""day"" and
""night"" phases of mammalian life. During the ""day"" phase a robotic system
stores the inbound information and is controlled by a light-weight rule-based
system in real time. In contrast to that, during the ""night"" phase the stored
information is been transferred to the supercomputing system to update the
realistic neural network: emotional and behavioral strategies.",Robot Dream,results  non stationary show manner data 
596,"We try a conceptual analysis of inheritance diagrams, first in abstract
terms, and then compare to ""normality"" and the ""small/big sets"" of preferential
and related reasoning. The main ideas are about nodes as truth values and
information sources, truth comparison by paths, accessibility or relevance of
information by paths, relative normality, and prototypical reasoning.",Remarks on Inheritance Systems,<unk> <unk> <unk>
597,"Generating diverse questions for given images is an important task for
computational education, entertainment and AI assistants. Different from many
conventional prediction techniques is the need for algorithms to generate a
diverse set of plausible questions, which we refer to as ""creativity"". In this
paper we propose a creative algorithm for visual question generation which
combines the advantages of variational autoencoders with long short-term memory
networks. We demonstrate that our framework is able to generate a large set of
varying questions given a single input image.",Creativity: Generating Diverse Questions using Variational Autoencoders,studied model data potential state two
598,"In this paper, we present a novel approach for initializing deep neural
networks, i.e., by turning PCA into neural layers. Usually, the initialization
of the weights of a deep neural network is done in one of the three following
ways: 1) with random values, 2) layer-wise, usually as Deep Belief Network or
as auto-encoder, and 3) re-use of layers from another network (transfer
learning). Therefore, typically, many training epochs are needed before
meaningful weights are learned, or a rather similar dataset is required for
seeding a fine-tuning of transfer learning. In this paper, we describe how to
turn a PCA into an auto-encoder, by generating an encoder layer of the PCA
parameters and furthermore adding a decoding layer. We analyze the
initialization technique on real documents. First, we show that a PCA-based
initialization is quick and leads to a very stable initialization. Furthermore,
for the task of layout analysis we investigate the effectiveness of PCA-based
initialization and show that it outperforms state-of-the-art random weight
initialization methods.",PCA-Initialized Deep Neural Networks Applied To Document Image Analysis,textual model <unk> combined
599,"Due to the lack of information such as the space environment condition and
resident space objects' (RSOs') body characteristics, current orbit predictions
that are solely grounded on physics-based models may fail to achieve required
accuracy for collision avoidance and have led to satellite collisions already.
This paper presents a methodology to predict RSOs' trajectories with higher
accuracy than that of the current methods. Inspired by the machine learning
(ML) theory through which the models are learned based on large amounts of
observed data and the prediction is conducted without explicitly modeling space
objects and space environment, the proposed ML approach integrates
physics-based orbit prediction algorithms with a learning-based process that
focuses on reducing the prediction errors. Using a simulation-based space
catalog environment as the test bed, the paper demonstrates three types of
generalization capability for the proposed ML approach: 1) the ML model can be
used to improve the same RSO's orbit information that is not available during
the learning process but shares the same time interval as the training data; 2)
the ML model can be used to improve predictions of the same RSO at future
epochs; and 3) the ML model based on a RSO can be applied to other RSOs that
share some common features.",Improving Orbit Prediction Accuracy through Supervised Machine Learning,along test #  video models
600,"We bound the future loss when predicting any (computably) stochastic sequence
online. Solomonoff finitely bounded the total deviation of his universal
predictor $M$ from the true distribution $mu$ by the algorithmic complexity of
$mu$. Here we assume we are at a time $t>1$ and already observed $x=x_1...x_t$.
We bound the future prediction performance on $x_{t+1}x_{t+2}...$ by a new
variant of algorithmic complexity of $mu$ given $x$, plus the complexity of the
randomness deficiency of $x$. The new complexity is monotone in its condition
in the sense that this complexity can only decrease if the condition is
prolonged. We also briefly discuss potential generalizations to Bayesian model
classes and to classification problems.",Algorithmic Complexity Bounds on Future Prediction Errors,kernel almost model <unk> using
601,"We propose a new approach for editing face images, which enables numerous
exciting applications including face relighting, makeup transfer and face
detail editing. Our face edits are based on a visual representation, which
includes geometry, face segmentation, albedo, illumination and detail map. To
recover our visual representation, we start by estimating geometry using a
morphable face model, then decompose the face image to recover the albedo, and
then shade the geometry with the albedo and illumination. The residual between
our shaded geometry and the input image produces our detail map, which carries
high frequency information that is either insufficiently or incorrectly
captured by our shading process. By manipulating the detail map, we can edit
face images with reality and identity preserved. Our representation allows
various applications. First, it allows a user to directly manipulate various
illumination. Second, it allows non-parametric makeup transfer with input
face's distinctive identity features preserved. Third, it allows non-parametric
modifications to the face appearance by transferring details. For face
relighting and detail editing, we evaluate via a user study and our method
outperforms other methods. For makeup transfer, we evaluate via an online
attractiveness evaluation system, and can reliably make people look younger and
more attractive. We also show extensive qualitative comparisons to existing
methods, and have significant improvements over previous techniques.",A Visual Representation for Editing Face Images,role demonstrate show
602,"Distributed training of deep neural networks has received significant
research interest, and its major approaches include implementations on multiple
GPUs and clusters. Parallelization can dramatically improve the efficiency of
training deep and complicated models with large-scale data. A fundamental
barrier against the speedup of DNN training, however, is the trade-off between
computation and communication time. In other words, increasing the number of
worker nodes decreases the time consumed in computation while simultaneously
increasing communication overhead under constrained network bandwidth,
especially in commodity hardware environments. To alleviate this trade-off, we
suggest the idea of homomorphic parameter compression, which compresses
parameters with the least expense and trains the DNN with the compressed
representation. Although the specific method is yet to be discovered, we
demonstrate that there is a high probability that the homomorphism can reduce
the communication overhead, thanks to little compression and decompression
times. We also provide theoretical speedup of homomorphic compression.",Homomorphic Parameter Compression for Distributed Deep Learning Training,networks based function  units two
603,"We consider the traffic data reconstruction problem. Suppose we have the
traffic data of an entire city that are incomplete because some road data are
unobserved. The problem is to reconstruct the unobserved parts of the data. In
this paper, we propose a new method to reconstruct incomplete traffic data
collected from various traffic sensors. Our approach is based on Markov random
field modeling of road traffic. The reconstruction is achieved by using
mean-field method and a machine learning method. We numerically verify the
performance of our method using realistic simulated traffic data for the real
road network of Sendai, Japan.",Traffic data reconstruction based on Markov random field modeling,complexity two uses proposed units function 
604,"Narrative intelligence is the ability to craft, tell, understand, and respond
affectively to stories. We argue that instilling artificial intelligences with
computational narrative intelligence affords a number of applications
beneficial to humans. We lay out some of the machine learning challenges
necessary to solve to achieve computational narrative intelligence. Finally, we
argue that computational narrative is a practical step towards machine
enculturation, the teaching of sociocultural values to machines.","Computational Narrative Intelligence: A Human-Centered Goal for
  Artificial Intelligence",model show distribution sparse applications 
605,"It is known that the majority of the human genome consists of repeated
sequences. Furthermore, it is believed that a significant part of the rest of
the genome also originated from repeated sequences and has mutated to its
current form. In this paper, we investigate the possibility of constructing an
exponentially large number of sequences from a short initial sequence and
simple replication rules, including those resembling genomic replication
processes. In other words, our goal is to find out the capacity, or the
expressive power, of these string-replication systems. Our results include
exact capacities, and bounds on the capacities, of four fundamental
string-replication systems.",The Capacity of String-Replication Systems,online model <unk> regression  networks
606,"We propose a simple two-step approach for speeding up convolution layers
within large convolutional neural networks based on tensor decomposition and
discriminative fine-tuning. Given a layer, we use non-linear least squares to
compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a
sum of a small number of rank-one tensors. At the second step, this
decomposition is used to replace the original convolutional layer with a
sequence of four convolutional layers with small kernels. After such
replacement, the entire network is fine-tuned on the training data using
standard backpropagation process.
  We evaluate this approach on two CNNs and show that it is competitive with
previous approaches, leading to higher obtained CPU speedups at the cost of
lower accuracy drops for the smaller of the two networks. Thus, for the
36-class character classification CNN, our approach obtains a 8.5x CPU speedup
of the whole network with only minor accuracy drop (1% from 91% to 90%). For
the standard ImageNet architecture (AlexNet), the approach speeds up the second
convolution layer by a factor of 4x at the cost of $1\%$ increase of the
overall top-5 classification error.","Speeding-up Convolutional Neural Networks Using Fine-tuned
  CP-Decomposition",using significant simple show least image <unk>
607,"Metric learning has been shown to be highly effective to improve the
performance of nearest neighbor classification. In this paper, we address the
problem of metric learning for Symmetric Positive Definite (SPD) matrices such
as covariance matrices, which arise in many real-world applications. Naively
using standard Mahalanobis metric learning methods under the Euclidean geometry
for SPD matrices is not appropriate, because the difference of SPD matrices can
be a non-SPD matrix and thus the obtained solution can be uninterpretable. To
cope with this problem, we propose to use a properly parameterized LogEuclidean
distance and optimize the metric with respect to kernel-target alignment, which
is a supervised criterion for kernel learning. Then the resulting non-trivial
optimization problem is solved by utilizing the Riemannian geometry. Finally,
we experimentally demonstrate the usefulness of our LogEuclidean metric
learning algorithm on real-world classification tasks for EEG signals and
texture patches.","Supervised LogEuclidean Metric Learning for Symmetric Positive Definite
  Matrices",potential forward method open best model networks
608,"Melanoma is the deadliest form of skin cancer. Computer systems can assist in
melanoma detection, but are not widespread in clinical practice. In 2016, an
open challenge in classification of dermoscopic images of skin lesions was
announced. A training set of 900 images with corresponding class labels and
semi-automatic/manual segmentation masks was released for the challenge. An
independent test set of 379 images was used to rank the participants. This
article demonstrates the impact of ranking criteria, segmentation method and
classifier, and highlights the clinical perspective. We compare five different
measures for diagnostic accuracy by analysing the resulting ranking of the
computer systems in the challenge. Choice of performance measure had great
impact on the ranking. Systems that were ranked among the top three for one
measure, dropped to the bottom half when changing performance measure. Nevus
Doctor, a computer system previously developed by the authors, was used to
investigate the impact of segmentation and classifier. The unexpected small
impact of automatic versus semi-automatic/manual segmentation suggests that
improvements of the automatic segmentation method w.r.t. resemblance to
semi-automatic/manual segmentation will not improve diagnostic accuracy
substantially. A small set of similar classification algorithms are used to
investigate the impact of classifier on the diagnostic accuracy. The
variability in diagnostic accuracy for different classifier algorithms was
larger than the variability for segmentation methods, and suggests a focus for
future investigations. From a clinical perspective, the misclassification of a
melanoma as benign has far greater cost than the misclassification of a benign
lesion. For computer systems to have clinical impact, their performance should
be ranked by a high-sensitivity measure.","Comparison of computer systems and ranking criteria for automatic
  melanoma detection in dermoscopic images",linear model <unk> <unk> different <unk> linear
609,"This article presents a mathematical framework to simultaneously tackle the
problems of 3D reconstruction, pose estimation and object classification, from
a single 2D image. In sharp contrast with state of the art methods that rely
primarily on 2D information and solve each of these three problems separately
or iteratively, we propose a mathematical framework that incorporates prior
""knowledge"" about the 3D shapes of different object classes and solves these
problems jointly and simultaneously, using a hypothesize-and-bound (H&B)
algorithm. In the proposed H&B algorithm one hypothesis is defined for each
possible pair [object class, object pose], and the algorithm selects the
hypothesis H that maximizes a function L(H) encoding how well each hypothesis
""explains"" the input image. To find this maximum efficiently, the function L(H)
is not evaluated exactly for each hypothesis H, but rather upper and lower
bounds for it are computed at a much lower cost. In order to obtain bounds for
L(H) that are tight yet inexpensive to compute, we extend the theory of shapes
described in [14] to handle projections of shapes. This extension allows us to
define a probabilistic relationship between the prior knowledge given in 3D and
the 2D input image. This relationship is derived from first principles and is
proven to be the only relationship having the properties that we intuitively
expect from a ""projection."" In addition to the efficiency and optimality
characteristics of H&B algorithms, the proposed framework has the desirable
property of integrating information in the 2D image with information in the 3D
prior to estimate the optimal reconstruction. While this article focuses
primarily on the problem mentioned above, we believe that the theory presented
herein has multiple other potential applications.","Hypothesize and Bound: A Computational Focus of Attention Mechanism for
  Simultaneous 3D Shape Reconstruction, Pose Estimation and Classification from
  a Single 2D Image",programming both we cross correlation data <unk> number
610,"Filters in a Convolutional Neural Network (CNN) contain model parameters
learned from enormous amounts of data. In this paper, we suggest to decompose
convolutional filters in CNN as a truncated expansion with pre-fixed bases,
namely the Decomposed Convolutional Filters network (DCFNet), where the
expansion coefficients remain learned from data. Such a structure not only
reduces the number of trainable parameters and computation, but also imposes
filter regularity by bases truncation. Through extensive experiments, we
consistently observe that DCFNet maintains accuracy for image classification
tasks with a significant reduction of model parameters, particularly with
Fourier-Bessel (FB) bases, and even with random bases. Theoretically, we
analyze the representation stability of DCFNet with respect to input
variations, and prove representation stability under generic assumptions on the
expansion coefficients. The analysis is consistent with the empirical
observations.",DCFNet: Deep Neural Network with Decomposed Convolutional Filters,around model kernel <unk> index
611,"Subset selection from massive data with noised information is increasingly
popular for various applications. This problem is still highly challenging as
current methods are generally slow in speed and sensitive to outliers. To
address the above two issues, we propose an accelerated robust subset selection
(ARSS) method. Specifically in the subset selection area, this is the first
attempt to employ the $\ell_{p}(0<p\leq1)$-norm based measure for the
representation loss, preventing large errors from dominating our objective. As
a result, the robustness against outlier elements is greatly enhanced.
Actually, data size is generally much larger than feature length, i.e. $N\gg
L$. Based on this observation, we propose a speedup solver (via ALM and
equivalent derivations) to highly reduce the computational cost, theoretically
from $O(N^{4})$ to $O(N{}^{2}L)$. Extensive experiments on ten benchmark
datasets verify that our method not only outperforms state of the art methods,
but also runs 10,000+ times faster than the most related method.","10,000+ Times Accelerated Robust Subset Selection (ARSS)",unsupervised show show preferences
612,"A large set of signals can sometimes be described sparsely using a
dictionary, that is, every element can be represented as a linear combination
of few elements from the dictionary. Algorithms for various signal processing
applications, including classification, denoising and signal separation, learn
a dictionary from a set of signals to be represented. Can we expect that the
representation found by such a dictionary for a previously unseen example from
the same source will have L_2 error of the same magnitude as those for the
given examples? We assume signals are generated from a fixed distribution, and
study this questions from a statistical learning theory perspective.
  We develop generalization bounds on the quality of the learned dictionary for
two types of constraints on the coefficient selection, as measured by the
expected L_2 error in representation when the dictionary is used. For the case
of l_1 regularized coefficient selection we provide a generalization bound of
the order of O(sqrt(np log(m lambda)/m)), where n is the dimension, p is the
number of elements in the dictionary, lambda is a bound on the l_1 norm of the
coefficient vector and m is the number of samples, which complements existing
results. For the case of representing a new signal as a combination of at most
k dictionary elements, we provide a bound of the order O(sqrt(np log(m k)/m))
under an assumption on the level of orthogonality of the dictionary (low Babel
function). We further show that this assumption holds for most dictionaries in
high dimensions in a strong probabilistic sense. Our results further yield fast
rates of order 1/m as opposed to 1/sqrt(m) using localized Rademacher
complexity. We provide similar results in a general setting using kernels with
weak smoothness requirements.",The Sample Complexity of Dictionary Learning,video various show reverse using states search
613,"Word similarities affect language acquisition and use in a multi-relational
way barely accounted for in the literature. We propose a multiplex network
representation of this mental lexicon of word similarities as a natural
framework for investigating large-scale cognitive patterns. Our representation
accounts for semantic, taxonomic, and phonological interactions and it
identifies a cluster of words which are used with greater frequency, are
identified, memorised, and learned more easily, and have more meanings than
expected at random. This cluster emerges around age 7 through an explosive
transition not reproduced by null models. We relate this explosive emergence to
polysemy -- redundancy in word meanings. Results indicate that the word cluster
acts as a core for the lexicon, increasing both lexical navigability and
robustness to linguistic degradation. Our findings provide quantitative
confirmation of existing conjectures about core structure in the mental lexicon
and the importance of integrating multi-relational word-word interactions in
psycholinguistic frameworks.",Multiplex model of mental lexicon reveals explosive learning in humans,word paper 
614,"We propose an extension of Stabler's version of clitics treatment for a wider
coverage of the French language. For this, we present the lexical entries
needed in the lexicon. Then, we show the recognition of complex syntactic
phenomena as (left and right) dislo- cation, clitic climbing over modal and
extraction from determiner phrase. The aim of this presentation is the
syntax-semantic interface for clitics analyses in which we will stress on
clitic climbing over verb and raising verb.",Treating clitics with minimalist grammars,using <unk> various public show distributions events 
615,"This paper considers multiprocessor task scheduling in a multistage hybrid
flow-shop environment. The problem even in its simplest form is NP-hard in the
strong sense. The great deal of interest for this problem, besides its
theoretical complexity, is animated by needs of various manufacturing and
computing systems. We propose a new approach based on limited discrepancy
search to solve the problem. Our method is tested with reference to a proposed
lower bound as well as the best-known solutions in literature. Computational
results show that the developed approach is efficient in particular for
large-size problems.","Climbing depth-bounded adjacent discrepancy search for solving hybrid
  flow shop scheduling problems with multiprocessor tasks",model <unk> discuss set method
616,"A recent framework of relativized hyperequivalence of programs offers a
unifying generalization of strong and uniform equivalence. It seems to be
especially well suited for applications in program optimization and modular
programming due to its flexibility that allows us to restrict, independently of
each other, the head and body alphabets in context programs. We study
relativized hyperequivalence for the three semantics of logic programs given by
stable, supported and supported minimal models. For each semantics, we identify
four types of contexts, depending on whether the head and body alphabets are
given directly or as the complement of a given set. Hyperequivalence relative
to contexts where the head and body alphabets are specified directly has been
studied before. In this paper, we establish the complexity of deciding
relativized hyperequivalence with respect to the three other types of context
programs.
  To appear in Theory and Practice of Logic Programming (TPLP).",Relativized hyperequivalence of logic programs for modular programming,however  complexity for show explain inference
617,"Given a training set with binary classification, the Support Vector Machine
identifies the hyperplane maximizing the margin between the two classes of
training data. This general formulation is useful in that it can be applied
without regard to variance differences between the classes. Ignoring these
differences is not optimal, however, as the general SVM will give the class
with lower variance an unjustifiably wide berth. This increases the chance of
misclassification of the other class and results in an overall loss of
predictive performance. An alternate construction is proposed in which the
margins of the separating hyperplane are different for each class, each
proportional to the standard deviation of its class along the direction
perpendicular to the hyperplane. The construction agrees with the SVM in the
case of equal class variances. This paper will then examine the impact to the
dual representation of the modified constraint equations.","A Modified Construction for a Support Vector Classifier to Accommodate
  Class Imbalances",estimate model video prove different different architectures  traditional
618,"Style-transfer is a process of migrating a style from a given image to the
content of another, synthesizing a new image which is an artistic mixture of
the two. Recent work on this problem adopting Convolutional Neural-networks
(CNN) ignited a renewed interest in this field, due to the very impressive
results obtained. There exists an alternative path towards handling the
style-transfer task, via generalization of texture-synthesis algorithms. This
approach has been proposed over the years, but its results are typically less
impressive compared to the CNN ones.
  In this work we propose a novel style-transfer algorithm that extends the
texture-synthesis work of Kwatra et. al. (2005), while aiming to get stylized
images that get closer in quality to the CNN ones. We modify Kwatra's algorithm
in several key ways in order to achieve the desired transfer, with emphasis on
a consistent way for keeping the content intact in selected regions, while
producing hallucinated and rich style in others. The results obtained are
visually pleasing and diverse, shown to be competitive with the recent CNN
style-transfer algorithms. The proposed algorithm is fast and flexible, being
able to process any pair of content + style images.",Style-Transfer via Texture-Synthesis,proposed <unk> research paper  system proposed account generate
619,"Current research in Computer Vision has shown that Convolutional Neural
Networks (CNN) give state-of-the-art performance in many classification tasks
and Computer Vision problems. The embedding of CNN, which is the internal
representation produced by the last layer, can indirectly learn topological and
relational properties. Moreover, by using a suitable loss function, CNN models
can learn invariance to a wide range of non-linear distortions such as
rotation, viewpoint angle or lighting condition. In this work, new insights are
discovered about CNN embeddings and a new loss function is proposed, derived
from the contrastive loss, that creates models with more predicable mappings
and also quantifies distortions. In typical distortion-dependent methods, there
is no simple relation between the features corresponding to one image and the
features of this image distorted. Therefore, these methods require to
feed-forward inputs under every distortions in order to find the corresponding
features representations. Our contribution makes a step towards embeddings
where features of distorted inputs are related and can be derived from each
others by the intensity of the distortion.",Towards Distortion-Predictable Embedding of Neural Networks,based problem 
620,"Model precision in a classification task is highly dependent on the feature
space that is used to train the model. Moreover, whether the features are
sequential or static will dictate which classification method can be applied as
most of the machine learning algorithms are designed to deal with either one or
another type of data. In real-life scenarios, however, it is often the case
that both static and dynamic features are present, or can be extracted from the
data. In this work, we demonstrate how generative models such as Hidden Markov
Models (HMM) and Long Short-Term Memory (LSTM) artificial neural networks can
be used to extract temporal information from the dynamic data. We explore how
the extracted information can be combined with the static features in order to
improve the classification performance. We evaluate the existing techniques and
suggest a hybrid approach, which outperforms other methods on several public
datasets.","Combining Static and Dynamic Features for Multivariate Sequence
  Classification",inference model achieved
621,"A novel efficient method for computing the Knowledge-Gradient policy for
Continuous Parameters (KGCP) for deterministic optimization is derived. The
differences with Expected Improvement (EI), a popular choice for Bayesian
optimization of deterministic engineering simulations, are explored. Both
policies and the Upper Confidence Bound (UCB) policy are compared on a number
of benchmark functions including a problem from structural dynamics. It is
empirically shown that KGCP has similar performance as the EI policy for many
problems, but has better convergence properties for complex (multi-modal)
optimization problems as it emphasizes more on exploration when the model is
confident about the shape of optimal regions. In addition, the relationship
between Maximum Likelihood Estimation (MLE) and slice sampling for estimation
of the hyperparameters of the underlying models, and the complexity of the
problem at hand, is studied.","Fast Calculation of the Knowledge Gradient for Optimization of
  Deterministic Engineering Simulations",naive show
622,"In this paper, we use a fully convolutional neural network (FCNN) for the
segmentation of gliomas from Magnetic Resonance Images (MRI). A fully
automatic, voxel based classification was achieved by training a 23 layer deep
FCNN on 2-D slices extracted from patient volumes. The network was trained on
slices extracted from 130 patients and validated on 50 patients. For the task
of survival prediction, texture and shape based features were extracted from T1
post contrast volume to train an XGBoost regressor. On BraTS 2017 validation
set, the proposed scheme achieved a mean whole tumor, tumor core and active
dice score of 0.83, 0.69 and 0.69 respectively and an accuracy of 52% for the
overall survival prediction.","Automatic Segmentation and Overall Survival Prediction in Gliomas using
  Fully Convolutional Neural Network and Texture Analysis",bandit show geometry linear
623,"Recently, significant improvement has been made on semantic object
segmentation due to the development of deep convolutional neural networks
(DCNNs). Training such a DCNN usually relies on a large number of images with
pixel-level segmentation masks, and annotating these images is very costly in
terms of both finance and human effort. In this paper, we propose a simple to
complex (STC) framework in which only image-level annotations are utilized to
learn DCNNs for semantic segmentation. Specifically, we first train an initial
segmentation network called Initial-DCNN with the saliency maps of simple
images (i.e., those with a single category of major object(s) and clean
background). These saliency maps can be automatically obtained by existing
bottom-up salient object detection techniques, where no supervision information
is needed. Then, a better network called Enhanced-DCNN is learned with
supervision from the predicted segmentation masks of simple images based on the
Initial-DCNN as well as the image-level annotations. Finally, more pixel-level
segmentation masks of complex images (two or more categories of objects with
cluttered background), which are inferred by using Enhanced-DCNN and
image-level annotations, are utilized as the supervision information to learn
the Powerful-DCNN for semantic segmentation. Our method utilizes $40$K simple
images from Flickr.com and 10K complex images from PASCAL VOC for step-wisely
boosting the segmentation network. Extensive experimental results on PASCAL VOC
2012 segmentation benchmark well demonstrate the superiority of the proposed
STC framework compared with other state-of-the-arts.","STC: A Simple to Complex Framework for Weakly-supervised Semantic
  Segmentation",present both we cross correlation data coordination using preferences
624,"This paper presents the systems developed by LIUM and CVC for the WMT16
Multimodal Machine Translation challenge. We explored various comparative
methods, namely phrase-based systems and attentional recurrent neural networks
models trained using monomodal or multimodal data. We also performed a human
evaluation in order to estimate the usefulness of multimodal data for human
machine translation and image description generation. Our systems obtained the
best results for both tasks according to the automatic evaluation metrics BLEU
and METEOR.","Does Multimodality Help Human and Machine for Translation and Image
  Captioning?",selection shown problem preferences
625,"Motion ability is one of the most important human properties, including gait
as a basis of human transitional movement. Gait, as a biometric for recognizing
human identities, can be non-intrusively captured signals using wearable or
portable smart devices. In this study gait patterns is collected using a
wireless platform of two sensors located at chest and right ankle of the
subjects. Then the raw data has undergone some preprocessing methods and
segmented into 5 seconds windows. Some time and frequency domain features is
extracted and the performance evaluated by 5 different classifiers. Decision
Tree (with all features) and K-Nearest Neighbors (with 10 selected features)
classifiers reached 99.4% and 100% respectively.",Gait Pattern Recognition Using Accelerometers,suggests level the required show input the
626,"Image Classification based on BOW (Bag-of-words) has broad application
prospect in pattern recognition field but the shortcomings are existed because
of single feature and low classification accuracy. To this end we combine three
ingredients: (i) Three features with functions of mutual complementation are
adopted to describe the images, including PHOW (Pyramid Histogram of Words),
PHOC (Pyramid Histogram of Color) and PHOG (Pyramid Histogram of Orientated
Gradients). (ii) The improvement of traditional BOW model is presented by using
dense sample and an improved K-means clustering method for constructing the
visual dictionary. (iii) An adaptive feature-weight adjusted image
categorization algorithm based on the SVM and the fusion of multiple features
is adopted. Experiments carried out on Caltech 101 database confirm the
validity of the proposed approach. From the experimental results can be seen
that the classification accuracy rate of the proposed method is improved by
7%-17% higher than that of the traditional BOW methods. This algorithm makes
full use of global, local and spatial information and has significant
improvements to the classification accuracy.","Image classification based on support vector machine and the fusion of
  complementary features",robotics doors
627,"The League Championship Algorithm (LCA) is sport-inspired optimization
algorithm that was introduced by Ali Husseinzadeh Kashan in the year 2009. It
has since drawn enormous interest among the researchers because of its
potential efficiency in solving many optimization problems and real-world
applications. The LCA has also shown great potentials in solving
non-deterministic polynomial time (NP-complete) problems. This survey presents
a brief synopsis of the LCA literatures in peer-reviewed journals, conferences
and book chapters. These research articles are then categorized according to
indexing in the major academic databases (Web of Science, Scopus, IEEE Xplore
and the Google Scholar). The analysis was also done to explore the prospects
and the challenges of the algorithm and its acceptability among researchers.
This systematic categorization can be used as a basis for future studies.",A Survey of League Championship Algorithm: Prospects and Challenges,studied model units comparing
628,"In this paper, we cast the scribble-based interactive image segmentation as a
semi-supervised learning problem. Our novel approach alleviates the need to
solve an expensive generalized eigenvector problem by approximating the
eigenvectors using efficiently computed eigenfunctions. The smoothness operator
defined on feature densities at the limit n tends to infinity recovers the
exact eigenvectors of the graph Laplacian, where n is the number of nodes in
the graph. To further reduce the computational complexity without scarifying
our accuracy, we select pivots pixels from user annotations. In our
experiments, we evaluate our approach using both human scribble and ""robot
user"" annotations to guide the foreground/background segmentation. We developed
a new unbiased collection of five annotated images datasets to standardize the
evaluation procedure for any scribble-based segmentation method. We
experimented with several variations, including different feature vectors,
pivot count and the number of eigenvectors. Experiments are carried out on
datasets that contain a wide variety of natural images. We achieve better
qualitative and quantitative results compared to state-of-the-art interactive
segmentation algorithms.","Seeded Laplaican: An Eigenfunction Solution for Scribble Based
  Interactive Image Segmentation",using conditional feature models show significant packing approaches
629,"We consider a non-stationary formulation of the stochastic multi-armed bandit
where the rewards are no longer assumed to be identically distributed. For the
best-arm identification task, we introduce a version of Successive Elimination
based on random shuffling of the $K$ arms. We prove that under a novel and mild
assumption on the mean gap $\Delta$, this simple but powerful modification
achieves the same guarantees in term of sample complexity and cumulative regret
than its original version, but in a much wider class of problems, as it is not
anymore constrained to stationary distributions. We also show that the original
{\sc Successive Elimination} fails to have controlled regret in this more
general scenario, thus showing the benefit of shuffling. We then remove our
mild assumption and adapt the algorithm to the best-arm identification task
with switching arms. We adapt the definition of the sample complexity for that
case and prove that, against an optimal policy with $N-1$ switches of the
optimal arm, this new algorithm achieves an expected sample complexity of
$O(\Delta^{-2}\sqrt{NK\delta^{-1} \log(K \delta^{-1})})$, where $\delta$ is the
probability of failure of the algorithm, and an expected cumulative regret of
$O(\Delta^{-1}{\sqrt{NTK \log (TK)}})$ after $T$ time steps.","Random Shuffling and Resets for the Non-stationary Stochastic Bandit
  Problem",using complexity show show significant significant model explain
630,"In recent years, the use of machine learning classifiers is of great value in
solving a variety of problems in text classification. Sentiment mining is a
kind of text classification in which, messages are classified according to
sentiment orientation such as positive or negative. This paper extends the idea
of evaluating the performance of various classifiers to show their
effectiveness in sentiment mining of online product reviews. The product
reviews are collected from Amazon reviews. To evaluate the performance of
classifiers various evaluation methods like random sampling, linear sampling
and bootstrap sampling are used. Our results shows that support vector machine
with bootstrap sampling method outperforms others classifiers and sampling
methods in terms of misclassification rate.","Performance Evaluation of Machine Learning Classifiers in Sentiment
  Mining",<unk> show made search
631,"In many multilingual text classification problems, the documents in different
languages often share the same set of categories. To reduce the labeling cost
of training a classification model for each individual language, it is
important to transfer the label knowledge gained from one language to another
language by conducting cross language classification. In this paper we develop
a novel subspace co-regularized multi-view learning method for cross language
text classification. This method is built on parallel corpora produced by
machine translation. It jointly minimizes the training error of each classifier
in each language while penalizing the distance between the subspace
representations of parallel documents. Our empirical study on a large set of
cross language text classification tasks shows the proposed method consistently
outperforms a number of inductive methods, domain adaptation methods, and
multi-view learning methods.","Cross Language Text Classification via Subspace Co-Regularized
  Multi-View Learning",image learning appear appear kernel stochastic
632,"Long Short-Term Memory (LSTM) is a recurrent neural network (RNN)
architecture that has been designed to address the vanishing and exploding
gradient problems of conventional RNNs. Unlike feedforward neural networks,
RNNs have cyclic connections making them powerful for modeling sequences. They
have been successfully used for sequence labeling and sequence prediction
tasks, such as handwriting recognition, language modeling, phonetic labeling of
acoustic frames. However, in contrast to the deep neural networks, the use of
RNNs in speech recognition has been limited to phone recognition in small scale
tasks. In this paper, we present novel LSTM based RNN architectures which make
more effective use of model parameters to train acoustic models for large
vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at
various numbers of parameters and configurations. We show that LSTM models
converge quickly and give state of the art speech recognition performance for
relatively small sized models.","Long Short-Term Memory Based Recurrent Neural Network Architectures for
  Large Vocabulary Speech Recognition",spectral <unk> <unk>
633,"In many domains, we are interested in analyzing the structure of the
underlying distribution, e.g., whether one variable is a direct parent of the
other. Bayesian model-selection attempts to find the MAP model and use its
structure to answer these questions. However, when the amount of available data
is modest, there might be many models that have non-negligible posterior. Thus,
we want compute the Bayesian posterior of a feature, i.e., the total posterior
probability of all models that contain it. In this paper, we propose a new
approach for this task. We first show how to efficiently compute a sum over the
exponential number of networks that are consistent with a fixed ordering over
network variables. This allows us to compute, for a given ordering, both the
marginal probability of the data and the posterior of a feature. We then use
this result as the basis for an algorithm that approximates the Bayesian
posterior of a feature. Our approach uses a Markov Chain Monte Carlo (MCMC)
method, but over orderings rather than over network structures. The space of
orderings is much smaller and more regular than the space of structures, and
has a smoother posterior `landscape'. We present empirical results on synthetic
and real-life datasets that compare our approach to full model averaging (when
possible), to MCMC over network structures, and to a non-Bayesian bootstrap
approach.",Being Bayesian about Network Structure,learning appear users model <unk> method accuracy annotating
634,"There exists a theory of a single general-purpose learning algorithm which
could explain the principles its operation. It assumes the initial rough
architecture, a small library of simple innate circuits which are prewired at
birth. and proposes that all significant mental algorithms are learned. Given
current understanding and observations, this paper reviews and lists the
ingredients of such an algorithm from architectural and functional
perspectives.",Thinking Required, f  bandit
635,"Imaging objects that are obscured by scattering and occlusion is an important
challenge for many applications. For example, navigation and mapping
capabilities of autonomous vehicles could be improved, vision in harsh weather
conditions or under water could be facilitated, or search and rescue scenarios
could become more effective. Unfortunately, conventional cameras cannot see
around corners. Emerging, time-resolved computational imaging systems, however,
have demonstrated first steps towards non-line-of-sight (NLOS) imaging. In this
paper, we develop an algorithmic framework for NLOS imaging that is robust to
partial occlusions within the hidden scenes. This is a common light transport
effect, but not adequately handled by existing NLOS reconstruction algorithms,
resulting in fundamental limitations in what types of scenes can be recovered.
We demonstrate state-of-the-art NLOS reconstructions in simulation and with a
prototype single photon avalanche diode (SPAD) based acquisition system.",Robust Non-line-of-sight Imaging with Single Photon Detectors,require using using categorization ## model correlations
636,"Traditional approaches for handwritten Chinese character recognition suffer
in classifying similar characters. In this paper, we propose to discriminate
similar handwritten Chinese characters by using weakly supervised learning. Our
approach learns a discriminative SVM for each similar pair which simultaneously
localizes the discriminative region of similar character and makes the
classification. For the first time, similar handwritten Chinese character
recognition (SHCCR) is formulated as an optimization problem extended from SVM.
We also propose a novel feature descriptor, Gradient Context, and apply
bag-of-words model to represent regions with different scales. In our method,
we do not need to select a sized-fixed sub-window to differentiate similar
characters. The unconstrained property makes our method well adapted to high
variance in the size and position of discriminative regions in similar
handwritten Chinese characters. We evaluate our proposed approach over the
CASIA Chinese character data set and the results show that our method
outperforms the state of the art.","Similar Handwritten Chinese Character Discrimination by Weakly
  Supervised Learning",models model data multi agent descriptors
637,"Suppose that we wish to estimate a vector $\mathbf{x}$ from a set of binary
paired comparisons of the form ""$\mathbf{x}$ is closer to $\mathbf{p}$ than to
$\mathbf{q}$"" for various choices of vectors $\mathbf{p}$ and $\mathbf{q}$. The
problem of estimating $\mathbf{x}$ from this type of observation arises in a
variety of contexts, including nonmetric multidimensional scaling, ""unfolding,""
and ranking problems, often because it provides a powerful and flexible model
of preference. We describe theoretical bounds for how well we can expect to
estimate $\mathbf{x}$ under a randomized model for $\mathbf{p}$ and
$\mathbf{q}$. We also present results for the case where the comparisons are
noisy and subject to some degree of error. Additionally, we show that under a
randomized model for $\mathbf{p}$ and $\mathbf{q}$, a suitable number of binary
paired comparisons yield a stable embedding of the space of target vectors.
Finally, we also that we can achieve significant gains by adaptively changing
the distribution for choosing $\mathbf{p}$ and $\mathbf{q}$.",As you like it: Localization via paired comparisons,physical search show learning handling using
638,"In real-world and online social networks, individuals receive and transmit
information in real time. Cascading information transmissions (e.g. phone
calls, text messages, social media posts) may be understood as a realization of
a diffusion process operating on the network, and its branching path can be
represented by a directed tree. The process only traverses and thus reveals a
limited portion of the edges. The network reconstruction/inference problem is
to infer the unrevealed connections. Most existing approaches derive a
likelihood and attempt to find the network topology maximizing the likelihood,
a problem that is highly intractable. In this paper, we focus on the network
reconstruction problem for a broad class of real-world diffusion processes,
exemplified by a network diffusion scheme called respondent-driven sampling
(RDS). We prove that under realistic and general models of network diffusion,
the posterior distribution of an observed RDS realization is a Bayesian
log-submodular model.We then propose VINE (Variational Inference for Network
rEconstruction), a novel, accurate, and computationally efficient variational
inference algorithm, for the network reconstruction problem under this model.
Crucially, we do not assume any particular probabilistic model for the
underlying network. VINE recovers any connected graph with high accuracy as
shown by our experimental results on real-life networks.",Submodular Variational Inference for Network Reconstruction,<unk> show <unk> different <unk> units function 
639,"Neural networks are known to be effective function approximators. Recently,
deep neural networks have proven to be very effective in pattern recognition,
classification tasks and human-level control to model highly nonlinear
realworld systems. This paper investigates the effectiveness of deep neural
networks in the modeling of dynamical systems with complex behavior. Three deep
neural network structures are trained on sequential data, and we investigate
the effectiveness of these networks in modeling associated characteristics of
the underlying dynamical systems. We carry out similar evaluations on select
publicly available system identification datasets. We demonstrate that deep
neural networks are effective model estimators from input-output data",Nonlinear Systems Identification Using Deep Dynamic Neural Networks,natural uses problem models preferences
640,"This paper presents a novel approach for lecture video indexing using a
boosted deep convolutional neural network system. The indexing is performed by
matching high quality slide images, for which text is either known or
extracted, to lower resolution video frames with possible noise, perspective
distortion, and occlusions. We propose a deep neural network integrated with a
boosting framework composed of two sub-networks targeting feature extraction
and similarity determination to perform the matching. The trained network is
given as input a pair of slide image and a candidate video frame image and
produces the similarity between them. A boosting framework is integrated into
our proposed network during the training process. Experimental results show
that the proposed approach is much more capable of handling occlusion, spatial
transformations, and other types of noises when compared with known approaches.",Lecture video indexing using boosted margin maximizing neural networks,data examples feature examples examples examples examples
641,"We propose an L-BFGS optimization algorithm on Riemannian manifolds using
minibatched stochastic variance reduction techniques for fast convergence with
constant step sizes, without resorting to linesearch methods designed to
satisfy Wolfe conditions. We provide a new convergence proof for strongly
convex functions without using curvature conditions on the manifold, as well as
a convergence discussion for nonconvex functions. We discuss a couple of ways
to obtain the correction pairs used to calculate the product of the gradient
with the inverse Hessian, and empirically demonstrate their use in synthetic
experiments on computation of Karcher means for symmetric positive definite
matrices and leading eigenvalues of large scale data matrices. We compare our
method to VR-PCA for the latter experiment, along with Riemannian SVRG for both
cases, and show strong convergence results for a range of datasets.",Accelerated Stochastic Quasi-Newton Optimization on Riemann Manifolds,system stochastic show <unk>
642,"We present opinion recommendation, a novel task of jointly predicting a
custom review with a rating score that a certain user would give to a certain
product or service, given existing reviews and rating scores to the product or
service by other users, and the reviews that the user has given to other
products and services. A characteristic of opinion recommendation is the
reliance of multiple data sources for multi-task joint learning, which is the
strength of neural models. We use a single neural network to model users and
products, capturing their correlation and generating customised product
representations using a deep memory network, from which customised ratings and
reviews are constructed jointly. Results show that our opinion recommendation
system gives ratings that are closer to real user ratings on Yelp.com data
compared with Yelp's own ratings, and our methods give better results compared
to several pipelines baselines using state-of-the-art sentiment rating and
summarization systems.",Opinion Recommendation using Neural Memory Model,networks show user two show user verify
643,"Skull-stripping separates the skull region of the head from the soft brain
tissues. In many cases of brain image analysis, this is an essential
preprocessing step in order to improve the final result. This is true for both
registration and segmentation tasks. In fact, skull-stripping of magnetic
resonance images (MRI) is a well-studied problem with numerous publications in
recent years. Many different algorithms have been proposed, a summary and
comparison of which can be found in [Fennema-Notestine, 2006]. Despite the
abundance of approaches, we discovered that the algorithms which had been
suggested so far, perform poorly when dealing with tumor-bearing brain images.
This is mostly due to additional difficulties in separating the brain from the
skull in this case, especially when the lesion is located very close to the
skull border. Additionally, images acquired according to standard clinical
protocols, often exhibit anisotropic resolution and only partial coverage,
which further complicates the task. Therefore, we developed a method which is
dedicated to skull-stripping for clinically acquired tumor-bearing brain
images.",Skull-stripping for Tumor-bearing Brain Images,models different feature system tree  show
644,"Given a knowledge base (KB) rich in facts about common nouns or generics,
such as ""all trees produce oxygen"" or ""some animals live in forests"", we
consider the problem of deriving additional such facts at a high precision.
While this problem has received much attention for named entity KBs such as
Freebase, little emphasis has been placed on generics despite their importance
for capturing general knowledge. Different from named entity KBs, generics KBs
involve implicit or explicit quantification, have more complex underlying
regularities, are substantially more incomplete, and violate the commonly used
locally closed world assumption (LCWA). Consequently, existing completion
methods struggle with this new task. We observe that external information, such
as relation schemas and entity taxonomies, if used correctly, can be
surprisingly powerful in addressing the challenges associated with generics.
Using this insight, we propose a simple yet effective knowledge guided tensor
factorization approach that achieves state-of-the-art results on two generics
KBs for science, doubling their size at 74\%-86\% precision. Further, to
address the paucity of facts about rare entities such as oriole (a bird), we
present a novel taxonomy guided submodular active learning method to collect
additional annotations that are over five times more effective in inferring
further new facts than multiple active learning baselines.",Knowledge Completion for Generics using Guided Tensor Factorization,potential using engineering  potential  ii  show fundamental of
645,"In this paper, we present ReaS, a technique that combines numerical
optimization with SAT solving to synthesize unknowns in a program that involves
discrete and floating point computation. ReaS makes the program end-to-end
differentiable by smoothing any Boolean expression that introduces
discontinuity such as conditionals and relaxing the Boolean unknowns so that
numerical optimization can be performed. On top of this, ReaS uses a SAT solver
to help the numerical search overcome local solutions by incrementally fixing
values to the Boolean expressions. We evaluated the approach on 5 case studies
involving hybrid systems and show that ReaS can synthesize programs that could
not be solved by previous SMT approaches.",REAS: Combining Numerical Optimization with SAT Solving,complexity <unk> show systematic theory
646,"Belief propagation and its variants are popular methods for approximate
inference, but their running time and even their convergence depend greatly on
the schedule used to send the messages. Recently, dynamic update schedules have
been shown to converge much faster on hard networks than static schedules,
namely the residual BP schedule of Elidan et al. [2006]. But that RBP algorithm
wastes message updates: many messages are computed solely to determine their
priority, and are never actually performed. In this paper, we show that
estimating the residual, rather than calculating it directly, leads to
significant decreases in the number of messages required for convergence, and
in the total running time. The residual is estimated using an upper bound based
on recent work on message errors in BP. On both synthetic and real-world
networks, this dramatically decreases the running time of BP, in some cases by
a factor of five, without affecting the quality of the solution.",Improved Dynamic Schedules for Belief Propagation,information model set model reformulating
647,"Batch Normalization is a commonly used trick to improve the training of deep
neural networks. These neural networks use L2 regularization, also called
weight decay, ostensibly to prevent overfitting. However, we show that L2
regularization has no regularizing effect when combined with normalization.
Instead, regularization has an influence on the scale of weights, and thereby
on the effective learning rate. We investigate this dependence, both in theory,
and experimentally. We show that popular optimization methods such as ADAM only
partially eliminate the influence of normalization on the learning rate. This
leads to a discussion on other ways to mitigate this issue.",L2 Regularization versus Batch and Weight Normalization,open improve proposed data significant
648,"In this article we show the duality between tensor networks and undirected
graphical models with discrete variables. We study tensor networks on
hypergraphs, which we call tensor hypernetworks. We show that the tensor
hypernetwork on a hypergraph exactly corresponds to the graphical model given
by the dual hypergraph. We translate various notions under duality. For
example, marginalization in a graphical model is dual to contraction in the
tensor network. Algorithms also translate under duality. We show that belief
propagation corresponds to a known algorithm for tensor network contraction.
This article is a reminder that the research areas of graphical models and
tensor networks can benefit from interaction.",Duality of Graphical Models and Tensor Networks,based certain deliver accelerator knowledge different feature models
649,"Recent research in computational linguistics has developed algorithms which
associate matrices with adjectives and verbs, based on the distribution of
words in a corpus of text. These matrices are linear operators on a vector
space of context words. They are used to construct the meaning of composite
expressions from that of the elementary constituents, forming part of a
compositional distributional approach to semantics. We propose a Matrix Theory
approach to this data, based on permutation symmetry along with Gaussian
weights and their perturbations. A simple Gaussian model is tested against word
matrices created from a large corpus of text. We characterize the cubic and
quartic departures from the model, which we propose, alongside the Gaussian
parameters, as signatures for comparison of linguistic corpora. We propose that
perturbed Gaussian models with permutation symmetry provide a promising
framework for characterizing the nature of universality in the statistical
properties of word matrices. The matrix theory framework developed here
exploits the view of statistics as zero dimensional perturbative quantum field
theory. It perceives language as a physical system realizing a universality
class of matrix statistics characterized by permutation symmetry.",Linguistic Matrix Theory,model  n  show programming set modelling
650,"We propose new algorithms for topic modeling when the number of topics is
unknown. Our approach relies on an analysis of the concentration of mass and
angular geometry of the topic simplex, a convex polytope constructed by taking
the convex hull of vertices representing the latent topics. Our algorithms are
shown in practice to have accuracy comparable to a Gibbs sampler in terms of
topic estimation, which requires the number of topics be given. Moreover, they
are one of the fastest among several state of the art parametric techniques.
Statistical consistency of our estimator is established under some conditions.",Conic Scan-and-Cover algorithms for nonparametric topic modeling,data semantic information show points  scores times 
651,"Many real-world engineering problems rely on human preferences to guide their
design and optimization. We present PrefOpt, an open source package to simplify
sequential optimization tasks that incorporate human preference feedback. Our
approach extends an existing latent variable model for binary preferences to
allow for observations of equivalent preference from users.",Sequential Preference-Based Optimization,models conditional today  two observation test observation existing
652,"Hamiltonian Monte Carlo (or hybrid Monte Carlo) with partial momentum
refreshment explores the state space more slowly than it otherwise would due to
the momentum reversals which occur on proposal rejection. These cause
trajectories to double back on themselves, leading to random walk behavior on
timescales longer than the typical rejection time, and leading to slower
mixing. I present a technique by which the number of momentum reversals can be
reduced. This is accomplished by maintaining the net exchange of probability
between states with opposite momenta, but reducing the rate of exchange in both
directions such that it is 0 in one direction. An experiment illustrates these
reduced momentum flips accelerating mixing for a particular distribution.",Hamiltonian Monte Carlo with Reduced Momentum Flips,video networks linear different potential using dependencies linear
653,"Objective of the current work is to develop an Optical Character Recognition
(OCR) engine for information Just In Time (iJIT) system that can be used for
recognition of handwritten textual annotations of lower case Roman script.
Tesseract open source OCR engine under Apache License 2.0 is used to develop
user-specific handwriting recognition models, viz., the language sets, for the
said system, where each user is identified by a unique identification tag
associated with the digital pen. To generate the language set for any user,
Tesseract is trained with labeled handwritten data samples of isolated and
free-flow texts of Roman script, collected exclusively from that user. The
designed system is tested on five different language sets with free- flow
handwritten annotations as test samples. The system could successfully segment
and subsequently recognize 87.92%, 81.53%, 92.88%, 86.75% and 90.80%
handwritten characters in the test samples of five different users.","Recognition of Handwritten Textual Annotations using Tesseract Open
  Source OCR Engine for information Just In Time (iJIT)",evaluation model show proposed distributions networks
654,"In this paper, we describe a methodology to infer Bullish or Bearish
sentiment towards companies/brands. More specifically, our approach leverages
affective lexica and word embeddings in combination with convolutional neural
networks to infer the sentiment of financial news headlines towards a target
company. Such architecture was used and evaluated in the context of the SemEval
2017 challenge (task 5, subtask 2), in which it obtained the best performance.","Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring
  Sentiment towards Brands from Financial News Headlines",predictive <unk>
655,"Using constraint-based local search, we effectively model and efficiently
solve the problem of balancing the traffic demands on portions of the European
airspace while ensuring that their capacity constraints are satisfied. The
traffic demand of a portion of airspace is the hourly number of flights planned
to enter it, and its capacity is the upper bound on this number under which
air-traffic controllers can work. Currently, the only form of demand-capacity
balancing we allow is ground holding, that is the changing of the take-off
times of not yet airborne flights. Experiments with projected European flight
plans of the year 2030 show that already this first form of demand-capacity
balancing is feasible without incurring too much total delay and that it can
lead to a significantly better demand-capacity balance.","Dynamic Demand-Capacity Balancing for Air Traffic Management Using
  Constraint-Based Local Search: First Results",<unk> show feature paper  for
656,"In this paper, we describe methods for handling multilingual
non-compositional constructions in the framework of GF. We specifically look at
methods to detect and extract non-compositional phrases from parallel texts and
propose methods to handle such constructions in GF grammars. We expect that the
methods to handle non-compositional constructions will enrich CNLs by providing
more flexibility in the design of controlled languages. We look at two specific
use cases of non-compositional constructions: a general-purpose method to
detect and extract multilingual multiword expressions and a procedure to
identify nominal compounds in German. We evaluate our procedure for multiword
expressions by performing a qualitative analysis of the results. For the
experiments on nominal compounds, we incorporate the detected compounds in a
full SMT pipeline and evaluate the impact of our method in machine translation
process.",Handling non-compositionality in multilingual CNLs,noise  traditional show high models
657,"Most existing person re-identification (ReID) methods rely only on the
spatial appearance information from either one or multiple person images,
whilst ignore the space-time cues readily available in video or image-sequence
data. Moreover, they often assume the availability of exhaustively labelled
cross-view pairwise data for every camera pair, making them non-scalable to
ReID applications in real-world large scale camera networks. In this work, we
introduce a novel video based person ReID method capable of accurately matching
people across views from arbitrary unaligned image-sequences without any
labelled pairwise data. Specifically, we introduce a new space-time person
representation by encoding multiple granularities of spatio-temporal dynamics
in form of time series. Moreover, a Time Shift Dynamic Time Warping (TS-DTW)
model is derived for performing automatically alignment whilst achieving data
selection and matching between inherently inaccurate and incomplete sequences
in a unified way. We further extend the TS-DTW model for accommodating multiple
feature-sequences of an image-sequence in order to fuse information from
different descriptions. Crucially, this model does not require pairwise
labelled training data (i.e. unsupervised) therefore readily scalable to large
scale camera networks of arbitrary camera pairs without the need for exhaustive
data annotation for every camera pair. We show the effectiveness and advantages
of the proposed method by extensive comparisons with related state-of-the-art
approaches using two benchmarking ReID datasets, PRID2011 and iLIDS-VID.",Person Re-Identification by Unsupervised Video Matching,proposed achieve model words using al 
658,"Machine Comprehension (MC) is a challenging task in Natural Language
Processing field, which aims to guide the machine to comprehend a passage and
answer the given question. Many existing approaches on MC task are suffering
the inefficiency in some bottlenecks, such as insufficient lexical
understanding, complex question-passage interaction, incorrect answer
extraction and so on. In this paper, we address these problems from the
viewpoint of how humans deal with reading tests in a scientific way.
Specifically, we first propose a novel lexical gating mechanism to dynamically
combine the words and characters representations. We then guide the machines to
read in an interactive way with attention mechanism and memory network. Finally
we add a checking layer to refine the answer for insurance. The extensive
experiments on two popular datasets SQuAD and TriviaQA show that our method
exceeds considerable performance than most state-of-the-art solutions at the
time of submission.",Smarnet: Teaching Machines to Read and Comprehend Like Human,words information show show deal agreement
659,"Even though modularity has been studied extensively in conventional logic
programming, there are few approaches on how to incorporate modularity into
Answer Set Programming, a prominent rule-based declarative programming
paradigm. A major approach is Oikarinnen and Janhunen's Gaifman-Shapiro-style
architecture of program modules, which provides the composition of program
modules. Their module theorem properly strengthens Lifschitz and Turner's
splitting set theorem for normal logic programs. However, this approach is
limited by module conditions that are imposed in order to ensure the
compatibility of their module system with the stable model semantics, namely
forcing output signatures of composing modules to be disjoint and disallowing
positive cyclic dependencies between different modules. These conditions turn
out to be too restrictive in practice and in this paper we discuss alternative
ways of lift both restrictions independently, effectively solving the first,
widening the applicability of this framework and the scope of the module
theorem.",Generalizing Modular Logic Programs,however   f  show show theory ranking
660,"We show that matrix completion with trace-norm regularization can be
significantly hurt when entries of the matrix are sampled non-uniformly. We
introduce a weighted version of the trace-norm regularizer that works well also
with non-uniform sampling. Our experimental results demonstrate that the
weighted trace-norm regularization indeed yields significant gains on the
(highly non-uniformly sampled) Netflix dataset.","Collaborative Filtering in a Non-Uniform World: Learning with the
  Weighted Trace Norm",discovery biological unsupervised show in
661,"Induction of common sense knowledge about prototypical sequences of events
has recently received much attention. Instead of inducing this knowledge in the
form of graphs, as in much of the previous work, in our method, distributed
representations of event realizations are computed based on distributed
representations of predicates and their arguments, and then these
representations are used to predict prototypical event orderings. The
parameters of the compositional process for computing the event representations
and the ranking component of the model are jointly estimated from texts. We
show that this approach results in a substantial boost in ordering performance
with respect to previous methods.",Learning Semantic Script Knowledge with Event Embeddings,conventional proposed <unk>
662,"Conditional random fields (CRFs) are usually specified by graphical models
but in this paper we propose to use probabilistic logic programs and specify
them generatively. Our intension is first to provide a unified approach to CRFs
for complex modeling through the use of a Turing complete language and second
to offer a convenient way of realizing generative-discriminative pairs in
machine learning to compare generative and discriminative models and choose the
best model. We implemented our approach as the D-PRISM language by modifying
PRISM, a logic-based probabilistic modeling language for generative modeling,
while exploiting its dynamic programming mechanism for efficient probability
computation. We tested D-PRISM with logistic regression, a linear-chain CRF and
a CRF-CFG and empirically confirmed their excellent discriminative performance
compared to their generative counterparts, i.e.\ naive Bayes, an HMM and a
PCFG. We also introduced new CRF models, CRF-BNCs and CRF-LCGs. They are CRF
versions of Bayesian network classifiers and probabilistic left-corner grammars
respectively and easily implementable in D-PRISM. We empirically showed that
they outperform their generative counterparts as expected.",A Logic-based Approach to Generatively Defined Discriminative Modeling,learning focus model show models method theorem for
663,"The L1-regularized Gaussian maximum likelihood estimator (MLE) has been shown
to have strong statistical guarantees in recovering a sparse inverse covariance
matrix, or alternatively the underlying graph structure of a Gaussian Markov
Random Field, from very limited samples. We propose a novel algorithm for
solving the resulting optimization problem which is a regularized
log-determinant program. In contrast to recent state-of-the-art methods that
largely use first order gradient information, our algorithm is based on
Newton's method and employs a quadratic approximation, but with some
modifications that leverage the structure of the sparse Gaussian MLE problem.
We show that our method is superlinearly convergent, and present experimental
results using synthetic and real-world application data that demonstrate the
considerable improvements in performance of our method when compared to other
state-of-the-art methods.","Sparse Inverse Covariance Matrix Estimation Using Quadratic
  Approximation",critical critical show certain learning <unk>
664,"Recently, several methods based on generative adversarial network (GAN) have
been proposed for the task of aligning cross-domain images or learning a joint
distribution of cross-domain images. One of the methods is to use conditional
GAN for alignment. However, previous attempts of adopting conditional GAN do
not perform as well as other methods. In this work we present an approach for
improving the capability of the methods which are based on conditional GAN. We
evaluate the proposed method on numerous tasks and the experimental results
show that it is able to align the cross-domain images successfully in absence
of paired samples. Furthermore, we also propose another model which conditions
on multiple information such as domain information and label information.
Conditioning on domain information and label information, we are able to
conduct label propagation from the source domain to the target domain. A 2-step
alternating training algorithm is proposed to learn this model.","AlignGAN: Learning to Align Cross-Domain Images with Conditional
  Generative Adversarial Networks",<unk> number based <unk> set domain
665,"We develop a probabilistic technique for colorizing grayscale natural images.
In light of the intrinsic uncertainty of this task, the proposed probabilistic
framework has numerous desirable properties. In particular, our model is able
to produce multiple plausible and vivid colorizations for a given grayscale
image and is one of the first colorization models to provide a proper
stochastic sampling scheme. Moreover, our training procedure is supported by a
rigorous theoretical framework that does not require any ad hoc heuristics and
allows for efficient modeling and learning of the joint pixel color
distribution. We demonstrate strong quantitative and qualitative experimental
results on the CIFAR-10 dataset and the challenging ILSVRC 2012 dataset.",Probabilistic Image Colorization,for model geometry using
666,"In this report, we will be interested at Dynamic Bayesian Network (DBNs) as a
model that tries to incorporate temporal dimension with uncertainty. We start
with basics of DBN where we especially focus in Inference and Learning concepts
and algorithms. Then we will present different levels and methods of creating
DBNs as well as approaches of incorporating temporal dimension in static
Bayesian network.",Characterization of Dynamic Bayesian Network,train similarity high
667,"A logical approach to object recognition on image is proposed. The main idea
of the approach is to perform the object recognition as a logical inference on
a set of rules describing an object shape.","Logical methods of object recognition on satellite images using spatial
  constraints",<unk> simultaneously simultaneously simultaneously models
668,"We propose to deal with sequential processes where only partial observations
are available by learning a latent representation space on which policies may
be accurately learned.",Learning States Representations in POMDP,video made search
669,"Image reconstruction plays a critical role in the implementation of all
contemporary imaging modalities across the physical and life sciences including
optical, MRI, CT, PET, and radio astronomy. During an image acquisition, the
sensor encodes an intermediate representation of an object in the sensor
domain, which is subsequently reconstructed into an image by an inversion of
the encoding function. Image reconstruction is challenging because analytic
knowledge of the inverse transform may not exist a priori, especially in the
presence of sensor non-idealities and noise. Thus, the standard reconstruction
approach involves approximating the inverse function with multiple ad hoc
stages in a signal processing chain whose composition depends on the details of
each acquisition strategy, and often requires expert parameter tuning to
optimize reconstruction performance. We present here a unified framework for
image reconstruction, AUtomated TransfOrm by Manifold APproximation (AUTOMAP),
which recasts image reconstruction as a data-driven, supervised learning task
that allows a mapping between sensor and image domain to emerge from an
appropriate corpus of training data. We implement AUTOMAP with a deep neural
network and exhibit its flexibility in learning reconstruction transforms for a
variety of MRI acquisition strategies, using the same network architecture and
hyperparameters. We further demonstrate its efficiency in sparsely representing
transforms along low-dimensional manifolds, resulting in superior immunity to
noise and reconstruction artifacts compared with conventional handcrafted
reconstruction methods. In addition to improving the reconstruction performance
of existing acquisition methodologies, we anticipate accelerating the discovery
of new acquisition strategies across modalities as the burden of reconstruction
becomes lifted by AUTOMAP and learned-reconstruction approaches.",Image reconstruction by domain transform manifold learning,model offers planning method planning quality proposed known
670,"Many interesting problems in machine learning are being revisited with new
deep learning tools. For graph-based semisupervised learning, a recent
important development is graph convolutional networks (GCNs), which nicely
integrate local vertex features and graph topology in the convolutional layers.
Although the GCN model compares favorably with other state-of-the-art methods,
its mechanisms are not clear and it still requires a considerable amount of
labeled data for validation and model selection. In this paper, we develop
deeper insights into the GCN model and address its fundamental limits. First,
we show that the graph convolution of the GCN model is actually a special form
of Laplacian smoothing, which is the key reason why GCNs work, but it also
brings potential concerns of over-smoothing with many convolutional layers.
Second, to overcome the limits of the GCN model with shallow architectures, we
propose both co-training and self-training approaches to train GCNs. Our
approaches significantly improve GCNs in learning with very few labels, and
exempt them from requiring additional labels for validation. Extensive
experiments on benchmarks have verified our theory and proposals.","Deeper Insights into Graph Convolutional Networks for Semi-Supervised
  Learning",based <unk>
671,"We describe a novel approach to monitoring high level behaviors using
concepts from AI planning. Our goal is to understand what a program is doing
based on its system call trace. This ability is particularly important for
detecting malware. We approach this problem by building an abstract model of
the operating system using the STRIPS planning language, casting system calls
as planning operators. Given a system call trace, we simulate the corresponding
operators on our model and by observing the properties of the state reached, we
learn about the nature of the original program and its behavior. Thus, unlike
most statistical detection methods that focus on syntactic features, our
approach is semantic in nature. Therefore, it is more robust against
obfuscation techniques used by malware that change the outward appearance of
the trace but not its effect. We demonstrate the efficacy of our approach by
evaluating it on actual system call traces.",A Planning Approach to Monitoring Behavior of Computer Programs,researchers art model <unk> combining
672,"We provide a brief technical description of an online platform for disease
monitoring, titled as the Flu Detector (fludetector.cs.ucl.ac.uk). Flu
Detector, in its current version (v.0.5), uses either Twitter or Google search
data in conjunction with statistical Natural Language Processing models to
estimate the rate of influenza-like illness in the population of England. Its
back-end is a live service that collects online data, utilises modern
technologies for large-scale text processing, and finally applies statistical
inference models that are trained offline. The front-end visualises the various
disease rate estimates. Notably, the models based on Google data achieve a high
level of accuracy with respect to the most recent four flu seasons in England
(2012/13 to 2015/16). This highlighted Flu Detector as having a great potential
of becoming a complementary source to the domestic traditional flu surveillance
schemes.","Flu Detector: Estimating influenza-like illness rates from online
  user-generated content",theory ranking show feature models
673,"We introduce a method to recover a continuous domain representation of a
piecewise constant two-dimensional image from few low-pass Fourier samples.
Assuming the edge set of the image is localized to the zero set of a
trigonometric polynomial, we show the Fourier coefficients of the partial
derivatives of the image satisfy a linear annihilation relation. We present
necessary and sufficient conditions for unique recovery of the image from
finite low-pass Fourier samples using the annihilation relation. We also
propose a practical two-stage recovery algorithm which is robust to
model-mismatch and noise. In the first stage we estimate a continuous domain
representation of the edge set of the image. In the second stage we perform an
extrapolation in Fourier domain by a least squares two-dimensional linear
prediction, which recovers the exact Fourier coefficients of the underlying
image. We demonstrate our algorithm on the super-resolution recovery of MRI
phantoms and real MRI data from low-pass Fourier samples, which shows benefits
over standard approaches for single-image super-resolution MRI.","Off-the-Grid Recovery of Piecewise Constant Images from Few Fourier
  Samples",efficiency set set examples
674,"There is a growing interest in using Kalman-filter models in brain modelling.
In turn, it is of considerable importance to make Kalman-filters amenable for
reinforcement learning. In the usual formulation of optimal control it is
computed off-line by solving a backward recursion. In this technical note we
show that slight modification of the linear-quadratic-Gaussian Kalman-filter
model allows the on-line estimation of optimal control and makes the bridge to
reinforcement learning. Moreover, the learning rule for value estimation
assumes a Hebbian form weighted by the error of the value estimation.",Kalman filter control in the reinforcement learning framework,played <unk> show show learning <unk>
675,"By considering the features of the airport runway image filtering, an
improved bilateral filtering method was proposed which can remove noise with
edge preserving. Firstly the steerable filtering decomposition is used to
calculate the sub-band parameters of 4 orients, and the texture feature matrix
is then obtained from the sub-band local median energy. The texture similar,
the spatial closer and the color similar functions are used to filter the
image.The effect of the weighting function parameters is qualitatively analyzed
also. In contrast with the standard bilateral filter and the simulation results
for the real airport runway image show that the multilateral filtering is more
effective than the standard bilateral filtering.",A multilateral filtering method applied to airplane runway image,models show <unk> significant noise behavior
676,"Proportional integral derivative (PID) controllers are important and widely
used tools in system control. Tuning of the controller gains is a laborious
task, especially for complex systems such as combustion engines. To minimize
the time of an engineer for tuning of the gains in a simulation software, we
propose to formulate a part of the problem as a black-box optimization task. In
this paper, we summarize the properties and practical limitations of tuning of
the gains in this particular application. We investigate the latest methods of
black-box optimization and conclude that the Covariance Matrix Adaptation
Evolution Strategy (CMA-ES) with bi-population restart strategy, elitist parent
selection and active covariance matrix adaptation is best suited for this task.
Details of the algorithm's experiment-based calibration are explained as well
as derivation of a suitable objective function. The method's performance is
compared with that of PSO and SHADE. Finally, its usability is verified on six
models of real engines.","Using CMA-ES for tuning coupled PID controllers within models of
  combustion engines",discrete models show differential controlled
677,"Improving information flow in deep networks helps to ease the training
difficulties and utilize parameters more efficiently. Here we propose a new
convolutional neural network architecture with alternately updated clique
(CliqueNet). In contrast to prior networks, there are both forward and backward
connections between any two layers in the same block. The layers are
constructed as a loop and are updated alternately. The CliqueNet has some
unique properties. For each layer, it is both the input and output of any other
layer in the same block, so that the information flow among layers is
maximized. During propagation, the newly updated layers are concatenated to
re-update previously updated layer, and parameters are reused for multiple
times. This recurrent feedback structure is able to bring higher level visual
information back to refine low-level filters and achieve spatial attention. We
analyze the features generated at different stages and observe that using
refined features leads to a better result. We adopt a multi-scale feature
strategy that effectively avoids the progressive growth of parameters.
Experiments on image recognition datasets including CIFAR-10, CIFAR-100, SVHN
and ImageNet show that our proposed models achieve the state-of-the-art
performance with fewer parameters.",Convolutional Neural Networks with Alternately Updated Clique,using model wind
678,"Classifier evasion consists in finding for a given instance $x$ the nearest
instance $x'$ such that the classifier predictions of $x$ and $x'$ are
different. We present two novel algorithms for systematically computing
evasions for tree ensembles such as boosted trees and random forests. Our first
algorithm uses a Mixed Integer Linear Program solver and finds the optimal
evading instance under an expressive set of constraints. Our second algorithm
trades off optimality for speed by using symbolic prediction, a novel algorithm
for fast finite differences on tree ensembles. On a digit recognition task, we
demonstrate that both gradient boosted trees and random forests are extremely
susceptible to evasions. Finally, we harden a boosted tree model without loss
of predictive accuracy by augmenting the training set of each boosting round
with evading instances, a technique we call adversarial boosting.",Evasion and Hardening of Tree Ensemble Classifiers,specifically method level approximations different level specifically
679,"Synthetic images rendered from 3D CAD models are useful for augmenting
training data for object recognition algorithms. However, the generated images
are non-photorealistic and do not match real image statistics. This leads to a
large domain discrepancy, causing models trained on synthetic data to perform
poorly on real domains. Recent work has shown the great potential of deep
convolutional neural networks to generate realistic images, but has not
utilized generative models to address synthetic-to-real domain adaptation. In
this work, we propose a Deep Generative Correlation Alignment Network (DGCAN)
to synthesize images using a novel domain adaption algorithm. DGCAN leverages a
shape preserving loss and a low level statistic matching loss to minimize the
domain discrepancy between synthetic and real images in deep feature space.
Experimentally, we show training off-the-shelf classifiers on the newly
generated data can significantly boost performance when testing on the real
image domains (PASCAL VOC 2007 benchmark and Office dataset), improving upon
several existing methods.","Synthetic to Real Adaptation with Generative Correlation Alignment
  Networks",data strategy show robotics real domain
680,"We present a method for automated segmentation of the vasculature in retinal
images. The method produces segmentations by classifying each image pixel as
vessel or non-vessel, based on the pixel's feature vector. Feature vectors are
composed of the pixel's intensity and continuous two-dimensional Morlet wavelet
transform responses taken at multiple scales. The Morlet wavelet is capable of
tuning to specific frequencies, thus allowing noise filtering and vessel
enhancement in a single step. We use a Bayesian classifier with
class-conditional probability density functions (likelihoods) described as
Gaussian mixtures, yielding a fast classification, while being able to model
complex decision surfaces and compare its performance with the linear minimum
squared error classifier. The probability distributions are estimated based on
a training set of labeled pixels obtained from manual segmentations. The
method's performance is evaluated on publicly available DRIVE and STARE
databases of manually labeled non-mydriatic images. On the DRIVE database, it
achieves an area under the receiver operating characteristic (ROC) curve of
0.9598, being slightly superior than that presented by the method of Staal et
al.","Retinal Vessel Segmentation Using the 2-D Morlet Wavelet and Supervised
  Classification",entries  model produces <unk>
681,"In this work, we focus on the challenge of taking partial observations of
highly-stylized text and generalizing the observations to generate unobserved
glyphs in the ornamented typeface. To generate a set of multi-content images
following a consistent style from very few examples, we propose an end-to-end
stacked conditional GAN model considering content along channels and style
along network layers. Our proposed network transfers the style of given glyphs
to the contents of unseen ones, capturing highly stylized fonts found in the
real-world such as those on movie posters or infographics. We seek to transfer
both the typographic stylization (ex. serifs and ears) as well as the textual
stylization (ex. color gradients and effects.) We base our experiments on our
collected data set including 10,000 fonts with different styles and demonstrate
effective generalization from a very small number of observed glyphs.",Multi-Content GAN for Few-Shot Font Style Transfer,<unk> cascade method properties
682,"The interest in accelerating black-box optimizers has resulted in several
surrogate model-assisted version of the Covariance Matrix Adaptation Evolution
Strategy, a state-of-the-art continuous black-box optimizer. The version called
Surrogate CMA-ES uses Gaussian processes or random forests surrogate models
with a generation-based evolution control. This paper presents an adaptive
improvement for S-CMA-ES based on a general procedure introduced with the
s*ACM-ES algorithm, in which the number of generations using the surrogate
model before retraining is adjusted depending on the performance of the last
instance of the surrogate. Three algorithms that differ in the measure of the
surrogate model's performance are evaluated on the COCO/BBOB framework. The
results show a minor improvement on S-CMA-ES with constant model lifelengths,
especially when larger lifelengths are considered.","Adaptive Generation-Based Evolution Control for Gaussian Process
  Surrogate Models",video real search different quantitative
683,"We present a supervised hyperspectral image segmentation algorithm based on a
convex formulation of a marginal maximum a posteriori segmentation with hidden
fields and structure tensor regularization: Segmentation via the Constraint
Split Augmented Lagrangian Shrinkage by Structure Tensor Regularization
(SegSALSA-STR). This formulation avoids the generally discrete nature of
segmentation problems and the inherent NP-hardness of the integer optimization
associated.
  We extend the Segmentation via the Constraint Split Augmented Lagrangian
Shrinkage (SegSALSA) algorithm by generalizing the vectorial total variation
prior using a structure tensor prior constructed from a patch-based Jacobian.
The resulting algorithm is convex, time-efficient and highly parallelizable.
This shows the potential of combining hidden fields with convex optimization
through the inclusion of different regularizers. The SegSALSA-STR algorithm is
validated in the segmentation of real hyperspectral images.","SegSALSA-STR: A convex formulation to supervised hyperspectral image
  segmentation using hidden fields and structure tensor regularization",noise tensor significant approaches different applied using objective
684,"In many combinatorial problems one may need to model the diversity or
similarity of assignments in a solution. For example, one may wish to maximise
or minimise the number of distinct values in a solution. To formulate problems
of this type, we can use soft variants of the well known AllDifferent and
AllEqual constraints. We present a taxonomy of six soft global constraints,
generated by combining the two latter ones and the two standard cost functions,
which are either maximised or minimised. We characterise the complexity of
achieving arc and bounds consistency on these constraints, resolving those
cases for which NP-hardness was neither proven nor disproven. In particular, we
explore in depth the constraint ensuring that at least k pairs of variables
have a common value. We show that achieving arc consistency is NP-hard, however
achieving bounds consistency can be done in polynomial time through dynamic
programming. Moreover, we show that the maximum number of pairs of equal
variables can be approximated by a factor 1/2 with a linear time greedy
algorithm. Finally, we provide a fixed parameter tractable algorithm with
respect to the number of values appearing in more than two distinct domains.
Interestingly, this taxonomy shows that enforcing equality is harder than
enforcing difference.",Soft Constraints of Difference and Equality,widely information show <unk> representations
685,"Collective classification models attempt to improve classification
performance by taking into account the class labels of related instances.
However, they tend not to learn patterns of interactions between classes and/or
make the assumption that instances of the same class link to each other
(assortativity assumption). Blockmodels provide a solution to these issues,
being capable of modelling assortative and disassortative interactions, and
learning the pattern of interactions in the form of a summary network. The
Supervised Blockmodel provides good classification performance using link
structure alone, whilst simultaneously providing an interpretable summary of
network interactions to allow a better understanding of the data. This work
explores three variants of supervised blockmodels of varying complexity and
tests them on four structurally different real world networks.",Supervised Blockmodelling,theory parameters  show research theory stochastic
686,"Multi-instance data, in which each object (bag) contains a collection of
instances, are widespread in machine learning, computer vision, bioinformatics,
signal processing, and social sciences. We present a maximum entropy (ME)
framework for learning from multi-instance data. In this approach each bag is
represented as a distribution using the principle of ME. We introduce the
concept of confidence-constrained ME (CME) to simultaneously learn the
structure of distribution space and infer each distribution. The shared
structure underlying each density is used to learn from instances inside each
bag. The proposed CME is free of tuning parameters. We devise a fast
optimization algorithm capable of handling large scale multi-instance data. In
the experimental section, we evaluate the performance of the proposed approach
in terms of exact rank recovery in the space of distributions and compare it
with the regularized ME approach. Moreover, we compare the performance of CME
with Multi-Instance Learning (MIL) state-of-the-art algorithms and show a
comparable performance in terms of accuracy with reduced computational
complexity.","Confidence-Constrained Maximum Entropy Framework for Learning from
  Multi-Instance Data",real demonstrate show in proposed proposed summarization associated
687,"Research on developing efficient and scalable ASP solvers can substantially
benefit by the availability of data sets to experiment with. KB_Bio_101
contains knowledge from a biology textbook, has been developed as part of
Project Halo, and has recently become available for research use. KB_Bio_101 is
one of the largest KBs available in ASP and the reasoning with it is
undecidable in general. We give a description of this KB and ASP programs for a
suite of queries that have been of practical interest. We explain why these
queries pose significant practical challenges for the current ASP solvers.","Query Answering in Object Oriented Knowledge Bases in Logic Programming:
  Description and Challenge for ASP",significant preferences test learning
688,"In this paper we present a simple yet effective approach to extend without
supervision any object proposal from static images to videos. Unlike previous
methods, these spatio-temporal proposals, to which we refer as tracks, are
generated relying on little or no visual content by only exploiting bounding
boxes spatial correlations through time. The tracks that we obtain are likely
to represent objects and are a general-purpose tool to represent meaningful
video content for a wide variety of tasks. For unannotated videos, tracks can
be used to discover content without any supervision. As further contribution we
also propose a novel and dataset-independent method to evaluate a generic
object proposal based on the entropy of a classifier output response. We
experiment on two competitive datasets, namely YouTube Objects and ILSVRC-2015
VID.",Segmentation Free Object Discovery in Video,need show show states shared
689,"Though suicide is a major public health problem in the US, machine learning
methods are not commonly used to predict an individual's risk of
attempting/committing suicide. In the present work, starting with an anonymized
collection of electronic health records for 522,056 unique, California-resident
adolescents, we develop neural network models to predict suicide attempts. We
frame the problem as a binary classification problem in which we use a
patient's data from 2006-2009 to predict either the presence (1) or absence (0)
of a suicide attempt in 2010. After addressing issues such as severely
imbalanced classes and the variable length of a patient's history, we build
neural networks with depths varying from two to eight hidden layers. For test
set observations where we have at least five ED/hospital visits' worth of data
on a patient, our depth-4 model achieves a sensitivity of 0.703, specificity of
0.980, and AUC of 0.958.",Predicting Adolescent Suicide Attempts with Neural Networks,research based musical show theory first different feature
690,"An important issue in neural network research is how to choose the number of
nodes and layers such as to solve a classification problem. We provide new
intuitions based on earlier results by An et al. (2015) by deriving an upper
bound on the number of nodes in networks with two hidden layers such that
linear separability can be achieved. Concretely, we show that if the data can
be described in terms of N finite sets and the used activation function f is
non-constant, increasing and has a left asymptote, we can derive how many nodes
are needed to linearly separate these sets. This will be an upper bound that
depends on the structure of the data. This structure can be analyzed using an
algorithm. For the leaky rectified linear activation function, we prove
separately that under some conditions on the slope, the same number of layers
and nodes as for the aforementioned activation functions is sufficient. We
empirically validate our claims.","Generalization of an Upper Bound on the Number of Nodes Needed to
  Achieve Linear Separability",efficiency <unk> show <unk> <unk> units inference
691,"We study the problem of learning influence functions under incomplete
observations of node activations. Incomplete observations are a major concern
as most (online and real-world) social networks are not fully observable. We
establish both proper and improper PAC learnability of influence functions
under randomly missing observations. Proper PAC learnability under the
Discrete-Time Linear Threshold (DLT) and Discrete-Time Independent Cascade
(DIC) models is established by reducing incomplete observations to complete
observations in a modified graph. Our improper PAC learnability result applies
for the DLT and DIC models as well as the Continuous-Time Independent Cascade
(CIC) model. It is based on a parametrization in terms of reachability
features, and also gives rise to an efficient and practical heuristic.
Experiments on synthetic and real-world datasets demonstrate the ability of our
method to compensate even for a fairly large fraction of missing observations.",Learning Influence Functions from Incomplete Observations,however  models show proposed leaving however  however 
692,"Many of the ordinal regression models that have been proposed in the
literature can be seen as methods that minimize a convex surrogate of the
zero-one, absolute, or squared loss functions. A key property that allows to
study the statistical implications of such approximations is that of Fisher
consistency. Fisher consistency is a desirable property for surrogate loss
functions and implies that in the population setting, i.e., if the probability
distribution that generates the data were available, then optimization of the
surrogate would yield the best possible model. In this paper we will
characterize the Fisher consistency of a rich family of surrogate loss
functions used in the context of ordinal regression, including support vector
ordinal regression, ORBoosting and least absolute deviation. We will see that,
for a family of surrogate loss functions that subsumes support vector ordinal
regression and ORBoosting, consistency can be fully characterized by the
derivative of a real-valued function at zero, as happens for convex
margin-based surrogates in binary classification. We also derive excess risk
bounds for a surrogate of the absolute error that generalize existing risk
bounds for binary classification. Finally, our analysis suggests a novel
surrogate of the squared error loss. We compare this novel surrogate with
competing approaches on 9 different datasets. Our method shows to be highly
competitive in practice, outperforming the least squares loss on 7 out of 9
datasets.",On the Consistency of Ordinal Regression Methods,video theory differential problem differential  ii 
693,"Authorship attribution, being an important problem in many areas in-cluding
information retrieval, computational linguistics, law and journalism etc., has
been identified as a subject of increasingly research interest in the re-cent
years. In case of Author Identification task in PAN at CLEF 2015, the main
focus was given on cross-genre and cross-topic author verification tasks. We
have used several word-based and style-based features to identify the
dif-ferences between the known and unknown problems of one given set and label
the unknown ones accordingly using a Random Forest based classifier.",Authorship Verification - An Approach based on Random Forest,preferences
694,"The evaluation of machine learning algorithms in biomedical fields for
applications involving sequential data lacks standardization. Common
quantitative scalar evaluation metrics such as sensitivity and specificity can
often be misleading depending on the requirements of the application.
Evaluation metrics must ultimately reflect the needs of users yet be
sufficiently sensitive to guide algorithm development. Feedback from critical
care clinicians who use automated event detection software in clinical
applications has been overwhelmingly emphatic that a low false alarm rate,
typically measured in units of the number of errors per 24 hours, is the single
most important criterion for user acceptance. Though using a single metric is
not often as insightful as examining performance over a range of operating
conditions, there is a need for a single scalar figure of merit. In this paper,
we discuss the deficiencies of existing metrics for a seizure detection task
and propose several new metrics that offer a more balanced view of performance.
We demonstrate these metrics on a seizure detection task based on the TUH EEG
Corpus. We show that two promising metrics are a measure based on a concept
borrowed from the spoken term detection literature, Actual Term-Weighted Value,
and a new metric, Time-Aligned Event Scoring (TAES), that accounts for the
temporal alignment of the hypothesis to the reference annotation. We also
demonstrate that state of the art technology based on deep learning, though
impressive in its performance, still needs significant improvement before it
will meet very strict user acceptance guidelines.",Objective evaluation metrics for automatic classification of EEG events,models model feature combining visual
695,"Even when a system is proven to be correct with respect to a specification,
there is still a question of how complete the specification is, and whether it
really covers all the behaviors of the system. Coverage metrics attempt to
check which parts of a system are actually relevant for the verification
process to succeed. Recent work on coverage in model checking suggests several
coverage metrics and algorithms for finding parts of the system that are not
covered by the specification. The work has already proven to be effective in
practice, detecting design errors that escape early verification efforts in
industrial settings. In this paper, we relate a formal definition of causality
given by Halpern and Pearl [2001] to coverage. We show that it gives
significant insight into unresolved issues regarding the definition of coverage
and leads to potentially useful extensions of coverage. In particular, we
introduce the notion of responsibility, which assigns to components of a system
a quantitative measure of their relevance to the satisfaction of the
specification.",What Causes a System to Satisfy a Specification?,currently model mammals tree  capabilities problem using observation
696,"Point patterns are sets or multi-sets of unordered elements that can be found
in numerous data sources. However, in data analysis tasks such as
classification and novelty detection, appropriate statistical models for point
pattern data have not received much attention. This paper proposes the
modelling of point pattern data via random finite sets (RFS). In particular, we
propose appropriate likelihood functions, and a maximum likelihood estimator
for learning a tractable family of RFS models. In novelty detection, we propose
novel ranking functions based on RFS models, which substantially improve
performance.",Model-based Classification and Novelty Detection For Point Pattern Data,offers approaches show technique proposed potential using potential
697,"We study a semi-supervised learning method based on the similarity graph and
RegularizedLaplacian. We give convenient optimization formulation of the
Regularized Laplacian method and establishits various properties. In
particular, we show that the kernel of the methodcan be interpreted in terms of
discrete and continuous time random walks and possesses several
importantproperties of proximity measures. Both optimization and linear algebra
methods can be used for efficientcomputation of the classification functions.
We demonstrate on numerical examples that theRegularized Laplacian method is
competitive with respect to the other state of the art semi-supervisedlearning
methods.",Semi-supervised Learning with Regularized Laplacian,<unk> show kernel fully
698,"Detecting seizure using brain neuroactivations recorded by intracranial
electroencephalogram (iEEG) has been widely used for monitoring, diagnosing,
and closed-loop therapy of epileptic patients, however, computational
efficiency gains are needed if state-of-the-art methods are to be implemented
in implanted devices. We present a novel method for automatic seizure detection
based on iEEG data that outperforms current state-of-the-art seizure detection
methods in terms of computational efficiency while maintaining the accuracy.
The proposed algorithm incorporates an automatic channel selection (ACS) engine
as a pre-processing stage to the seizure detection procedure. The ACS engine
consists of supervised classifiers which aim to find iEEGchannelswhich
contribute the most to a seizure. Seizure detection stage involves feature
extraction and classification. Feature extraction is performed in both
frequency and time domains where spectral power and correlation between channel
pairs are calculated. Random Forest is used in classification of interictal,
ictal and early ictal periods of iEEG signals. Seizure detection in this paper
is retrospective and patient-specific. iEEG data is accessed via Kaggle,
provided by International Epilepsy Electro-physiology Portal. The dataset
includes a training set of 6.5 hours of interictal data and 41 minin ictal data
and a test set of 9.14 hours. Compared to the state-of-the-art on the same
dataset, we achieve 49.4% increase in computational efficiency and 400 mins
better in average for detection delay. The proposed model is able to detect a
seizure onset at 91.95% sensitivity and 94.05% specificity with a mean
detection delay of 2.77 s. The area under the curve (AUC) is 96.44%, that is
comparable to the current state-of-the-art with AUC of 96.29%.","Supervised Learning in Automatic Channel Selection for Epileptic Seizure
  Detection",provide model feature word paper  for
699,"Approximate nearest neighbor (ANN) search has achieved great success in many
tasks. However, existing popular methods for ANN search, such as hashing and
quantization methods, are designed for static databases only. They cannot
handle well the database with data distribution evolving dynamically, due to
the high computational effort for retraining the model based on the new
database. In this paper, we address the problem by developing an online product
quantization (online PQ) model and incrementally updating the quantization
codebook that accommodates to the incoming streaming data. Moreover, to further
alleviate the issue of large scale computation for the online PQ update, we
design two budget constraints for the model to update partial PQ codebook
instead of all. We derive a loss bound which guarantees the performance of our
online PQ model. Furthermore, we develop an online PQ model over a sliding
window with both data insertion and deletion supported, to reflect the
real-time behaviour of the data. The experiments demonstrate that our online PQ
model is both time-efficient and effective for ANN search in dynamic large
scale databases compared with baseline methods and the idea of partial PQ
codebook update further reduces the update cost.",Online Product Quantization,high models new fundamental with 
700,"Existing machine translation decoding algorithms generate translations in a
strictly monotonic fashion and never revisit previous decisions. As a result,
earlier mistakes cannot be corrected at a later stage. In this paper, we
present a translation scheme that starts from an initial guess and then makes
iterative improvements that may revisit previous decisions. We parameterize our
model as a convolutional neural network that predicts discrete substitutions to
an existing translation based on an attention mechanism over both the source
sentence as well as the current translation output. By making less than one
modification per sentence, we improve the output of a phrase-based translation
system by up to 0.4 BLEU on WMT15 German-English translation.",Iterative Refinement for Machine Translation,programming information show extensive information the
701,"We present a stochastic first-order optimization algorithm, named BCSC, that
adds a cyclic constraint to stochastic block-coordinate descent. It uses
different subsets of the data to update different subsets of the parameters,
thus limiting the detrimental effect of outliers in the training set. Empirical
tests in benchmark datasets show that our algorithm outperforms
state-of-the-art optimization methods in both accuracy as well as convergence
speed. The improvements are consistent across different architectures, and can
be combined with other training techniques and regularization methods.",Block-Cyclic Stochastic Coordinate Descent for Deep Neural Networks,kernel kernel efficient show kernel <unk>
702,"With the advance of fluorescence imaging technologies, recently cell
biologists are able to record the movement of protein vesicles within a living
cell. Automatic tracking of the movements of these vesicles become key for
qualitative analysis of dynamics of theses vesicles. In this thesis, we
formulate such tracking problem as video object tracking problem, and design a
dynamic programming method for tracking single object. Our experiments on
simulation data show that the method can identify a track with high accuracy
which is robust to the choose of tracking parameters and presence of high level
noise. We then extend this method to the tracking multiple objects using the
track elimination strategy. In multiple object tracking, the above approach
often fails to correctly identify a track when two tracks cross. We solve this
problem by incorporating the Kalman filter into the dynamic programming
framework. Our experiments on simulated data show that the tracking accuracy is
significantly improved.",Automatic tracking of protein vesicles,constraint stochastic use show achieve segmentation 
703,"The growth of data, the need for scalability and the complexity of models
used in modern machine learning calls for distributed implementations. Yet, as
of today, distributed machine learning frameworks have largely ignored the
possibility of arbitrary (i.e., Byzantine) failures. In this paper, we study
the robustness to Byzantine failures at the fundamental level of stochastic
gradient descent (SGD), the heart of most machine learning algorithms. Assuming
a set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can
SGD be, without limiting the dimension, nor the size of the parameter space.
  We first show that no gradient descent update rule based on a linear
combination of the vectors proposed by the workers (i.e, current approaches)
tolerates a single Byzantine failure. We then formulate a resilience property
of the update rule capturing the basic requirements to guarantee convergence
despite $f$ Byzantine workers. We finally propose Krum, an update rule that
satisfies the resilience property aforementioned. For a $d$-dimensional
learning problem, the time complexity of Krum is $O(n^2 \cdot (d + \log n))$.",Byzantine-Tolerant Machine Learning,using <unk> the model <unk>
704,"Learning meaningful representations using deep neural networks involves
designing efficient training schemes and well-structured networks. Currently,
the method of stochastic gradient descent that has a momentum with dropout is
one of the most popular training protocols. Based on that, more advanced
methods (i.e., Maxout and Batch Normalization) have been proposed in recent
years, but most still suffer from performance degradation caused by small
perturbations, also known as adversarial examples. To address this issue, we
propose manifold regularized networks (MRnet) that utilize a novel training
objective function that minimizes the difference between multi-layer embedding
results of samples and those adversarial. Our experimental results demonstrated
that MRnet is more resilient to adversarial examples and helps us to generalize
representations on manifolds. Furthermore, combining MRnet and dropout allowed
us to achieve competitive classification performances for three well-known
benchmarks: MNIST, CIFAR-10, and SVHN.",Manifold Regularized Deep Neural Networks using Adversarial Examples,representations the the method <unk> using
705,"The Generalized Traveling Salesman Problem (GTSP) is one of the NP-hard
combinatorial optimization problems. A variant of GTSP is E-GTSP where E,
meaning equality, has the constraint: exactly one node from a cluster of a
graph partition is visited. The main objective of the E-GTSP is to find a
minimum cost tour passing through exactly one node from each cluster of an
undirected graph. Agent-based approaches involving are successfully used
nowadays for solving real life complex problems. The aim of the current paper
is to illustrate some variants of agent-based algorithms including ant-based
models with specific properties for solving E-GTSP.","A Unifying Survey of Reinforced, Sensitive and Stigmergic Agent-Based
  Approaches for E-GTSP",data model noisy show preferences
706,"RESTful services on the Web expose information through retrievable resource
representations that represent self-describing descriptions of resources, and
through the way how these resources are interlinked through the hyperlinks that
can be found in those representations. This basic design of RESTful services
means that for extracting the most useful information from a service, it is
necessary to understand a service's representations, which means both the
semantics in terms of describing a resource, and also its semantics in terms of
describing its linkage with other resources. Based on the Resource Linking
Language (ReLL), this paper describes a framework for how RESTful services can
be described, and how these descriptions can then be used to harvest
information from these services. Building on this framework, a layered model of
RESTful service semantics allows to represent a service's information in
RDF/OWL. Because REST is based on the linkage between resources, the same model
can be used for aggregating and interlinking multiple services for extracting
RDF data from sets of RESTful services.",From RESTful Services to RDF: Connecting the Web and the Semantic Web,models model common proposed poses first
707,"Automation of brain matter segmentation from MR images is a challenging task
due to the irregular boundaries between the grey and white matter regions. In
addition, the presence of intensity inhomogeneity in the MR images further
complicates the problem. In this paper, we propose a texture and vesselness
incorporated version of the ScatterNet Hybrid Deep Learning Network (TS-SHDL)
that extracts hierarchical invariant mid-level features, used by fisher vector
encoding and a conditional random field (CRF) to perform the desired
segmentation. The performance of the proposed network is evaluated by extensive
experimentation and comparison with the state-of-the-art methods on several 2D
MRI scans taken from the synthetic McGill Brain Web as well as on the MRBrainS
dataset of real 3D MRI scans. The advantages of the TS-SHDL network over
supervised deep learning networks is also presented in addition to its superior
performance over the state-of-the-art.","Texture and Structure Incorporated ScatterNet Hybrid Deep Learning
  Network (TS-SHDL) For Brain Matter Segmentation",mechanism weight video  kl  state model learning domain 
708,"In this paper we present our web application SeRE designed to explore
semantically related concepts. Wikipedia and DBpedia are rich data sources to
extract related entities for a given topic, like in- and out-links, broader and
narrower terms, categorisation information etc. We use the Wikipedia full text
body to compute the semantic relatedness for extracted terms, which results in
a list of entities that are most relevant for a topic. For any given query, the
user interface of SeRE visualizes these related concepts, ordered by semantic
relatedness; with snippets from Wikipedia articles that explain the connection
between those two entities. In a user study we examine how SeRE can be used to
find important entities and their relationships for a given topic and to answer
the question of how the classification system can be used for filtering.",Exploring semantically-related concepts from Wikipedia: the case of SeRE,dimension constraint show show correspondence still extraction two
709,"In this work, we propose a semi-supervised method for short text clustering,
where we represent texts as distributed vectors with neural networks, and use a
small amount of labeled data to specify our intention for clustering. We design
a novel objective to combine the representation learning process and the
k-means clustering process together, and optimize the objective with both
labeled data and unlabeled data iteratively until convergence through three
steps: (1) assign each short text to its nearest centroid based on its
representation from the current neural networks; (2) re-estimate the cluster
centroids based on cluster assignments from step (1); (3) update neural
networks according to the objective by keeping centroids and cluster
assignments fixed. Experimental results on four datasets show that our method
works significantly better than several other text clustering methods.","Semi-supervised Clustering for Short Text via Deep Representation
  Learning",networks <unk> <unk> <unk> remains
710,"This paper proposes a deep leaning method to address the challenging facial
attractiveness prediction problem. The method constructs a convolutional neural
network of facial beauty prediction using a new deep cascaded fine-turning
scheme with various face inputting channels, such as the original RGB face
image, the detail layer image, and the lighting layer image. With a carefully
designed CNN model of deep structure, large input size and small convolutional
kernels, we have achieved a high prediction correlation of 0.88. This result
convinces us that the problem of facial attractiveness prediction can be solved
by deep learning approach, and it also shows the important roles of the facial
smoothness, lightness, and color information that were involved in facial
beauty perception, which is consistent with the result of recent psychology
studies. Furthermore, we analyze the high-level features learnt by CNN through
visualization of its hidden layers, and some interesting phenomena were
observed. It is found that the contours and appearance of facial features,
especially eyes and moth, are the most significant facial attributes for facial
attractiveness prediction, which is also consistent with the visual perception
intuition of human.","A new humanlike facial attractiveness predictor with cascaded
  fine-tuning deep learning model",broadly feature models demonstrate show maximum first 
711,"This paper reports on modern approaches in Information Extraction (IE) and
its two main sub-tasks of Named Entity Recognition (NER) and Relation
Extraction (RE). Basic concepts and the most recent approaches in this area are
reviewed, which mainly include Machine Learning (ML) based approaches and the
more recent trend to Deep Learning (DL) based methods.",A Study of Recent Contributions on Information Extraction,information show significant inductive approaches method extraction evaluation
712,"The number of categories for action recognition is growing rapidly and it has
become increasingly hard to label sufficient training data for learning
conventional models for all categories. Instead of collecting ever more data
and labelling them exhaustively for all categories, an attractive alternative
approach is zero-shot learning"" (ZSL). To that end, in this study we construct
a mapping between visual features and a semantic descriptor of each action
category, allowing new categories to be recognised in the absence of any visual
training data. Existing ZSL studies focus primarily on still images, and
attribute-based semantic representations. In this work, we explore word-vectors
as the shared semantic space to embed videos and category labels for ZSL action
recognition. This is a more challenging problem than existing ZSL of still
images and/or attributes, because the mapping between video spacetime features
of actions and the semantic space is more complex and harder to learn for the
purpose of generalising over any cross-category domain shift. To solve this
generalisation problem in ZSL action recognition, we investigate a series of
synergistic strategies to improve upon the standard ZSL pipeline. Most of these
strategies are transductive in nature which means access to testing data in the
training phase.",Transductive Zero-Shot Action Recognition by Word-Vector Embedding,<unk> <unk>
713,"Planning in partially observable Markov decision processes (POMDPs) remains a
challenging topic in the artificial intelligence community, in spite of recent
impressive progress in approximation techniques. Previous research has
indicated that online planning approaches are promising in handling large-scale
POMDP domains efficiently as they make decisions ""on demand"" instead of
proactively for the entire state space. We present a Factored Hybrid Heuristic
Online Planning (FHHOP) algorithm for large POMDPs. FHHOP gets its power by
combining a novel hybrid heuristic search strategy with a recently developed
factored state representation. On several benchmark problems, FHHOP
substantially outperformed state-of-the-art online heuristic search approaches
in terms of both scalability and quality.","FHHOP: A Factored Hybrid Heuristic Online Planning Algorithm for Large
  POMDPs",query similarity quality linear model <unk>
714,"We introduce a novel schema for sequence to sequence learning with a Deep
Q-Network (DQN), which decodes the output sequence iteratively. The aim here is
to enable the decoder to first tackle easier portions of the sequences, and
then turn to cope with difficult parts. Specifically, in each iteration, an
encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the
input sequence, automatically create features to represent the internal states
of and formulate a list of potential actions for the DQN. Take rephrasing a
natural sentence as an example. This list can contain ranked potential words.
Next, the DQN learns to make decision on which action (e.g., word) will be
selected from the list to modify the current decoded sequence. The newly
modified output sequence is subsequently used as the input to the DQN for the
next decoding iteration. In each iteration, we also bias the reinforcement
learning's attention to explore sequence portions which are previously
difficult to be decoded. For evaluation, the proposed strategy was trained to
decode ten thousands natural sentences. Our experiments indicate that, when
compared to a left-to-right greedy beam search LSTM decoder, the proposed
method performed competitively well when decoding sentences from the training
set, but significantly outperformed the baseline when decoding unseen
sentences, in terms of BLEU score obtained.",Generating Text with Deep Reinforcement Learning,preferences show show preferences
715,"A nonparametric family of conditional distributions is introduced, which
generalizes conditional exponential families using functional parameters in a
suitable RKHS. An algorithm is provided for learning the generalized natural
parameter, and consistency of the estimator is established in the well
specified case. In experiments, the new method generally outperforms a
competing approach with consistency guarantees, and is competitive with a deep
conditional density model on datasets that exhibit abrupt transitions and
heteroscedasticity.",Kernel Conditional Exponential Family,data models show <unk> approaches using domain
716,"In this paper, we consider the problem of machine reading task when the
questions are in the form of keywords, rather than natural language. In recent
years, researchers have achieved significant success on machine reading
comprehension tasks, such as SQuAD and TriviaQA. These datasets provide a
natural language question sentence and a pre-selected passage, and the goal is
to answer the question according to the passage. However, in the situation of
interacting with machines by means of text, people are more likely to raise a
query in form of several keywords rather than a complete sentence. The
keyword-based query comprehension is a new challenge, because small variations
to a question may completely change its semantical information, thus yield
different answers. In this paper, we propose a novel neural network system that
consists a Demand Optimization Model based on a passage-attention neural
machine translation and a Reader Model that can find the answer given the
optimized question. The Demand Optimization Model optimizes the original query
and output multiple reconstructed questions, then the Reader Model takes the
new questions as input and locate the answers from the passage. To make
predictions robust, an evaluation mechanism will score the reconstructed
questions so the final answer strike a good balance between the quality of both
the Demand Optimization Model and the Reader Model. Experimental results on
several datasets show that our framework significantly improves multiple strong
baselines on this challenging task.","Keyword-based Query Comprehending via Multiple Optimized-Demand
  Augmentation",combining paper  show paper  metric 
717,"In this exploratory note we ask the question of what a measure of performance
for all tasks is like if we use a weighting of tasks based on a difficulty
function. This difficulty function depends on the complexity of the
(acceptable) solution for the task (instead of a universal distribution over
tasks or an adaptive test). The resulting aggregations and decompositions are
(now retrospectively) seen as the natural (and trivial) interactive
generalisation of the C-tests.",A note about the generalisation of the C-tests, #  show significant complexity feature models
718,"Deep dictionary learning seeks multiple dictionaries at different image
scales to capture complementary coherent characteristics. We propose a method
for learning a hierarchy of synthesis dictionaries with an image classification
goal. The dictionaries and classification parameters are trained by a
classification objective, and the sparse features are extracted by reducing a
reconstruction loss in each layer. The reconstruction objectives in some sense
regularize the classification problem and inject source signal information in
the extracted features. The performance of the proposed hierarchical method
increases by adding more layers, which consequently makes this model easier to
tune and adapt. The proposed algorithm furthermore, shows remarkably lower
fooling rate in presence of adversarial perturbation. The validation of the
proposed approach is based on its classification performance using four
benchmark datasets and is compared to a CNN of similar size.",Deep Dictionary Learning: A PARametric NETwork Approach,method maximal generated method learning users model utility
719,"Hedonic games are meant to model how coalitions of people form and break
apart in the real world. However, it is difficult to run simulations when
everything must be done by hand on paper. We present an online software that
allows fast and visual simulation of several types of hedonic games.
http://lukemiles.org/hedonic-games/",A Simulator for Hedonic Games,using performing significant significant significant
720,"We propose two practical non-convex approaches for learning near-isometric,
linear embeddings of finite sets of data points. Given a set of training points
$\mathcal{X}$, we consider the secant set $S(\mathcal{X})$ that consists of all
pairwise difference vectors of $\mathcal{X}$, normalized to lie on the unit
sphere. The problem can be formulated as finding a symmetric and positive
semi-definite matrix $\boldsymbol{\Psi}$ that preserves the norms of all the
vectors in $S(\mathcal{X})$ up to a distortion parameter $\delta$. Motivated by
non-negative matrix factorization, we reformulate our problem into a Frobenius
norm minimization problem, which is solved by the Alternating Direction Method
of Multipliers (ADMM) and develop an algorithm, FroMax. Another method solves
for a projection matrix $\boldsymbol{\Psi}$ by minimizing the restricted
isometry property (RIP) directly over the set of symmetric, postive
semi-definite matrices. Applying ADMM and a Moreau decomposition on a proximal
mapping, we develop another algorithm, NILE-Pro, for dimensionality reduction.
FroMax is shown to converge faster for smaller $\delta$ while NILE-Pro
converges faster for larger $\delta$. Both non-convex approaches are then
empirically demonstrated to be more computationally efficient than prior convex
approaches for a number of applications in machine learning and signal
processing.",Practical Algorithms for Learning Near-Isometric Linear Embeddings,various present misses two show
721,"Although learning-based methods have great potential for robotics, one
concern is that a robot that updates its parameters might cause large amounts
of damage before it learns the optimal policy. We formalize the idea of safe
learning in a probabilistic sense by defining an optimization problem: we
desire to maximize the expected return while keeping the expected damage below
a given safety limit. We study this optimization for the case of a robot
manipulator with safety-based torque limits. We would like to ensure that the
damage constraint is maintained at every step of the optimization and not just
at convergence. To achieve this aim, we introduce a novel method which predicts
how modifying the torque limit, as well as how updating the policy parameters,
might affect the robot's safety. We show through a number of experiments that
our approach allows the robot to improve its performance while ensuring that
the expected damage constraint is not violated during the learning process.",Probabilistically Safe Policy Transfer,np hard <unk> ####  new constraint ensemble
722,"In this paper, we provide a new framework to obtain the generalization bounds
of the learning process for domain adaptation, and then apply the derived
bounds to analyze the asymptotical convergence of the learning process. Without
loss of generality, we consider two kinds of representative domain adaptation:
one is with multiple sources and the other is combining source and target data.
  In particular, we use the integral probability metric to measure the
difference between two domains. For either kind of domain adaptation, we
develop a related Hoeffding-type deviation inequality and a symmetrization
inequality to achieve the corresponding generalization bound based on the
uniform entropy number. We also generalized the classical McDiarmid's
inequality to a more general setting where independent random variables can
take values from different domains. By using this inequality, we then obtain
generalization bounds based on the Rademacher complexity. Afterwards, we
analyze the asymptotic convergence and the rate of convergence of the learning
process for such kind of domain adaptation. Meanwhile, we discuss the factors
that affect the asymptotic behavior of the learning process and the numerical
experiments support our theoretical findings as well.",Generalization Bounds for Domain Adaptation,time varying domain model <unk> proposed applied unlabeled offers
723,"The online Markov decision process (MDP) is a generalization of the classical
Markov decision process that incorporates changing reward functions. In this
paper, we propose practical online MDP algorithms with policy iteration and
theoretically establish a sublinear regret bound. A notable advantage of the
proposed algorithm is that it can be easily combined with function
approximation, and thus large and possibly continuous state spaces can be
efficiently handled. Through experiments, we demonstrate the usefulness of the
proposed algorithm.",Online Markov decision processes with policy iteration,despite strategy visualization
724,"We present a general framework for proving polynomial sample complexity
bounds for the problem of learning from samples the best auction in a class of
""simple"" auctions. Our framework captures all of the most prominent examples of
""simple"" auctions, including anonymous and non-anonymous item and bundle
pricings, with either a single or multiple buyers. The technique we propose is
to break the analysis of auctions into two natural pieces. First, one shows
that the set of allocation rules have large amounts of structure; second,
fixing an allocation on a sample, one shows that the set of auctions agreeing
with this allocation on that sample have revenue functions with low
dimensionality. Our results effectively imply that whenever it's possible to
compute a near-optimal simple auction with a known prior, it is also possible
to compute such an auction with an unknown prior (given a polynomial number of
samples).",Learning Simple Auctions,data semantic number show video search
725,"The convergence, convergence rate and expected hitting time play fundamental
roles in the analysis of randomised search heuristics. This paper presents a
unified Markov chain approach to studying them. Using the approach, the
sufficient and necessary conditions of convergence in distribution are
established. Then the average convergence rate is introduced to randomised
search heuristics and its lower and upper bounds are derived. Finally, novel
average drift analysis and backward drift analysis are proposed for bounding
the expected hitting time. A computational study is also conducted to
investigate the convergence, convergence rate and expected hitting time. The
theoretical study belongs to a prior and general study while the computational
study belongs to a posterior and case study.","A Unified Markov Chain Approach to Analysing Randomised Search
  Heuristics",model video along model data languages rapid
726,"This work deals with content-based video indexing. Our viewpoint is
semi-automatic analysis of compressed video. We consider the possible
applications of motion analysis and moving object detection : assisting moving
object indexing, summarising videos, and allowing image and motion queries. We
propose an approach based on interest points. As first results, we test and
compare the stability of different types of interest point detectors in
compressed sequences.","Assisted Video Sequences Indexing : Motion Analysis Based on Interest
  Points",train similarity layer show units inference
727,"Expectation Propagation (EP) provides a framework for approximate inference.
When the model under consideration is over a latent Gaussian field, with the
approximation being Gaussian, we show how these approximations can
systematically be corrected. A perturbative expansion is made of the exact but
intractable correction, and can be applied to the model's partition function
and other moments of interest. The correction is expressed over the
higher-order cumulants which are neglected by EP's local matching of moments.
Through the expansion, we see that EP is correct to first order. By considering
higher orders, corrections of increasing polynomial complexity can be applied
to the approximation. The second order provides a correction in quadratic time,
which we apply to an array of Gaussian process and Ising models. The
corrections generalize to arbitrarily complex approximating families, which we
illustrate on tree-structured Ising model approximations. Furthermore, they
provide a polynomial-time assessment of the approximation error. We also
provide both theoretical and practical insights on the exactness of the EP
solution.","Perturbative Corrections for Approximate Inference in Gaussian Latent
  Variable Models",using rapid size learning <unk>
728,"In an attempt at exploring the limitations of simple approaches to the task
of piano transcription (as usually defined in MIR), we conduct an in-depth
analysis of neural network-based framewise transcription. We systematically
compare different popular input representations for transcription systems to
determine the ones most suitable for use with neural networks. Exploiting
recent advances in training techniques and new regularizers, and taking into
account hyper-parameter tuning, we show that it is possible, by simple
bottom-up frame-wise processing, to obtain a piano transcriber that outperforms
the current published state of the art on the publicly available MAPS dataset
-- without any complex post-processing steps. Thus, we propose this simple
approach as a new baseline for this dataset, for future transcription research
to build on and improve.",On the Potential of Simple Framewise Approaches to Piano Transcription,segmentation data machine use show show objects sentence
729,"We propose a probabilistic model for the parallel execution of Las Vegas
algorithms, i.e., randomized algorithms whose runtime might vary from one
execution to another, even with the same input. This model aims at predicting
the parallel performances (i.e., speedups) by analysis the runtime distribution
of the sequential runs of the algorithm. Then, we study in practice the case of
a particular Las Vegas algorithm for combinatorial optimization, on three
classical problems, and compare with an actual parallel implementation up to
256 cores. We show that the prediction can be quite accurate, matching the
actual speedups very well up to 100 parallel cores and then with a deviation of
about 20% up to 256 cores.",Prediction of Parallel Speed-ups for Las Vegas Algorithms,using learning techniques problem <unk> propose objects models
730,"Non-contact estimation of respiratory pattern (RP) and respiration rate (RR)
has multiple applications. Existing methods for RP and RR measurement fall into
one of the three categories - (i) estimation through nasal air flow
measurement, (ii) estimation from video-based remote photoplethysmography, and
(iii) estimation by measurement of motion induced by respiration using motion
detectors. These methods, however, require specialized sensors, are
computationally expensive and/or critically depend on selection of a region of
interest (ROI) for processing. In this paper a general framework is described
for estimating a periodic signal driving noisy LTI channels connected in
parallel with unknown dynamics. The method is then applied to derive a
computationally inexpensive method for estimating RP using 2D cameras that does
not critically depend on ROI. Specifically, RP is estimated by imaging the
changes in the reflected light caused by respiration-induced motion. Each
spatial location in the field of view of the camera is modeled as a
noise-corrupted linear time-invariant (LTI) measurement channel with unknown
system dynamics, driven by a single generating respiratory signal. Estimation
of RP is cast as a blind deconvolution problem and is solved through a method
comprising subspace projection and statistical aggregation. Experiments are
carried out on 31 healthy human subjects by generating multiple RPs and
comparing the proposed estimates with simultaneously acquired ground truth from
an impedance pneumograph device. The proposed estimator agrees well with the
ground truth device in terms of correlation measures, despite variability in
clothing pattern, angle of view and ROI.","Estimation of respiratory pattern from video using selective ensemble
  aggregation",operator for based certain time varying proposed frank wolfe phenomena 
731,"Quantification of intra-retinal boundaries in optical coherence tomography
(OCT) is a crucial task for studying and diagnosing neurological and ocular
diseases. Since manual segmentation of layers is usually a time consuming task
and relay on user, a lot of attempts done to do it automatically and without
interference of user. Although for extracting all layers usually same procedure
is applied but finding the first layer is usually more difficult due to
vanishing it in some region specially close to Fobia. To have a general
software, beside using common methods like applying shortest path algorithm on
global gradient of image, some extra steps are used here to confine search area
for Dijstra algorithm especially for the second layer. Results demonstrates
high accuracy in segmenting all present layers, especially the first one that
is important for diagnosing issue.","Accurate automatic segmentation of retina layers with emphasis on first
  layer",potential  ii  proposed known
732,"This paper introduces a new approach to automatically quantify the severity
of knee OA using X-ray images. Automatically quantifying knee OA severity
involves two steps: first, automatically localizing the knee joints; next,
classifying the localized knee joint images. We introduce a new approach to
automatically detect the knee joints using a fully convolutional neural network
(FCN). We train convolutional neural networks (CNN) from scratch to
automatically quantify the knee OA severity optimizing a weighted ratio of two
loss functions: categorical cross-entropy and mean-squared loss. This joint
training further improves the overall quantification of knee OA severity, with
the added benefit of naturally producing simultaneous multi-class
classification and regression outputs. Two public datasets are used to evaluate
our approach, the Osteoarthritis Initiative (OAI) and the Multicenter
Osteoarthritis Study (MOST), with extremely promising results that outperform
existing approaches.","Automatic Detection of Knee Joints and Quantification of Knee
  Osteoarthritis Severity using Convolutional Neural Networks",paper  for show technique  svm 
733,"Recent electrophysiological experiments have shown that dopamine (D1)
modulation of pyramidal cells in prefrontal cortex reduces spike frequency
adaptation and enhances NMDA transmission. Using four models, from
multicompartmental to integrate and fire, we examine the effects of these
modulations on sustained (delay) activity in a reverberatory network. We find
that D1 modulation may enable robust network bistability yielding selective
reverberation among cells that code for a particular item or location. We
further show that the tuning curve of such cells is sharpened, and that
signal-to-noise ratio is increased. We postulate that D1 modulation affects the
tuning of ""memory fields"" and yield efficient distributed dynamic
representations.","Dopamine modulation of prefrontal delay activity-reverberatory activity
  and sharpness of tuning curves",values  show
734,"Learning distributed representations of documents has pushed the
state-of-the-art in several natural language processing tasks and was
successfully applied to the field of recommender systems recently. In this
paper, we propose a novel content-based recommender system based on learned
representations and a generative model of user interest. Our method works as
follows: First, we learn representations on a corpus of text documents. Then,
we capture a user's interest as a generative model in the space of the document
representations. In particular, we model the distribution of interest for each
user as a Gaussian mixture model (GMM). Recommendations can be obtained
directly by sampling from a user's generative model. Using Latent semantic
analysis (LSA) as comparison, we compute and explore document representations
on the Delicious bookmarks dataset, a standard benchmark for recommender
systems. We then perform density estimation in both spaces and show that
learned representations outperform LSA in terms of predictive performance.",Generative Interest Estimation for Document Recommendations,two strategy uses show models images  mixture demonstrate
735,"The identification and quantification of markers in medical images is
critical for diagnosis, prognosis and management of patients in clinical
practice. Supervised- or weakly supervised training enables the detection of
findings that are known a priori. It does not scale well, and a priori
definition limits the vocabulary of markers to known entities reducing the
accuracy of diagnosis and prognosis. Here, we propose the identification of
anomalies in large-scale medical imaging data using healthy examples as a
reference. We detect and categorize candidates for anomaly findings untypical
for the observed data. A deep convolutional autoencoder is trained on healthy
retinal images. The learned model generates a new feature representation, and
the distribution of healthy retinal patches is estimated by a one-class support
vector machine. Results demonstrate that we can identify pathologic regions in
images without using expert annotations. A subsequent clustering categorizes
findings into clinically meaningful classes. In addition the learned features
outperform standard embedding approaches in a classification task.",Identifying and Categorizing Anomalies in Retinal Imaging Data,the model signals  general online
736,"Despite the recent successes in robotics, artificial intelligence and
computer vision, a complete artificial agent necessarily must include active
perception. A multitude of ideas and methods for how to accomplish this have
already appeared in the past, their broader utility perhaps impeded by
insufficient computational power or costly hardware. The history of these
ideas, perhaps selective due to our perspectives, is presented with the goal of
organizing the past literature and highlighting the seminal contributions. We
argue that those contributions are as relevant today as they were decades ago
and, with the state of modern computational tools, are poised to find new life
in the robotic perception systems of the next decade.",Revisiting Active Perception,linear model types mathematical
737,"As the popularity of social media platforms continues to rise, an
ever-increasing amount of human communication and self- expression takes place
online. Most recent research has focused on mining social media for public user
opinion about external entities such as product reviews or sentiment towards
political news. However, less attention has been paid to analyzing users'
internalized thoughts and emotions from a mental health perspective. In this
paper, we quantify the semantic difference between public Tweets and private
mental health journals used in online cognitive behavioral therapy. We will use
deep transfer learning techniques for analyzing the semantic gap between the
two domains. We show that for the task of emotional valence prediction, social
media can be successfully harnessed to create more accurate, robust, and
personalized mental health models. Our results suggest that the semantic gap
between public and private self-expression is small, and that utilizing the
abundance of available social media is one way to overcome the small sample
sizes of mental health data, which are commonly limited by availability and
privacy concerns.","Hashtag Healthcare: From Tweets to Mental Health Journals Using Deep
  Transfer Learning",propose embeddings information show score method  scalar
738,"In this paper, we propose a new texture analysis method using the
deterministic partially self-avoiding walk performed on maps modified with
thresholds. In this method, two pixels of the map are neighbors if the
Euclidean distance between them is less than $\sqrt{2}$ and the weight
(difference between its intensities) is less than a given threshold. The maps
obtained by using different thresholds highlight several properties of the
image that are extracted by the deterministic walk. To compose the feature
vector, deterministic walks are performed with different thresholds and its
statistics are concatenated. Thus, this approach can be considered as a
multi-scale analysis. We validate our method on the Brodatz database, which is
very well known public image database and widely used by texture analysis
methods. Experimental results indicate that the proposed method presents a good
texture discrimination, overcoming traditional texture methods.","Texture analysis using deterministic partially self-avoiding walk with
  thresholds",learning currently model currently show save semantics
739,"A short review of similarities between dolphins and humans with the help of
quantitative linguistics and information theory.",Parallels of human language in the behavior of bottlenose dolphins,using exceeds sorting
740,"This paper addresses the problem of corpus-level entity typing, i.e.,
inferring from a large corpus that an entity is a member of a class such as
""food"" or ""artist"". The application of entity typing we are interested in is
knowledge base completion, specifically, to learn which classes an entity is a
member of. We propose FIGMENT to tackle this problem. FIGMENT is
embedding-based and combines (i) a global model that scores based on aggregated
contextual information of an entity and (ii) a context model that first scores
the individual occurrences of an entity and then aggregates the scores. In our
evaluation, FIGMENT strongly outperforms an approach to entity typing that
relies on relations obtained by an open information extraction system.",Corpus-level Fine-grained Entity Typing Using Contextual Information,offers observation show modified
741,"The user-level brokers in grids consider individual application QoS
requirements and minimize their cost without considering demands from other
users. This results in contention for resources and sub-optimal schedules.
Meta-scheduling in grids aims to address this scheduling problem, which is NP
hard due to its combinatorial nature. Thus, many heuristic-based solutions
using Genetic Algorithm (GA) have been proposed, apart from traditional
algorithms such as Greedy and FCFS.
  We propose a Linear Programming/Integer Programming model (LP/IP) for
scheduling these applications to multiple resources. We also propose a novel
algorithm LPGA (Linear programming driven Genetic Algorithm) which combines the
capabilities of LP and GA. The aim of this algorithm is to obtain the best
metaschedule for utility grids which minimize combined cost of all users in a
coordinated manner. Simulation results show that our proposed integrated
algorithm offers the best schedule having the minimum processing cost with
negligible time overhead.","A Linear Programming Driven Genetic Algorithm for Meta-Scheduling on
  Utility Grids",model method <unk> for
742,"This article presents a fragment of a new comparative dictionary ""A
comparative dictionary of names of expansive action in Russian and Bulgarian
languages"". Main features of the new web-based comparative dictionary are
placed, the principles of its formation are shown, primary links between the
word-matches are classified. The principal difference between translation
dictionaries and the model of double comparison is also shown. The
classification scheme of the pages is proposed. New concepts and keywords have
been introduced. The real prototype of the dictionary with a few key pages is
published. The broad debate about the possibility of this prototype to become a
version of Russian-Bulgarian comparative dictionary of a new generation is
available.","Materials to the Russian-Bulgarian Comparative Dictionary ""EAD""",potential  ii 
743,"We applied machine learning to predict whether a gene is involved in axon
regeneration. We extracted 31 features from different databases and trained
five machine learning models. Our optimal model, a Random Forest Classifier
with 50 submodels, yielded a test score of 85.71%, which is 4.1% higher than
the baseline score. We concluded that our models have some predictive
capability. Similar methodology and features could be applied to predict other
Gene Ontology (GO) terms.",Gene Ontology (GO) Prediction using Machine Learning Methods,the show set inference different
744,"In this paper, a novel framework for automated liver segmentation via a level
set formulation is presented. A sparse representation of both global
(region-based) and local (voxel-wise) image information is embedded in a level
set formulation to innovate a new cost function. Two dictionaries are build: A
region-based feature dictionary and a voxel-wise dictionary. These dictionaries
are learned, using the K-SVD method, from a public database of liver
segmentation challenge (MICCAI-SLiver07). The learned dictionaries provide
prior knowledge to the level set formulation. For the quantitative evaluation,
the proposed method is evaluated using the testing data of MICCAI-SLiver07
database. The results are evaluated using different metric scores computed by
the challenge organizers. The experimental results demonstrate the superiority
of the proposed framework by achieving the highest segmentation accuracy
(79.6\%) in comparison to the state-of-the-art methods.","Automatic 3D Liver Segmentation Using Sparse Representation of Global
  and Local Image Information via Level Set Formulation",software <unk>
745,"Many interesting real world domains involve reinforcement learning (RL) in
partially observable environments. Efficient learning in such domains is
important, but existing sample complexity bounds for partially observable RL
are at least exponential in the episode length. We give, to our knowledge, the
first partially observable RL algorithm with a polynomial bound on the number
of episodes on which the algorithm may not achieve near-optimal performance.
Our algorithm is suitable for an important class of episodic POMDPs. Our
approach builds on recent advances in method of moments for latent variable
model estimation.",A PAC RL Algorithm for Episodic POMDPs,ones using show brain
746,"In this paper, we propose a simple and effective {geometric} model fitting
method to fit and segment multi-structure data even in the presence of severe
outliers. We cast the task of geometric model fitting as a representative
mode-seeking problem on hypergraphs. Specifically, a hypergraph is firstly
constructed, where the vertices represent model hypotheses and the hyperedges
denote data points. The hypergraph involves higher-order similarities (instead
of pairwise similarities used on a simple graph), and it can characterize
complex relationships between model hypotheses and data points. {In addition,
we develop a hypergraph reduction technique to remove ""insignificant"" vertices
while retaining as many ""significant"" vertices as possible in the hypergraph}.
Based on the {simplified hypergraph, we then propose a novel mode-seeking
algorithm to search for representative modes within reasonable time. Finally,
the} proposed mode-seeking algorithm detects modes according to two key
elements, i.e., the weighting scores of vertices and the similarity analysis
between vertices. Overall, the proposed fitting method is able to efficiently
and effectively estimate the number and the parameters of model instances in
the data simultaneously. Experimental results demonstrate that the proposed
method achieves significant superiority over {several} state-of-the-art model
fitting methods on both synthetic data and real images.","Searching for Representative Modes on Hypergraphs for Robust Geometric
  Model Fitting",models show discovery natural cover model study known
747,"Convolutional Neural Networks (Convnets) have achieved good results in a
range of computer vision tasks the recent years. Though given a lot of
attention, visualizing the learned representations to interpret Convnets, still
remains a challenging task. The high dimensionality of internal representations
and the high abstractions of deep layers are the main challenges when
visualizing Convnet functionality. We present in this paper a technique based
on clustering internal Convnet representations with a Dirichlet Process
Gaussian Mixture Model, for visualization of learned representations in
Convnets. Our method copes with the high dimensionality of a Convnet by
clustering representations across all nodes of each layer. We will discuss how
this application is useful when considering transfer learning, i.e.\
transferring a model trained on one dataset to solve a task on a different one.","Analyzing Learned Convnet Features with Dirichlet Process Gaussian
  Mixture Models",state show <unk> texture using evaluation outliers 
748,"Aligning video sequences is a fundamental yet still unsolved component for a
broad range of applications in computer graphics and vision. Most classical
image processing methods cannot be directly applied to related video problems
due to the high amount of underlying data and their limit to small changes in
appearance. We present a scalable and robust method for computing a non-linear
temporal video alignment. The approach autonomously manages its training data
for learning a meaningful representation in an iterative procedure each time
increasing its own knowledge. It leverages on the nature of the videos
themselves to remove the need for manually created labels. While previous
alignment methods similarly consider weather conditions, season and
illumination, our approach is able to align videos from data recorded months
apart.",Learning Robust Video Synchronization without Annotations,learning us model data evolution show set
749,"In this paper an accurate real-time sequence-based system for representation,
recognition, interpretation, and analysis of the facial action units (AUs) and
expressions is presented. Our system has the following characteristics: 1)
employing adaptive-network-based fuzzy inference systems (ANFIS) and temporal
information, we developed a classification scheme based on neuro-fuzzy modeling
of the AU intensity, which is robust to intensity variations, 2) using both
geometric and appearance-based features, and applying efficient dimension
reduction techniques, our system is robust to illumination changes and it can
represent the subtle changes as well as temporal information involved in
formation of the facial expressions, and 3) by continuous values of intensity
and employing top-down hierarchical rule-based classifiers, we can develop
accurate human-interpretable AU-to-expression converters. Extensive experiments
on Cohn-Kanade database show the superiority of the proposed method, in
comparison with support vector machines, hidden Markov models, and neural
network classifiers. Keywords: biased discriminant analysis (BDA), classifier
design and evaluation, facial action units (AUs), hybrid learning, neuro-fuzzy
modeling.","Analysis, Interpretation, and Recognition of Facial Action Units and
  Expressions Using Neuro-Fuzzy Modeling",#  video show #  method  apply
750,"Recent years have witnessed amazing progress in AI related fields such as
computer vision, machine learning and autonomous vehicles. As with any rapidly
growing field, however, it becomes increasingly difficult to stay up-to-date or
enter the field as a beginner. While several topic specific survey papers have
been written, to date no general survey on problems, datasets and methods in
computer vision for autonomous vehicles exists. This paper attempts to narrow
this gap by providing a state-of-the-art survey on this topic. Our survey
includes both the historically most relevant literature as well as the current
state-of-the-art on several specific topics, including recognition,
reconstruction, motion estimation, tracking, scene understanding and end-to-end
learning. Towards this goal, we first provide a taxonomy to classify each
approach and then analyze the performance of the state-of-the-art on several
challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes.
Besides, we discuss open problems and current research challenges. To ease
accessibility and accommodate missing references, we will also provide an
interactive platform which allows to navigate topics and methods, and provides
additional information and project links for each paper.","Computer Vision for Autonomous Vehicles: Problems, Datasets and
  State-of-the-Art",models show yielded networks
751,"We present a new similarity measure based on information theoretic measures
which is superior than Normalized Compression Distance for clustering problems
and inherits the useful properties of conditional Kolmogorov complexity. We
show that Normalized Compression Dictionary Size and Normalized Compression
Dictionary Entropy are computationally more efficient, as the need to perform
the compression itself is eliminated. Also they scale linearly with exponential
vector size growth and are content independent. We show that normalized
compression dictionary distance is compressor independent, if limited to
lossless compressors, which gives space for optimizations and implementation
speed improvement for real-time and big data applications. The introduced
measure is applicable for machine learning tasks of parameter-free unsupervised
clustering, supervised learning such as classification and regression, feature
selection, and is applicable for big data problems with order of magnitude
speed increase.","Generalized Compression Dictionary Distance as Universal Similarity
  Measure",paper  for show handwriting simple
752,"With the range and sensitivity of algorithmic decisions expanding at a
break-neck speed, it is imperative that we aggressively investigate whether
programs are biased. We propose a novel probabilistic program analysis
technique and apply it to quantifying bias in decision-making programs.
Specifically, we (i) present a sound and complete automated verification
technique for proving quantitative properties of probabilistic programs; (ii)
show that certain notions of bias, recently proposed in the fairness
literature, can be phrased as quantitative correctness properties; and (iii)
present FairSquare, the first verification tool for quantifying program bias,
and evaluate it on a range of decision-making programs.",Quantifying Program Bias,data always paper  model technique previous method
753,"2D face analysis techniques, such as face landmarking, face recognition and
face verification, are reasonably dependent on illumination conditions which
are usually uncontrolled and unpredictable in the real world. An illumination
robust preprocessing method thus remains a significant challenge in reliable
face analysis. In this paper we propose a novel approach for improving lighting
normalization through building the underlying reflectance model which
characterizes interactions between skin surface, lighting source and camera
sensor, and elaborates the formation of face color appearance. Specifically,
the proposed illumination processing pipeline enables the generation of
Chromaticity Intrinsic Image (CII) in a log chromaticity space which is robust
to illumination variations. Moreover, as an advantage over most prevailing
methods, a photo-realistic color face image is subsequently reconstructed which
eliminates a wide variety of shadows whilst retaining the color information and
identity details. Experimental results under different scenarios and using
various face databases show the effectiveness of the proposed approach to deal
with lighting variations, including both soft and hard shadows, in face
recognition.",Improving Shadow Suppression for Illumination Robust Face Recognition,objects theorem models show engineering  using
754,"We present Deeply Supervised Object Detector (DSOD), a framework that can
learn object detectors from scratch. State-of-the-art object objectors rely
heavily on the off-the-shelf networks pre-trained on large-scale classification
datasets like ImageNet, which incurs learning bias due to the difference on
both the loss functions and the category distributions between classification
and detection tasks. Model fine-tuning for the detection task could alleviate
this bias to some extent but not fundamentally. Besides, transferring
pre-trained models from classification to detection between discrepant domains
is even more difficult (e.g. RGB to depth images). A better solution to tackle
these two critical problems is to train object detectors from scratch, which
motivates our proposed DSOD. Previous efforts in this direction mostly failed
due to much more complicated loss functions and limited training data in object
detection. In DSOD, we contribute a set of design principles for training
object detectors from scratch. One of the key findings is that deep
supervision, enabled by dense layer-wise connections, plays a critical role in
learning a good detector. Combining with several other principles, we develop
DSOD following the single-shot detection (SSD) framework. Experiments on PASCAL
VOC 2007, 2012 and MS COCO datasets demonstrate that DSOD can achieve better
results than the state-of-the-art solutions with much more compact models. For
instance, DSOD outperforms SSD on all three benchmarks with real-time detection
speed, while requires only 1/2 parameters to SSD and 1/10 parameters to Faster
RCNN. Our code and models are available at: https://github.com/szq0214/DSOD .",DSOD: Learning Deeply Supervised Object Detectors from Scratch,word proposed proposed word for
755,"We present an approach for real-time, robust and accurate hand pose
estimation from moving egocentric RGB-D cameras in cluttered real environments.
Existing methods typically fail for hand-object interactions in cluttered
scenes imaged from egocentric viewpoints, common for virtual or augmented
reality applications. Our approach uses two subsequently applied Convolutional
Neural Networks (CNNs) to localize the hand and regress 3D joint locations.
Hand localization is achieved by using a CNN to estimate the 2D position of the
hand center in the input, even in the presence of clutter and occlusions. The
localized hand position, together with the corresponding input depth value, is
used to generate a normalized cropped image that is fed into a second CNN to
regress relative 3D hand joint locations in real time. For added accuracy,
robustness and temporal stability, we refine the pose estimates using a
kinematic pose tracking energy. To train the CNNs, we introduce a new
photorealistic dataset that uses a merged reality approach to capture and
synthesize large amounts of annotated data of natural hand interaction in
cluttered scenes. Through quantitative and qualitative evaluation, we show that
our method is robust to self-occlusion and occlusions by objects, particularly
in moving egocentric perspectives.",Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor,diffusion show hardness for
756,"Accurate segmentation of the heart is an important step towards evaluating
cardiac function. In this paper, we present a fully automated framework for
segmentation of the left (LV) and right (RV) ventricular cavities and the
myocardium (Myo) on short-axis cardiac MR images. We investigate various 2D and
3D convolutional neural network architectures for this task. We investigate the
suitability of various state-of-the art 2D and 3D convolutional neural network
architectures, as well as slight modifications thereof, for this task.
Experiments were performed on the ACDC 2017 challenge training dataset
comprising cardiac MR images of 100 patients, where manual reference
segmentations were made available for end-diastolic (ED) and end-systolic (ES)
frames. We find that processing the images in a slice-by-slice fashion using 2D
networks is beneficial due to a relatively large slice thickness. However, the
exact network architecture only plays a minor role. We report mean Dice
coefficients of $0.950$ (LV), $0.893$ (RV), and $0.899$ (Myo), respectively
with an average evaluation time of 1.1 seconds per volume on a modern GPU.","An Exploration of 2D and 3D Deep Learning Techniques for Cardiac MR
  Image Segmentation",step problem different significant unlabeled improve online
757,"Tensor decompositions are invaluable tools in analyzing multimodal datasets.
In many real-world scenarios, such datasets are far from being static, to the
contrary they tend to grow over time. For instance, in an online social network
setting, as we observe new interactions over time, our dataset gets updated in
its ""time"" mode. How can we maintain a valid and accurate tensor decomposition
of such a dynamically evolving multimodal dataset, without having to re-compute
the entire decomposition after every single update? In this paper we introduce
SaMbaTen, a Sampling-based Batch Incremental Tensor Decomposition algorithm,
which incrementally maintains the decomposition given new updates to the tensor
dataset. SaMbaTen is able to scale to datasets that the state-of-the-art in
incremental tensor decomposition is unable to operate on, due to its ability to
effectively summarize the existing tensor and the incoming updates, and perform
all computations in the reduced summary space. We extensively evaluate SaMbaTen
using synthetic and real datasets. Indicatively, SaMbaTen achieves comparable
accuracy to state-of-the-art incremental and non-incremental techniques, while
being 25-30 times faster. Furthermore, SaMbaTen scales to very large sparse and
dense dynamically evolving tensors of dimensions up to 100K x 100K x 100K where
state-of-the-art incremental approaches were not able to operate.",SamBaTen: Sampling-based Batch Incremental Tensor Decomposition,advantage updates approaches
758,"Normally a decision support system is build to solve problem where
multi-criteria decisions are involved. The knowledge base is the vital part of
the decision support containing the information or data that is used in
decision-making process. This is the field where engineers and scientists have
applied several intelligent techniques and heuristics to obtain optimal
decisions from imprecise information. In this paper, we present a hybrid
neuro-genetic learning approach for the adaptation a Mamdani fuzzy inference
system for the Tactical Air Combat Decision Support System (TACDSS). Some
simulation results demonstrating the difference of the learning techniques and
are also provided.","Adaptation of Mamdani Fuzzy Inference System Using Neuro - Genetic
  Approach for Tactical Air Combat Decision Support System",paper  for
759,"Clothing retrieval is a challenging problem in computer vision. With the
advance of Convolutional Neural Networks (CNNs), the accuracy of clothing
retrieval has been significantly improved. FashionNet[1], a recent study,
proposes to employ a set of artificial features in the form of landmarks for
clothing retrieval, which are shown to be helpful for retrieval. However, the
landmark detection module is trained with strong supervision which requires
considerable efforts to obtain. In this paper, we propose a self-learning
Visual Attention Model (VAM) to extract attention maps from clothing images.
The VAM is further connected to a global network to form an end-to-end network
structure through Impdrop connection which randomly Dropout on the feature maps
with the probabilities given by the attention map. Extensive experiments on
several widely used benchmark clothing retrieval data sets have demonstrated
the promise of the proposed method. We also show that compared to the trivial
Product connection, the Impdrop connection makes the network structure more
robust when training sets of limited size are used.",Clothing Retrieval with Visual Attention Model,data require feature show preferences information show domain 
760,"We propose a novel tree classification system called Treelogy, that fuses
deep representations with hand-crafted features obtained from leaf images to
perform leaf-based plant classification. Key to this system are segmentation of
the leaf from an untextured background, using convolutional neural networks
(CNNs) for learning deep representations, extracting hand-crafted features with
a number of image processing techniques, training a linear SVM with feature
vectors, merging SVM and CNN results, and identifying the species from a
dataset of 57 trees. Our classification results show that fusion of deep
representations with hand-crafted features leads to the highest accuracy. The
proposed algorithm is embedded in a smart-phone application, which is publicly
available. Furthermore, our novel dataset comprised of 5408 leaf images is also
made public for use of other researchers.","Treelogy: A Novel Tree Classifier Utilizing Deep and Hand-crafted
  Representations",representations for show set hand crafted successful
761,"We present a novel detection method using a deep convolutional neural network
(CNN), named AttentionNet. We cast an object detection problem as an iterative
classification problem, which is the most suitable form of a CNN. AttentionNet
provides quantized weak directions pointing a target object and the ensemble of
iterative predictions from AttentionNet converges to an accurate object
boundary box. Since AttentionNet is a unified network for object detection, it
detects objects without any separated models from the object proposal to the
post bounding-box regression. We evaluate AttentionNet by a human detection
task and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC
2007/2012 with an 8-layered architecture only.",AttentionNet: Aggregating Weak Directions for Accurate Object Detection,studied model signals  improve online different feature models
762,"Recent advances have enabled ""oracle"" classifiers that can classify across
many classes and input distributions with high accuracy without retraining.
However, these classifiers are relatively heavyweight, so that applying them to
classify video is costly. We show that day-to-day video exhibits highly skewed
class distributions over the short term, and that these distributions can be
classified by much simpler models. We formulate the problem of detecting the
short-term skews online and exploiting models based on it as a new sequential
decision making problem dubbed the Online Bandit Problem, and present a new
algorithm to solve it. When applied to recognizing faces in TV shows and
movies, we realize end-to-end classification speedups of 2.4-7.8x/2.6-11.2x (on
GPU/CPU) relative to a state-of-the-art convolutional neural network, at
competitive accuracy.",Fast Video Classification via Adaptive Cascading of Deep Models,paper  considerably show previously directions  model
763,"A central challenge in sensory neuroscience is describing how the activity of
populations of neurons can represent useful features of the external
environment. However, while neurophysiologists have long been able to record
the responses of neurons in awake, behaving animals, it is another matter
entirely to say what a given neuron does. A key problem is that in many sensory
domains, the space of all possible stimuli that one might encounter is
effectively infinite; in vision, for instance, natural scenes are
combinatorially complex, and an organism will only encounter a tiny fraction of
possible stimuli. As a result, even describing the response properties of
sensory neurons is difficult, and investigations of neuronal functions are
almost always critically limited by the number of stimuli that can be
considered. In this paper, we propose a closed-loop, optimization-based
experimental framework for characterizing the response properties of sensory
neurons, building on past efforts in closed-loop experimental methods, and
leveraging recent advances in artificial neural networks to serve as as a
proving ground for our techniques. Specifically, using deep convolutional
neural networks, we asked whether modern black-box optimization techniques can
be used to interrogate the ""tuning landscape"" of an artificial neuron in a
deep, nonlinear system, without imposing significant constraints on the space
of stimuli under consideration. We introduce a series of measures to quantify
the tuning landscapes, and show how these relate to the performances of the
networks in an object recognition task. To the extent that deep convolutional
neural networks increasingly serve as de facto working hypotheses for
biological vision, we argue that developing a unified approach for studying
both artificial and biological systems holds great potential to advance both
fields together.","Measuring and Understanding Sensory Representations within Deep Networks
  Using a Numerical Optimization Framework",sensor preference similarity significant state
764,"Optimal transportation distances are a fundamental family of parameterized
distances for histograms. Despite their appealing theoretical properties,
excellent performance in retrieval tasks and intuitive formulation, their
computation involves the resolution of a linear program whose cost is
prohibitive whenever the histograms' dimension exceeds a few hundreds. We
propose in this work a new family of optimal transportation distances that look
at transportation problems from a maximum-entropy perspective. We smooth the
classical optimal transportation problem with an entropic regularization term,
and show that the resulting optimum is also a distance which can be computed
through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several
orders of magnitude faster than that of transportation solvers. We also report
improved performance over classical optimal transportation distances on the
MNIST benchmark problem.","Sinkhorn Distances: Lightspeed Computation of Optimal Transportation
  Distances",data model show preferences
765,"Many real-world reinforcement learning problems have a hierarchical nature,
and often exhibit some degree of partial observability. While hierarchy and
partial observability are usually tackled separately (for instance by combining
recurrent neural networks and options), we show that addressing both problems
simultaneously is simpler and more efficient in many cases. More specifically,
we make the initiation set of options conditional on the previously-executed
option, and show that options with such Option-Observation Initiation Sets
(OOIs) are at least as expressive as Finite State Controllers (FSCs), a
state-of-the-art approach for learning in POMDPs. OOIs are easy to design based
on an intuitive description of the task, lead to explainable policies and keep
the top-level and option policies memoryless. Our experiments show that OOIs
allow agents to learn optimal policies in challenging POMDPs, while being much
more sample-efficient than a recurrent neural network over options.","Reinforcement Learning in POMDPs with Memoryless Options and
  Option-Observation Initiation Sets",<unk> partial system
766,"Stein kernel has recently shown promising performance on classifying images
represented by symmetric positive definite (SPD) matrices. It evaluates the
similarity between two SPD matrices through their eigenvalues. In this paper,
we argue that directly using the original eigenvalues may be problematic
because: i) Eigenvalue estimation becomes biased when the number of samples is
inadequate, which may lead to unreliable kernel evaluation; ii) More
importantly, eigenvalues only reflect the property of an individual SPD matrix.
They are not necessarily optimal for computing Stein kernel when the goal is to
discriminate different sets of SPD matrices. To address the two issues in one
shot, we propose a discriminative Stein kernel, in which an extra parameter
vector is defined to adjust the eigenvalues of the input SPD matrices. The
optimal parameter values are sought by optimizing a proxy of classification
performance. To show the generality of the proposed method, three different
kernel learning criteria that are commonly used in the literature are employed
respectively as a proxy. A comprehensive experimental study is conducted on a
variety of image classification tasks to compare our proposed discriminative
Stein kernel with the original Stein kernel and other commonly used methods for
evaluating the similarity between SPD matrices. The experimental results
demonstrate that, the discriminative Stein kernel can attain greater
discrimination and better align with classification tasks by altering the
eigenvalues. This makes it produce higher classification performance than the
original Stein kernel and other commonly used methods.","Learning Discriminative Stein Kernel for SPD Matrices and Its
  Applications",two model show trajectories
767,"In just three years, Variational Autoencoders (VAEs) have emerged as one of
the most popular approaches to unsupervised learning of complicated
distributions. VAEs are appealing because they are built on top of standard
function approximators (neural networks), and can be trained with stochastic
gradient descent. VAEs have already shown promise in generating many kinds of
complicated data, including handwritten digits, faces, house numbers, CIFAR
images, physical models of scenes, segmentation, and predicting the future from
static images. This tutorial introduces the intuitions behind VAEs, explains
the mathematics behind them, and describes some empirical behavior. No prior
knowledge of variational Bayesian methods is assumed.",Tutorial on Variational Autoencoders,data research via problem people approaches
768,"While there is currently a lot of enthusiasm about ""big data"", useful data is
usually ""small"" and expensive to acquire. In this paper, we present a new
paradigm of learning partial differential equations from {\em small} data. In
particular, we introduce \emph{hidden physics models}, which are essentially
data-efficient learning machines capable of leveraging the underlying laws of
physics, expressed by time dependent and nonlinear partial differential
equations, to extract patterns from high-dimensional data generated from
experiments. The proposed methodology may be applied to the problem of
learning, system identification, or data-driven discovery of partial
differential equations. Our framework relies on Gaussian processes, a powerful
tool for probabilistic inference over functions, that enables us to strike a
balance between model complexity and data fitting. The effectiveness of the
proposed approach is demonstrated through a variety of canonical problems,
spanning a number of scientific domains, including the Navier-Stokes,
Schr\""odinger, Kuramoto-Sivashinsky, and time dependent linear fractional
equations. The methodology provides a promising new direction for harnessing
the long-standing developments of classical methods in applied mathematics and
mathematical physics to design learning machines with the ability to operate in
complex domains without requiring large quantities of data.","Hidden Physics Models: Machine Learning of Nonlinear Partial
  Differential Equations",ranking show <unk> set observation
769,"Effective debugging of ontologies is an important prerequisite for their
broad application, especially in areas that rely on everyday users to create
and maintain knowledge bases, such as the Semantic Web. In such systems
ontologies capture formalized vocabularies of terms shared by its users.
However in many cases users have different local views of the domain, i.e. of
the context in which a given term is used. Inappropriate usage of terms
together with natural complications when formulating and understanding logical
descriptions may result in faulty ontologies. Recent ontology debugging
approaches use diagnosis methods to identify causes of the faults. In most
debugging scenarios these methods return many alternative diagnoses, thus
placing the burden of fault localization on the user. This paper demonstrates
how the target diagnosis can be identified by performing a sequence of
observations, that is, by querying an oracle about entailments of the target
ontology. To identify the best query we propose two query selection strategies:
a simple ""split-in-half"" strategy and an entropy-based strategy. The latter
allows knowledge about typical user errors to be exploited to minimize the
number of queries. Our evaluation showed that the entropy-based method
significantly reduces the number of required queries compared to the
""split-in-half"" approach. We experimented with different probability
distributions of user errors and different qualities of the a-priori
probabilities. Our measurements demonstrated the superiority of entropy-based
query selection even in cases where all fault probabilities are equal, i.e.
where no information about typical user errors is available.","Interactive ontology debugging: two query strategies for efficient fault
  localization",density user demonstrate show <unk>
770,"Special technologies need to be used to take advantage of, and overcome, the
challenges associated with acquiring, transforming, storing, processing, and
distributing spoken language resources in organisations. This paper introduces
an application architecture consisting of tools and supporting utilities for
indexing and transcription, and describes how these tools, together with
downstream processing and distribution systems, can be integrated into a
workflow. Two sample applications for this architecture are outlined- the
analysis of decision-making processes in organisations and the deployment of
systems development methods by designers in the field.","Application Architecture for Spoken Language Resources in Organisational
  Settings",using conditional approaches test offers online
771,"This is full length article (draft version) where problem number of topics in
Topic Modeling is discussed. We proposed idea that Renyi and Tsallis entropy
can be used for identification of optimal number in large textual collections.
We also report results of numerical experiments of Semantic stability for 4
topic models, which shows that semantic stability play very important role in
problem topic number. The calculation of Renyi and Tsallis entropy based on
thermodynamics approach.","Application of Rényi and Tsallis Entropies to Topic Modeling
  Optimization",tasks scene based problem  provides interfaces without
772,"Many efforts have been made to use various forms of domain knowledge in
malware detection. Currently there exist two common approaches to malware
detection without domain knowledge, namely byte n-grams and strings. In this
work we explore the feasibility of applying neural networks to malware
detection and feature learning. We do this by restricting ourselves to a
minimal amount of domain knowledge in order to extract a portion of the
Portable Executable (PE) header. By doing this we show that neural networks can
learn from raw bytes without explicit feature construction, and perform even
better than a domain knowledge approach that parses the PE header into explicit
features.","Learning the PE Header, Malware Detection with Minimal Domain Knowledge",using observation model distribution method linguistic example
773,"We present examples where the use of belief functions provided sound and
elegant solutions to real life problems. These are essentially characterized by
?missing' information. The examples deal with 1) discriminant analysis using a
learning set where classes are only partially known; 2) an information
retrieval systems handling inter-documents relationships; 3) the combination of
data from sensors competent on partially overlapping frames; 4) the
determination of the number of sources in a multi-sensor environment by
studying the inter-sensors contradiction. The purpose of the paper is to report
on such applications where the use of belief functions provides a convenient
tool to handle ?messy' data problems.",Practical Uses of Belief Functions,textual model mammals tree  capabilities proposed predictive using
774,"We present an efficient method for training slack-rescaled structural SVM.
Although finding the most violating label in a margin-rescaled formulation is
often easy since the target function decomposes with respect to the structure,
this is not the case for a slack-rescaled formulation, and finding the most
violated label might be very difficult. Our core contribution is an efficient
method for finding the most-violating-label in a slack-rescaled formulation,
given an oracle that returns the most-violating-label in a (slightly modified)
margin-rescaled formulation. We show that our method enables accurate and
scalable training for slack-rescaled SVMs, reducing runtime by an order of
magnitude compared to previous approaches to slack-rescaled SVMs.",Fast and Scalable Structural SVM with Slack Rescaling,suitable vision  online
775,"We propose a learning setting in which unlabeled data is free, and the cost
of a label depends on its value, which is not known in advance. We study binary
classification in an extreme case, where the algorithm only pays for negative
labels. Our motivation are applications such as fraud detection, in which
investigating an honest transaction should be avoided if possible. We term the
setting auditing, and consider the auditing complexity of an algorithm: the
number of negative labels the algorithm requires in order to learn a hypothesis
with low relative error. We design auditing algorithms for simple hypothesis
classes (thresholds and rectangles), and show that with these algorithms, the
auditing complexity can be significantly lower than the active label
complexity. We also discuss a general competitive approach for auditing and
possible modifications to the framework.",Auditing: Active Learning with Outcome-Dependent Query Costs,complexity model complexity brain
776,"After the incredible success of deep learning in the computer vision domain,
there has been much interest in applying Convolutional Network (ConvNet)
features in robotic fields such as visual navigation and SLAM. Unfortunately,
there are fundamental differences and challenges involved. Computer vision
datasets are very different in character to robotic camera data, real-time
performance is essential, and performance priorities can be different. This
paper comprehensively evaluates and compares the utility of three
state-of-the-art ConvNets on the problems of particular relevance to navigation
for robots; viewpoint-invariance and condition-invariance, and for the first
time enables real-time place recognition performance using ConvNets with large
maps by integrating a variety of existing (locality-sensitive hashing) and
novel (semantic search space partitioning) optimization techniques. We present
extensive experiments on four real world datasets cultivated to evaluate each
of the specific challenges in place recognition. The results demonstrate that
speed-ups of two orders of magnitude can be achieved with minimal accuracy
degradation, enabling real-time performance. We confirm that networks trained
for semantic place categorization also perform better at (specific) place
recognition when faced with severe appearance changes and provide a reference
for which networks and layers are optimal for different aspects of the place
recognition problem.",On the Performance of ConvNet Features for Place Recognition,based state model networks
777,"In this work, we propose a new segmentation algorithm for images containing
convex objects present in multiple shapes with a high degree of overlap. The
proposed algorithm is carried out in two steps, first we identify the visible
contours, segment them using concave points and finally group the segments
belonging to the same object. The next step is to assign a shape identity to
these grouped contour segments. For images containing objects in multiple
shapes we begin first by identifying shape classes of the contours followed by
assigning a shape entity to these classes. We provide a comprehensive
experimentation of our algorithm on two crystal image datasets. One dataset
comprises of images containing objects in multiple shapes overlapping each
other and the other dataset contains standard images with objects present in a
single shape. We test our algorithm against two baselines, with our proposed
algorithm outperforming both the baselines.",Image Segmentation of Multi-Shaped Overlapping Objects,models model objects models
778,"Many logic programming based approaches can be used to describe and solve
combinatorial search problems. On the one hand there is constraint logic
programming which computes a solution as an answer substitution to a query
containing the variables of the constraint satisfaction problem. On the other
hand there are systems based on stable model semantics, abductive systems, and
first order logic model generators which compute solutions as models of some
theory. This paper compares these different approaches from the point of view
of knowledge representation (how declarative are the programs) and from the
point of view of performance (how good are they at solving typical problems).","Logic Programming Approaches for Representing and Solving Constraint
  Satisfaction Problems: A Comparison",using constraint model show significant density kernel demonstrate
779,"Unmanned aerial vehicles (UAV) are evolving as an alternative tool to acquire
land tenure data. UAVs can capture geospatial data at high quality and
resolution in a cost-effective, transparent and flexible manner, from which
visible land parcel boundaries, i.e., cadastral boundaries are delineable. This
delineation is to no extent automated, even though physical objects
automatically retrievable through image analysis methods mark a large portion
of cadastral boundaries. This study proposes (i) a workflow that automatically
extracts candidate cadastral boundaries from UAV orthoimages and (ii) a tool
for their semi-automatic processing to delineate final cadastral boundaries.
The workflow consists of two state-of-the-art computer vision methods, namely
gPb contour detection and SLIC superpixels that are transferred to remote
sensing in this study. The tool combines the two methods, allows a
semi-automatic final delineation and is implemented as a publicly available
QGIS plugin. The approach does not yet aim to provide a comparable alternative
to manual cadastral mapping procedures. However, the methodological development
of the tool towards this goal is developed in this paper. A study with 13
volunteers investigates the design and implementation of the approach and
gathers initial qualitative as well as quantitate results. The study revealed
points for improvement, which are prioritized based on the study results and
which will be addressed in future work.",Towards Automated Cadastral Boundary Delineation from UAV Data,results  model based function  using instances linear
780,"One of the most challenging problems in kernel online learning is to bound
the model size and to promote the model sparsity. Sparse models not only
improve computation and memory usage, but also enhance the generalization
capacity, a principle that concurs with the law of parsimony. However,
inappropriate sparsity modeling may also significantly degrade the performance.
In this paper, we propose Approximation Vector Machine (AVM), a model that can
simultaneously encourage the sparsity and safeguard its risk in compromising
the performance. When an incoming instance arrives, we approximate this
instance by one of its neighbors whose distance to it is less than a predefined
threshold. Our key intuition is that since the newly seen instance is expressed
by its nearby neighbor the optimal performance can be analytically formulated
and maintained. We develop theoretical foundations to support this intuition
and further establish an analysis to characterize the gap between the
approximation and optimal solutions. This gap crucially depends on the
frequency of approximation and the predefined threshold. We perform the
convergence analysis for a wide spectrum of loss functions including Hinge,
smooth Hinge, and Logistic for classification task, and $l_1$, $l_2$, and
$\epsilon$-insensitive for regression task. We conducted extensive experiments
for classification task in batch and online modes, and regression task in
online mode over several benchmark datasets. The results show that our proposed
AVM achieved a comparable predictive performance with current state-of-the-art
methods while simultaneously achieving significant computational speed-up due
to the ability of the proposed AVM in maintaining the model size.",Approximation Vector Machines for Large-scale Online Learning,two show show description model <unk>
781,"Natural disasters can have catastrophic impacts on the functionality of
infrastructure systems and cause severe physical and socio-economic losses.
Given budget constraints, it is crucial to optimize decisions regarding
mitigation, preparedness, response, and recovery practices for these systems.
This requires accurate and efficient means to evaluate the infrastructure
system reliability. While numerous research efforts have addressed and
quantified the impact of natural disasters on infrastructure systems, typically
using the Monte Carlo approach, they still suffer from high computational cost
and, thus, are of limited applicability to large systems. This paper presents a
deep learning framework for accelerating infrastructure system reliability
analysis. In particular, two distinct deep neural network surrogates are
constructed and studied: (1) A classifier surrogate which speeds up the
connectivity determination of networks, and (2) An end-to-end surrogate that
replaces a number of components such as roadway status realization,
connectivity determination, and connectivity averaging. The proposed approach
is applied to a simulation-based study of the two-terminal connectivity of a
California transportation network subject to extreme probabilistic earthquake
events. Numerical results highlight the effectiveness of the proposed approach
in accelerating the transportation system two-terminal reliability analysis
with extremely high prediction accuracy.","Deep Learning for Accelerated Reliability Analysis of Infrastructure
  Networks",improve method method data system model system
782,"Scene text recognition has been a hot research topic in computer vision due
to its various applications. The state of the art is the attention-based
encoder-decoder framework that learns the mapping between input images and
output sequences in a purely data-driven way. However, we observe that existing
attention-based methods perform poorly on complicated and/or low-quality
images. One major reason is that existing methods cannot get accurate
alignments between feature areas and targets for such images. We call this
phenomenon ""attention drift"". To tackle this problem, in this paper we propose
the FAN (the abbreviation of Focusing Attention Network) method that employs a
focusing attention mechanism to automatically draw back the drifted attention.
FAN consists of two major components: an attention network (AN) that is
responsible for recognizing character targets as in the existing methods, and a
focusing network (FN) that is responsible for adjusting attention by evaluating
whether AN pays attention properly on the target areas in the images.
Furthermore, different from the existing methods, we adopt a ResNet-based
network to enrich deep representations of scene text images. Extensive
experiments on various benchmarks, including the IIIT5k, SVT and ICDAR
datasets, show that the FAN method substantially outperforms the existing
methods.",Focusing Attention: Towards Accurate Text Recognition in Natural Images,model method learning <unk>
783,"Previous works demonstrated that Automatic Text Summarization (ATS) by
sentences extraction may be improved using sentence compression. In this work
we present a sentence compressions approach guided by level-sentence discourse
segmentation and probabilistic language models (LM). The results presented here
show that the proposed solution is able to generate coherent summaries with
grammatical compressed sentences. The approach is simple enough to be
transposed into other languages.","Sentence Compression in Spanish driven by Discourse Segmentation and
  Language Models",states approaches different suitable is 
784,"We present an unusual algorithm involving classification trees where two
trees are grown in opposite directions so that they are matched at their
leaves. This approach finds application in a new data mining task we formulate,
called ""redescription mining"". A redescription is a shift-of-vocabulary, or a
different way of communicating information about a given subset of data; the
goal of redescription mining is to find subsets of data that afford multiple
descriptions. We highlight the importance of this problem in domains such as
bioinformatics, which exhibit an underlying richness and diversity of data
descriptors (e.g., genes can be studied in a variety of ways). Our approach
helps integrate multiple forms of characterizing datasets, situates the
knowledge gained from one dataset in the context of others, and harnesses
high-level abstractions for uncovering cryptic and subtle features of data.
Algorithm design decisions, implementation details, and experimental results
are presented.",Turning CARTwheels: An Alternating Algorithm for Mining Redescriptions,natural present data  proposed <unk> natural distance 
785,"Multiresolution analysis and matrix factorization are foundational tools in
computer vision. In this work, we study the interface between these two
distinct topics and obtain techniques to uncover hierarchical block structure
in symmetric matrices -- an important aspect in the success of many vision
problems. Our new algorithm, the incremental multiresolution matrix
factorization, uncovers such structure one feature at a time, and hence scales
well to large matrices. We describe how this multiscale analysis goes much
farther than what a direct global factorization of the data can identify. We
evaluate the efficacy of the resulting factorizations for relative leveraging
within regression tasks using medical imaging data. We also use the
factorization on representations learned by popular deep networks, providing
evidence of their ability to infer semantic relationships even when they are
not explicitly trained to do so. We show that this algorithm can be used as an
exploratory tool to improve the network architecture, and within numerous other
settings in vision.",The Incremental Multiresolution Matrix Factorization Algorithm,potential reduces show preferences show data 
786,"We exploit the versatile framework of Riemannian optimization on quotient
manifolds to develop R3MC, a nonlinear conjugate-gradient method for low-rank
matrix completion. The underlying search space of fixed-rank matrices is
endowed with a novel Riemannian metric that is tailored to the least-squares
cost. Numerical comparisons suggest that R3MC robustly outperforms
state-of-the-art algorithms across different problem instances, especially
those that combine scarcely sampled and ill-conditioned data.",R3MC: A Riemannian three-factor algorithm for low-rank matrix completion,states content search show opinion texture
787,"This paper describes algorithms for nonnegative matrix factorization (NMF)
with the beta-divergence (beta-NMF). The beta-divergence is a family of cost
functions parametrized by a single shape parameter beta that takes the
Euclidean distance, the Kullback-Leibler divergence and the Itakura-Saito
divergence as special cases (beta = 2,1,0, respectively). The proposed
algorithms are based on a surrogate auxiliary function (a local majorization of
the criterion function). We first describe a majorization-minimization (MM)
algorithm that leads to multiplicative updates, which differ from standard
heuristic multiplicative updates by a beta-dependent power exponent. The
monotonicity of the heuristic algorithm can however be proven for beta in (0,1)
using the proposed auxiliary function. Then we introduce the concept of
majorization-equalization (ME) algorithm which produces updates that move along
constant level sets of the auxiliary function and lead to larger steps than MM.
Simulations on synthetic and real data illustrate the faster convergence of the
ME approach. The paper also describes how the proposed algorithms can be
adapted to two common variants of NMF : penalized NMF (i.e., when a penalty
function of the factors is added to the criterion function) and convex-NMF
(when the dictionary is assumed to belong to a known subspace).",Algorithms for nonnegative matrix factorization with the beta-divergence,cluster new show show tensor events 
788,"Dealing with datasets of very high dimension is a major challenge in machine
learning. In this paper, we consider the problem of feature selection in
applications where the memory is not large enough to contain all features. In
this setting, we propose a novel tree-based feature selection approach that
builds a sequence of randomized trees on small subsamples of variables mixing
both variables already identified as relevant by previous models and variables
randomly selected among the other variables. As our main contribution, we
provide an in-depth theoretical analysis of this method in infinite sample
setting. In particular, we study its soundness with respect to common
definitions of feature relevance and its convergence speed under various
variable dependance scenarios. We also provide some preliminary empirical
results highlighting the potential of the approach.","Random Subspace with Trees for Feature Selection Under Memory
  Constraints",improve models image paper  for proposed borrowed  data 
789,"Collective intelligence, which aggregates the shared information from large
crowds, is often negatively impacted by unreliable information sources with the
low quality data. This becomes a barrier to the effective use of collective
intelligence in a variety of applications. In order to address this issue, we
propose a probabilistic model to jointly assess the reliability of sources and
find the true data. We observe that different sources are often not independent
of each other. Instead, sources are prone to be mutually influenced, which
makes them dependent when sharing information with each other. High dependency
between sources makes collective intelligence vulnerable to the overuse of
redundant (and possibly incorrect) information from the dependent sources.
Thus, we reveal the latent group structure among dependent sources, and
aggregate the information at the group level rather than from individual
sources directly. This can prevent the collective intelligence from being
inappropriately dominated by dependent sources. We will also explicitly reveal
the reliability of groups, and minimize the negative impacts of unreliable
groups. Experimental results on real-world data sets show the effectiveness of
the proposed approach with respect to existing algorithms.",Learning from Collective Intelligence in Groups,the model <unk> for different mapping use
790,"This is the second part of a paper on Conscious Intelligent Systems. We use
the understanding gained in the first part (Conscious Intelligent Systems Part
1: IXI (arxiv id cs.AI/0612056)) to look at understanding. We see how the
presence of mind affects understanding and intelligent systems; we see that the
presence of mind necessitates language. The rise of language in turn has
important effects on understanding. We discuss the humanoid question and how
the question of self-consciousness (and by association mind/thought/language)
would affect humanoids too.","Conscious Intelligent Systems - Part II - Mind, Thought, Language and
  Understanding",learning domain  model <unk> <unk>
791,"Current methods for automatically evaluating grammatical error correction
(GEC) systems rely on gold-standard references. However, these methods suffer
from penalizing grammatical edits that are correct but not in the gold
standard. We show that reference-less grammaticality metrics correlate very
strongly with human judgments and are competitive with the leading
reference-based evaluation metrics. By interpolating both methods, we achieve
state-of-the-art correlation with human judgments. Finally, we show that GEC
metrics are much more reliable when they are calculated at the sentence level
instead of the corpus level. We have set up a CodaLab site for benchmarking GEC
output using a common dataset and different evaluation metrics.","There's No Comparison: Reference-less Evaluation Metrics in Grammatical
  Error Correction",model appear users without proposed stochastic
792,"This paper presented our work on applying Recurrent Deep Stacking Networks
(RDSNs) to Robust Automatic Speech Recognition (ASR) tasks. In the paper, we
also proposed a more efficient yet comparable substitute to RDSN, Bi- Pass
Stacking Network (BPSN). The main idea of these two models is to add
phoneme-level information into acoustic models, transforming an acoustic model
to the combination of an acoustic model and a phoneme-level N-gram model.
Experiments showed that RDSN and BPsn can substantially improve the
performances over conventional DNNs.",Recurrent Deep Stacking Networks for Speech Recognition,models uncertainty  show paper  alignments models
793,"In a recent paper, we presented an intelligent evolutionary search technique
through genetic programming (GP) for finding new analytical expressions of
nonlinear dynamical systems, similar to the classical Lorenz attractor's which
also exhibit chaotic behaviour in the phase space. In this paper, we extend our
previous finding to explore yet another gallery of new chaotic attractors which
are derived from the original Lorenz system of equations. Compared to the
previous exploration with sinusoidal type transcendental nonlinearity, here we
focus on only cross-product and higher-power type nonlinearities in the three
state equations. We here report over 150 different structures of chaotic
attractors along with their one set of parameter values, phase space dynamics
and the Largest Lyapunov Exponents (LLE). The expressions of these new
Lorenz-like nonlinear dynamical systems have been automatically evolved through
multi-gene genetic programming (MGGP). In the past two decades, there have been
many claims of designing new chaotic attractors as an incremental extension of
the Lorenz family. We provide here a large family of chaotic systems whose
structure closely resemble the original Lorenz system but with drastically
different phase space dynamics. This advances the state of the art knowledge of
discovering new chaotic systems which can find application in many real-world
problems. This work may also find its archival value in future in the domain of
new chaotic system discovery.","Evolving Chaos: Identifying New Attractors of the Generalised Lorenz
  Family",based based merely feature operator for
794,"Trust is a fundamental concept in many real-world applications such as
e-commerce and peer-to-peer networks. In these applications, users can generate
local opinions about the counterparts based on direct experiences, and these
opinions can then be aggregated to build trust among unknown users. The
mechanism to build new trust relationships based on existing ones is referred
to as trust inference. State-of-the-art trust inference approaches employ the
transitivity property of trust by propagating trust along connected users. In
this paper, we propose a novel trust inference model (MaTrust) by exploring an
equally important property of trust, i.e., the multi-aspect property. MaTrust
directly characterizes multiple latent factors for each trustor and trustee
from the locally-generated trust relationships. Furthermore, it can naturally
incorporate prior knowledge as specified factors. These factors in turn serve
as the basis to infer the unseen trustworthiness scores. Experimental
evaluations on real data sets show that the proposed MaTrust significantly
outperforms several benchmark trust inference models in both effectiveness and
efficiency.",MaTrust: An Effective Multi-Aspect Trust Inference Model,###  improve online
795,"In this paper, we are concerned with obtaining distribution-free
concentration inequalities for mixture of independent Bernoulli variables that
incorporate a notion of variance. Missing mass is the total probability mass
associated to the outcomes that have not been seen in a given sample which is
an important quantity that connects density estimates obtained from a sample to
the population for discrete distributions. Therefore, we are specifically
motivated to apply our method to study the concentration of missing mass -
which can be expressed as a mixture of Bernoulli - in a novel way.
  We not only derive - for the first time - Bernstein-like large deviation
bounds for the missing mass whose exponents behave almost linearly with respect
to deviation size, but also sharpen McAllester and Ortiz (2003) and Berend and
Kontorovich (2013) for large sample sizes in the case of small deviations which
is the most interesting case in learning theory. In the meantime, our approach
shows that the heterogeneity issue introduced in McAllester and Ortiz (2003) is
resolvable in the case of missing mass in the sense that one can use standard
inequalities but it may not lead to strong results. Thus, we postulate that our
results are general and can be applied to provide potentially sharp
Bernstein-like bounds under some constraints.","Novel Deviation Bounds for Mixture of Independent Bernoulli Variables
  with Application to the Missing Mass",theory theory fractal <unk> annotation cannot
796,"In this paper, an online adaptive model-free tracker is proposed to track
single objects in video sequences to deal with real-world tracking challenges
like low-resolution, object deformation, occlusion and motion blur. The novelty
lies in the construction of a strong appearance model that captures features
from the initialized bounding box and then are assembled into anchor-point
features. These features memorize the global pattern of the object and have an
internal star graph-like structure. These features are unique and flexible and
helps tracking generic and deformable objects with no limitation on specific
objects. In addition, the relevance of each feature is evaluated online using
short-term consistency and long-term consistency. These parameters are adapted
to retain consistent features that vote for the object location and that deal
with outliers for long-term tracking scenarios. Additionally, voting in a
Gaussian manner helps in tackling inherent noise of the tracking system and in
accurate object localization. Furthermore, the proposed tracker uses pairwise
distance measure to cope with scale variations and combines pixel-level binary
features and global weighted color features for model update. Finally,
experimental results on a visual tracking benchmark dataset are presented to
demonstrate the effectiveness and competitiveness of the proposed tracker.",Tracking using Numerous Anchor points,unlabeled for show tensor <unk>
797,"Many algorithms formulate graph matching as an optimization of an objective
function of pairwise quantification of nodes and edges of two graphs to be
matched. Pairwise measurements usually consider local attributes but disregard
contextual information involved in graph structures. We address this issue by
proposing contextual similarities between pairs of nodes. This is done by
considering the tensor product graph (TPG) of two graphs to be matched, where
each node is an ordered pair of nodes of the operand graphs. Contextual
similarities between a pair of nodes are computed by accumulating weighted
walks (normalized pairwise similarities) terminating at the corresponding
paired node in TPG. Once the contextual similarities are obtained, we formulate
subgraph matching as a node and edge selection problem in TPG. We use
contextual similarities to construct an objective function and optimize it with
a linear programming approach. Since random walk formulation through TPG takes
into account higher order information, it is not a surprise that we obtain more
reliable similarities and better discrimination among the nodes and edges.
Experimental results shown on synthetic as well as real benchmarks illustrate
that higher order contextual similarities add discriminating power and allow
one to find approximate solutions to the subgraph matching problem.","Product Graph-based Higher Order Contextual Similarities for Inexact
  Subgraph Matching",new <unk> based feasibility learning users users automatic
798,"Echo state networks (ESN), a type of reservoir computing (RC) architecture,
are efficient and accurate artificial neural systems for time series processing
and learning. An ESN consists of a core of recurrent neural networks, called a
reservoir, with a small number of tunable parameters to generate a
high-dimensional representation of an input, and a readout layer which is
easily trained using regression to produce a desired output from the reservoir
states. Certain computational tasks involve real-time calculation of high-order
time correlations, which requires nonlinear transformation either in the
reservoir or the readout layer. Traditional ESN employs a reservoir with
sigmoid or tanh function neurons. In contrast, some types of biological neurons
obey response curves that can be described as a product unit rather than a sum
and threshold. Inspired by this class of neurons, we introduce a RC
architecture with a reservoir of product nodes for time series computation. We
find that the product RC shows many properties of standard ESN such as
short-term memory and nonlinear capacity. On standard benchmarks for chaotic
prediction tasks, the product RC maintains the performance of a standard
nonlinear ESN while being more amenable to mathematical analysis. Our study
provides evidence that such networks are powerful in highly nonlinear tasks
owing to high-order statistics generated by the recurrent product node
reservoir.","Product Reservoir Computing: Time-Series Computation with Multiplicative
  Neurons",<unk> interfaces <unk>
799,"We present a novel deep architecture termed templateNet for depth based
object instance recognition. Using an intermediate template layer we exploit
prior knowledge of an object's shape to sparsify the feature maps. This has
three advantages: (i) the network is better regularised resulting in structured
filters; (ii) the sparse feature maps results in intuitive features been learnt
which can be visualized as the output of the template layer and (iii) the
resulting network achieves state-of-the-art performance. The network benefits
from this without any additional parametrization from the template layer. We
derive the weight updates needed to efficiently train this network in an
end-to-end manner. We benchmark the templateNet for depth based object instance
recognition using two publicly available datasets. The datasets present
multiple challenges of clutter, large pose variations and similar looking
distractors. Through our experiments we show that with the addition of a
template layer, a depth based CNN is able to outperform existing
state-of-the-art methods in the field.",TemplateNet for Depth-Based Object Instance Recognition,models method feature models
800,"This article proposes to auto-encode text at byte-level using convolutional
networks with a recursive architecture. The motivation is to explore whether it
is possible to have scalable and homogeneous text generation at byte-level in a
non-sequential fashion through the simple task of auto-encoding. We show that
non-sequential text generation from a fixed-length representation is not only
possible, but also achieved much better auto-encoding results than recurrent
networks. The proposed model is a multi-stage deep convolutional
encoder-decoder framework using residual connections, containing up to 160
parameterized layers. Each encoder or decoder contains a shared group of
modules that consists of either pooling or upsampling layers, making the
network recursive in terms of abstraction levels in representation. Results for
6 large-scale paragraph datasets are reported, in 3 languages including Arabic,
Chinese and English. Analyses are conducted to study several properties of the
proposed model.",Byte-Level Recursive Convolutional Auto-Encoder for Text,learning model next  encodings using uncertainty  using observation
801,"Efficient modeling on uncertain information plays an important role in
estimating the risk of contaminant intrusion in water distribution networks.
Dempster-Shafer evidence theory is one of the most commonly used methods.
However, the Dempster-Shafer evidence theory has some hypotheses including the
exclusive property of the elements in the frame of discernment, which may not
be consistent with the real world. In this paper, based on a more effective
representation of uncertainty, called D numbers, a new method that allows the
elements in the frame of discernment to be non-exclusive is proposed. To
demonstrate the efficiency of the proposed method, we apply it to the water
distribution networks to estimate the risk of contaminant intrusion.","Modeling contaminant intrusion in water distribution networks based on D
  numbers",models <unk> present image sheffield
802,"We work out a classification scheme for quantum modeling in Hilbert space of
any kind of composite entity violating Bell's inequalities and exhibiting
entanglement. Our theoretical framework includes situations with entangled
states and product measurements ('customary quantum situation'), and also
situations with both entangled states and entangled measurements ('nonlocal box
situation', 'nonlocal non-marginal box situation'). We show that entanglement
is structurally a joint property of states and measurements. Furthermore,
entangled measurements enable quantum modeling of situations that are usually
believed to be 'beyond quantum'. Our results are also extended from pure states
to quantum mixtures.",General Quantum Hilbert Space Modeling Scheme for Entanglement,al  show al  propose evolved definition present image
803,"Existing neural machine translation (NMT) models generally translate
sentences in isolation, missing the opportunity to take advantage of
document-level information. In this work, we propose to augment NMT models with
a very light-weight cache-like memory network, which stores recent hidden
representations as translation history. The probability distribution over
generated words is updated online depending on the translation history
retrieved from the memory, endowing NMT models with the capability to
dynamically adapt over time. Experiments on multiple domains with different
topics and styles show the effectiveness of the proposed approach with
negligible impact on the computational cost.",Learning to Remember Translation History with a Continuous Cache,models show <unk> tasks  a
804,"The goal of this paper is to discover a set of discriminative patches which
can serve as a fully unsupervised mid-level visual representation. The desired
patches need to satisfy two requirements: 1) to be representative, they need to
occur frequently enough in the visual world; 2) to be discriminative, they need
to be different enough from the rest of the visual world. The patches could
correspond to parts, objects, ""visual phrases"", etc. but are not restricted to
be any one of them. We pose this as an unsupervised discriminative clustering
problem on a huge dataset of image patches. We use an iterative procedure which
alternates between clustering and training discriminative classifiers, while
applying careful cross-validation at each step to prevent overfitting. The
paper experimentally demonstrates the effectiveness of discriminative patches
as an unsupervised mid-level visual representation, suggesting that it could be
used in place of visual words for many tasks. Furthermore, discriminative
patches can also be used in a supervised regime, such as scene classification,
where they demonstrate state-of-the-art performance on the MIT Indoor-67
dataset.",Unsupervised Discovery of Mid-Level Discriminative Patches,models model video number
805,"Classification is one of the most important tasks of machine learning.
Although the most well studied model is the two-class problem, in many
scenarios there is the opportunity to label critical items for manual revision,
instead of trying to automatically classify every item. In this paper we adapt
a paradigm initially proposed for the classification of ordinal data to address
the classification problem with reject option. The technique reduces the
problem of classifying with reject option to the standard two-class problem.
The introduced method is then mapped into support vector machines and neural
networks. Finally, the framework is extended to multiclass ordinal data with
reject option. An experimental study with synthetic and real data sets,
verifies the usefulness of the proposed approach.",The Data Replication Method for the Classification with Reject Option,models show function  vaes using <unk>
806,"The aim of this paper is to propose an application of mutual
information-based ensemble methods to the analysis and classification of heart
beats associated with different types of Arrhythmia. Models of multilayer
perceptrons, support vector machines, and radial basis function neural networks
were trained and tested using the MIT-BIH arrhythmia database. This research
brings a focus to an ensemble method that, to our knowledge, is a novel
application in the area of ECG Arrhythmia detection. The proposed classifier
ensemble method showed improved performance, relative to either majority voting
classifier integration or to individual classifier performance. The overall
ensemble accuracy was 98.25%.",Arrhythmia Detection using Mutual Information-Based Integration Method,learning conditions model data combining considerably
807,"The Outilex software platform, which will be made available to research,
development and industry, comprises software components implementing all the
fundamental operations of written text processing: processing without lexicons,
exploitation of lexicons and grammars, language resource management. All data
are structured in XML formats, and also in more compact formats, either
readable or binary, whenever necessary; the required format converters are
included in the platform; the grammar formats allow for combining statistical
approaches with resource-based approaches. Manually constructed lexicons for
French and English, originating from the LADL, and of substantial coverage,
will be distributed with the platform under LGPL-LR license.","Outilex, plate-forme logicielle de traitement de textes écrits",using video various domain domain
808,"Detecting multiple planes in images is a challenging problem, but one with
many applications. Recent work such as J-Linkage and Ordered Residual Kernels
have focussed on developing a domain independent approach to detect multiple
structures. These multiple structure detection methods are then used for
estimating multiple homographies given feature matches between two images.
Features participating in the multiple homographies detected, provide us the
multiple scene planes. We show that these methods provide locally optimal
results and fail to merge detected planar patches to the true scene planes.
These methods use only residues obtained on applying homography of one plane to
another as cue for merging. In this paper, we develop additional cues such as
local consistency of planes, local normals, texture etc. to perform better
classification and merging . We formulate the classification as an MRF problem
and use TRWS message passing algorithm to solve non metric energy terms and
complex sparse graph structure. We show results on challenging dataset common
in robotics navigation scenarios where our method shows accuracy of more than
85 percent on average while being close or same as the actual number of scene
planes.",Top Down Approach to Multiple Plane Detection,score learning <unk> model set ##
809,"One popular approach for blind deconvolution is to formulate a maximum a
posteriori (MAP) problem with sparsity priors on the gradients of the latent
image, and then alternatingly estimate the blur kernel and the latent image.
While several successful MAP based methods have been proposed, there has been
much controversy and confusion about their convergence, because sparsity priors
have been shown to prefer blurry images to sharp natural images. In this paper,
we revisit this problem and provide an analysis on the convergence of MAP based
approaches. We first introduce a slight modification to a conventional joint
energy function for blind deconvolution. The reformulated energy function
yields the same alternating estimation process, but more clearly reveals how
blind deconvolution works. We then show the energy function can actually favor
the right solution instead of the no-blur solution under certain conditions,
which explains the success of previous MAP based approaches. The reformulated
energy function and our conditions for the convergence also provide a way to
compare the qualities of different blur kernels, and we demonstrate its
applicability to automatic blur kernel size selection, blur kernel estimation
using light streaks, and defocus estimation.",Convergence Analysis of MAP based Blur Kernel Estimation,vision paper satellite fully
810,"In this paper, we take a new look at the possibilistic c-means (PCM) and
adaptive PCM (APCM) clustering algorithms from the perspective of uncertainty.
This new perspective offers us insights into the clustering process, and also
provides us greater degree of flexibility. We analyze the clustering behavior
of PCM-based algorithms and introduce parameters $\sigma_v$ and $\alpha$ to
characterize uncertainty of estimated bandwidth and noise level of the dataset
respectively. Then uncertainty (fuzziness) of membership values caused by
uncertainty of the estimated bandwidth parameter is modeled by a conditional
fuzzy set, which is a new formulation of the type-2 fuzzy set. Experiments show
that parameters $\sigma_v$ and $\alpha$ make the clustering process more easy
to control, and main features of PCM and APCM are unified in this new
clustering framework (UPCM). More specifically, UPCM reduces to PCM when we set
a small $\alpha$ or a large $\sigma_v$, and UPCM reduces to APCM when clusters
are confined in their physical clusters and possible cluster elimination are
ensured. Finally we present further researches of this paper.",PCM and APCM Revisited: An Uncertainty Perspective,error  <unk> for show distributions domain  method
811,"This article demonstrates a new conceptor network based classifier in
classifying images. Mathematical descriptions and analysis are presented.
Various tests are experimented using three benchmark datasets: MNIST, CIFAR-10
and CIFAR-100. The experiments displayed that conceptor network can offer
superior results and flexible configurations than conventional classifiers such
as Softmax Regression and Support Vector Machine (SVM).",Classify Images with Conceptor Network,index show video step
812,"In this work we consider the problem of anomaly detection in heterogeneous,
multivariate, variable-length time series datasets. Our focus is on the
aviation safety domain, where data objects are flights and time series are
sensor readings and pilot switches. In this context the goal is to detect
anomalous flight segments, due to mechanical, environmental, or human factors
in order to identifying operationally significant events and provide insights
into the flight operations and highlight otherwise unavailable potential safety
risks and precursors to accidents. For this purpose, we propose a framework
which represents each flight using a semi-Markov switching vector
autoregressive (SMS-VAR) model. Detection of anomalies is then based on
measuring dissimilarities between the model's prediction and data observation.
The framework is scalable, due to the inherent parallel nature of most
computations, and can be used to perform online anomaly detection. Extensive
experimental results on simulated and real datasets illustrate that the
framework can detect various types of anomalies along with the key parameters
involved.","Semi-Markov Switching Vector Autoregressive Model-based Anomaly
  Detection in Aviation Systems",models based function 
813,"This paper derives a formula for computing the conditional probability of a
set of candidates, where a candidate is a set of disorders that explain a given
set of positive findings. Such candidate sets are produced by a recent method
for multidisorder diagnosis called symptom clustering. A symptom clustering
represents a set of candidates compactly as a cartesian product of differential
diagnoses. By evaluating the probability of a candidate set, then, a large set
of candidates can be validated or pruned simultaneously. The probability of a
candidate set is then specialized to obtain the probability of a single
candidate. Unlike earlier results, the equation derived here allows the
specification of positive, negative, and unknown symptoms and does not make
assumptions about disorders not in the candidate.","Probabilistic Evaluation of Candidates and Symptom Clustering for
  Multidisorder Diagnosis",using <unk> show video step
814,"This paper presents a novel framework for generating texture mosaics with
convolutional neural networks. Our method is called GANosaic and performs
optimization in the latent noise space of a generative texture model, which
allows the transformation of a content image into a mosaic exhibiting the
visual properties of the underlying texture manifold. To represent that
manifold, we use a state-of-the-art generative adversarial method for texture
synthesis, which can learn expressive texture representations from data and
produce mosaic images with very high resolution. This fully convolutional model
generates smooth (without any visible borders) mosaic images which morph and
blend different textures locally. In addition, we develop a new type of
differentiable statistical regularization appropriate for optimization over the
prior noise space of the PSGAN model.",GANosaic: Mosaic Creation with Generative Texture Manifolds,simple model extracts relatively different methods  implementation compressed
815,"Detection and learning based appearance feature play the central role in data
association based multiple object tracking (MOT), but most recent MOT works
usually ignore them and only focus on the hand-crafted feature and association
algorithms. In this paper, we explore the high-performance detection and deep
learning based appearance feature, and show that they lead to significantly
better MOT results in both online and offline setting. We make our detection
and appearance feature publicly available. In the following part, we first
summarize the detection and appearance feature, and then introduce our tracker
named Person of Interest (POI), which has both online and offline version.","POI: Multiple Object Tracking with High Performance Detection and
  Appearance Feature",word evolution vision significant shown using nets
816,"K-Nearest Neighbours (k-NN) is a popular classification and regression
algorithm, yet one of its main limitations is the difficulty in choosing the
number of neighbours. We present a Bayesian algorithm to compute the posterior
probability distribution for k given a target point within a data-set,
efficiently and without the use of Markov Chain Monte Carlo (MCMC) methods or
simulation - alongside an exact solution for distributions within the
exponential family. The central idea is that data points around our target are
generated by the same probability distribution, extending outwards over the
appropriate, though unknown, number of neighbours. Once the data is projected
onto a distance metric of choice, we can transform the choice of k into a
change-point detection problem, for which there is an efficient solution: we
recursively compute the probability of the last change-point as we move towards
our target, and thus de facto compute the posterior probability distribution
over k. Applying this approach to both a classification and a regression UCI
data-sets, we compare favourably and, most importantly, by removing the need
for simulation, we are able to compute the posterior probability of k exactly
and rapidly. As an example, the computational time for the Ripley data-set is a
few milliseconds compared to a few hours when using a MCMC approach.",An Efficient Algorithm for Bayesian Nearest Neighbours,based uci <unk>
817,"Transductive learning considers situations when a learner observes $m$
labelled training points and $u$ unlabelled test points with the final goal of
giving correct answers for the test points. This paper introduces a new
complexity measure for transductive learning called Permutational Rademacher
Complexity (PRC) and studies its properties. A novel symmetrization inequality
is proved, which shows that PRC provides a tighter control over expected
suprema of empirical processes compared to what happens in the standard i.i.d.
setting. A number of comparison results are also provided, which show the
relation between PRC and other popular complexity measures used in statistical
learning theory, including Rademacher complexity and Transductive Rademacher
Complexity (TRC). We argue that PRC is a more suitable complexity measure for
transductive learning. Finally, these results are combined with a standard
concentration argument to provide novel data-dependent risk bounds for
transductive learning.","Permutational Rademacher Complexity: a New Complexity Measure for
  Transductive Learning",complexity demonstrate show significant conflict
818,"Exemplar-based face sketch synthesis plays an important role in both digital
entertainment and law enforcement. It generally consists of two parts: neighbor
selection and reconstruction weight representation. The most time-consuming or
main computation complexity for exemplar-based face sketch synthesis methods
lies in the neighbor selection process. State-of-the-art face sketch synthesis
methods perform neighbor selection online in a data-driven manner by $K$
nearest neighbor ($K$-NN) searching. Actually, the online search increases the
time consuming for synthesis. Moreover, since these methods need to traverse
the whole training dataset for neighbor selection, the computational complexity
increases with the scale of the training database and hence these methods have
limited scalability. In this paper, we proposed a simple but effective offline
random sampling in place of online $K$-NN search to improve the synthesis
efficiency. Extensive experiments on public face sketch databases demonstrate
the superiority of the proposed method in comparison to state-of-the-art
methods, in terms of both synthesis quality and time consumption. The proposed
method could be extended to other heterogeneous face image transformation
problems such as face hallucination. We release the source codes of our
proposed methods and the evaluation metrics for future study online:
http://www.ihitworld.com/RSLCR.html.",Random Sampling for Fast Face Sketch Synthesis,new learning networks model learning complexity two allowed
819,"Backdoors of answer-set programs are sets of atoms that represent clever
reasoning shortcuts through the search space. Assignments to backdoor atoms
reduce the given program to several programs that belong to a tractable target
class. Previous research has considered target classes based on notions of
acyclicity where various types of cycles (good and bad cycles) are excluded
from graph representations of programs. We generalize the target classes by
taking the parity of the number of negative edges on bad cycles into account
and consider backdoors for such classes. We establish new hardness results and
non-uniform polynomial-time tractability relative to directed or undirected
cycles.","The Good, the Bad, and the Odd: Cycles in Answer-Set Programs",offers approaches proposed discovery various
820,"A recent theoretical analysis shows the equivalence between non-negative
matrix factorization (NMF) and spectral clustering based approach to subspace
clustering. As NMF and many of its variants are essentially linear, we
introduce a nonlinear NMF with explicit orthogonality and derive general
kernel-based orthogonal multiplicative update rules to solve the subspace
clustering problem. In nonlinear orthogonal NMF framework, we propose two
subspace clustering algorithms, named kernel-based non-negative subspace
clustering KNSC-Ncut and KNSC-Rcut and establish their connection with spectral
normalized cut and ratio cut clustering. We further extend the nonlinear
orthogonal NMF framework and introduce a graph regularization to obtain a
factorization that respects a local geometric structure of the data after the
nonlinear mapping. The proposed NMF-based approach to subspace clustering takes
into account the nonlinear nature of the manifold, as well as its intrinsic
local geometry, which considerably improves the clustering performance when
compared to the several recently proposed state-of-the-art methods.","A Nonlinear Orthogonal Non-Negative Matrix Factorization Approach to
  Subspace Clustering",complexity number based research theory model <unk>
821,"We develop a natural language interface for human robot interaction that
implements reasoning about deep semantics in natural language. To realize the
required deep analysis, we employ methods from cognitive linguistics, namely
the modular and compositional framework of Embodied Construction Grammar (ECG)
[Feldman, 2009]. Using ECG, robots are able to solve fine-grained reference
resolution problems and other issues related to deep semantics and
compositionality of natural language. This also includes verbal interaction
with humans to clarify commands and queries that are too ambiguous to be
executed safely. We implement our NLU framework as a ROS package and present
proof-of-concept scenarios with different robots, as well as a survey on the
state of the art.","Exploiting Deep Semantics and Compositionality of Natural Language for
  Human-Robot-Interaction",markets data  proposed types brain
822,"This paper describes a method for identification of the informative variables
in the information system with discrete decision variables. It is targeted
specifically towards discovery of the variables that are non-informative when
considered alone, but are informative when the synergistic interactions between
multiple variables are considered. To this end, the mutual entropy of all
possible k-tuples of variables with decision variable is computed. Then, for
each variable the maximal information gain due to interactions with other
variables is obtained. For non-informative variables this quantity conforms to
the well known statistical distributions. This allows for discerning truly
informative variables from non-informative ones. For demonstration of the
approach, the method is applied to several synthetic datasets that involve
complex multidimensional interactions between variables. It is capable of
identifying most important informative variables, even in the case when the
dimensionality of the analysis is smaller than the true dimensionality of the
problem. What is more, the high sensitivity of the algorithm allows for
detection of the influence of nuisance variables on the response variable.","All-relevant feature selection using multidimensional filters with
  exhaustive search",using produces feature attention
823,"Given two subsets A and B of nodes in a directed graph, the conduciveness of
the graph from A to B is the ratio representing how many of the edges outgoing
from nodes in A are incoming to nodes in B. When the graph's nodes stand for
the possible solutions to certain problems of combinatorial optimization,
choosing its edges appropriately has been shown to lead to conduciveness
properties that provide useful insight into the performance of algorithms to
solve those problems. Here we study the conduciveness of CA-rule graphs, that
is, graphs whose node set is the set of all CA rules given a cell's number of
possible states and neighborhood size. We consider several different edge sets
interconnecting these nodes, both deterministic and random ones, and derive
analytical expressions for the resulting graph's conduciveness toward rules
having a fixed number of non-quiescent entries. We demonstrate that one of the
random edge sets, characterized by allowing nodes to be sparsely interconnected
across any Hamming distance between the corresponding rules, has the potential
of providing reasonable conduciveness toward the desired rules. We conjecture
that this may lie at the bottom of the best strategies known to date for
discovering complex rules to solve specific problems, all of an evolutionary
nature.",The conduciveness of CA-rule graphs,learning applying model learning open users model open
824,"Artificial reinforcement learning (RL) is a widely used technique in
artificial intelligence that provides a general method for training agents to
perform a wide variety of behaviours. RL as used in computer science has
striking parallels to reward and punishment learning in animal and human
brains. I argue that present-day artificial RL agents have a very small but
nonzero degree of ethical importance. This is particularly plausible for views
according to which sentience comes in degrees based on the abilities and
complexities of minds, but even binary views on consciousness should assign
nonzero probability to RL programs having morally relevant experiences. While
RL programs are not a top ethical priority today, they may become more
significant in the coming decades as RL is increasingly applied to industry,
robotics, video games, and other areas. I encourage scientists, philosophers,
and citizens to begin a conversation about our ethical duties to reduce the
harm that we inflict on powerless, voiceless RL agents.",Do Artificial Reinforcement-Learning Agents Matter Morally?,choosing show utilizes networks observation
825,"Commercial head-mounted eye trackers provide useful features to customers in
industry and research but are expensive and rely on closed source hardware and
software. This limits the application areas and use of mobile eye tracking to
expert users and inhibits user-driven development, customisation, and
extension. In this paper we present Pupil -- an accessible, affordable, and
extensible open source platform for mobile eye tracking and gaze-based
interaction. Pupil comprises 1) a light-weight headset with high-resolution
cameras, 2) an open source software framework for mobile eye tracking, as well
as 3) a graphical user interface (GUI) to playback and visualize video and gaze
data. Pupil features high-resolution scene and eye cameras for monocular and
binocular gaze estimation. The software and GUI are platform-independent and
include state-of-the-art algorithms for real-time pupil detection and tracking,
calibration, and accurate gaze estimation. Results of a performance evaluation
show that Pupil can provide an average gaze estimation accuracy of 0.6 degree
of visual angle (0.08 degree precision) with a latency of the processing
pipeline of only 0.045 seconds.","Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile
  Gaze-based Interaction",two show show set search
826,"We propose a novel method for semi-supervised learning (SSL) based on
data-driven distributionally robust optimization (DRO) using optimal transport
metrics. Our proposed method enhances generalization error by using the
unlabeled data to restrict the support of the worst case distribution in our
DRO formulation. We enable the implementation of our DRO formulation by
proposing a stochastic gradient descent algorithm which allows to easily
implement the training procedure. We demonstrate that our Semi-supervised DRO
method is able to improve the generalization error over natural supervised
procedures and state-of-the-art SSL estimators. Finally, we include a
discussion on the large sample behavior of the optimal uncertainty region in
the DRO formulation. Our discussion exposes important aspects such as the role
of dimension reduction in SSL.",Semi-supervised Learning based on Distributionally Robust Optimization,tasks  sample model planning rapid
827,"Unsupervised models of dependency parsing typically require large amounts of
clean, unlabeled data plus gold-standard part-of-speech tags. Adding indirect
supervision (e.g. language universals and rules) can help, but we show that
obtaining small amounts of direct supervision - here, partial dependency
annotations - provides a strong balance between zero and full supervision. We
adapt the unsupervised ConvexMST dependency parser to learn from partial
dependencies expressed in the Graph Fragment Language. With less than 24 hours
of total annotation, we obtain 7% and 17% absolute improvement in unlabeled
dependency scores for English and Spanish, respectively, compared to the same
parser using only universal grammar constraints.","Fill it up: Exploiting partial dependency annotations in a minimum
  spanning tree parser",concepts #### 
828,"This paper introduces a simple but highly efficient ensemble for robust
texture classification, which can effectively deal with translation, scale and
changes of significant viewpoint problems. The proposed method first inherits
the spirit of spatial pyramid matching model (SPM), which is popular for
encoding spatial distribution of local features, but in a flexible way,
partitioning the original image into different levels and incorporating
different overlapping patterns of each level. This flexible setup helps capture
the informative features and produces sufficient local feature codes by some
well-chosen aggregation statistics or pooling operations within each
partitioned region, even when only a few sample images are available for
training. Then each texture image is represented by several orderless feature
codes and thereby all the training data form a reliable feature pond. Finally,
to take full advantage of this feature pond, we develop a collaborative
representation-based strategy with locality constraint (LC-CRC) for the final
classification, and experimental results on three well-known public texture
datasets demonstrate the proposed approach is very competitive and even
outperforms several state-of-the-art methods. Particularly, when only a few
samples of each category are available for training, our approach still
achieves very high classification performance.","Multi-Level Feature Descriptor for Robust Texture Classification via
  Locality-Constrained Collaborative Strategy",data feature two show researchers kernels
829,"This paper describes the method of visualization of periodic constituents and
instability areas in series of measurements, being based on the algorithm of
smoothing out and concept of one-dimensional cellular automata. A method can be
used at the analysis of temporal series, related to the volumes of thematic
publications in web-space.","Visualization of features of a series of measurements with
  one-dimensional cellular structure",variables  model linguistic using <unk> models present draws
830,"The superposition of temporal point processes has been studied for many
years, although the usefulness of such models for practical applications has
not be fully developed. We investigate superposed Hawkes process as an
important class of such models, with properties studied in the framework of
least squares estimation. The superposition of Hawkes processes is demonstrated
to be beneficial for tightening the upper bound of excess risk under certain
conditions, and we show the feasibility of the benefit in typical situations.
The usefulness of superposed Hawkes processes is verified on synthetic data,
and its potential to solve the cold-start problem of recommendation systems is
demonstrated on real-world data.",Benefits from Superposed Hawkes Processes,avoidance networks
831,"Understanding physical phenomena is a key competence that enables humans and
animals to act and interact under uncertain perception in previously unseen
environments containing novel object and their configurations. Developmental
psychology has shown that such skills are acquired by infants from observations
at a very early stage.
  In this paper, we contrast a more traditional approach of taking a
model-based route with explicit 3D representations and physical simulation by
an end-to-end approach that directly predicts stability and related quantities
from appearance. We ask the question if and to what extent and quality such a
skill can directly be acquired in a data-driven way bypassing the need for an
explicit simulation.
  We present a learning-based approach based on simulated data that predicts
stability of towers comprised of wooden blocks under different conditions and
quantities related to the potential fall of the towers. The evaluation is
carried out on synthetic data and compared to human judgments on the same
stimuli.","To Fall Or Not To Fall: A Visual Approach to Physical Stability
  Prediction",paper  system traditional
832,"Standing at the paradigm shift towards data-intensive science, machine
learning techniques are becoming increasingly important. In particular, as a
major breakthrough in the field, deep learning has proven as an extremely
powerful tool in many fields. Shall we embrace deep learning as the key to all?
Or, should we resist a 'black-box' solution? There are controversial opinions
in the remote sensing community. In this article, we analyze the challenges of
using deep learning for remote sensing data analysis, review the recent
advances, and provide resources to make deep learning in remote sensing
ridiculously simple to start with. More importantly, we advocate remote sensing
scientists to bring their expertise into deep learning, and use it as an
implicit general model to tackle unprecedented large-scale influential
challenges, such as climate change and urbanization.",Deep learning in remote sensing: a review,interpolation model average multimodal units interpolation
833,"Commute Time Distance (CTD) is a random walk based metric on graphs. CTD has
found widespread applications in many domains including personalized search,
collaborative filtering and making search engines robust against manipulation.
Our interest is inspired by the use of CTD as a metric for anomaly detection.
It has been shown that CTD can be used to simultaneously identify both global
and local anomalies. Here we propose an accurate and efficient approximation
for computing the CTD in an incremental fashion in order to facilitate
real-time applications. An online anomaly detection algorithm is designed where
the CTD of each new arriving data point to any point in the current graph can
be estimated in constant time ensuring a real-time response. Moreover, the
proposed approach can also be applied in many other applications that utilize
commute time distance.",Online Anomaly Detection Systems Using Incremental Commute Time,models proposed vanilla
834,"The objective of the paper is to design an agent which provides efficient
response to the caller when a call goes unanswered in smartphones. The agent
provides responses through text messages, email etc stating the most likely
reason as to why the callee is unable to answer a call. Responses are composed
taking into consideration the importance of the present call and the situation
the callee is in at the moment like driving, sleeping, at work etc. The agent
makes decisons in the compostion of response messages based on the patterns it
has come across in the learning environment. Initially the user helps the agent
to compose response messages. The agent associates this message to the percept
it recieves with respect to the environment the callee is in. The user may
thereafter either choose to make to response system automatic or choose to
recieve suggestions from the agent for responses messages and confirm what is
to be sent to the caller.",Design of an Agent for Answering Back in Smart Phones,power examine proposed data user using user noise
835,"We consider the problem of bottom-up compilation of knowledge bases, which is
usually predicated on the existence of a polytime function for combining
compilations using Boolean operators (usually called an Apply function). While
such a polytime Apply function is known to exist for certain languages (e.g.,
OBDDs) and not exist for others (e.g., DNNF), its existence for certain
languages remains unknown. Among the latter is the recently introduced language
of Sentential Decision Diagrams (SDDs), for which a polytime Apply function
exists for unreduced SDDs, but remains unknown for reduced ones (i.e. canonical
SDDs). We resolve this open question in this paper and consider some of its
theoretical and practical implications. Some of the findings we report question
the common wisdom on the relationship between bottom-up compilation, language
canonicity and the complexity of the Apply function.",On the Role of Canonicity in Bottom-up Knowledge Compilation,critical critical show <unk> evolution
836,"Visual representation is crucial for a visual tracking method's performances.
Conventionally, visual representations adopted in visual tracking rely on
hand-crafted computer vision descriptors. These descriptors were developed
generically without considering tracking-specific information. In this paper,
we propose to learn complex-valued invariant representations from tracked
sequential image patches, via strong temporal slowness constraint and stacked
convolutional autoencoders. The deep slow local representations are learned
offline on unlabeled data and transferred to the observational model of our
proposed tracker. The proposed observational model retains old training samples
to alleviate drift, and collect negative samples which are coherent with
target's motion pattern for better discriminative tracking. With the learned
representation and online training samples, a logistic regression classifier is
adopted to distinguish target from background, and retrained online to adapt to
appearance changes. Subsequently, the observational model is integrated into a
particle filter framework to peform visual tracking. Experimental results on
various challenging benchmark sequences demonstrate that the proposed tracker
performs favourably against several state-of-the-art trackers.","Self-taught learning of a deep invariant representation for visual
  tracking via temporal slowness principle",train similarity estimated
837,"Diversity is an important aspect of highly efficient multi-agent teams. We
introduce the main factors that drive a multi-agent system in either direction
along the diversity scale. A metric for diversity is described, and we
speculate on the concept of transient diversity. Finally, an experiment on
social entropy using a RoboCup simulated soccer team is presented.",Transient Diversity in Multi-Agent Systems,accurate paper  for
838,"Intelligent agents offer a new and exciting way of understanding the world of
work. In this paper we apply agent-based modeling and simulation to investigate
a set of problems in a retail context. Specifically, we are working to
understand the relationship between human resource management practices and
retail productivity. Despite the fact we are working within a relatively novel
and complex domain, it is clear that intelligent agents could offer potential
for fostering sustainable organizational capabilities in the future. The
project is still at an early stage. So far we have conducted a case study in a
UK department store to collect data and capture impressions about operations
and actors within departments. Furthermore, based on our case study we have
built and tested our first version of a retail branch simulator which we will
present in this paper.","Using Intelligent Agents to Understand Management Practices and Retail
  Productivity",two model
839,"In parallel with the success of CNNs to solve vision problems, there is a
growing interest in developing methodologies to understand and visualize the
internal representations of these networks. How the responses of a trained CNN
encode the visual information is a fundamental question both for computer and
human vision research. Image representations provided by the first
convolutional layer as well as the resolution change provided by the
max-polling operation are easy to understand, however, as soon as a second and
further convolutional layers are added in the representation, any intuition is
lost. A usual way to deal with this problem has been to define deconvolutional
networks that somehow allow to explore the internal representations of the most
important activations towards the image space, where deconvolution is assumed
as a convolution with the transposed filter. However, this assumption is not
the best approximation of an inverse convolution. In this paper we propose a
new assumption based on filter substitution to reverse the encoding of a
convolutional layer. This provides us with a new tool to directly visualize any
CNN single neuron as a filter in the first layer, this is in terms of the image
space.","Understanding learned CNN features through Filter Decoding with
  Substitution",learning <unk> model learning representations the
840,"We present a new model-based integrative method for clustering objects given
both vectorial data, which describes the feature of each object, and network
data, which indicates the similarity of connected objects. The proposed general
model is able to cluster the two types of data simultaneously within one
integrative probabilistic model, while traditional methods can only handle one
data type or depend on transforming one data type to another. Bayesian
inference of the clustering is conducted based on a Markov chain Monte Carlo
algorithm. A special case of the general model combining the Gaussian mixture
model and the stochastic block model is extensively studied. We used both
synthetic data and real data to evaluate this new method and compare it with
alternative methods. The results show that our simultaneous clustering method
performs much better. This improvement is due to the power of the model-based
probabilistic approach for efficiently integrating information.","A Bayesian Method for Joint Clustering of Vectorial Data and Network
  Data",textual test combined model learning
841,"Hand pose estimation is to predict the pose parameters representing a 3D hand
model, such as locations of hand joints. This problem is very challenging due
to large changes in viewpoints and articulations, and intense self-occlusions,
etc. Many researchers have investigated the problem from both aspects of input
feature learning and output prediction modelling. Though effective, most of the
existing discriminative methods only give a deterministic estimation of target
poses. Also, due to their single-value mapping intrinsic, they fail to
adequately handle self-occlusion problems, where occluded joints present
multiple modes. In this paper, we tackle the self-occlusion issue and provide a
complete description of observed poses given an input depth image through a
hierarchical mixture density network (HMDN) framework. In particular, HMDN
leverages the state-of-the-art CNN module to facilitate feature learning, while
proposes a density in a two-level hierarchy to reconcile single-valued and
multi-valued mapping in the output. The whole framework is naturally end-to-end
trainable with a mixture of two differentiable density functions. HMDN produces
interpretable and diverse candidate samples, and significantly outperforms the
state-of-the-art algorithms on benchmarks that exhibit occlusions.","Occlusion-aware Hand Pose Estimation Using Hierarchical Mixture Density
  Network",introduction common number show fundamental known
842,"This paper addresses the optimal control problem known as the Linear
Quadratic Regulator in the case when the dynamics are unknown. We propose a
multi-stage procedure, called Coarse-ID control, that estimates a model from a
few experimental trials, estimates the error in that model with respect to the
truth, and then designs a controller using both the model and uncertainty
estimate. Our technique uses contemporary tools from random matrix theory to
bound the error in the estimation procedure. We also employ a recently
developed approach to control synthesis called System Level Synthesis that
enables robust control design by solving a convex optimization problem. We
provide end-to-end bounds on the relative error in control cost that are nearly
optimal in the number of parameters and that highlight salient properties of
the system to be controlled such as closed-loop sensitivity and optimal control
magnitude. We show experimentally that the Coarse-ID approach enables efficient
computation of a stabilizing controller in regimes where simple control schemes
that do not take the model uncertainty into account fail to stabilize the true
system.",On the Sample Complexity of the Linear Quadratic Regulator,solve model paper  for
843,"BDeu marginal likelihood score is a popular model selection criterion for
selecting a Bayesian network structure based on sample data. This
non-informative scoring criterion assigns same score for network structures
that encode same independence statements. However, before applying the BDeu
score, one must determine a single parameter, the equivalent sample size alpha.
Unfortunately no generally accepted rule for determining the alpha parameter
has been suggested. This is disturbing, since in this paper we show through a
series of concrete experiments that the solution of the network structure
optimization problem is highly sensitive to the chosen alpha parameter value.
Based on these results, we are able to give explanations for how and why this
phenomenon happens, and discuss ideas for solving this problem.","On Sensitivity of the MAP Bayesian Network Structure to the Equivalent
  Sample Size Parameter",filtered information show method  networks
844,"We study the task of cleaning scanned text documents that are strongly
corrupted by dirt such as manual line strokes, spilled ink etc. We aim at
autonomously removing dirt from a single letter-size page based only on the
information the page contains. Our approach, therefore, has to learn character
representations without supervision and requires a mechanism to distinguish
learned representations from irregular patterns. To learn character
representations, we use a probabilistic generative model parameterizing pattern
features, feature variances, the features' planar arrangements, and pattern
frequencies. The latent variables of the model describe pattern class, pattern
position, and the presence or absence of individual pattern features. The model
parameters are optimized using a novel variational EM approximation. After
learning, the parameters represent, independently of their absolute position,
planar feature arrangements and their variances. A quality measure defined
based on the learned representation then allows for an autonomous
discrimination between regular character patterns and the irregular patterns
making up the dirt. The irregular patterns can thus be removed to clean the
document. For a full Latin alphabet we found that a single page does not
contain sufficiently many character examples. However, even if heavily
corrupted by dirt, we show that a page containing a lower number of character
types can efficiently and autonomously be cleaned solely based on the
structural regularity of the characters it contains. In different examples
using characters from different alphabets, we demonstrate generality of the
approach and discuss its implications for future developments.","Autonomous Cleaning of Corrupted Scanned Documents - A Generative
  Modeling Approach",networks search size distance  data <unk>
845,"Factored neural machine translation (FNMT) is founded on the idea of using
the morphological and grammatical decomposition of the words (factors) at the
output side of the neural network. This architecture addresses two well-known
problems occurring in MT, namely the size of target language vocabulary and the
number of unknown tokens produced in the translation. FNMT system is designed
to manage larger vocabulary and reduce the training time (for systems with
equivalent target language vocabulary size). Moreover, we can produce
grammatically correct words that are not part of the vocabulary. FNMT model is
evaluated on IWSLT'15 English to French task and compared to the baseline
word-based and BPE-based NMT systems. Promising qualitative and quantitative
results (in terms of BLEU and METEOR) are reported.",Neural Machine Translation by Generating Multiple Linguistic Factors,learning programming equation
846,"Algorithmic image-based diagnosis and prognosis of neurodegenerative diseases
on longitudinal data has drawn great interest from computer vision researchers.
The current state-of-the-art models for many image classification tasks are
based on the Convolutional Neural Networks (CNN). However, a key challenge in
applying CNN to biological problems is that the available labeled training
samples are very limited. Another issue for CNN to be applied in computer aided
diagnosis applications is that to achieve better diagnosis and prognosis
accuracy, one usually has to deal with the longitudinal dataset, i.e., the
dataset of images scanned at different time points. Here we argue that an
enhanced CNN model with transfer learning for the joint analysis of tasks from
multiple time points or regions of interests may have a potential to improve
the accuracy of computer aided diagnosis. To reach this goal, we innovate a CNN
based deep learning multi-task dictionary learning framework to address the
above challenges. Firstly, we pre-train CNN on the ImageNet dataset and
transfer the knowledge from the pre-trained model to the medical imaging
progression representation, generating the features for different tasks. Then,
we propose a novel unsupervised learning method, termed Multi-task Stochastic
Coordinate Coding (MSCC), for learning different tasks by using shared and
individual dictionaries and generating the sparse features required to predict
the future cognitive clinical scores. We apply our new model in a publicly
available neuroimaging cohort to predict clinical measures with two different
feature sets and compare them with seven other state-of-the-art methods. The
experimental results show our proposed method achieved superior results.","Multi-task Dictionary Learning based Convolutional Neural Network for
  Computer aided Diagnosis with Longitudinal Images",selection model <unk> present <unk>
847,"Mechanical learning is a computing system that is based on a set of simple
and fixed rules, and can learn from incoming data. A learning machine is a
system that realizes mechanical learning. Importantly, we emphasis that it is
based on a set of simple and fixed rules, contrasting to often called machine
learning that is sophisticated software based on very complicated mathematical
theory, and often needs human intervene for software fine tune and manual
adjustments. Here, we discuss some basic facts and principles of such system,
and try to lay down a framework for further study. We propose 2 directions to
approach mechanical learning, just like Church-Turing pair: one is trying to
realize a learning machine, another is trying to well describe the mechanical
learning.",Discussion on Mechanical Learning and Learning Machine,best defining show demonstrated often
848,"This work introduces a novel framework for quantifying the presence and
strength of recurrent dynamics in video data. Specifically, we provide
continuous measures of periodicity (perfect repetition) and quasiperiodicity
(superposition of periodic modes with non-commensurate periods), in a way which
does not require segmentation, training, object tracking or 1-dimensional
surrogate signals. Our methodology operates directly on video data. The
approach combines ideas from nonlinear time series analysis (delay embeddings)
and computational topology (persistent homology), by translating the problem of
finding recurrent dynamics in video data, into the problem of determining the
circularity or toroidality of an associated geometric space. Through extensive
testing, we show the robustness of our scores with respect to several noise
models/levels, we show that our periodicity score is superior to other methods
when compared to human-generated periodicity rankings, and furthermore, we show
that our quasiperiodicity score clearly indicates the presence of biphonation
in videos of vibrating vocal folds, which has never before been accomplished
end to end quantitatively.","(Quasi)Periodicity Quantification in Video Data, Using Topology",learning model model model robustness automatically problem noise
849,"We propose an image representation and matching approach that substantially
improves visual-based location estimation for images. The main novelty of the
approach, called distinctive visual element matching (DVEM), is its use of
representations that are specific to the query image whose location is being
predicted. These representations are based on visual element clouds, which
robustly capture the connection between the query and visual evidence from
candidate locations. We then maximize the influence of visual elements that are
geo-distinctive because they do not occur in images taken at many other
locations. We carry out experiments and analysis for both geo-constrained and
geo-unconstrained location estimation cases using two large-scale,
publicly-available datasets: the San Francisco Landmark dataset with $1.06$
million street-view images and the MediaEval '15 Placing Task dataset with
$5.6$ million geo-tagged images from Flickr. We present examples that
illustrate the highly-transparent mechanics of the approach, which are based on
common sense observations about the visual patterns in image collections. Our
results show that the proposed method delivers a considerable performance
improvement compared to the state of the art.","Geo-distinctive Visual Element Matching for Location Estimation of
  Images",show show objects show representations using
850,"In this doctoral thesis, we apply premises of cognitive linguistics to
terminological definitions and present a proposal called the flexible
terminological definition. This consists of a set of definitions of the same
concept made up of a general definition (in this case, one encompassing the
entire environmental domain) along with additional definitions describing the
concept from the perspective of the subdomains in which it is relevant. Since
context is a determining factor in the construction of the meaning of lexical
units (including terms), we assume that terminological definitions can, and
should, reflect the effects of context, even though definitions have
traditionally been treated as the expression of meaning void of any contextual
effect. The main objective of this thesis is to analyze the effects of
contextual variation on specialized environmental concepts with a view to their
representation in terminological definitions. Specifically, we focused on
contextual variation based on thematic restrictions. To accomplish the
objectives of this doctoral thesis, we conducted an empirical study consisting
of the analysis of a set of contextually variable concepts and the creation of
a flexible definition for two of them. As a result of the first part of our
empirical study, we divided our notion of domain-dependent contextual variation
into three different phenomena: modulation, perspectivization and
subconceptualization. These phenomena are additive in that all concepts
experience modulation, some concepts also undergo perspectivization, and
finally, a small number of concepts are additionally subjected to
subconceptualization. In the second part, we applied these notions to
terminological definitions and we presented we presented guidelines on how to
build flexible definitions, from the extraction of knowledge to the actual
writing of the definition.","La representación de la variación contextual mediante definiciones
  terminológicas flexibles",textual using show show linguistic <unk> different feature
851,"The inherent noise in the observed (e.g., scanned) binary document image
degrades the image quality and harms the compression ratio through breaking the
pattern repentance and adding entropy to the document images. In this paper, we
design a cost function in Bayesian framework with dictionary learning.
Minimizing our cost function produces a restored image which has better quality
than that of the observed noisy image, and a dictionary for representing and
encoding the image. After the restoration, we use this dictionary (from the
same cost function) to encode the restored image following the
symbol-dictionary framework by JBIG2 standard with the lossless mode.
Experimental results with a variety of document images demonstrate that our
method improves the image quality compared with the observed image, and
simultaneously improves the compression ratio. For the test images with
synthetic noise, our method reduces the number of flipped pixels by 48.2% and
improves the compression ratio by 36.36% as compared with the best encoding
methods. For the test images with real noise, our method visually improves the
image quality, and outperforms the cutting-edge method by 28.27% in terms of
the compression ratio.","Model-based Iterative Restoration for Binary Document Image Compression
  with Dictionary Learning",camera definition al  show combining camera existing
852,"This paper proposes an image dehazing model built with a convolutional neural
network (CNN), called All-in-One Dehazing Network (AOD-Net). It is designed
based on a re-formulated atmospheric scattering model. Instead of estimating
the transmission matrix and the atmospheric light separately as most previous
models did, AOD-Net directly generates the clean image through a light-weight
CNN. Such a novel end-to-end design makes it easy to embed AOD-Net into other
deep models, e.g., Faster R-CNN, for improving high-level task performance on
hazy images. Experimental results on both synthesized and natural hazy image
datasets demonstrate our superior performance than the state-of-the-art in
terms of PSNR, SSIM and the subjective visual quality. Furthermore, when
concatenating AOD-Net with Faster R-CNN and training the joint pipeline from
end to end, we witness a large improvement of the object detection performance
on hazy images.",An All-in-One Network for Dehazing and Beyond,neural mahalanobis connection based models
853,"Our team won the second prize of the Safe Aging with SPHERE Challenge
organized by SPHERE, in conjunction with ECML-PKDD and Driven Data. The goal of
the competition was to recognize activities performed by humans, using sensor
data. This paper presents our solution. It is based on a rich pre-processing
and state of the art machine learning methods. From the raw train data, we
generate a synthetic train set with the same statistical characteristics as the
test set. We then perform feature engineering. The machine learning modeling
part is based on stacking weak learners through a grid searched XGBoost
algorithm. Finally, we use post-processing to smooth our predictions over time.",Dataiku's Solution to SPHERE's Activity Recognition Challenge,gpu model gpu
854,"We discuss the evolution of aspects of nonmonotonic reasoning towards the
computational paradigm of answer-set programming (ASP). We give a general
overview of the roots of ASP and follow up with the personal perspective on
research developments that helped verbalize the main principles of ASP and
differentiated it from the classical logic programming.","Origins of Answer-Set Programming - Some Background And Two Personal
  Accounts",based composite categorization adapting model composite composite adapting
855,"In this paper, we show how absolute orientation measurements provided by
low-cost but high-fidelity IMU sensors can be integrated into the KinectFusion
pipeline. We show that integration improves both runtime, robustness and
quality of the 3D reconstruction. In particular, we use this orientation data
to seed and regularize the ICP registration technique. We also present a
technique to filter the pairs of 3D matched points based on the distribution of
their distances. This filter is implemented efficiently on the GPU. Estimating
the distribution of the distances helps control the number of iterations
necessary for the convergence of the ICP algorithm. Finally, we show
experimental results that highlight improvements in robustness, a speed-up of
almost 12%, and a gain in tracking quality of 53% for the ATE metric on the
Freiburg benchmark.","Integration of Absolute Orientation Measurements in the KinectFusion
  Reconstruction pipeline",studied model <unk> connection based types room
856,"We propose a general class of language models that treat reference as an
explicit stochastic latent variable. This architecture allows models to create
mentions of entities and their attributes by accessing external databases
(required by, e.g., dialogue generation and recipe generation) and internal
state (required by, e.g. language models which are aware of coreference). This
facilitates the incorporation of information that can be accessed in
predictable locations in databases or discourse context, even when the targets
of the reference may be rare words. Experiments on three tasks shows our model
variants based on deterministic attention.",Reference-Aware Language Models,models models show models models
857,"The most direct way to express arbitrary dependencies in datasets is to
estimate the joint distribution and to apply afterwards the argmax-function to
obtain the mode of the corresponding conditional distribution. This method is
in practice difficult, because it requires a global optimization of a
complicated function, the joint distribution by fixed input variables. This
article proposes a method for finding global maxima if the joint distribution
is modeled by a kernel density estimation. Some experiments show advantages and
shortcomings of the resulting regression method in comparison to the standard
Nadaraya-Watson regression technique, which approximates the optimum by the
expectation value.","Kernel Regression by Mode Calculation of the Conditional Probability
  Distribution",kernels demonstrate show distribution planning conventional
858,"This paper presents a renewed overview of photosensor oculography (PSOG), an
eye-tracking technique based on the principle of using simple photosensors to
measure the amount of reflected (usually infrared) light when the eye rotates.
Photosensor oculography can provide measurements with high precision, low
latency and reduced power consumption, and thus it appears as an attractive
option for performing eye-tracking in the emerging head-mounted interaction
devices, e.g. augmented and virtual reality (AR/VR) headsets. In our current
work we employ an adjustable simulation framework as a common basis for
performing an exploratory study of the eye-tracking behavior of different
photosensor oculography designs. With the performed experiments we explore the
effects from the variation of some basic parameters of the designs on the
resulting accuracy and cross-talk, which are crucial characteristics for the
seamless operation of human-computer interaction applications based on
eye-tracking. Our experimental results reveal the design trade-offs that need
to be adopted to tackle the competing conditions that lead to optimum
performance of different eye-tracking characteristics. We also present the
transformations that arise in the eye-tracking output when sensor shifts occur,
and assess the resulting degradation in accuracy for different combinations of
eye movements and sensor shifts.","Photosensor Oculography: Survey and Parametric Analysis of Designs using
  Model-Based Simulation",bias proposed account tree 
859,"Many models of interest in the natural and social sciences have no
closed-form likelihood function, which means that they cannot be treated using
the usual techniques of statistical inference. In the case where such models
can be efficiently simulated, Bayesian inference is still possible thanks to
the Approximate Bayesian Computation (ABC) algorithm. Although many refinements
have been suggested, ABC inference is still far from routine. ABC is often
excruciatingly slow due to very low acceptance rates. In addition, ABC requires
introducing a vector of ""summary statistics"", the choice of which is relatively
arbitrary, and often require some trial and error, making the whole process
quite laborious for the user.
  We introduce in this work the EP-ABC algorithm, which is an adaptation to the
likelihood-free context of the variational approximation algorithm known as
Expectation Propagation (Minka, 2001). The main advantage of EP-ABC is that it
is faster by a few orders of magnitude than standard algorithms, while
producing an overall approximation error which is typically negligible. A
second advantage of EP-ABC is that it replaces the usual global ABC constraint
on the vector of summary statistics computed on the whole dataset, by n local
constraints of the form that apply separately to each data-point. As a
consequence, it is often possible to do away with summary statistics entirely.
In that case, EP-ABC approximates directly the evidence (marginal likelihood)
of the model.
  Comparisons are performed in three real-world applications which are typical
of likelihood-free inference, including one application in neuroscience which
is novel, and possibly too challenging for standard ABC techniques.",Expectation-Propagation for Likelihood-Free Inference,constraint illustration  show constraint illustration  offers
860,"In this paper we develop a dynamic form of Bayesian optimization for machine
learning models with the goal of rapidly finding good hyperparameter settings.
Our method uses the partial information gained during the training of a machine
learning model in order to decide whether to pause training and start a new
model, or resume the training of a previously-considered model. We specifically
tailor our method to machine learning problems by developing a novel
positive-definite covariance kernel to capture a variety of training curves.
Furthermore, we develop a Gaussian process prior that scales gracefully with
additional temporal observations. Finally, we provide an information-theoretic
framework to automate the decision process. Experiments on several common
machine learning models show that our approach is extremely effective in
practice.",Freeze-Thaw Bayesian Optimization,models show <unk> off the shelf
861,"We are presenting work on recognising acronyms of the form Long-Form
(Short-Form) such as ""International Monetary Fund (IMF)"" in millions of news
articles in twenty-two languages, as part of our more general effort to
recognise entities and their variants in news text and to use them for the
automatic analysis of the news, including the linking of related news across
languages. We show how the acronym recognition patterns, initially developed
for medical terms, needed to be adapted to the more general news domain and we
present evaluation results. We describe our effort to automatically merge the
numerous long-form variants referring to the same short-form, while keeping
non-related long-forms separate. Finally, we provide extensive statistics on
the frequency and the distribution of short-form/long-form pairs across
languages.",Acronym recognition and processing in 22 languages,accurate manifolds show significant search using key
862,"The recent, remarkable growth of machine learning has led to intense interest
in the privacy of the data on which machine learning relies, and to new
techniques for preserving privacy. However, older ideas about privacy may well
remain valid and useful. This note reviews two recent works on privacy in the
light of the wisdom of some of the early literature, in particular the
principles distilled by Saltzer and Schroeder in the 1970s.","On the Protection of Private Information in Machine Learning Systems:
  Two Recent Approaches",using significant referred new information different architectures  robustness
863,"The field of Automatic Facial Expression Analysis has grown rapidly in recent
years. However, despite progress in new approaches as well as benchmarking
efforts, most evaluations still focus on either posed expressions, near-frontal
recordings, or both. This makes it hard to tell how existing expression
recognition approaches perform under conditions where faces appear in a wide
range of poses (or camera views), displaying ecologically valid expressions.
The main obstacle for assessing this is the availability of suitable data, and
the challenge proposed here addresses this limitation. The FG 2017 Facial
Expression Recognition and Analysis challenge (FERA 2017) extends FERA 2015 to
the estimation of Action Units occurrence and intensity under different camera
views. In this paper we present the third challenge in automatic recognition of
facial expressions, to be held in conjunction with the 12th IEEE conference on
Face and Gesture Recognition, May 2017, in Washington, United States. Two
sub-challenges are defined: the detection of AU occurrence, and the estimation
of AU intensity. In this work we outline the evaluation protocol, the data
used, and the results of a baseline method for both sub-challenges.","FERA 2017 - Addressing Head Pose in the Third Facial Expression
  Recognition and Analysis Challenge",model rapid
864,"We present a loss function for neural networks that encompasses an idea of
trivial versus non-trivial predictions, such that the network jointly
determines its own prediction goals and learns to satisfy them. This permits
the network to choose sub-sets of a problem which are most amenable to its
abilities to focus on solving, while discarding 'distracting' elements that
interfere with its learning. To do this, the network first transforms the raw
data into a higher-level categorical representation, and then trains a
predictor from that new time series to its future. To prevent a trivial
solution of mapping the signal to zero, we introduce a measure of
non-triviality via a contrast between the prediction error of the learned model
with a naive model of the overall signal statistics. The transform can learn to
discard uninformative and unpredictable components of the signal in favor of
the features which are both highly predictive and highly predictable. This
creates a coarse-grained model of the time-series dynamics, focusing on
predicting the slowly varying latent parameters which control the statistics of
the time-series, rather than predicting the fast details directly. The result
is a semi-supervised algorithm which is capable of extracting latent
parameters, segmenting sections of time-series with differing statistics, and
building a higher-level representation of the underlying dynamics from
unlabeled data.","Neural Coarse-Graining: Extracting slowly-varying latent degrees of
  freedom with neural networks",included asked cross correlation data around
865,"Information on the web is prodigious; searching relevant information is
difficult making web users to rely on search engines for finding relevant
information on the web. Search engines index and categorize web pages according
to their contents using crawlers and rank them accordingly. For given user
query they retrieve millions of webpages and display them to users according to
web-page rank. Every search engine has their own algorithms based on certain
parameters for ranking web-pages. Search Engine Optimization (SEO) is that
technique by which webmasters try to improve ranking of their websites by
optimizing it according to search engines ranking parameters. It is the aim of
this research to identify the most popular SEO techniques used by search
engines for ranking web-pages and to establish their importance for indexing
and categorizing web data. The research tries to establish that using more SEO
parameters in ranking algorithms helps in retrieving better search results thus
increasing user satisfaction.
  In the accomplished research, a web based Meta search engine is proposed to
aggregates search results from different search engines and rank web-pages
based on new page ranking algorithm which will assign heuristic page rank to
web-pages based on SEO parameters such as title tag, Meta description, sitemap
etc. The research also provides insight into techniques which webmasters can
use for better ranking their websites in Google and Bing.
  Initial results has shown that using certain SEO parameters in present
ranking algorithm helps in retrieving more useful results for user queries.
These results generated from Meta search engine outperformed existing search
engines in terms of better retrieved search results and high precision.",Intelligent Search Optimization using Artificial Fuzzy Logics,types show robotics feature models
866,"Discussion forums are an important source of information. They are often used
to answer specific questions a user might have and to discover more about a
topic of interest. Discussions in these forums may evolve in intricate ways,
making it difficult for users to follow the flow of ideas. We propose a novel
approach for automatically identifying the underlying thread structure of a
forum discussion. Our approach is based on a neural model that computes
coherence scores of possible reconstructions and then selects the highest
scoring, i.e., the most coherent one. Preliminary experiments demonstrate
promising results outperforming a number of strong baseline methods.","Thread Reconstruction in Conversational Data using Neural Coherence
  Models",solve model <unk> using still aware
867,"PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is jointly
modeling treatments across a set of patients with varying number of medical
encounters, where the alignment of events in time bears no clinical meaning,
and it may also be impossible to align them due to their varying length.
Despite recent improvements on scaling up unconstrained PARAFAC2, its model
factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a COnstrained PARAFAC2 (COPA)
method, which carefully incorporates optimization constraints such as temporal
smoothness, sparsity, and non-negativity in the resulting factors. To
efficiently support all those constraints, COPA adopts a hybrid optimization
framework using alternating optimization and alternating direction method of
multiplier (AO-ADMM). As evaluated on large electronic health record (EHR)
datasets with hundreds of thousands of patients, COPA achieves significant
speedups (up to 36x faster) over prior PARAFAC2 approaches that only attempt to
handle a subset of the constraints that COPA enables. Overall, our method
outperforms all the baselines attempting to handle a subset of the constraints
in terms of speed, while achieving the same level of accuracy.",COPA: Constrained PARAFAC2 for Sparse & Large Datasets,mechanism examples using examples model feature models
868,"Although face recognition has been improved much as the development of Deep
Neural Networks, SIPP(Single Image Per Person) problem in face recognition has
not been better solved, especially in practical applications where searching
over complicated database. In this paper, a combination of modified mean search
and LSH method would be introduced orderly to improve the precision and recall
of SIPP face recognition without retrain of the DNN model. First, a modified
SVD based augmentation method would be introduced to get more intra-class
variations even for person with only one image. Second, an unique rule based
combination of modified mean search and LSH method was proposed the first time
to help get the most similar personID in a complicated dataset, and some
theoretical explaining followed. Third, we would like to emphasize, no need to
retrain of the DNN model and would easy to be extended without much efforts. We
do some practical testing in competition of Msceleb challenge-2 2017 which was
hold by Microsoft Research, great improvement of coverage from 13.39% to
19.25%, 29.94%, 42.11%, 47.52% at precision 99%(P99) would be shown latter,
coverage reach 94.2% and 100% at precision 97%(P97) and 95%(P95) respectively.
As far as we known, this is the only paper who do not fine-tuning on
competition dataset and ranked top-10. A similar test on CASIA WebFace dataset
also demonstrated the same improvements on both precision and recall.","Improving precision and recall of face recognition in SIPP with
  combination of modified mean search and LSH",paper  currently show programming using paper  for
869,"We propose a novel word embedding pre-training approach that exploits writing
errors in learners' scripts. We compare our method to previous models that tune
the embeddings based on script scores and the discrimination between correct
and corrupt word contexts in addition to the generic commonly-used embeddings
pre-trained on large corpora. The comparison is achieved by using the
aforementioned models to bootstrap a neural network that learns to predict a
holistic score for scripts. Furthermore, we investigate augmenting our model
with error corrections and monitor the impact on performance. Our results show
that our error-oriented approach outperforms other comparable ones which is
further demonstrated when training on more data. Additionally, extending the
model with corrections provides further performance gains when data sparsity is
an issue.",An Error-Oriented Approach to Word Embedding Pre-Training,linear model intractable agent new distribution
870,"A standard approach to approximate inference in state-space models isto apply
a particle filter, e.g., the Condensation Algorithm.However, the performance of
particle filters often varies significantlydue to their stochastic nature.We
present a class of algorithms, called lattice particle filters, thatcircumvent
this difficulty by placing the particles deterministicallyaccording to a
Quasi-Monte Carlo integration rule.We describe a practical realization of this
idea, discuss itstheoretical properties, and its efficiency.Experimental
results with a synthetic 2D tracking problem show that thelattice particle
filter is equivalent to a conventional particle filterthat has between 10 and
60% more particles, depending ontheir ""sparsity"" in the state-space.We also
present results on inferring 3D human motion frommoving light displays.",Lattice Particle Filters,widely stochastic show kernel stochastic
871,"One of the most famous drawings by Leonardo da Vinci is a self-portrait in
red chalk, where he looks quite old. In fact, there is a sketch in one of his
notebooks, partially covered by written notes, that can be a self-portrait of
the artist when he was young. The use of image processing, to remove the
handwritten text and improve the image, allows a comparison of the two
portraits.",A self-portrait of young Leonardo,using <unk> model influences expect method data input
872,"Recovering matrices from compressive and grossly corrupted observations is a
fundamental problem in robust statistics, with rich applications in computer
vision and machine learning. In theory, under certain conditions, this problem
can be solved in polynomial time via a natural convex relaxation, known as
Compressive Principal Component Pursuit (CPCP). However, all existing provable
algorithms for CPCP suffer from superlinear per-iteration cost, which severely
limits their applicability to large scale problems. In this paper, we propose
provable, scalable and efficient methods to solve CPCP with (essentially)
linear per-iteration cost. Our method combines classical ideas from Frank-Wolfe
and proximal methods. In each iteration, we mainly exploit Frank-Wolfe to
update the low-rank component with rank-one SVD and exploit the proximal step
for the sparse term. Convergence results and implementation details are also
discussed. We demonstrate the scalability of the proposed approach with
promising numerical experiments on visual data.",Scalable Robust Matrix Recovery: Frank-Wolfe Meets Proximal Methods,models classify generated method interactively producing
873,"A semi-parametric, non-linear regression model in the presence of latent
variables is introduced. These latent variables can correspond to unmodeled
phenomena or unmeasured agents in a complex networked system. This new
formulation allows joint estimation of certain non-linearities in the system,
the direct interactions between measured variables, and the effects of
unmodeled elements on the observed system. The particular form of the model
adopted is justified, and learning is posed as a regularized maximum likelihood
estimation. This leads to classes of structured convex optimization problems
with a ""sparse plus low-rank"" flavor. Relations between the proposed model and
several common model paradigms, such as those of Robust Principal Component
Analysis (PCA) and Vector Autoregression (VAR), are established. Particularly
in the VAR setting, the low-rank contributions can come from broad trends
exhibited in the time series. Details of the algorithm for learning the model
are presented. Experiments demonstrate the performance of the model and the
estimation algorithm on simulated and real data.",SILVar: Single Index Latent Variable Models,forward method np hardness system
874,"An inductive probabilistic classification rule must generally obey the
principles of Bayesian predictive inference, such that all observed and
unobserved stochastic quantities are jointly modeled and the parameter
uncertainty is fully acknowledged through the posterior predictive
distribution. Several such rules have been recently considered and their
asymptotic behavior has been characterized under the assumption that the
observed features or variables used for building a classifier are conditionally
independent given a simultaneous labeling of both the training samples and
those from an unknown origin. Here we extend the theoretical results to
predictive classifiers acknowledging feature dependencies either through
graphical models or sparser alternatives defined as stratified graphical
models. We also show through experimentation with both synthetic and real data
that the predictive classifiers based on stratified graphical models have
consistently best accuracy compared with the predictive classifiers based on
either conditionally independent features or on ordinary graphical models.","Marginal and simultaneous predictive classification using stratified
  graphical models",data engineering  descriptors
875,"Learning by children and animals occurs effortlessly and largely without
obvious supervision. Successes in automating supervised learning have not
translated to the more ambiguous realm of unsupervised learning where goals and
labels are not provided. Barlow (1961) suggested that the signal that brains
leverage for unsupervised learning is dependence, or redundancy, in the sensory
environment. Dependence can be characterized using the information-theoretic
multivariate mutual information measure called total correlation. The principle
of Total Cor-relation Ex-planation (CorEx) is to learn representations of data
that ""explain"" as much dependence in the data as possible. We review some
manifestations of this principle along with successes in unsupervised learning
problems across diverse domains including human behavior, biology, and
language.",Unsupervised Learning via Total Correlation Explanation,programming conventional model data demonstrate show research video
876,"The heuristic identification of peaks from noisy complex spectra often leads
to misunderstanding of the physical and chemical properties of matter. In this
paper, we propose a framework based on Bayesian inference, which enables us to
separate multipeak spectra into single peaks statistically and consists of two
steps. The first step is estimating both the noise variance and the number of
peaks as hyperparameters based on Bayes free energy, which generally is not
analytically tractable. The second step is fitting the parameters of each peak
function to the given spectrum by calculating the posterior density, which has
a problem of local minima and saddles since multipeak models are nonlinear and
hierarchical. Our framework enables the escape from local minima or saddles by
using the exchange Monte Carlo method and calculates Bayes free energy via the
multiple histogram method. We discuss a simulation demonstrating how efficient
our framework is and show that estimating both the noise variance and the
number of peaks prevents overfitting, overpenalizing, and misunderstanding the
precision of parameter estimation.","Simultaneous Estimation of Noise Variance and Number of Peaks in
  Bayesian Spectral Deconvolution",noise models show various set domain
877,"This paper proposes an unsupervised learning technique by using Multi-layer
Mirroring Neural Network and Forgy's clustering algorithm. Multi-layer
Mirroring Neural Network is a neural network that can be trained with
generalized data inputs (different categories of image patterns) to perform
non-linear dimensionality reduction and the resultant low-dimensional code is
used for unsupervised pattern classification using Forgy's algorithm. By
adapting the non-linear activation function (modified sigmoidal function) and
initializing the weights and bias terms to small random values, mirroring of
the input pattern is initiated. In training, the weights and bias terms are
changed in such a way that the input presented is reproduced at the output by
back propagating the error. The mirroring neural network is capable of reducing
the input vector to a great degree (approximately 1/30th the original size) and
also able to reconstruct the input pattern at the output layer from this
reduced code units. The feature set (output of central hidden layer) extracted
from this network is fed to Forgy's algorithm, which classify input data
patterns into distinguishable classes. In the implementation of Forgy's
algorithm, initial seed points are selected in such a way that they are distant
enough to be perfectly grouped into different categories. Thus a new method of
unsupervised learning is formulated and demonstrated in this paper. This method
gave impressive results when applied to classification of different image
patterns.","Automatic Pattern Classification by Unsupervised Learning Using
  Dimensionality Reduction of Data with Mirroring Neural Networks",proposed assumption types common
878,"Authorship attribution mainly deals with undecided authorship of literary
texts. Authorship attribution is useful in resolving issues like uncertain
authorship, recognize authorship of unknown texts, spot plagiarism so on.
Statistical methods can be used to set apart the approach of an author
numerically. The basic methodologies that are made use in computational
stylometry are word length, sentence length, vocabulary affluence, frequencies
etc. Each author has an inborn style of writing, which is particular to
himself. Statistical quantitative techniques can be used to differentiate the
approach of an author in a numerical way. The problem can be broken down into
three sub problems as author identification, author characterization and
similarity detection. The steps involved are pre-processing, extracting
features, classification and author identification. For this different
classifiers can be used. Here fuzzy learning classifier and SVM are used. After
author identification the SVM was found to have more accuracy than Fuzzy
classifier. Later combined the classifiers to obtain a better accuracy when
compared to individual SVM and fuzzy classifier.",Text Classification For Authorship Attribution Analysis,<unk> <unk> <unk> axes for
879,"POMDPs are standard models for probabilistic planning problems, where an
agent interacts with an uncertain environment. We study the problem of
almost-sure reachability, where given a set of target states, the question is
to decide whether there is a policy to ensure that the target set is reached
with probability 1 (almost-surely). While in general the problem is
EXPTIME-complete, in many practical cases policies with a small amount of
memory suffice. Moreover, the existing solution to the problem is explicit,
which first requires to construct explicitly an exponential reduction to a
belief-support MDP. In this work, we first study the existence of
observation-stationary strategies, which is NP-complete, and then small-memory
strategies. We present a symbolic algorithm by an efficient encoding to SAT and
using a SAT solver for the problem. We report experimental results
demonstrating the scalability of our symbolic (SAT-based) approach.","A Symbolic SAT-based Algorithm for Almost-sure Reachability with Small
  Strategies in POMDPs",new of new data including recently discuss
880,"While deep reinforcement learning (RL) methods have achieved unprecedented
successes in a range of challenging problems, their applicability has been
mainly limited to simulation or game domains due to the high sample complexity
of the trial-and-error learning process. However, real-world robotic
applications often need a data-efficient learning process with safety-critical
constraints. In this paper, we consider the challenging problem of learning
unmanned aerial vehicle (UAV) control for tracking a moving target. To acquire
a strategy that combines perception and control, we represent the policy by a
convolutional neural network. We develop a hierarchical approach that combines
a model-free policy gradient method with a conventional feedback
proportional-integral-derivative (PID) controller to enable stable learning
without catastrophic failure. The neural network is trained by a combination of
supervised learning from raw images and reinforcement learning from games of
self-play. We show that the proposed approach can learn a target following
policy in a simulator efficiently and the learned behavior can be successfully
transferred to the DJI quadrotor platform for real-world UAV control.",Learning Unmanned Aerial Vehicle Control for Autonomous Target Following,kernel encoder decoder show representations complexity data 
881,"When designing systems that are complex, dynamic and stochastic in nature,
simulation is generally recognised as one of the best design support
technologies, and a valuable aid in the strategic and tactical decision making
process. A simulation model consists of a set of rules that define how a system
changes over time, given its current state. Unlike analytical models, a
simulation model is not solved but is run and the changes of system states can
be observed at any point in time. This provides an insight into system dynamics
rather than just predicting the output of a system based on specific inputs.
Simulation is not a decision making tool but a decision support tool, allowing
better informed decisions to be made. Due to the complexity of the real world,
a simulation model can only be an approximation of the target system. The
essence of the art of simulation modelling is abstraction and simplification.
Only those characteristics that are important for the study and analysis of the
target system should be included in the simulation model.",Introduction to Multi-Agent Simulation,<unk> various public model agents  data 
882,"One of the important requirements in image retrieval, indexing,
classification, clustering and etc. is extracting efficient features from
images. The color feature is one of the most widely used visual features. Use
of color histogram is the most common way for representing color feature. One
of disadvantage of the color histogram is that it does not take the color
spatial distribution into consideration. In this paper dynamic color
distribution entropy of neighborhoods method based on color distribution
entropy is presented, which effectively describes the spatial information of
colors. The image retrieval results in compare to improved color distribution
entropy show the acceptable efficiency of this approach.","A New Color Feature Extraction Method Based on Dynamic Color
  Distribution Entropy of Neighborhoods",planning conventional proposed data <unk>
883,"Recent studies have highlighted the vulnerability of deep neural networks
(DNNs) to adversarial examples - a visually indistinguishable adversarial image
can easily be crafted to cause a well-trained model to misclassify. Existing
methods for crafting adversarial examples are based on $L_2$ and $L_\infty$
distortion metrics. However, despite the fact that $L_1$ distortion accounts
for the total variation and encourages sparsity in the perturbation, little has
been developed for crafting $L_1$-based adversarial examples. In this paper, we
formulate the process of attacking DNNs via adversarial examples as an
elastic-net regularized optimization problem. Our elastic-net attacks to DNNs
(EAD) feature $L_1$-oriented adversarial examples and include the
state-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,
CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial
examples with small $L_1$ distortion and attains similar attack performance to
the state-of-the-art methods in different attack scenarios. More importantly,
EAD leads to improved attack transferability and complements adversarial
training for DNNs, suggesting novel insights on leveraging $L_1$ distortion in
adversarial machine learning and security implications of DNNs.","EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial
  Examples",video <unk> based distribution tasks  equivalence
884,"Two types of probabilistic maps are popular in the mobile robotics
literature: occupancy grids and geometric maps. Occupancy grids have the
advantages of simplicity and speed, but they represent only a restricted class
of maps and they make incorrect independence assumptions. On the other hand,
current geometric approaches, which characterize the environment by features
such as line segments, can represent complex environments compactly. However,
they do not reason explicitly about occupancy, a necessity for motion planning;
and, they lack a complete probability model over environmental structures. In
this paper we present a probabilistic mapping technique based on polygonal
random fields (PRF), which combines the advantages of both approaches. Our
approach explicitly represents occupancy using a geometric representation, and
it is based upon a consistent probability distribution over environments which
avoids the incorrect independence assumptions made by occupancy grids. We show
how sampling techniques for PRFs can be applied to localized laser and sonar
data, and we demonstrate significant improvements in mapping performance over
occupancy grids.",Robotic Mapping with Polygonal Random Fields,robust mapping existing show score learning execution model
885,"Unlike traditional programs (such as operating systems or word processors)
which have large amounts of code, machine learning tasks use programs with
relatively small amounts of code (written in machine learning libraries), but
voluminous amounts of data. Just like developers of traditional programs debug
errors in their code, developers of machine learning tasks debug and fix errors
in their data. However, algorithms and tools for debugging and fixing errors in
data are less common, when compared to their counterparts for detecting and
fixing errors in code. In this paper, we consider classification tasks where
errors in training data lead to misclassifications in test points, and propose
an automated method to find the root causes of such misclassifications. Our
root cause analysis is based on Pearl's theory of causation, and uses Pearl's
PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,
encodes the computation of PS as a probabilistic program, and uses recent work
on probabilistic programs and transformations on probabilistic programs (along
with gray-box models of machine learning algorithms) to efficiently compute PS.
Psi is able to identify root causes of data errors in interesting data sets.",Debugging Machine Learning Tasks,models model paper  variables  uses method color  metric 
886,"The present study is focused on the automatic identification and description
of frozen similes in British and French novels written between the 19 th
century and the beginning of the 20 th century. Two main patterns of frozen
similes were considered: adjectival ground + simile marker + nominal vehicle
(e.g. happy as a lark) and eventuality + simile marker + nominal vehicle (e.g.
sleep like a top). All potential similes and their components were first
extracted using a rule-based algorithm. Then, frozen similes were identified
based on reference lists of existing similes and semantic distance between the
tenor and the vehicle. The results obtained tend to confirm the fact that
frozen similes are not used haphazardly in literary texts. In addition,
contrary to how they are often presented, frozen similes often go beyond the
ground or the eventuality and the vehicle to also include the tenor.","""Pale as death"" or ""pâle comme la mort"" : Frozen similes used as
  literary clichés",empirical the proposed images  sample
887,"Symbol detection techniques in online handwritten graphics (e.g. diagrams and
mathematical expressions) consist of methods specifically designed for a single
graphic type. In this work, we evaluate the Faster R-CNN object detection
algorithm as a general method for detection of symbols in handwritten graphics.
We evaluate different configurations of the Faster R-CNN method, and point out
issues relative to the handwritten nature of the data. Considering the online
recognition context, we evaluate efficiency and accuracy trade-offs of using
Deep Neural Networks of different complexities as feature extractors. We
evaluate the method on publicly available flowchart and mathematical expression
(CROHME-2016) datasets. Results show that Faster R-CNN can be effectively used
on both datasets, enabling the possibility of developing general methods for
symbol detection, and furthermore, general graphic understanding methods that
could be built on top of the algorithm.",Symbol detection in online handwritten graphics using Faster R-CNN,models model games similarity estimated
888,"Our study applies statistical methods to French and Italian corpora to
examine the phenomenon of multi-word term reduction in specialty languages.
There are two kinds of reduction: anaphoric and lexical. We show that anaphoric
reduction depends on the discourse type (vulgarization, pedagogical,
specialized) but is independent of both domain and language; that lexical
reduction depends on domain and is more frequent in technical, rapidly evolving
domains; and that anaphoric reductions tend to follow full terms rather than
precede them. We define the notion of the anaphoric tree of the term and study
its properties. Concerning lexical reduction, we attempt to prove statistically
that there is a notion of term lifecycle, where the full form is progressively
replaced by a lexical reduction. ----- Nous \'etudions par des m\'ethodes
statistiques sur des corpus fran\c{c}ais et italiens, le ph\'enom\`ene de
r\'eduction des termes complexes dans les langues de sp\'ecialit\'e. Il existe
deux types de r\'eductions : anaphorique et lexicale. Nous montrons que la
r\'eduction anaphorique d\'epend du type de discours (de vulgarisation,
p\'edagogique, sp\'ecialis\'e) mais ne d\'epend ni du domaine, ni de la langue,
alors que la r\'eduction lexicale d\'epend du domaine et est plus fr\'equente
dans les domaines techniques \`a \'evolution rapide. D'autre part, nous
montrons que la r\'eduction anaphorique a tendance \`a suivre la forme pleine
du terme, nous d\'efinissons une notion d'arbre anaphorique de terme et nous
\'etudions ses propri\'et\'es. Concernant la r\'eduction lexicale, nous tentons
de d\'emontrer statistiquement qu'il existe une notion de cycle de vie de
terme, o\`u la forme pleine est progressivement remplac\'ee par une r\'eduction
lexicale.",La réduction de termes complexes dans les langues de spécialité,words show show feature real approaches
889,"Distance metric learning is of fundamental interest in machine learning
because the distance metric employed can significantly affect the performance
of many learning methods. Quadratic Mahalanobis metric learning is a popular
approach to the problem, but typically requires solving a semidefinite
programming (SDP) problem, which is computationally expensive. Standard
interior-point SDP solvers typically have a complexity of $O(D^{6.5})$ (with
$D$ the dimension of input data), and can thus only practically solve problems
exhibiting less than a few thousand variables. Since the number of variables is
$D (D+1) / 2 $, this implies a limit upon the size of problem that can
practically be solved of around a few hundred dimensions. The complexity of the
popular quadratic Mahalanobis metric learning approach thus limits the size of
problem to which metric learning can be applied. Here we propose a
significantly more efficient approach to the metric learning problem based on
the Lagrange dual formulation of the problem. The proposed formulation is much
simpler to implement, and therefore allows much larger Mahalanobis metric
learning problems to be solved. The time complexity of the proposed method is
$O (D ^ 3) $, which is significantly lower than that of the SDP approach.
Experiments on a variety of datasets demonstrate that the proposed method
achieves an accuracy comparable to the state-of-the-art, but is applicable to
significantly larger problems. We also show that the proposed method can be
applied to solve more general Frobenius-norm regularized SDP problems
approximately.",An Efficient Dual Approach to Distance Metric Learning,square method complexity for
890,"Bounds on the log partition function are important in a variety of contexts,
including approximate inference, model fitting, decision theory, and large
deviations analysis. We introduce a new class of upper bounds on the log
partition function, based on convex combinations of distributions in the
exponential domain, that is applicable to an arbitrary undirected graphical
model. In the special case of convex combinations of tree-structured
distributions, we obtain a family of variational problems, similar to the Bethe
free energy, but distinguished by the following desirable properties: i. they
are cnvex, and have a unique global minimum; and ii. the global minimum gives
an upper bound on the log partition function. The global minimum is defined by
stationary conditions very similar to those defining fixed points of belief
propagation or tree-based reparameterization Wainwright et al., 2001. As with
BP fixed points, the elements of the minimizing argument can be used as
approximations to the marginals of the original model. The analysis described
here can be extended to structures of higher treewidth e.g., hypertrees,
thereby making connections with more advanced approximations e.g., Kikuchi and
variants Yedidia et al., 2001; Minka, 2001.",A New Class of Upper Bounds on the Log Partition Function,train model <unk> show researchers models model
891,"In the last decade, special purpose computing systems, such as Neuromorphic
computing, have become very popular in the field of computer vision and machine
learning for classification tasks. In 2015, IBM's released the TrueNorth
Neuromorphic system, kick-starting a new era of Neuromorphic computing.
Alternatively, Deep Learning approaches such as Deep Convolutional Neural
Networks (DCNN) show almost human-level accuracies for detection and
classification tasks. IBM's 2016 release of a deep learning framework for
DCNNs, called Energy Efficient Deep Neuromorphic Networks (Eedn). Eedn shows
promise for delivering high accuracies across a number of different benchmarks,
while consuming very low power, using IBM's TrueNorth chip. However, there are
many things that remained undiscovered using the Eedn framework for
classification tasks on a Neuromorphic system. In this paper, we have
empirically evaluated the performance of different DCNN architectures
implemented within the Eedn framework. The goal of this work was discover the
most efficient way to implement DCNN models for object classification tasks
using the TrueNorth system. We performed our experiments using benchmark data
sets such as MNIST, COIL 20, and COIL 100. The experimental results show very
promising classification accuracies with very low power consumption on IBM's
NS1e Neurosynaptic system. The results show that for datasets with large
numbers of classes, wider networks perform better when compared to deep
networks comprised of nearly the same core complexity on IBM's TrueNorth
system.","Deep Versus Wide Convolutional Neural Networks for Object Recognition on
  Neuromorphic System",natural using show the
892,"Recent advances in AI and robotics have claimed many incredible results with
deep learning, yet no work to date has applied deep learning to the problem of
liquid perception and reasoning. In this paper, we apply fully-convolutional
deep neural networks to the tasks of detecting and tracking liquids. We
evaluate three models: a single-frame network, multi-frame network, and a LSTM
recurrent network. Our results show that the best liquid detection results are
achieved when aggregating data over multiple frames and that the LSTM network
outperforms the other two in both tasks. This suggests that LSTM-based neural
networks have the potential to be a key component for enabling robots to handle
liquids using robust, closed-loop controllers.",Towards Learning to Perceive and Reason About Liquids,for show cross correlation data still improve
893,"Let $G=(V,E,w)$ be a finite, connected graph with weighted edges. We are
interested in the problem of finding a subset $W \subset V$ of vertices and
weights $a_w$ such that $$ \frac{1}{|V|}\sum_{v \in V}^{}{f(v)} \sim \sum_{w
\in W}{a_w f(w)}$$ for functions $f:V \rightarrow \mathbb{R}$ that are `smooth'
with respect to the geometry of the graph. The main application are problems
where $f$ is known to somehow depend on the underlying graph but is expensive
to evaluate on even a single vertex. We prove an inequality showing that the
integration problem can be rewritten as a geometric problem (`the optimal
packing of heat balls'). We discuss how one would construct approximate
solutions of the heat ball packing problem; numerical examples demonstrate the
efficiency of the method.",Numerical Integration on Graphs: where to sample and how to weigh,models model <unk>
894,"The implicit bias of gradient descent is not fully understood even in simple
linear classification tasks (e.g., logistic regression). Soudry et al. (2018)
studied this bias on separable data, where there are multiple solutions that
correctly classify the data. It was found that, when optimizing monotonically
decreasing loss functions with exponential tails using gradient descent, the
linear classifier specified by the gradient descent iterates converge to the
$L_2$ max margin separator. However, the convergence rate to the maximum margin
solution with fixed step size was found to be extremely slow: $1/\log(t)$.
  Here we examine how the convergence is influenced by using different loss
functions and by using variable step sizes. First, we calculate the convergence
rate for loss functions with poly-exponential tails near $\exp(-u^{\nu})$. We
prove that $\nu=1$ yields the optimal convergence rate in the range $\nu>0.25$.
Based on further analysis we conjecture that this remains the optimal rate for
$\nu \leq 0.25$, and even for sub-poly-exponential tails --- until loss
functions with polynomial tails no longer converge to the max margin. Second,
we prove the convergence rate could be improved to $(\log t) /\sqrt{t}$ for the
exponential loss, by using aggressive step sizes which compensate for the
rapidly vanishing gradients.",Convergence of Gradient Descent on Separable Data,model video theory show uncertainties
895,"Artificial bee colony (ABC), an optimization algorithm is a recent addition
to the family of population based search algorithm. ABC has taken its
inspiration from the collective intelligent foraging behavior of honey bees. In
this study we have incorporated golden section search mechanism in the
structure of basic ABC to improve the global convergence and prevent to stick
on a local solution. The proposed variant is termed as ILS-ABC. Comparative
numerical results with the state-of-art algorithms show the performance of the
proposal when applied to the set of unconstrained engineering design problems.
The simulated results show that the proposed variant can be successfully
applied to solve real life problems.","Improved Local Search in Artificial Bee Colony using Golden Section
  Search",step size <unk>
896,"In this paper, we propose a new network architecture for Chinese typography
transformation based on deep learning. The architecture consists of two
sub-networks: (1)a fully convolutional network(FCN) aiming at transferring
specified typography style to another in condition of preserving structure
information; (2)an adversarial network aiming at generating more realistic
strokes in some details. Unlike models proposed before 2012 relying on the
complex segmentation of Chinese components or strokes, our model treats every
Chinese character as an inseparable image, so pre-processing or
post-preprocessing are abandoned. Besides, our model adopts end-to-end training
without pre-trained used in other deep models. The experiments demonstrates
that our model can synthesize realistic-looking target typography from any
source typography both on printed style and handwriting style.",Chinese Typography Transfer,model boxes method <unk>
897,"Deep neural networks have been shown to achieve state-of-the-art performance
in several machine learning tasks. Stochastic Gradient Descent (SGD) is the
preferred optimization algorithm for training these networks and asynchronous
SGD (ASGD) has been widely adopted for accelerating the training of large-scale
deep networks in a distributed computing environment. However, in practice it
is quite challenging to tune the training hyperparameters (such as learning
rate) when using ASGD so as achieve convergence and linear speedup, since the
stability of the optimization algorithm is strongly influenced by the
asynchronous nature of parameter updates. In this paper, we propose a variant
of the ASGD algorithm in which the learning rate is modulated according to the
gradient staleness and provide theoretical guarantees for convergence of this
algorithm. Experimental verification is performed on commonly-used image
classification benchmarks: CIFAR10 and Imagenet to demonstrate the superior
effectiveness of the proposed approach, compared to SSGD (Synchronous SGD) and
the conventional ASGD algorithm.",Staleness-aware Async-SGD for Distributed Deep Learning,model model <unk> using made common
898,"In this paper, we propose an automated evaluation metric for text entry. We
also consider possible improvements to existing text entry evaluation metrics,
such as the minimum string distance error rate, keystrokes per character, cost
per correction, and a unified approach proposed by MacKenzie, so they can
accommodate the special characteristics of Chinese text. Current methods lack
an integrated concern about both typing speed and accuracy for Chinese text
entry evaluation. Our goal is to remove the bias that arises due to human
factors. First, we propose a new metric, called the correction penalty (P),
based on Fitts' law and Hick's law. Next, we transform it into the approximate
amortized cost (AAC) of information theory. An analysis of the AAC of Chinese
text input methods with different context lengths is also presented.",An Automated Evaluation Metric for Chinese Text Entry,<unk> improve method process radar
899,"Recent literature on online learning has focused on developing adaptive
algorithms that take advantage of a regularity of the sequence of observations,
yet retain worst-case performance guarantees. A complementary direction is to
develop prediction methods that perform well against complex benchmarks. In
this paper, we address these two directions together. We present a fully
adaptive method that competes with dynamic benchmarks in which regret guarantee
scales with regularity of the sequence of cost functions and comparators.
Notably, the regret bound adapts to the smaller complexity measure in the
problem environment. Finally, we apply our results to drifting zero-sum,
two-player games where both players achieve no regret guarantees against best
sequences of actions in hindsight.",Online Optimization : Competing with Dynamic Comparators,using train similarity estimated show networks data 
900,"While the research on convolutional neural networks (CNNs) is progressing
quickly, the real-world deployment of these models is often limited by
computing resources and memory constraints. In this paper, we address this
issue by proposing a novel filter pruning method to compress and accelerate
CNNs. Our work is based on the linear relationship identified in different
feature map subspaces via visualization of feature maps. Such linear
relationship implies that the information in CNNs is redundant. Our method
eliminates the redundancy in convolutional filters by applying subspace
clustering to feature maps. In this way, most of the representative information
in the network can be retained in each cluster. Therefore, our method provides
an effective solution to filter pruning for which most existing methods
directly remove filters based on simple heuristics. The proposed method is
independent of the network structure, thus it can be adopted by any
off-the-shelf deep learning libraries. Experiments on different networks and
tasks show that our method outperforms existing techniques before fine-tuning,
and achieves the state-of-the-art results after fine-tuning.","Exploring Linear Relationship in Feature Map Subspace for ConvNets
  Compression",feature word for show signals  offers online
901,"We review the problem of defining and inferring a ""state"" for a control
system based on complex, high-dimensional, highly uncertain measurement streams
such as videos. Such a state, or representation, should contain all and only
the information needed for control, and discount nuisance variability in the
data. It should also have finite complexity, ideally modulated depending on
available resources. This representation is what we want to store in memory in
lieu of the data, as it ""separates"" the control task from the measurement
process. For the trivial case with no dynamics, a representation can be
inferred by minimizing the Information Bottleneck Lagrangian in a function
class realized by deep neural networks. The resulting representation has much
higher dimension than the data, already in the millions, but it is smaller in
the sense of information content, retaining only what is needed for the task.
This process also yields representations that are invariant to nuisance factors
and having maximally independent components. We extend these ideas to the
dynamic case, where the representation is the posterior density of the task
variable given the measurements up to the current time, which is in general
much simpler than the prediction density maintained by the classical Bayesian
filter. Again this can be finitely-parametrized using a deep neural network,
and already some applications are beginning to emerge. No explicit assumption
of Markovianity is needed; instead, complexity trades off approximation of an
optimal representation, including the degree of Markovianity.",A Separation Principle for Control in the Age of Deep Learning,representations reduces model <unk>
902,"Metaheuristic algorithms are becoming an important part of modern
optimization. A wide range of metaheuristic algorithms have emerged over the
last two decades, and many metaheuristics such as particle swarm optimization
are becoming increasingly popular. Despite their popularity, mathematical
analysis of these algorithms lacks behind. Convergence analysis still remains
unsolved for the majority of metaheuristic algorithms, while efficiency
analysis is equally challenging. In this paper, we intend to provide an
overview of convergence and efficiency studies of metaheuristics, and try to
provide a framework for analyzing metaheuristics in terms of convergence and
efficiency. This can form a basis for analyzing other algorithms. We also
outline some open questions as further research topics.",Metaheuristic Optimization: Algorithm Analysis and Open Problems,video <unk> show users users model discovery minimal
903,"We consider the exploration-exploitation tradeoff in linear quadratic (LQ)
control problems, where the state dynamics is linear and the cost function is
quadratic in states and controls. We analyze the regret of Thompson sampling
(TS) (a.k.a. posterior-sampling for reinforcement learning) in the frequentist
setting, i.e., when the parameters characterizing the LQ dynamics are fixed.
Despite the empirical and theoretical success in a wide range of problems from
multi-armed bandit to linear bandit, we show that when studying the frequentist
regret TS in control problems, we need to trade-off the frequency of sampling
optimistic parameters and the frequency of switches in the control policy. This
results in an overall regret of $O(T^{2/3})$, which is significantly worse than
the regret $O(\sqrt{T})$ achieved by the optimism-in-face-of-uncertainty
algorithm in LQ control problems.",Thompson Sampling for Linear-Quadratic Control Problems,input model input input
904,"For analysis of a high-dimensional dataset, a common approach is to test a
null hypothesis of statistical independence on all variable pairs using a
non-parametric measure of dependence. However, because this approach attempts
to identify any non-trivial relationship no matter how weak, it often
identifies too many relationships to be useful. What is needed is a way of
identifying a smaller set of relationships that merit detailed further
analysis.
  Here we formally present and characterize equitability, a property of
measures of dependence that aims to overcome this challenge. Notionally, an
equitable statistic is a statistic that, given some measure of noise, assigns
similar scores to equally noisy relationships of different types [Reshef et al.
2011]. We begin by formalizing this idea via a new object called the
interpretable interval, which functions as an interval estimate of the amount
of noise in a relationship of unknown type. We define an equitable statistic as
one with small interpretable intervals.
  We then draw on the equivalence of interval estimation and hypothesis testing
to show that under moderate assumptions an equitable statistic is one that
yields well powered tests for distinguishing not only between trivial and
non-trivial relationships of all kinds but also between non-trivial
relationships of different strengths. This means that equitability allows us to
specify a threshold relationship strength $x_0$ and to search for relationships
of all kinds with strength greater than $x_0$. Thus, equitability can be
thought of as a strengthening of power against independence that enables
fruitful analysis of data sets with a small number of strong, interesting
relationships and a large number of weaker ones. We conclude with a
demonstration of how our two equivalent characterizations of equitability can
be used to evaluate the equitability of a statistic in practice.","Equitability, interval estimation, and statistical power",based images  role show categorization dynamics dynamics online
905,"We consider the problem of distributed statistical machine learning in
adversarial settings, where some unknown and time-varying subset of working
machines may be compromised and behave arbitrarily to prevent an accurate model
from being learned. This setting captures the potential adversarial attacks
faced by Federated Learning -- a modern machine learning paradigm that is
proposed by Google researchers and has been intensively studied for ensuring
user privacy. Formally, we focus on a distributed system consisting of a
parameter server and $m$ working machines. Each working machine keeps $N/m$
data samples, where $N$ is the total number of samples. The goal is to
collectively learn the underlying true model parameter of dimension $d$.
  In classical batch gradient descent methods, the gradients reported to the
server by the working machines are aggregated via simple averaging, which is
vulnerable to a single Byzantine failure. In this paper, we propose a Byzantine
gradient descent method based on the geometric median of means of the
gradients. We show that our method can tolerate $q \le (m-1)/2$ Byzantine
failures, and the parameter estimate converges in $O(\log N)$ rounds with an
estimation error of $\sqrt{d(2q+1)/N}$, hence approaching the optimal error
rate $\sqrt{d/N}$ in the centralized and failure-free setting. The total
computational complexity of our algorithm is of $O((Nd/m) \log N)$ at each
working machine and $O(md + kd \log^3 N)$ at the central server, and the total
communication cost is of $O(m d \log N)$. We further provide an application of
our general results to the linear regression problem.
  A key challenge arises in the above problem is that Byzantine failures create
arbitrary and unspecified dependency among the iterations and the aggregated
gradients. We prove that the aggregated gradient converges uniformly to the
true gradient function.","Distributed Statistical Machine Learning in Adversarial Settings:
  Byzantine Gradient Descent",models model however  complexity for
906,"Seasonality is a distinctive characteristic which is often observed in many
practical time series. Artificial Neural Networks (ANNs) are a class of
promising models for efficiently recognizing and forecasting seasonal patterns.
In this paper, the Particle Swarm Optimization (PSO) approach is used to
enhance the forecasting strengths of feedforward ANN (FANN) as well as Elman
ANN (EANN) models for seasonal data. Three widely popular versions of the basic
PSO algorithm, viz. Trelea-I, Trelea-II and Clerc-Type1 are considered here.
The empirical analysis is conducted on three real-world seasonal time series.
Results clearly show that each version of the PSO algorithm achieves notably
better forecasting accuracies than the standard Backpropagation (BP) training
method for both FANN and EANN models. The neural network forecasting results
are also compared with those from the three traditional statistical models,
viz. Seasonal Autoregressive Integrated Moving Average (SARIMA), Holt-Winters
(HW) and Support Vector Machine (SVM). The comparison demonstrates that both
PSO and BP based neural networks outperform SARIMA, HW and SVM models for all
three time series datasets. The forecasting performances of ANNs are further
improved through combining the outputs from the three PSO based models.","PSO based Neural Networks vs. Traditional Statistical Models for
  Seasonal Time Series Forecasting",theory ranking show significant search
907,"Word embeddings are now a standard technique for inducing meaning
representations for words. For getting good representations, it is important to
take into account different senses of a word. In this paper, we propose a
mixture model for learning multi-sense word embeddings. Our model generalizes
the previous works in that it allows to induce different weights of different
senses of a word. The experimental results show that our model outperforms
previous models on standard evaluation tasks.",A Mixture Model for Learning Multi-Sense Word Embeddings,models show <unk> graph bandit
908,"Symmetric matrices are widely used in machine learning problems such as
kernel machines and manifold learning. Using large datasets often requires
computing low-rank approximations of these symmetric matrices so that they fit
in memory. In this paper, we present a novel method based on biharmonic
interpolation for low-rank matrix approximation. The method exploits knowledge
of the data manifold to learn an interpolation operator that approximates
values using a subset of randomly selected landmark points. This operator is
readily sparsified, reducing memory requirements by at least two orders of
magnitude without significant loss in accuracy. We show that our method can
approximate very large datasets using twenty times more landmarks than other
methods. Further, numerical results suggest that our method is stable even when
numerical difficulties arise for other methods.","Sparse and low-rank approximations of large symmetric matrices using
  biharmonic interpolation",model data <unk> use show types paper  for
909,"Globally, in 2016, one out of eleven adults suffered from Diabetes Mellitus.
Diabetic Foot Ulcers (DFU) are a major complication of this disease, which if
not managed properly can lead to amputation. Current clinical approaches to DFU
treatment rely on patient and clinician vigilance, which has significant
limitations such as the high cost involved in the diagnosis, treatment and
lengthy care of the DFU. We collected an extensive dataset of foot images,
which contain DFU from different patients. In this paper, we have proposed the
use of traditional computer vision features for detecting foot ulcers among
diabetic patients, which represent a cost-effective, remote and convenient
healthcare solution. Furthermore, we used Convolutional Neural Networks (CNNs)
for the first time in DFU classification. We have proposed a novel
convolutional neural network architecture, DFUNet, with better feature
extraction to identify the feature differences between healthy skin and the
DFU. Using 10-fold cross-validation, DFUNet achieved an AUC score of 0.962.
This outperformed both the machine learning and deep learning classifiers we
have tested. Here we present the development of a novel and highly sensitive
DFUNet for objectively detecting the presence of DFUs. This novel approach has
the potential to deliver a paradigm shift in diabetic foot care.","DFUNet: Convolutional Neural Networks for Diabetic Foot Ulcer
  Classification",data  show rational <unk>
910,"We propose a simple and fast algorithm called PatchLift for computing
distances between patches (contiguous block of samples) extracted from a given
one-dimensional signal. PatchLift is based on the observation that the patch
distances can be efficiently computed from a matrix that is derived from the
one-dimensional signal using lifting; importantly, the number of operations
required to compute the patch distances using this approach does not scale with
the patch length. We next demonstrate how PatchLift can be used for patch-based
denoising of images corrupted with Gaussian noise. In particular, we propose a
separable formulation of the classical Non-Local Means (NLM) algorithm that can
be implemented using PatchLift. We demonstrate that the PatchLift-based
implementation of separable NLM is few orders faster than standard NLM, and is
competitive with existing fast implementations of NLM. Moreover, its denoising
performance is shown to be consistently superior to that of NLM and some of its
variants, both in terms of PSNR/SSIM and visual quality.",Fast Separable Non-Local Means,models model connection problem models new bias
911,"Generative Adversarial Networks (GANs) have been shown to be able to sample
impressively realistic images. GAN training consists of a saddle point
optimization problem that can be thought of as an adversarial game between a
generator which produces the images, and a discriminator, which judges if the
images are real. Both the generator and the discriminator are commonly
parametrized as deep convolutional neural networks. The goal of this paper is
to disentangle the contribution of the optimization procedure and the network
parametrization to the success of GANs. To this end we introduce and study
Generative Latent Optimization (GLO), a framework to train a generator without
the need to learn a discriminator, thus avoiding challenging adversarial
optimization problems. We show experimentally that GLO enjoys many of the
desirable properties of GANs: learning from large data, synthesizing
visually-appealing samples, interpolating meaningfully between samples, and
performing linear arithmetic with noise vectors.",Optimizing the Latent Space of Generative Networks,observation partial shown
912,"In this work a new way to calculate the multivariate joint entropy is
presented. This measure is the basis for a fast information-theoretic based
evaluation of gene relevance in a Microarray Gene Expression data context. Its
low complexity is based on the reuse of previous computations to calculate
current feature relevance. The mu-TAFS algorithm --named as such to
differentiate it from previous TAFS algorithms-- implements a simulated
annealing technique specially designed for feature subset selection. The
algorithm is applied to the maximization of gene subset relevance in several
public-domain microarray data sets. The experimental results show a notoriously
high classification performance and low size subsets formed by biologically
meaningful genes.","Feature Selection for Microarray Gene Expression Data using Simulated
  Annealing guided by the Multivariate Joint Entropy",<unk> preferences
913,"We propose the first fully-adaptive algorithm for pure exploration in linear
bandits---the task to find the arm with the largest expected reward, which
depends on an unknown parameter linearly. While existing methods partially or
entirely fix sequences of arm selections before observing rewards, our method
adaptively changes the arm selection strategy based on past observations at
each round. We show our sample complexity matches the achievable lower bound up
to a constant factor in an extreme case. Furthermore, we evaluate the
performance of the methods by simulations based on both synthetic setting and
real-world data, in which our method shows vast improvement over existing
methods.",Fully adaptive algorithm for pure exploration in linear bandits,kernels show predictive <unk>
914,"Existing learning-based atmospheric particle-removal approaches such as those
used for rainy and hazy images are designed with strong assumptions regarding
spatial frequency, trajectory, and translucency. However, the removal of snow
particles is more complicated because it possess the additional attributes of
particle size and shape, and these attributes may vary within a single image.
Currently, hand-crafted features are still the mainstream for snow removal,
making significant generalization difficult to achieve. In response, we have
designed a multistage network codenamed DesnowNet to in turn deal with the
removal of translucent and opaque snow particles. We also differentiate snow
into attributes of translucency and chromatic aberration for accurate
estimation. Moreover, our approach individually estimates residual complements
of the snow-free images to recover details obscured by opaque snow.
Additionally, a multi-scale design is utilized throughout the entire network to
model the diversity of snow. As demonstrated in experimental results, our
approach outperforms state-of-the-art learning-based atmospheric phenomena
removal methods and one semantic segmentation baseline on the proposed Snow100K
dataset in both qualitative and quantitative comparisons. The results indicate
our network would benefit applications involving computer vision and graphics.",DesnowNet: Context-Aware Deep Network for Snow Removal,theorem models show however  quality
915,"A good clustering algorithm should not only be able to discover clusters of
arbitrary shapes (global view) but also provide additional information, which
can be used to gain more meaningful insights into the internal structure of the
clusters (local view). In this work we use the mathematical framework of factor
graphs and message passing algorithms to optimize a pairwise similarity based
cost function, in the same spirit as was done in Affinity Propagation. Using
this framework we develop two variants of a new clustering algorithm, EAP and
SHAPE. EAP/SHAPE can not only discover clusters of arbitrary shapes but also
provide a rich local view in the form of meaningful local representatives
(exemplars) and connections between these local exemplars. We discuss how this
local information can be used to gain various insights about the clusters
including varying relative cluster densities and indication of local strength
in different regions of a cluster . We also discuss how this can help an
analyst in discovering and resolving potential inconsistencies in the results.
The efficacy of EAP/SHAPE is shown by applying it to various synthetic and real
world benchmark datasets.","Clustering with Simultaneous Local and Global View of Data: A message
  passing based approach",models new channel  networks
916,"We present a neural encoder-decoder model to convert images into
presentational markup based on a scalable coarse-to-fine attention mechanism.
Our method is evaluated in the context of image-to-LaTeX generation, and we
introduce a new dataset of real-world rendered mathematical expressions paired
with LaTeX markup. We show that unlike neural OCR techniques using CTC-based
models, attention-based approaches can tackle this non-standard OCR task. Our
approach outperforms classical mathematical OCR systems by a large margin on
in-domain rendered data, and, with pretraining, also performs well on
out-of-domain handwritten data. To reduce the inference complexity associated
with the attention-based approaches, we introduce a new coarse-to-fine
attention layer that selects a support region before applying attention.",Image-to-Markup Generation with Coarse-to-Fine Attention,kernel demonstrate show <unk> search
917,"Deep Web databases contain more than 90% of pertinent information of the Web.
Despite their importance, users don't profit of this treasury. Many deep web
services are offering competitive services in term of prices, quality of
service, and facilities. As the number of services is growing rapidly, users
have difficulty to ask many web services in the same time. In this paper, we
imagine a system where users have the possibility to formulate one query using
one query interface and then the system translates query to the rest of query
interfaces. However, interfaces are created by designers in order to be
interpreted visually by users, machines can not interpret query from a given
interface. We propose a new approach which emulates capacity of interpretation
of users and extracts query from deep web query interfaces. Our approach has
proved good performances on two standard datasets.","VIQI: A New Approach for Visual Interpretation of Deep Web Query
  Interfaces",linear image query irregular
918,"We propose a deep neural network for the prediction of future frames in
natural video sequences. To effectively handle complex evolution of pixels in
videos, we propose to decompose the motion and content, two key components
generating dynamics in videos. Our model is built upon the Encoder-Decoder
Convolutional Neural Network and Convolutional LSTM for pixel-level prediction,
which independently capture the spatial layout of an image and the
corresponding temporal dynamics. By independently modeling motion and content,
predicting the next frame reduces to converting the extracted content features
into the next frame content by the identified motion features, which simplifies
the task of prediction. Our model is end-to-end trainable over multiple time
steps, and naturally learns to decompose motion and content without separate
training. We evaluate the proposed network architecture on human activity
videos using KTH, Weizmann action, and UCF-101 datasets. We show
state-of-the-art performance in comparison to recent approaches. To the best of
our knowledge, this is the first end-to-end trainable network architecture with
motion and content separation to model the spatiotemporal dynamics for
pixel-level future prediction in natural videos.",Decomposing Motion and Content for Natural Video Sequence Prediction,video search
919,"We present an approach towards convex optimization that relies on a novel
scheme which converts online adaptive algorithms into offline methods. In the
offline optimization setting, our derived methods are shown to obtain
favourable adaptive guarantees which depend on the harmonic sum of the queried
gradients. We further show that our methods implicitly adapt to the objective's
structure: in the smooth case fast convergence rates are ensured without any
prior knowledge of the smoothness parameter, while still maintaining guarantees
in the non-smooth setting. Our approach has a natural extension to the
stochastic setting, resulting in a lazy version of SGD (stochastic GD), where
minibathces are chosen \emph{adaptively} depending on the magnitude of the
gradients. Thus providing a principled approach towards choosing minibatch
sizes.","Online to Offline Conversions, Universality and Adaptive Minibatch Sizes",noise k nearest develop partial show critical model convex
920,"We introduce LAMBADA, a dataset to evaluate the capabilities of computational
models for text understanding by means of a word prediction task. LAMBADA is a
collection of narrative passages sharing the characteristic that human subjects
are able to guess their last word if they are exposed to the whole passage, but
not if they only see the last sentence preceding the target word. To succeed on
LAMBADA, computational models cannot simply rely on local context, but must be
able to keep track of information in the broader discourse. We show that
LAMBADA exemplifies a wide range of linguistic phenomena, and that none of
several state-of-the-art language models reaches accuracy above 1% on this
novel benchmark. We thus propose LAMBADA as a challenging test set, meant to
encourage the development of new models capable of genuine understanding of
broad context in natural language text.",The LAMBADA dataset: Word prediction requiring a broad discourse context,new bayes optimal behavior show resistive fundamental
921,"Dominance-based Rough Set Approach (DRSA), as the extension of Pawlak's Rough
Set theory, is effective and fundamentally important in Multiple Criteria
Decision Analysis (MCDA). In previous DRSA models, the definitions of the upper
and lower approximations are preserving the class unions rather than the
singleton class. In this paper, we propose a new Class-based Rough
Approximation with respect to a series of previous DRSA models, including
Classical DRSA model, VC-DRSA model and VP-DRSA model. In addition, the new
class-based reducts are investigated.",Class-based Rough Approximation with Dominance Principle,capacity  model eigenvalues inference on
922,"We address the issue of adapting optical images-based edge detection
techniques for use in Polarimetric Synthetic Aperture Radar (PolSAR) imagery.
We modify the gravitational edge detection technique (inspired by the Law of
Universal Gravity) proposed by Lopez-Molina et al, using the non-standard
neighbourhood configuration proposed by Fu et al, to reduce the speckle noise
in polarimetric SAR imagery. We compare the modified and unmodified versions of
the gravitational edge detection technique with the well-established one
proposed by Canny, as well as with a recent multiscale fuzzy-based technique
proposed by Lopez-Molina et Alejandro We also address the issues of aggregation
of gray level images before and after edge detection and of filtering. All
techniques addressed here are applied to a mosaic built using class
distributions obtained from a real scene, as well as to the true PolSAR image;
the mosaic results are assessed using Baddeley's Delta Metric. Our experiments
show that modifying the gravitational edge detection technique with a
non-standard neighbourhood configuration produces better results than the
original technique, as well as the other techniques used for comparison. The
experiments show that adapting edge detection methods from Computational
Intelligence for use in PolSAR imagery is a new field worthy of exploration.",Optical images-based edge detection in Synthetic Aperture Radar images,noise system show based based discrete
923,"A problem faced by many instructors is that of designing exams that
accurately assess the abilities of the students. Typically these exams are
prepared several days in advance, and generic question scores are used based on
rough approximation of the question difficulty and length. For example, for a
recent class taught by the author, there were 30 multiple choice questions
worth 3 points, 15 true/false with explanation questions worth 4 points, and 5
analytical exercises worth 10 points. We describe a novel framework where
algorithms from machine learning are used to modify the exam question weights
in order to optimize the exam scores, using the overall class grade as a proxy
for a student's true ability. We show that significant error reduction can be
obtained by our approach over standard weighting schemes, and we make several
new observations regarding the properties of the ""good"" and ""bad"" exam
questions that can have impact on the design of improved future evaluation
methods.",Optimal Weighting for Exam Composition,quality model differential using
924,"Visual representation is crucial for a visual tracking method's performances.
Conventionally, visual representations adopted in visual tracking rely on
hand-crafted computer vision descriptors. These descriptors were developed
generically without considering tracking-specific information. In this paper,
we propose to learn complex-valued invariant representations from tracked
sequential image patches, via strong temporal slowness constraint and stacked
convolutional autoencoders. The deep slow local representations are learned
offline on unlabeled data and transferred to the observational model of our
proposed tracker. The proposed observational model retains old training samples
to alleviate drift, and collect negative samples which are coherent with
target's motion pattern for better discriminative tracking. With the learned
representation and online training samples, a logistic regression classifier is
adopted to distinguish target from background, and retrained online to adapt to
appearance changes. Subsequently, the observational model is integrated into a
particle filter framework to peform visual tracking. Experimental results on
various challenging benchmark sequences demonstrate that the proposed tracker
performs favourably against several state-of-the-art trackers.","Self-taught learning of a deep invariant representation for visual
  tracking via temporal slowness principle",train similarity estimated
925,"To recover the three dimensional (3D) volumetric distribution of matter in an
object, images of the object are captured from multiple directions and
locations. Using these images tomographic computations extract the
distribution. In highly scattering media and constrained, natural irradiance,
tomography must explicitly account for off-axis scattering. Furthermore, the
tomographic model and recovery must function when imaging is done in-situ, as
occurs in medical imaging and ground-based atmospheric sensing. We formulate
tomography that handles arbitrary orders of scattering, using a monte-carlo
model. Moreover, the model is highly parallelizable in our formulation. This
enables large scale rendering and recovery of volumetric scenes having a large
number of variables. We solve stability and conditioning problems that stem
from radiative transfer (RT) modeling in-situ.",In-situ multi-scattering tomography,programming natural agent show preferences
926,"We describe our system for SemEval-2018 Shared Task on Semantic Relation
Extraction and Classification in Scientific Papers where we focus on the
Classification task. Our simple piecewise convolution neural encoder performs
decently in an end to end manner. A simple inter-task data augmentation
signifi- cantly boosts the performance of the model. Our best-performing
systems stood 8th out of 20 teams on the classification task on noisy data and
12th out of 28 teams on the classification task on clean data.","OhioState at SemEval-2018 Task 7: Exploiting Data Augmentation for
  Relation Classification in Scientific Papers using Piecewise Convolutional
  Neural Networks",model <unk> <unk> show <unk> the
927,"We present a method of training a differentiable function approximator for a
regression task using negative examples. We effect this training using negative
learning rates. We also show how this method can be used to perform direct
policy learning in a reinforcement learning setting.",Negative Learning Rates and P-Learning,deal models  deal outperform
928,"Many high dimensional sparse learning problems are formulated as nonconvex
optimization. A popular approach to solve these nonconvex optimization problems
is through convex relaxations such as linear and semidefinite programming. In
this paper, we study the statistical limits of convex relaxations.
Particularly, we consider two problems: Mean estimation for sparse principal
submatrix and edge probability estimation for stochastic block model. We
exploit the sum-of-squares relaxation hierarchy to sharply characterize the
limits of a broad class of convex relaxations. Our result shows statistical
optimality needs to be compromised for achieving computational tractability
using convex relaxations. Compared with existing results on computational lower
bounds for statistical problems, which consider general polynomial-time
algorithms and rely on computational hardness hypotheses on problems like
planted clique detection, our theory focuses on a broad class of convex
relaxations and does not rely on unproven hypotheses.",Statistical Limits of Convex Relaxations,model despite technique concise density uncertainty  using query
929,"We propose RoBiRank, a ranking algorithm that is motivated by observing a
close connection between evaluation metrics for learning to rank and loss
functions for robust classification. The algorithm shows a very competitive
performance on standard benchmark datasets against other representative
algorithms in the literature. On the other hand, in large scale problems where
explicit feature vectors and scores are not given, our algorithm can be
efficiently parallelized across a large number of machines; for a task that
requires 386,133 x 49,824,519 pairwise interactions between items to be ranked,
our algorithm finds solutions that are of dramatically higher quality than that
can be found by a state-of-the-art competitor algorithm, given the same amount
of wall-clock time for computation.","Ranking via Robust Binary Classification and Parallel Parameter
  Estimation in Large-Scale Data",non conjugate measure show query models using measure variance
930,"We investigate a novel cluster-of-bandit algorithm CAB for collaborative
recommendation tasks that implements the underlying feedback sharing mechanism
by estimating the neighborhood of users in a context-dependent manner. CAB
makes sharp departures from the state of the art by incorporating collaborative
effects into inference as well as learning processes in a manner that
seamlessly interleaving explore-exploit tradeoffs and collaborative steps. We
prove regret bounds under various assumptions on the data, which exhibit a
crisp dependence on the expected number of clusters over the users, a natural
measure of the statistical difficulty of the learning task. Experiments on
production and real-world datasets show that CAB offers significantly increased
prediction performance against a representative pool of state-of-the-art
methods.",On Context-Dependent Clustering of Bandits,paper  both we cross correlation data around
931,"How universal is human conceptual structure? The way concepts are organized
in the human brain may reflect distinct features of cultural, historical, and
environmental background in addition to properties universal to human
cognition. Semantics, or meaning expressed through language, provides direct
access to the underlying conceptual structure, but meaning is notoriously
difficult to measure, let alone parameterize. Here we provide an empirical
measure of semantic proximity between concepts using cross-linguistic
dictionaries. Across languages carefully selected from a phylogenetically and
geographically stratified sample of genera, translations of words reveal cases
where a particular language uses a single polysemous word to express concepts
represented by distinct words in another. We use the frequency of polysemies
linking two concepts as a measure of their semantic proximity, and represent
the pattern of such linkages by a weighted network. This network is highly
uneven and fragmented: certain concepts are far more prone to polysemy than
others, and there emerge naturally interpretable clusters loosely connected to
each other. Statistical analysis shows such structural properties are
consistent across different language groups, largely independent of geography,
environment, and literacy. It is therefore possible to conclude the conceptual
structure connecting basic vocabulary studied is primarily due to universal
features of human cognition and language use.",On the universal structure of human lexical semantics,systematic images  improve models show <unk>
932,"We propose an end-to-end learning framework for segmenting generic objects in
videos. Our method learns to combine appearance and motion information to
produce pixel level segmentation masks for all prominent objects in videos. We
formulate this task as a structured prediction problem and design a two-stream
fully convolutional neural network which fuses together motion and appearance
in a unified framework. Since large-scale video datasets with pixel level
segmentations are problematic, we show how to bootstrap weakly annotated videos
together with existing image recognition datasets for training. Through
experiments on three challenging video segmentation benchmarks, our method
substantially improves the state-of-the-art for segmenting generic (unseen)
objects. Code and pre-trained models are available on the project website.","FusionSeg: Learning to combine motion and appearance for fully automatic
  segmention of generic objects in videos",model model monotone monotone fully
933,"Principal component analysis (PCA) is largely adopted for chemical process
monitoring and numerous PCA-based systems have been developed to solve various
fault detection and diagnosis problems. Since PCA-based methods assume that the
monitored process is linear, nonlinear PCA models, such as autoencoder models
and kernel principal component analysis (KPCA), has been proposed and applied
to nonlinear process monitoring. However, KPCA-based methods need to perform
eigen-decomposition (ED) on the kernel Gram matrix whose dimensions depend on
the number of training data. Moreover, prefixed kernel parameters cannot be
most effective for different faults which may need different parameters to
maximize their respective detection performances. Autoencoder models lack the
consideration of orthogonal constraints which is crucial for PCA-based
algorithms. To address these problems, this paper proposes a novel nonlinear
method, called neural component analysis (NCA), which intends to train a
feedforward neural work with orthogonal constraints such as those used in PCA.
NCA can adaptively learn its parameters through backpropagation and the
dimensionality of the nonlinear features has no relationship with the number of
training samples. Extensive experimental results on the Tennessee Eastman (TE)
benchmark process show the superiority of NCA in terms of missed detection rate
(MDR) and false alarm rate (FAR). The source code of NCA can be found in
https://github.com/haitaozhao/Neural-Component-Analysis.git.",Neural Component Analysis for Fault Detection,studied model kernel show show significant scene provide
934,"This paper contributes to the human-machine interface community in two ways:
as a critique of the closed-loop AC (augmented cognition) approach, and as a
way to introduce concepts from complex systems and systems physiology into the
field. Of particular relevance is a comparison of the inverted-U (or Gaussian)
model of optimal performance and multidimensional fitness landscape model.
Hypothetical examples will be given from human physiology and learning and
memory. In particular, a four-step model will be introduced that is proposed as
a better means to characterize multivariate systems during behavioral processes
with complex dynamics such as learning. Finally, the alternate approach
presented herein is considered as a preferable design alternate in
human-machine systems. It is within this context that future directions are
discussed.","The adaptability of physiological systems optimizes performance: new
  directions in augmentation",using review deal approaches
935,"After data selection, pre-processing, transformation, and feature extraction,
knowledge extraction is not the final step in a data mining process. It is then
necessary to understand this knowledge in order to apply it efficiently and
effectively. Up to now, there is a lack of appropriate techniques that support
this significant step. This is partly due to the fact that the assessment of
knowledge is often highly subjective, e.g., regarding aspects such as novelty
or usefulness. These aspects depend on the specific knowledge and requirements
of the data miner. There are, however, a number of aspects that are objective
and for which it is possible to provide appropriate measures. In this article
we focus on classification problems and use probabilistic generative
classifiers based on mixture density models that are quite common in data
mining applications. We define objective measures to assess the
informativeness, uniqueness, importance, discrimination, representativity,
uncertainty, and distinguishability of rules contained in these classifiers
numerically. These measures not only support a data miner in evaluating results
of a data mining process based on such classifiers. As we will see in
illustrative case studies, they may also be used to improve the data mining
process itself or to support the later application of the extracted knowledge.","Towards Automation of Knowledge Understanding: An Approach for
  Probabilistic Generative Classifiers",<unk> implementation art model significant using significant simple
936,"We consider a team of reinforcement learning agents that concurrently learn
to operate in a common environment. We identify three properties - adaptivity,
commitment, and diversity - which are necessary for efficient coordinated
exploration and demonstrate that straightforward extensions to single-agent
optimistic and posterior sampling approaches fail to satisfy them. As an
alternative, we propose seed sampling, which extends posterior sampling in a
manner that meets these requirements. Simulation results investigate how
per-agent regret decreases as the number of agents grows, establishing
substantial advantages of seed sampling over alternative exploration schemes.",Coordinated Exploration in Concurrent Reinforcement Learning,textual proposed including probability inference test camera texture
937,"We consider the problem of learning Bayesian network classifiers that
maximize the marginover a set of classification variables. We find that this
problem is harder for Bayesian networks than for undirected graphical models
like maximum margin Markov networks. The main difficulty is that the parameters
in a Bayesian network must satisfy additional normalization constraints that an
undirected graphical model need not respect. These additional constraints
complicate the optimization task. Nevertheless, we derive an effective training
algorithm that solves the maximum margin training problem for a range of
Bayesian network topologies, and converges to an approximate solution for
arbitrary network topologies. Experimental results show that the method can
demonstrate improved generalization performance over Markov networks when the
directed graphical structure encodes relevant knowledge. In practice, the
training technique allows one to combine prior knowledge expressed as a
directed (causal) model with state of the art discriminative learning methods.",Maximum Margin Bayesian Networks,models proposed order data consistent <unk>
938,"This paper addresses the problem of correlation estimation in sets of
compressed images. We consider a framework where images are represented under
the form of linear measurements due to low complexity sensing or security
requirements. We assume that the images are correlated through the displacement
of visual objects due to motion or viewpoint change and the correlation is
effectively represented by optical flow or motion field models. The correlation
is estimated in the compressed domain by jointly processing the linear
measurements. We first show that the correlated images can be efficiently
related using a linear operator. Using this linear relationship we then
describe the dependencies between images in the compressed domain. We further
cast a regularized optimization problem where the correlation is estimated in
order to satisfy both data consistency and motion smoothness objectives with a
Graph Cut algorithm. We analyze in detail the correlation estimation
performance and quantify the penalty due to image compression. Extensive
experiments in stereo and video imaging applications show that our novel
solution stays competitive with methods that implement complex image
reconstruction steps prior to correlation estimation. We finally use the
estimated correlation in a novel joint image reconstruction scheme that is
based on an optimization problem with sparsity priors on the reconstructed
images. Additional experiments show that our correlation estimation algorithm
leads to an effective reconstruction of pairs of images in distributed image
coding schemes that outperform independent reconstruction algorithms by 2 to 4
dB.",Correlation Estimation from Compressed Images,models model hardness for
939,"We consider the problem of maximizing submodular functions; while this
problem is known to be NP-hard, several numerically efficient local search
techniques with approximation guarantees are available. In this paper, we
propose a novel convex relaxation which is based on the relationship between
submodular functions, entropies and probabilistic graphical models. In a
graphical model, the entropy of the joint distribution decomposes as a sum of
marginal entropies of subsets of variables; moreover, for any distribution, the
entropy of the closest distribution factorizing in the graphical model provides
an bound on the entropy. For directed graphical models, this last property
turns out to be a direct consequence of the submodularity of the entropy
function, and allows the generalization of graphical-model-based upper bounds
to any submodular functions. These upper bounds may then be jointly maximized
with respect to a set, while minimized with respect to the graph, leading to a
convex variational inference scheme for maximizing submodular functions, based
on outer approximations of the marginal polytope and maximum likelihood bounded
treewidth structures. By considering graphs of increasing treewidths, we may
then explore the trade-off between computational complexity and tightness of
the relaxation. We also present extensions to constrained problems and
maximizing the difference of submodular functions, which include all possible
set functions.",Maximizing submodular functions using probabilistic graphical models,data video knowledge show automate interpretation 
940,"In a recent article we described a new type of deep neural network - a
Perpetual Learning Machine (PLM) - which is capable of learning 'on the fly'
like a brain by existing in a state of Perpetual Stochastic Gradient Descent
(PSGD). Here, by simulating the process of practice, we demonstrate both
selective memory and selective forgetting when we introduce statistical recall
biases during PSGD. Frequently recalled memories are remembered, whilst
memories recalled rarely are forgotten. This results in a 'use it or lose it'
stimulus driven memory process that is similar to human memory.","Use it or Lose it: Selective Memory and Forgetting in a Perpetual
  Learning Machine",best input ##  error method provides inside
941,"Skin cancer, the most common human malignancy, is primarily diagnosed
visually by physicians [1]. Classification with an automated method like CNN
[2, 3] shows potential for challenging tasks [1]. By now, the deep
convolutional neural networks are on par with human dermatologist [1]. This
abstract is dedicated on developing a Deep Learning method for ISIC [5] 2017
Skin Lesion Detection Competition hosted at [6] to classify the dermatology
pictures, which is aimed at improving the diagnostic accuracy rate and general
level of the human health. The challenge falls into three sub-challenges,
including Lesion Segmentation, Lesion Dermoscopic Feature Extraction and Lesion
Classification. This project only participates in the Lesion Classification
part. This algorithm is comprised of three steps: (1) original images
preprocessing, (2) modelling the processed images using CNN [2, 3] in Caffe [4]
framework, (3) predicting the test images and calculating the scores that
represent the likelihood of corresponding classification. The models are built
on the source images are using the Caffe [4] framework. The scores in
prediction step are obtained by two different models from the source images.","Using Deep Learning Method for Classification: A Proposed Algorithm for
  the ISIC 2017 Skin Lesion Classification Challenge",problem away model feature using curve
942,"Neural networks are analogous in many ways to spin glasses, systems which are
known for their rich set of dynamics and equally complex phase diagrams. We
apply well-known techniques in the study of spin glasses to a convolutional
sparsely encoding neural network and observe power law finite-size scaling
behavior in the sparsity and reconstruction error as the network denoises
32$\times$32 RGB CIFAR-10 images. This finite-size scaling indicates the
presence of a continuous phase transition at a critical value of this sparsity.
By using the power law scaling relations inherent to finite-size scaling, we
can determine the optimal value of sparsity for any network size by tuning the
system to the critical point and operate the system at the minimum denoising
error.","Phase Transitions in Image Denoising via Sparsely Coding Convolutional
  Neural Networks",estimation method region show region
943,"Automatic segmentation of medical images is an important task for many
clinical applications. In practice, a wide range of anatomical structures are
visualised using different imaging modalities. In this paper, we investigate
whether a single convolutional neural network (CNN) can be trained to perform
different segmentation tasks.
  A single CNN is trained to segment six tissues in MR brain images, the
pectoral muscle in MR breast images, and the coronary arteries in cardiac CTA.
The CNN therefore learns to identify the imaging modality, the visualised
anatomical structures, and the tissue classes.
  For each of the three tasks (brain MRI, breast MRI and cardiac CTA), this
combined training procedure resulted in a segmentation performance equivalent
to that of a CNN trained specifically for that task, demonstrating the high
capacity of CNN architectures. Hence, a single system could be used in clinical
practice to automatically perform diverse segmentation tasks without
task-specific training.","Deep Learning for Multi-Task Medical Image Segmentation in Multiple
  Modalities",networks model learning networks model <unk>
944,"In this paper we present a unified framework for modeling multi-relational
representations, scoring, and learning, and conduct an empirical study of
several recent multi-relational embedding models under the framework. We
investigate the different choices of relation operators based on linear and
bilinear transformations, and also the effects of entity representations by
incorporating unsupervised vectors pre-trained on extra textual resources. Our
results show several interesting findings, enabling the design of a simple
embedding model that achieves the new state-of-the-art performance on a popular
knowledge base completion task evaluated on Freebase.",Learning Multi-Relational Semantics Using Neural-Embedding Models,method show validation solving method validation method
945,"The classical mixture of Gaussians model is related to K-means via
small-variance asymptotics: as the covariances of the Gaussians tend to zero,
the negative log-likelihood of the mixture of Gaussians model approaches the
K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis
& Jordan (2012) used this observation to obtain a novel K-means-like algorithm
from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead
consider applying small-variance asymptotics directly to the posterior in
Bayesian nonparametric models. This framework is independent of any specific
Bayesian inference algorithm, and it has the major advantage that it
generalizes immediately to a range of models beyond the DP mixture. To
illustrate, we apply our framework to the feature learning setting, where the
beta process and Indian buffet process provide an appropriate Bayesian
nonparametric prior. We obtain a novel objective function that goes beyond
clustering to learn (and penalize new) groupings for which we relax the mutual
exclusivity and exhaustivity assumptions of clustering. We demonstrate several
other algorithms, all of which are scalable and simple to implement. Empirical
results demonstrate the benefits of the new framework.",MAD-Bayes: MAP-based Asymptotic Derivations from Bayes,focus models models proposed important actions
946,"Conventional dual-frequency fringe projection algorithm often suffers from
phase unwrapping failure when the frequency ratio between the high frequency
and the low one is too large. Zhang et.al. proposed an enhanced two-frequency
phase-shifting method to use geometric constraints of digital fringe
projection(DFP) to reduce the noise impact due to the large frequency ratio.
However, this method needs to calibrate the DFP system and calculate the
minimum phase map at the nearest position from the camera perspective, these
procedures are are relatively complex and more time-cosuming. In this paper, we
proposed an improved method, which eliminates the system calibration and
determination in Zhang's method,meanwhile does not need to use the low
frequency fringe pattern. In the proposed method,we only need a set of high
frequency fringe patterns to measure the object after the high frequency is
directly estimated by the experiment. Thus the proposed method can simplify the
procedure and improve the speed. Finally, the experimental evaluation is
conducted to prove the validity of the proposed method.The results demonstrate
that the proposed method can overcome the main disadvantages encountered by
Zhang's method.",Improved phase-unwrapping method using geometric constraints,models show show input field
947,"In this work, we present a new dataset for computational humor, specifically
comparative humor ranking, which attempts to eschew the ubiquitous binary
approach to humor detection. The dataset consists of tweets that are humorous
responses to a given hashtag. We describe the motivation for this new dataset,
as well as the collection process, which includes a description of our
semi-automated system for data collection. We also present initial experiments
for this dataset using both unsupervised and supervised approaches. Our best
supervised system achieved 63.7% accuracy, suggesting that this task is much
more difficult than comparable humor detection tasks. Initial experiments
indicate that a character-level model is more suitable for this task than a
token-level model, likely due to a large amount of puns that can be captured by
a character-level model.",#HashtagWars: Learning a Sense of Humor,policy model research decentralized
948,"Over the past century, personality theory and research has successfully
identified core sets of characteristics that consistently describe and explain
fundamental differences in the way people think, feel and behave. Such
characteristics were derived through theory, dictionary analyses, and survey
research using explicit self-reports. The availability of social media data
spanning millions of users now makes it possible to automatically derive
characteristics from language use -- at large scale. Taking advantage of
linguistic information available through Facebook, we study the process of
inferring a new set of potential human traits based on unprompted language use.
We subject these new traits to a comprehensive set of evaluations and compare
them with a popular five factor model of personality. We find that our
language-based trait construct is often more generalizable in that it often
predicts non-questionnaire-based outcomes better than questionnaire-based
traits (e.g. entities someone likes, income and intelligence quotient), while
the factors remain nearly as stable as traditional factors. Our approach
suggests a value in new constructs of personality derived from everyday human
language use.","Latent Human Traits in the Language of Social Media: An Open-Vocabulary
  Approach",advantage using <unk>
949,"We first present our work in machine translation, during which we used
aligned sentences to train a neural network to embed n-grams of different
languages into an $d$-dimensional space, such that n-grams that are the
translation of each other are close with respect to some metric. Good n-grams
to n-grams translation results were achieved, but full sentences translation is
still problematic. We realized that learning semantics of sentences and
documents was the key for solving a lot of natural language processing
problems, and thus moved to the second part of our work: sentence compression.
We introduce a flexible neural network architecture for learning embeddings of
words and sentences that extract their semantics, propose an efficient
implementation in the Torch framework and present embedding results comparable
to the ones obtained with classical neural language models, while being more
powerful.",Semantic Vector Machines,networks networks new process samples data researchers number
950,"This paper describes a new kind of knowledge representation and mining system
which we are calling the Semantic Knowledge Graph. At its heart, the Semantic
Knowledge Graph leverages an inverted index, along with a complementary
uninverted index, to represent nodes (terms) and edges (the documents within
intersecting postings lists for multiple terms/nodes). This provides a layer of
indirection between each pair of nodes and their corresponding edge, enabling
edges to materialize dynamically from underlying corpus statistics. As a
result, any combination of nodes can have edges to any other nodes materialize
and be scored to reveal latent relationships between the nodes. This provides
numerous benefits: the knowledge graph can be built automatically from a
real-world corpus of data, new nodes - along with their combined edges - can be
instantly materialized from any arbitrary combination of preexisting nodes
(using set operations), and a full model of the semantic relationships between
all entities within a domain can be represented and dynamically traversed using
a highly compact representation of the graph. Such a system has widespread
applications in areas as diverse as knowledge modeling and reasoning, natural
language processing, anomaly detection, data cleansing, semantic search,
analytics, data classification, root cause analysis, and recommendations
systems. The main contribution of this paper is the introduction of a novel
system - the Semantic Knowledge Graph - which is able to dynamically discover
and score interesting relationships between any arbitrary combination of
entities (words, phrases, or extracted concepts) through dynamically
materializing nodes and edges from a compact graphical representation built
automatically from a corpus of data representative of a knowledge domain.","The Semantic Knowledge Graph: A compact, auto-generated model for
  real-time traversal and ranking of any relationship within a domain",weighted in examples proposed <unk> for
951,"In previous work we developed a method of learning Bayesian Network models
from raw data. This method relies on the well known minimal description length
(MDL) principle. The MDL principle is particularly well suited to this task as
it allows us to tradeoff, in a principled way, the accuracy of the learned
network against its practical usefulness. In this paper we present some new
results that have arisen from our work. In particular, we present a new local
way of computing the description length. This allows us to make significant
improvements in our search algorithm. In addition, we modify our algorithm so
that it can take into account partial domain information that might be provided
by a domain expert. The local computation of description length also opens the
door for local refinement of an existent network. The feasibility of our
approach is demonstrated by experiments involving networks of a practical size.",Using Causal Information and Local Measures to Learn Bayesian Networks,models model function  vaes
952,"Non-negative Matrix Factorization (NMF) has already been applied to learn
speaker characterizations from single or non-simultaneous speech for speaker
recognition applications. It is also known for its good performance in (blind)
source separation for simultaneous speech. This paper explains how NMF can be
used to jointly solve the two problems in a multichannel speaker recognizer for
simultaneous speech. It is shown how state-of-the-art multichannel NMF for
blind source separation can be easily extended to incorporate speaker
recognition. Experiments on the CHiME corpus show that this method outperforms
the sequential approach of first applying source separation, followed by
speaker recognition that uses state-of-the-art i-vector techniques.",Joint Sound Source Separation and Speaker Recognition,art model art show scene feature for
953,"The majority of online reviews consist of plain-text feedback together with a
single numeric score. However, there are multiple dimensions to products and
opinions, and understanding the `aspects' that contribute to users' ratings may
help us to better understand their individual preferences. For example, a
user's impression of an audiobook presumably depends on aspects such as the
story and the narrator, and knowing their opinions on these aspects may help us
to recommend better products. In this paper, we build models for rating systems
in which such dimensions are explicit, in the sense that users leave separate
ratings for each aspect of a product. By introducing new corpora consisting of
five million reviews, rated with between three and six aspects, we evaluate our
models on three prediction tasks: First, we use our model to uncover which
parts of a review discuss which of the rated aspects. Second, we use our model
to summarize reviews, which for us means finding the sentences that best
explain a user's rating. Finally, since aspect ratings are optional in many of
the datasets we consider, we use our model to recover those ratings that are
missing from a user's evaluation. Our model matches state-of-the-art approaches
on existing small-scale datasets, while scaling to the real-world datasets we
introduce. Moreover, our model is able to `disentangle' content and sentiment
words: we automatically learn content words that are indicative of a particular
aspect as well as the aspect-specific sentiment words that are indicative of a
particular rating.",Learning Attitudes and Attributes from Multi-Aspect Reviews,models show rational <unk> new want
954,"Deep generative models (DGMs) are effective on learning multilayered
representations of complex data and performing inference of input data by
exploring the generative ability. However, it is relatively insufficient to
empower the discriminative ability of DGMs on making accurate predictions. This
paper presents max-margin deep generative models (mmDGMs) and a
class-conditional variant (mmDCGMs), which explore the strongly discriminative
principle of max-margin learning to improve the predictive performance of DGMs
in both supervised and semi-supervised learning, while retaining the generative
capability. In semi-supervised learning, we use the predictions of a max-margin
classifier as the missing labels instead of performing full posterior inference
for efficiency; we also introduce additional max-margin and label-balance
regularization terms of unlabeled data for effectiveness. We develop an
efficient doubly stochastic subgradient algorithm for the piecewise linear
objectives in different settings. Empirical results on various datasets
demonstrate that: (1) max-margin learning can significantly improve the
prediction performance of DGMs and meanwhile retain the generative ability; (2)
in supervised learning, mmDGMs are competitive to the best fully discriminative
networks when employing convolutional neural networks as the generative and
recognition models; and (3) in semi-supervised learning, mmDCGMs can perform
efficient inference and achieve state-of-the-art classification results on
several benchmarks.",Max-Margin Deep Generative Models for (Semi-)Supervised Learning,empty syntactic show various solving
955,"Positron Emission Tomography (PET) scan images are one of the bio medical
imaging techniques similar to that of MRI scan images but PET scan images are
helpful in finding the development of tumors.The PET scan images requires
expertise in the segmentation where clustering plays an important role in the
automation process.The segmentation of such images is manual to automate the
process clustering is used.Clustering is commonly known as unsupervised
learning process of n dimensional data sets are clustered into k groups so as
to maximize the inter cluster similarity and to minimize the intra cluster
similarity.This paper is proposed to implement the commonly used K Means and
Fuzzy CMeans (FCM) clustering algorithm.This work is implemented using MATrix
LABoratory (MATLAB) and tested with sample PET scan image. The sample data is
collected from Alzheimers Disease Neuro imaging Initiative ADNI. Medical Image
Processing and Visualization Tool (MIPAV) are used to compare the resultant
images.",Segmentation of Alzheimers Disease in PET scan datasets using MATLAB,frames model <unk> method percent variational
956,"This paper describes the results of some experiments exploring statistical
methods to infer syntactic behavior of words and morphemes from a raw corpus in
an unsupervised fashion. It shares certain points in common with Brown et al
(1992) and work that has grown out of that: it employs statistical techniques
to analyze syntactic behavior based on what words occur adjacent to a given
word. However, we use an eigenvector decomposition of a nearest-neighbor graph
to produce a two-dimensional rendering of the words of a corpus in which words
of the same syntactic category tend to form neighborhoods. We exploit this
technique for extending the value of automatic learning of morphology. In
particular, we look at the suffixes derived from a corpus by unsupervised
learning of morphology, and we ask which of these suffixes have a consistent
syntactic function (e.g., in English, -tion is primarily a mark of nouns, but
-s marks both noun plurals and 3rd person present on verbs), and we determine
that this method works well for this task.",Using eigenvectors of the bigram graph to infer morpheme identity,closer for show leaving achieve texture
957,"Filters in convolutional networks are typically parameterized in a pixel
basis, that does not take prior knowledge about the visual world into account.
We investigate the generalized notion of frames designed with image properties
in mind, as alternatives to this parametrization. We show that frame-based
ResNets and Densenets can improve performance on Cifar-10+ consistently, while
having additional pleasant properties like steerability. By exploiting these
transformation properties explicitly, we arrive at dynamic steerable blocks.
They are an extension of residual blocks, that are able to seamlessly transform
filters under pre-defined transformations, conditioned on the input at training
and inference time. Dynamic steerable blocks learn the degree of invariance
from data and locally adapt filters, allowing them to apply a different
geometrical variant of the same filter to each location of the feature map.
When evaluated on the Berkeley Segmentation contour detection dataset, our
approach outperforms all competing approaches that do not utilize pre-training.
Our results highlight the benefits of image-based regularization to deep
networks.",Dynamic Steerable Blocks in Deep Residual Networks,learning appear model model domain 
958,"Machine learning and computer vision have driven many of the greatest
advances in the modeling of Deep Convolutional Neural Networks (DCNNs).
Nowadays, most of the research has been focused on improving recognition
accuracy with better DCNN models and learning approaches. The recurrent
convolutional approach is not applied very much, other than in a few DCNN
architectures. On the other hand, Inception-v4 and Residual networks have
promptly become popular among computer the vision community. In this paper, we
introduce a new DCNN model called the Inception Recurrent Residual
Convolutional Neural Network (IRRCNN), which utilizes the power of the
Recurrent Convolutional Neural Network (RCNN), the Inception network, and the
Residual network. This approach improves the recognition accuracy of the
Inception-residual network with same number of network parameters. In addition,
this proposed architecture generalizes the Inception network, the RCNN, and the
Residual network with significantly improved training accuracy. We have
empirically evaluated the performance of the IRRCNN model on different
benchmarks including CIFAR-10, CIFAR-100, TinyImageNet-200, and CU3D-100. The
experimental results show higher recognition accuracy against most of the
popular DCNN models including the RCNN. We have also investigated the
performance of the IRRCNN approach against the Equivalent Inception Network
(EIN) and the Equivalent Inception Residual Network (EIRN) counterpart on the
CIFAR-100 dataset. We report around 4.53%, 4.49% and 3.56% improvement in
classification accuracy compared with the RCNN, EIN, and EIRN on the CIFAR-100
dataset respectively. Furthermore, the experiment has been conducted on the
TinyImageNet-200 and CU3D-100 datasets where the IRRCNN provides better testing
accuracy compared to the Inception Recurrent CNN (IRCNN), the EIN, and the
EIRN.","Improved Inception-Residual Convolutional Neural Network for Object
  Recognition",## method learning
959,"We consider the problem of computing a lightest derivation of a global
structure using a set of weighted rules. A large variety of inference problems
in AI can be formulated in this framework. We generalize A* search and
heuristics derived from abstractions to a broad class of lightest derivation
problems. We also describe a new algorithm that searches for lightest
derivations using a hierarchy of abstractions. Our generalization of A* gives a
new algorithm for searching AND/OR graphs in a bottom-up fashion. We discuss
how the algorithms described here provide a general architecture for addressing
the pipeline problem --- the problem of passing information back and forth
between various stages of processing in a perceptual system. We consider
examples in computer vision and natural language processing. We apply the
hierarchical search algorithm to the problem of estimating the boundaries of
convex objects in grayscale images and compare it to other search methods. A
second set of experiments demonstrate the use of a new compositional model for
finding salient curves in images.",The Generalized A* Architecture,reward set observation
960,"Often, when dealing with real-world recognition problems, we do not need, and
often cannot have, knowledge of the entire set of possible classes that might
appear during operational testing. Moreover, sometimes some of these classes
may be ill-sampled, not sampled at all or undefined. In such cases, we need to
think of robust classification methods able to deal with the ""unknown"" and
properly reject samples belonging to classes never seen during training.
Notwithstanding, almost all existing classifiers to date were mostly developed
for the closed-set scenario, i.e., the classification setup in which it is
assumed that all test samples belong to one of the classes with which the
classifier was trained. In the open-set scenario, however, a test sample can
belong to none of the known classes and the classifier must properly reject it
by classifying it as unknown. In this work, we extend upon the well-known
Support Vector Machines (SVM) classifier and introduce the Specialized Support
Vector Machines (SSVM), which is suitable for recognition in open-set setups.
SSVM balances the empirical risk and the risk of the unknown and ensures that
the region of the feature space in which a test sample would be classified as
known (one of the known classes) is always bounded, ensuring a finite risk of
the unknown. The same cannot be guaranteed by the traditional SVM formulation,
even when using the Radial Basis Function (RBF) kernel. In this work, we also
highlight the properties of the SVM classifier related to the open-set
scenario, and provide necessary and sufficient conditions for an RBF SVM to
have bounded open-space risk. An extensive set of experiments compares the
proposed method with existing solutions in the literature for open-set
recognition and the reported results show its effectiveness.",Specialized Support Vector Machines for open-set recognition,<unk> model based problem  using first 
961,"In this paper, we propose a recurrent framework for Joint Unsupervised
LEarning (JULE) of deep representations and image clusters. In our framework,
successive operations in a clustering algorithm are expressed as steps in a
recurrent process, stacked on top of representations output by a Convolutional
Neural Network (CNN). During training, image clusters and representations are
updated jointly: image clustering is conducted in the forward pass, while
representation learning in the backward pass. Our key idea behind this
framework is that good representations are beneficial to image clustering and
clustering results provide supervisory signals to representation learning. By
integrating two processes into a single model with a unified weighted triplet
loss and optimizing it end-to-end, we can obtain not only more powerful
representations, but also more precise image clusters. Extensive experiments
show that our method outperforms the state-of-the-art on image clustering
across a variety of image datasets. Moreover, the learned representations
generalize well when transferred to other tasks.",Joint Unsupervised Learning of Deep Representations and Image Clusters,models problem <unk>
962,"A database of objects discovered in houses in the Roman city of Pompeii
provides a unique view of ordinary life in an ancient city. Experts have used
this collection to study the structure of Roman households, exploring the
distribution and variability of tasks in architectural spaces, but such
approaches are necessarily affected by modern cultural assumptions. In this
study we present a data-driven approach to household archeology, treating it as
an unsupervised labeling problem. This approach scales to large data sets and
provides a more objective complement to human interpretation.",Reconstructing Pompeian Households,imaging  model however  proposed imaging 
963,"Image orientation detection requires high-level scene understanding. Humans
use object recognition and contextual scene information to correctly orient
images. In literature, the problem of image orientation detection is mostly
confronted by using low-level vision features, while some approaches
incorporate few easily detectable semantic cues to gain minor improvements. The
vast amount of semantic content in images makes orientation detection
challenging, and therefore there is a large semantic gap between existing
methods and human behavior. Also, existing methods in literature report highly
discrepant detection rates, which is mainly due to large differences in
datasets and limited variety of test images used for evaluation. In this work,
for the first time, we leverage the power of deep learning and adapt
pre-trained convolutional neural networks using largest training dataset
to-date for the image orientation detection task. An extensive evaluation of
our model on different public datasets shows that it remarkably generalizes to
correctly orient a large set of unconstrained images; it also significantly
outperforms the state-of-the-art and achieves accuracy very close to that of
humans.","Why my photos look sideways or upside down? Detecting Canonical
  Orientation of Images using Convolutional Neural Networks",points de train similarity ##
964,"Conditional random fields (CRFs) are commonly employed as a post-processing
tool for image segmentation tasks. The unary potentials of the CRF are often
learnt independently by a classifier, thereby decoupling the inference in CRF
from the training of classifier. Such a scheme works effectively, when
pixel-level labelling is available for all the images. However, in absence of
pixel-level labels, the classifier is faced with the uphill task of selectively
assigning the image-level labels to the pixels of the image. Prior work often
relied on localization cues, such as saliency maps, objectness priors, bounding
boxes etc., to address this challenging problem. In contrast, we model the
labels of the pixels as latent variables of a CRF. The pixels and the
image-level labels are the observed variables of the latent CRF. We amortize
the cost of inference in the latent CRF over the entire dataset, by training an
inference network to approximate the posterior distribution of the latent
variables given the observed variables. The inference network can be trained in
an end-to-end fashion, and requires no localization cues for training.
Moreover, unlike other approaches for weakly-supervised segmentation, the
proposed model doesn't require further post-processing. The proposed model
achieves performance comparable with other approaches that employ saliency
masks for the task of weakly-supervised semantic image segmentation on the
challenging VOC 2012 dataset.","Amortized Inference and Learning in Latent Conditional Random Fields for
  Weakly-Supervised Semantic Image Segmentation",using model brain different critical models
965,"Millions of hearing impaired people around the world routinely use some
variants of sign languages to communicate, thus the automatic translation of a
sign language is meaningful and important. Currently, there are two
sub-problems in Sign Language Recognition (SLR), i.e., isolated SLR that
recognizes word by word and continuous SLR that translates entire sentences.
Existing continuous SLR methods typically utilize isolated SLRs as building
blocks, with an extra layer of preprocessing (temporal segmentation) and
another layer of post-processing (sentence synthesis). Unfortunately, temporal
segmentation itself is non-trivial and inevitably propagates errors into
subsequent steps. Worse still, isolated SLR methods typically require strenuous
labeling of each word separately in a sentence, severely limiting the amount of
attainable training data. To address these challenges, we propose a novel
continuous sign recognition framework, the Hierarchical Attention Network with
Latent Space (LS-HAN), which eliminates the preprocessing of temporal
segmentation. The proposed LS-HAN consists of three components: a two-stream
Convolutional Neural Network (CNN) for video feature representation generation,
a Latent Space (LS) for semantic gap bridging, and a Hierarchical Attention
Network (HAN) for latent space based recognition. Experiments are carried out
on two large scale datasets. Experimental results demonstrate the effectiveness
of the proposed framework.",Video-based Sign Language Recognition without Temporal Segmentation,based learning behavior we word paper  for
966,"Robust foreground object segmentation via background modelling is a difficult
problem in cluttered environments, where obtaining a clear view of the
background to model is almost impossible. In this paper, we propose a method
capable of robustly estimating the background and detecting regions of interest
in such environments. In particular, we propose to extend the background
initialisation component of a recent patch-based foreground detection algorithm
with an elaborate technique based on Markov Random Fields, where the optimal
labelling solution is computed using iterated conditional modes. Rather than
relying purely on local temporal statistics, the proposed technique takes into
account the spatial continuity of the entire background. Experiments with
several tracking algorithms on the CAVIAR dataset indicate that the proposed
method leads to considerable improvements in object tracking accuracy, when
compared to methods based on Gaussian mixture models and feature histograms.","MRF-based Background Initialisation for Improved Foreground Detection in
  Cluttered Surveillance Videos",means role brain show significant significant
967,"Canonical correlation analysis (CCA) is a multivariate statistical technique
for finding the linear relationship between two sets of variables. The kernel
generalization of CCA named kernel CCA has been proposed to find nonlinear
relations between datasets. Despite their wide usage, they have one common
limitation that is the lack of sparsity in their solution. In this paper, we
consider sparse kernel CCA and propose a novel sparse kernel CCA algorithm
(SKCCA). Our algorithm is based on a relationship between kernel CCA and least
squares. Sparsity of the dual transformations is introduced by penalizing the
$\ell_{1}$-norm of dual vectors. Experiments demonstrate that our algorithm not
only performs well in computing sparse dual transformations but also can
alleviate the over-fitting problem of kernel CCA.",Sparse Kernel Canonical Correlation Analysis via $\ell_1$-regularization,best model still <unk> for
968,"Learning acoustic models directly from the raw waveform data with minimal
processing is challenging. Current waveform-based models have generally used
very few (~2) convolutional layers, which might be insufficient for building
high-level discriminative features. In this work, we propose very deep
convolutional neural networks (CNNs) that directly use time-domain waveforms as
inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over
very long sequences (e.g., vector of size 32000), necessary for processing
acoustic waveforms. This is achieved through batch normalization, residual
learning, and a careful design of down-sampling in the initial layers. Our
networks are fully convolutional, without the use of fully connected layers and
dropout, to maximize representation learning. We use a large receptive field in
the first convolutional layer to mimic bandpass filters, but very small
receptive fields subsequently to control the model capacity. We demonstrate the
performance gains with the deeper models. Our evaluation shows that the CNN
with 18 weight layers outperform the CNN with 3 weight layers by over 15% in
absolute accuracy for an environmental sound recognition task and matches the
performance of models using log-mel features.",Very Deep Convolutional Neural Networks for Raw Waveforms,models model <unk> method <unk> system
969,"We study here the well-known propagation rules for Boolean constraints. First
we propose a simple notion of completeness for sets of such rules and establish
a completeness result. Then we show an equivalence in an appropriate sense
between Boolean constraint propagation and unit propagation, a form of
resolution for propositional logic.
  Subsequently we characterize one set of such rules by means of the notion of
hyper-arc consistency introduced in (Mohr and Masini 1988). Also, we clarify
the status of a similar, though different, set of rules introduced in (Simonis
1989a) and more fully in (Codognet and Diaz 1996).",Some Remarks on Boolean Constraint Propagation,using method tensor significant video search using account
970,"We propose a clustering-based iterative algorithm to solve certain
optimization problems in machine learning, where we start the algorithm by
aggregating the original data, solving the problem on aggregated data, and then
in subsequent steps gradually disaggregate the aggregated data. We apply the
algorithm to common machine learning problems such as the least absolute
deviation regression problem, support vector machines, and semi-supervised
support vector machines. We derive model-specific data aggregation and
disaggregation procedures. We also show optimality, convergence, and the
optimality gap of the approximated solution in each iteration. A computational
study is provided.","An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality
  in Machine Learning",using <unk> for show video loss
971,"As with articles and journals, the customary methods for measuring books'
academic impact mainly involve citations, which is easy but limited to
interrogating traditional citation databases and scholarly book reviews,
Researchers have attempted to use other metrics, such as Google Books,
libcitation, and publisher prestige. However, these approaches lack
content-level information and cannot determine the citation intentions of
users. Meanwhile, the abundant online review resources concerning academic
books can be used to mine deeper information and content utilizing altmetric
perspectives. In this study, we measure the impacts of academic books by
multi-granularity mining online reviews, and we identify factors that affect a
book's impact. First, online reviews of a sample of academic books on Amazon.cn
are crawled and processed. Then, multi-granularity review mining is conducted
to identify review sentiment polarities and aspects' sentiment values. Lastly,
the numbers of positive reviews and negative reviews, aspect sentiment values,
star values, and information regarding helpfulness are integrated via the
entropy method, and lead to the calculation of the final book impact scores.
The results of a correlation analysis of book impact scores obtained via our
method versus traditional book citations show that, although there are
substantial differences between subject areas, online book reviews tend to
reflect the academic impact. Thus, we infer that online reviews represent a
promising source for mining book impact within the altmetric perspective and at
the multi-granularity content level. Moreover, our proposed method might also
be a means by which to measure other books besides academic publications.","Measuring Book Impact Based on the Multi-granularity Online Review
  Mining",real size data feature word paper  system
972,"In many applications of finance, biology and sociology, complex systems
involve entities interacting with each other. These processes have the
peculiarity of evolving over time and of comprising latent factors, which
influence the system without being explicitly measured. In this work we present
latent variable time-varying graphical lasso (LTGL), a method for multivariate
time-series graphical modelling that considers the influence of hidden or
unmeasurable factors. The estimation of the contribution of the latent factors
is embedded in the model which produces both sparse and low-rank components for
each time point. In particular, the first component represents the connectivity
structure of observable variables of the system, while the second represents
the influence of hidden factors, assumed to be few with respect to the observed
variables. Our model includes temporal consistency on both components,
providing an accurate evolutionary pattern of the system. We derive a tractable
optimisation algorithm based on alternating direction method of multipliers,
and develop a scalable and efficient implementation which exploits proximity
operators in closed form. LTGL is extensively validated on synthetic data,
achieving optimal performance in terms of accuracy, structure learning and
scalability with respect to ground truth and state-of-the-art methods for
graphical inference. We conclude with the application of LTGL to real case
studies, from biology and finance, to illustrate how our method can be
successfully employed to gain insights on multivariate time-series data.",Latent variable time-varying network inference,models data feature examples using successful
973,"Community detection is a fundamental problem in network analysis which is
made more challenging by overlaps between communities which often occur in
practice. Here we propose a general, flexible, and interpretable generative
model for overlapping communities, which can be thought of as a generalization
of the degree-corrected stochastic block model. We develop an efficient
spectral algorithm for estimating the community memberships, which deals with
the overlaps by employing the K-medians algorithm rather than the usual K-means
for clustering in the spectral domain. We show that the algorithm is
asymptotically consistent when networks are not too sparse and the overlaps
between communities not too large. Numerical experiments on both simulated
networks and many real social networks demonstrate that our method performs
very well compared to a number of benchmark methods for overlapping community
detection.",Detecting Overlapping Communities in Networks Using Spectral Methods,cues show significant stochastic
974,"Often multiple instances of an object occur in the same scene, for example in
a warehouse. Unsupervised multi-instance object discovery algorithms are able
to detect and identify such objects. We use such an algorithm to provide object
proposals to a convolutional neural network (CNN) based classifier. This
results in fewer regions to evaluate, compared to traditional region proposal
algorithms. Additionally, it enables using the joint probability of multiple
instances of an object, resulting in improved classification accuracy. The
proposed technique can also split a single class into multiple sub-classes
corresponding to the different object types, enabling hierarchical
classification.","Detecting and Grouping Identical Objects for Region Proposal and
  Classification",technique domain 
975,"In this work, we present a new Vector Space Model (VSM) of speech utterances
for the task of spoken dialect identification. Generally, DID systems are built
using two sets of features that are extracted from speech utterances; acoustic
and phonetic. The acoustic and phonetic features are used to form vector
representations of speech utterances in an attempt to encode information about
the spoken dialects. The Phonotactic and Acoustic VSMs, thus formed, are used
for the task of DID. The aim of this paper is to construct a single VSM that
encodes information about spoken dialects from both the Phonotactic and
Acoustic VSMs. Given the two views of the data, we make use of a well known
multi-view dimensionality reduction technique known as Canonical Correlation
Analysis (CCA), to form a single vector representation for each speech
utterance that encodes dialect specific discriminative information from both
the phonetic and acoustic representations. We refer to this approach as feature
space combination approach and show that our CCA based feature vector
representation performs better on the Arabic DID task than the phonetic and
acoustic feature representations used alone. We also present the feature space
combination approach as a viable alternative to the model based combination
approach, where two DID systems are built using the two VSMs (Phonotactic and
Acoustic) and the final prediction score is the output score combination from
the two systems.","Multi-view Dimensionality Reduction for Dialect Identification of Arabic
  Broadcast Speech",learning model <unk> public method <unk> representations for
976,"Detecting outliers which are grossly different from or inconsistent with the
remaining dataset is a major challenge in real-world KDD applications. Existing
outlier detection methods are ineffective on scattered real-world datasets due
to implicit data patterns and parameter setting issues. We define a novel
""Local Distance-based Outlier Factor"" (LDOF) to measure the {outlier-ness} of
objects in scattered datasets which addresses these issues. LDOF uses the
relative location of an object to its neighbours to determine the degree to
which the object deviates from its neighbourhood. Properties of LDOF are
theoretically analysed including LDOF's lower bound and its false-detection
probability, as well as parameter settings. In order to facilitate parameter
settings in real-world applications, we employ a top-n technique in our outlier
detection approach, where only the objects with the highest LDOF values are
regarded as outliers. Compared to conventional approaches (such as top-n KNN
and top-n LOF), our method top-n LDOF is more effective at detecting outliers
in scattered data. It is also easier to set parameters, since its performance
is relatively stable over a large range of parameter values, as illustrated by
experimental results on both real-world and synthetic datasets.","A New Local Distance-Based Outlier Detection Approach for Scattered
  Real-World Data",set domain
977,"Recurrent Neural Networks (RNNs) have been widely used in natural language
processing and computer vision. Among them, the Hierarchical Multi-scale RNN
(HM-RNN), a kind of multi-scale hierarchical RNN proposed recently, can learn
the hierarchical temporal structure from data automatically. In this paper, we
extend the work to solve the computer vision task of action recognition.
However, in sequence-to-sequence models like RNN, it is normally very hard to
discover the relationships between inputs and outputs given static inputs. As a
solution, attention mechanism could be applied to extract the relevant
information from input thus facilitating the modeling of input-output
relationships. Based on these considerations, we propose a novel attention
network, namely Hierarchical Multi-scale Attention Network (HM-AN), by
combining the HM-RNN and the attention mechanism and apply it to action
recognition. A newly proposed gradient estimation method for stochastic
neurons, namely Gumbel-softmax, is exploited to implement the temporal boundary
detectors and the stochastic hard attention mechanism. To amealiate the
negative effect of sensitive temperature of the Gumbel-softmax, an adaptive
temperature training method is applied to better the system performance. The
experimental results demonstrate the improved effect of HM-AN over LSTM with
attention on the vision task. Through visualization of what have been learnt by
the networks, it can be observed that both the attention regions of images and
the hierarchical temporal structure can be captured by HM-AN.",Hierarchical Multi-scale Attention Networks for Action Recognition,using self driving network exposing specifically 
978,"Person Re-Identification (re-id) is a challenging task in computer vision,
especially when there are limited training data from multiple camera views. In
this paper, we pro- pose a deep learning based person re-identification method
by transferring knowledge of mid-level attribute features and high-level
classification features. Building on the idea that identity classification,
attribute recognition and re- identification share the same mid-level semantic
representations, they can be trained sequentially by fine-tuning one based on
another. In our framework, we train identity classification and attribute
recognition tasks from deep Convolutional Neural Network (dCNN) to learn person
information. The information can be transferred to the person re-id task and
improves its accuracy by a large margin. Further- more, a Long Short Term
Memory(LSTM) based Recurrent Neural Network (RNN) component is extended by a
spacial gate. This component is used in the re-id model to pay attention to
certain spacial parts in each recurrent unit. Experimental results show that
our method achieves 78.3% of rank-1 recognition accuracy on the CUHK03
benchmark.",Cross Domain Knowledge Transfer for Person Re-identification,video theory fractal data feature models number
979,"Policy optimization methods have shown great promise in solving complex
reinforcement and imitation learning tasks. While model-free methods are
broadly applicable, they often require many samples to optimize complex
policies. Model-based methods greatly improve sample-efficiency but at the cost
of poor generalization, requiring a carefully handcrafted model of the system
dynamics for each task. Recently, hybrid methods have been successful in
trading off applicability for improved sample-complexity. However, these have
been limited to continuous action spaces. In this work, we present a new hybrid
method based on an approximation of the dynamics as an expectation over the
next state under the current policy. This relaxation allows us to derive a
novel hybrid policy gradient estimator, combining score function and pathwise
derivative estimators, that is applicable to discrete action spaces. We show
significant gains in sample complexity, ranging between $1.7$ and $25\times$,
when learning parameterized policies on Cart Pole, Acrobot, Mountain Car and
Hand Mass. Our method is applicable to both discrete and continuous action
spaces, when competing pathwise methods are limited to the latter.","Deterministic Policy Optimization by Combining Pathwise and Score
  Function Estimators for Discrete Action Spaces",means method kernel user the
980,"Possibilistic answer set programming (PASP) extends answer set programming
(ASP) by attaching to each rule a degree of certainty. While such an extension
is important from an application point of view, existing semantics are not
well-motivated, and do not always yield intuitive results. To develop a more
suitable semantics, we first introduce a characterization of answer sets of
classical ASP programs in terms of possibilistic logic where an ASP program
specifies a set of constraints on possibility distributions. This
characterization is then naturally generalized to define answer sets of PASP
programs. We furthermore provide a syntactic counterpart, leading to a
possibilistic generalization of the well-known Gelfond-Lifschitz reduct, and we
show how our framework can readily be implemented using standard ASP solvers.",Possibilistic Answer Set Programming Revisited,representations set
981,"We extend the work of Narasimhan and Bilmes [30] for minimizing set functions
representable as a difference between submodular functions. Similar to [30],
our new algorithms are guaranteed to monotonically reduce the objective
function at every step. We empirically and theoretically show that the
per-iteration cost of our algorithms is much less than [30], and our algorithms
can be used to efficiently minimize a difference between submodular functions
under various combinatorial constraints, a problem not previously addressed. We
provide computational bounds and a hardness result on the mul- tiplicative
inapproximability of minimizing the difference between submodular functions. We
show, however, that it is possible to give worst-case additive bounds by
providing a polynomial time computable lower-bound on the minima. Finally we
show how a number of machine learning problems can be modeled as minimizing the
difference between submodular functions. We experimentally show the validity of
our algorithms by testing them on the problem of feature selection with
submodular cost features.","Algorithms for Approximate Minimization of the Difference Between
  Submodular Functions, with Applications",models method word paper  for
982,"We interpret HyperNetworks within the framework of variational inference
within implicit distributions. Our method, Bayes by Hypernet, is able to model
a richer variational distribution than previous methods. Experiments show that
it achieves comparable predictive performance on the MNIST classification task
while providing higher predictive uncertainties compared to MC-Dropout and
regular maximum likelihood training.",Implicit Weight Uncertainty in Neural Networks,predictive <unk> show <unk> sense computing 
983,"We propose an automatic method to infer high dynamic range illumination from
a single, limited field-of-view, low dynamic range photograph of an indoor
scene. In contrast to previous work that relies on specialized image capture,
user input, and/or simple scene models, we train an end-to-end deep neural
network that directly regresses a limited field-of-view photo to HDR
illumination, without strong assumptions on scene geometry, material
properties, or lighting. We show that this can be accomplished in a three step
process: 1) we train a robust lighting classifier to automatically annotate the
location of light sources in a large dataset of LDR environment maps, 2) we use
these annotations to train a deep neural network that predicts the location of
lights in a scene from a single limited field-of-view photo, and 3) we
fine-tune this network using a small dataset of HDR environment maps to predict
light intensities. This allows us to automatically recover high-quality HDR
illumination estimates that significantly outperform previous state-of-the-art
methods. Consequently, using our illumination estimates for applications like
3D object insertion, we can achieve results that are photo-realistic, which is
validated via a perceptual user study.",Learning to Predict Indoor Illumination from a Single Image,group use show research <unk> model data probabilistic
984,"This paper describes a novel storyboarding scheme that uses a model trained
on pairwise image comparisons to identify images likely to be of interest to a
mobile robot user. Traditional storyboarding schemes typically attempt to
summarise robot observations using predefined novelty or image quality
objectives, but we propose a user training stage that allows the incorporation
of user interest when storyboarding. Our approach dramatically reduces the
number of image comparisons required to infer image interest by applying a
Gaussian process smoothing algorithm on image features extracted using a
pre-trained convolutional neural network. As a particularly valuable
by-product, the proposed approach allows the generation of user-specific
saliency or attention maps.","User-driven mobile robot storyboarding: Learning image interest and
  saliency from pairwise image comparisons",data variables  two show identify approaches
985,"Change detection is one of the central problems in earth observation and was
extensively investigated over recent decades. In this paper, we propose a novel
recurrent convolutional neural network (ReCNN) architecture, which is trained
to learn a joint spectral-spatial-temporal feature representation in a unified
framework for change detection in multispectral images. To this end, we bring
together a convolutional neural network (CNN) and a recurrent neural network
(RNN) into one end-to-end network. The former is able to generate rich
spectral-spatial feature representations, while the latter effectively analyzes
temporal dependency in bi-temporal images. In comparison with previous
approaches to change detection, the proposed network architecture possesses
three distinctive properties: 1) It is end-to-end trainable, in contrast to
most existing methods whose components are separately trained or computed; 2)
it naturally harnesses spatial information that has been proven to be
beneficial to change detection task; 3) it is capable of adaptively learning
the temporal dependency between multitemporal images, unlike most of algorithms
that use fairly simple operation like image differencing or stacking. As far as
we know, this is the first time that a recurrent convolutional network
architecture has been proposed for multitemporal remote sensing image analysis.
The proposed network is validated on real multispectral data sets. Both visual
and quantitative analysis of experimental results demonstrates competitive
performance in the proposed mode.","Learning Spectral-Spatial-Temporal Features via a Recurrent
  Convolutional Neural Network for Change Detection in Multispectral Imagery",model regression  networks image propose semantics paper  for
986,"Feature representations, both hand-designed and learned ones, are often hard
to analyze and interpret, even when they are extracted from visual data. We
propose a new approach to study image representations by inverting them with an
up-convolutional neural network. We apply the method to shallow representations
(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our
approach provides significantly better reconstructions than existing methods,
revealing that there is surprisingly rich information contained in these
features. Inverting a deep network trained on ImageNet provides several
insights into the properties of the feature representation learned by the
network. Most strikingly, the colors and the rough contours of an image can be
reconstructed from activations in higher network layers and even from the
predicted class probabilities.",Inverting Visual Representations with Convolutional Networks,video step
987,"We investigate neural techniques for end-to-end computational argumentation
mining (AM). We frame AM both as a token-based dependency parsing and as a
token-based sequence tagging problem, including a multi-task learning setup.
Contrary to models that operate on the argument component level, we find that
framing AM as dependency parsing leads to subpar performance results. In
contrast, less complex (local) tagging models based on BiLSTMs perform robustly
across classification scenarios, being able to catch long-range dependencies
inherent to the AM problem. Moreover, we find that jointly learning 'natural'
subtasks, in a multi-task learning setup, improves performance.",Neural End-to-End Learning for Computational Argumentation Mining,<unk> show <unk> similarity high
988,"Face alignment is a classic problem in the computer vision field. Previous
works mostly focus on sparse alignment with a limited number of facial landmark
points, i.e., facial landmark detection. In this paper, for the first time, we
aim at providing a very dense 3D alignment for large-pose face images. To
achieve this, we train a CNN to estimate the 3D face shape, which not only
aligns limited facial landmarks but also fits face contours and SIFT feature
points. Moreover, we also address the bottleneck of training CNN with multiple
datasets, due to different landmark markups on different datasets, such as 5,
34, 68. Experimental results show our method not only provides high-quality,
dense 3D face fitting but also outperforms the state-of-the-art facial landmark
detection methods on the challenging datasets. Our model can run at real time
during testing.",Dense Face Alignment,train similarity correspondence robotics limited
989,"In the domain of image processing, often real-time constraints are required.
In particular, in safety-critical applications, such as X-ray computed
tomography in medical imaging or advanced driver assistance systems in the
automotive domain, timing is of utmost importance. A common approach to
maintain real-time capabilities of compute-intensive applications is to offload
those computations to dedicated accelerator hardware, such as Field
Programmable Gate Arrays (FPGAs). Programming such architectures is a
challenging task, with respect to the typical FPGA-specific design criteria:
Achievable overall algorithm latency and resource usage of FPGA primitives
(BRAM, FF, LUT, and DSP). High-Level Synthesis (HLS) dramatically simplifies
this task by enabling the description of algorithms in well-known higher
languages (C/C++) and its automatic synthesis that can be accomplished by HLS
tools. However, algorithm developers still need expert knowledge about the
target architecture, in order to achieve satisfying results. Therefore, in
previous work, we have shown that elevating the description of image algorithms
to an even higher abstraction level, by using a Domain-Specific Language (DSL),
can significantly cut down the complexity for designing such algorithms for
FPGAs. To give the developer even more control over the common trade-off,
latency vs. resource usage, we will present an automatic optimization process
where these criteria are analyzed and fed back to the DSL compiler, in order to
generate code that is closer to the desired design specifications. Finally, we
generate code for stereo block matching algorithms and compare it with
handwritten implementations to quantify the quality of our results.",Automatic Optimization of Hardware Accelerators for Image Processing,complexity show show efficiency high using scheme 
990,"We study the problem of inducing interpretability in KG embeddings.
Specifically, we explore the Universal Schema (Riedel et al., 2013) and propose
a method to induce interpretability. There have been many vector space models
proposed for the problem, however, most of these methods don't address the
interpretability (semantics) of individual dimensions. In this work, we study
this problem and propose a method for inducing interpretability in KG
embeddings using entity co-occurrence statistics. The proposed method
significantly improves the interpretability, while maintaining comparable
performance in other KG tasks.",Inducing Interpretability in Knowledge Graph Embeddings,learning  simple
991,"Deep learning has significantly advanced the state of the art in artificial
intelligence, gaining wide popularity from both industry and academia. Special
interest is around Convolutional Neural Networks (CNN), which take inspiration
from the hierarchical structure of the visual cortex, to form deep layers of
convolutional operations, along with fully connected classifiers. Hardware
implementations of these deep CNN architectures are challenged with memory
bottlenecks that require many convolution and fully-connected layers demanding
large amount of communication for parallel computation. Multi-core CPU based
solutions have demonstrated their inadequacy for this problem due to the memory
wall and low parallelism. Many-core GPU architectures show superior performance
but they consume high power and also have memory constraints due to
inconsistencies between cache and main memory. FPGA design solutions are also
actively being explored, which allow implementing the memory hierarchy using
embedded BlockRAM. This boosts the parallel use of shared memory elements
between multiple processing units, avoiding data replicability and
inconsistencies. This makes FPGAs potentially powerful solutions for real-time
classification of CNNs. Both Altera and Xilinx have adopted OpenCL co-design
framework from GPU for FPGA designs as a pseudo-automatic development solution.
In this paper, a comprehensive evaluation and comparison of Altera and Xilinx
OpenCL frameworks for a 5-layer deep CNN is presented. Hardware resources,
temporal performance and the OpenCL architecture for CNNs are discussed. Xilinx
demonstrates faster synthesis, better FPGA resource utilization and more
compact boards. Altera provides multi-platforms tools, mature design community
and better execution times.","Comprehensive Evaluation of OpenCL-based Convolutional Neural Network
  Accelerators in Xilinx and Altera FPGAs",textual proposed require networks different learning  role uncertainty 
992,"We introduce GAMSEL (Generalized Additive Model Selection), a penalized
likelihood approach for fitting sparse generalized additive models in high
dimension. Our method interpolates between null, linear and additive models by
allowing the effect of each variable to be estimated as being either zero,
linear, or a low-complexity curve, as determined by the data. We present a
blockwise coordinate descent procedure for efficiently optimizing the penalized
likelihood objective over a dense grid of the tuning parameter, producing a
regularization path of additive models. We demonstrate the performance of our
method on both real and simulated data examples, and compare it with existing
techniques for additive model selection.",Generalized Additive Model Selection,data level linear two show ordering contextual networks
993,"There have been intensive research interests in ship detection and
segmentation due to high demands on a wide range of civil applications in the
last two decades. However, existing approaches, which are mainly based on
statistical properties of images, fail to detect smaller ships and boats.
Specifically, known techniques are not robust enough in view of inevitable
small geometric and photometric changes in images consisting of ships. In this
paper a novel approach for ship detection is proposed based on correlation of
maritime images. The idea comes from the observation that a fine pattern of the
sea surface changes considerably from time to time whereas the ship appearance
basically keeps unchanged. We want to examine whether the images have a common
unaltered part, a ship in this case. To this end, we developed a method -
Focused Correlation (FC) to achieve robustness to geometric distortions of the
image content. Various experiments have been conducted to evaluate the
effectiveness of the proposed approach.",Ship Detection and Segmentation using Image Correlation,concepts <unk> show types types structure  <unk>
994,"Close-range Photogrammetry is widely used in many industries because of the
cost effectiveness and efficiency of the technique. In this research, we
introduce an automated coded target detection method which can be used to
enhance the efficiency of the Photogrammetry.",Automatic Detection and Decoding of Photogrammetric Coded Targets,efficiency tensor video tensor suitable successful
995,"Developers often wonder how to implement a certain functionality (e.g., how
to parse XML files) using APIs. Obtaining an API usage sequence based on an
API-related natural language query is very helpful in this regard. Given a
query, existing approaches utilize information retrieval models to search for
matching API sequences. These approaches treat queries and APIs as bag-of-words
(i.e., keyword matching or word-to-word alignment) and lack a deep
understanding of the semantics of the query.
  We propose DeepAPI, a deep learning based approach to generate API usage
sequences for a given natural language query. Instead of a bags-of-words
assumption, it learns the sequence of words in a query and the sequence of
associated APIs. DeepAPI adapts a neural language model named RNN
Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length
context vector, and generates an API sequence based on the context vector. We
also augment the RNN Encoder-Decoder by considering the importance of
individual APIs. We empirically evaluate our approach with more than 7 million
annotated code snippets collected from GitHub. The results show that our
approach generates largely accurate API sequences and outperforms the related
approaches.",Deep API Learning,feature <unk> show feature models present image data
996,"Maximally stable component detection is a very popular method for feature
analysis in images, mainly due to its low computation cost and high
repeatability. With the recent advance of feature-based methods in geometric
shape analysis, there is significant interest in finding analogous approaches
in the 3D world. In this paper, we formulate a diffusion-geometric framework
for stable component detection in non-rigid 3D shapes, which can be used for
geometric feature detection and description. A quantitative evaluation of our
method on the SHREC'10 feature detection benchmark shows its potential as a
source of high-quality features.","Diffusion-geometric maximally stable component detection in deformable
  shapes",suggests evaluation <unk>
997,"In recent years genetic algorithms have emerged as a useful tool for the
heuristic solution of complex discrete optimisation problems. In particular
there has been considerable interest in their use in tackling problems arising
in the areas of scheduling and timetabling. However, the classical genetic
algorithm paradigm is not well equipped to handle constraints and successful
implementations usually require some sort of modification to enable the search
to exploit problem specific knowledge in order to overcome this shortcoming.
This paper is concerned with the development of a family of genetic algorithms
for the solution of a nurse rostering problem at a major UK hospital. The
hospital is made up of wards of up to 30 nurses. Each ward has its own group of
nurses whose shifts have to be scheduled on a weekly basis. In addition to
fulfilling the minimum demand for staff over three daily shifts, nurses' wishes
and qualifications have to be taken into account. The schedules must also be
seen to be fair, in that unpopular shifts have to be spread evenly amongst all
nurses, and other restrictions, such as team nursing and special conditions for
senior staff, have to be satisfied. The basis of the family of genetic
algorithms is a classical genetic algorithm consisting of n-point crossover,
single-bit mutation and a rank-based selection. The solution space consists of
all schedules in which each nurse works the required number of shifts, but the
remaining constraints, both hard and soft, are relaxed and penalised in the
fitness function. The talk will start with a detailed description of the
problem and the initial implementation and will go on to highlight the
shortcomings of such an approach, in terms of the key element of balancing
feasibility, i.e. covering the demand and work regulations, and quality, as
measured by the nurses' preferences. A series of experiments involving
parameter adaptation, niching, intelligent weights, delta coding, local hill
climbing, migration and special selection rules will then be outlined and it
will be shown how a series of these enhancements were able to eradicate these
difficulties. Results based on several months' real data will be used to
measure the impact of each modification, and to show that the final algorithm
is able to compete with a tabu search approach currently employed at the
hospital. The talk will conclude with some observations as to the overall
quality of this approach to this and similar problems.",Nurse Rostering with Genetic Algorithms,impact using impact texture show probabilistic the
998,"Automatic continuous speech recognition (CSR) is sufficiently mature that a
variety of real world applications are now possible including large vocabulary
transcription and interactive spoken dialogues. This paper reviews the
evolution of the statistical modelling techniques which underlie current-day
systems, specifically hidden Markov models (HMMs) and N-grams. Starting from a
description of the speech signal and its parameterisation, the various
modelling assumptions and their consequences are discussed. It then describes
various techniques by which the effects of these assumptions can be mitigated.
Despite the progress that has been made, the limitations of current modelling
techniques are still evident. The paper therefore concludes with a brief review
of some of the more fundamental modelling work now in progress.","Statistical Modeling in Continuous Speech Recognition (CSR)(Invited
  Talk)",learning various various texture show previously learning dimension the
999,"This paper deals with the revision of partially ordered beliefs. It proposes
a semantic representation of epistemic states by partial pre-orders on
interpretations and a syntactic representation by partially ordered belief
bases. Two revision operations, the revision stemming from the history of
observations and the possibilistic revision, defined when the epistemic state
is represented by a total pre-order, are generalized, at a semantic level, to
the case of a partial pre-order on interpretations, and at a syntactic level,
to the case of a partially ordered belief base. The equivalence between the two
representations is shown for the two revision operations.",Revising Partially Ordered Beliefs,two model architectures  using
